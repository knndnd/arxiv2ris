
TY  - Preprint
T1  - Semi-supervised Deep Reinforcement Learning in Support of IoT and Smart City Services
A1  - Mehdi Mohammadi
A1  - Ala Al-Fuqaha
A1  - Mohsen Guizani
A1  - Jun-Seok Oh
JO  - ArXiv e-prints
Y1  - 9 October, 2018
UR  - https://arxiv.org/abs/1810.04118
N2  - Smart services are an important element of the smart cities and the Internet of Things (IoT) ecosystems where the intelligence behind the services is obtained and improved through the sensory data. Providing a large amount of training data is not always feasible; therefore, we need to consider alternative ways that incorporate unlabeled data as well. In recent years, Deep reinforcement learning (DRL) has gained great success in several application domains. It is an applicable method for IoT and smart city scenarios where auto-generated data can be partially labeled by users&#39; feedback for training purposes. In this paper, we propose a semi-supervised deep reinforcement learning model that fits smart city applications as it consumes both labeled and unlabeled data to improve the performance and accuracy of the learning agent. The model utilizes Variational Autoencoders (VAE) as the inference engine for generalizing optimal policies. To the best of our knowledge, the proposed model is the first investigation that extends deep reinforcement learning to the semi-supervised paradigm. As a case study of smart city applications, we focus on smart buildings and apply the proposed model to the problem of indoor localization based on BLE signal strength. Indoor localization is the main component of smart city services since people spend significant time in indoor environments. Our model learns the best action policies that lead to a close estimation of the target locations with an improvement of 23% in terms of distance to the target and at least 67% more received rewards compared to the supervised DRL model.
ER  -


TY  - Preprint
T1  - Deep learning with differential Gaussian process flows
A1  - Pashupati Hegde
A1  - Markus Heinonen
A1  - Harri LÃ¤hdesmÃ¤ki
A1  - Samuel Kaski
JO  - ArXiv e-prints
Y1  - 9 October, 2018
UR  - https://arxiv.org/abs/1810.04066
N2  - We propose a novel deep learning paradigm of differential flows that learn a stochastic differential equation transformations of inputs prior to a standard classification or regression function. The key property of differential Gaussian processes is the warping of inputs through infinitely deep, but infinitesimal, differential fields, that generalise discrete layers into a dynamical system. We demonstrate state-of-the-art results that exceed the performance of deep Gaussian processes and neural networks
ER  -


TY  - Preprint
T1  - Hartley Spectral Pooling for Deep Learning
A1  - Hao Zhang
A1  - Jianwei Ma
JO  - ArXiv e-prints
Y1  - 7 October, 2018
UR  - https://arxiv.org/abs/1810.04028
N2  - In most convolution neural networks (CNNs), downsampling hidden layers is adopted for increasing computation efficiency and the receptive field size. Such operation is commonly so-called pooling. Maximation and averaging over sliding windows (max/average pooling), and plain downsampling in the form of strided convolution are popular pooling methods. Since the pooling is a lossy procedure, a motivation of our work is to design a new pooling approach for less lossy in the dimensionality reduction. Inspired by the Fourier spectral pooling(FSP) proposed by Rippel et. al. [1], we present the Hartley transform based spectral pooling method in CNNs. Compared with FSP, the proposed spectral pooling avoids the use of complex arithmetic for frequency representation and reduces the computation. Spectral pooling preserves more structure features for network&#39;s discriminability than max and average pooling. We empirically show that Hartley spectral pooling gives rise to the convergence of training CNNs on MNIST and CIFAR-10 datasets.
ER  -


TY  - Preprint
T1  - Deep Geodesic Learning for Segmentation and Anatomical Landmarking
A1  - Neslisah Torosdagli
A1  - Denise K. Liberton
A1  - Payal Verma
A1  - Murat Sincan
A1  - Janice S. Lee
A1  - Ulas Bagci
JO  - ArXiv e-prints
Y1  - 6 October, 2018
UR  - https://arxiv.org/abs/1810.04021
N2  - In this paper, we propose a novel deep learning framework for anatomy segmentation and automatic landmark- ing. Specifically, we focus on the challenging problem of mandible segmentation from cone-beam computed tomography (CBCT) scans and identification of 9 anatomical landmarks of the mandible on the geodesic space. The overall approach employs three inter-related steps. In step 1, we propose a deep neu- ral network architecture with carefully designed regularization, and network hyper-parameters to perform image segmentation without the need for data augmentation and complex post- processing refinement. In step 2, we formulate the landmark localization problem directly on the geodesic space for sparsely- spaced anatomical landmarks. In step 3, we propose to use a long short-term memory (LSTM) network to identify closely- spaced landmarks, which is rather difficult to obtain using other standard detection networks. The proposed fully automated method showed superior efficacy compared to the state-of-the- art mandible segmentation and landmarking approaches in craniofacial anomalies and diseased states. We used a very challenging CBCT dataset of 50 patients with a high-degree of craniomaxillofacial (CMF) variability that is realistic in clinical practice. Complementary to the quantitative analysis, the qualitative visual inspection was conducted for distinct CBCT scans from 250 patients with high anatomical variability. We have also shown feasibility of the proposed work in an independent dataset from MICCAI Head-Neck Challenge (2015) achieving the state-of-the-art performance. Lastly, we present an in-depth analysis of the proposed deep networks with respect to the choice of hyper-parameters such as pooling and activation functions.
ER  -


TY  - Preprint
T1  - A Comprehensive Study of Deep Learning for Image Captioning
A1  - Md. Zakir Hossain
A1  - Ferdous Sohel
A1  - Mohd Fairuz Shiratuddin
A1  - Hamid Laga
JO  - ArXiv e-prints
Y1  - 6 October, 2018
UR  - https://arxiv.org/abs/1810.04020
N2  - Generating a description of an image is called image captioning. Image captioning requires to recognize the important objects, their attributes and their relationships in an image. It also needs to generate syntactically and semantically correct sentences. Deep learning-based techniques are capable of handling the complexities and challenges of image captioning. In this survey paper, we aim to present a comprehensive review of existing deep learning-based image captioning techniques. We discuss the foundation of the techniques to analyze their performances, strengths and limitations. We also discuss the datasets and the evaluation metrics popularly used in deep learning based automatic image captioning.
ER  -


TY  - Preprint
T1  - Learning Converged Propagations with Deep Prior Ensemble for Image Enhancement
A1  - Risheng Liu
A1  - Long Ma
A1  - Yiyang Wang
A1  - Lei Zhang
JO  - ArXiv e-prints
Y1  - 9 October, 2018
UR  - https://arxiv.org/abs/1810.04012
N2  - Enhancing visual qualities of images plays very important roles in various vision and learning applications. In the past few years, both knowledge-driven maximum a posterior (MAP) with prior modelings and fully data-dependent convolutional neural network (CNN) techniques have been investigated to address specific enhancement tasks. In this paper, by exploiting the advantages of these two types of mechanisms within a complementary propagation perspective, we propose a unified framework, named deep prior ensemble (DPE), for solving various image enhancement tasks. Specifically, we first establish the basic propagation scheme based on the fundamental image modeling cues and then introduce residual CNNs to help predicting the propagation direction at each stage. By designing prior projections to perform feedback control, we theoretically prove that even with experience-inspired CNNs, DPE is definitely converged and the output will always satisfy our fundamental task constraints. The main advantage against conventional optimization-based MAP approaches is that our descent directions are learned from collected training data, thus are much more robust to unwanted local minimums. While, compared with existing CNN type networks, which are often designed in heuristic manners without theoretical guarantees, DPE is able to gain advantages from rich task cues investigated on the bases of domain knowledges. Therefore, DPE actually provides a generic ensemble methodology to integrate both knowledge and data-based cues for different image enhancement tasks. More importantly, our theoretical investigations verify that the feedforward propagations of DPE are properly controlled toward our desired solution. Experimental results demonstrate that the proposed DPE outperforms state-of-the-arts on a variety of image enhancement tasks in terms of both quantitative measure and visual perception quality.
ER  -


TY  - Preprint
T1  - DeepImageSpam: Deep Learning based Image Spam Detection
A1  - Amara Dinesh Kumar
A1  - Vinayakumar R
A1  - Soman KP
JO  - ArXiv e-prints
Y1  - 3 October, 2018
UR  - https://arxiv.org/abs/1810.03977
N2  - Hackers and spammers are employing innovative and novel techniques to deceive novice and even knowledgeable internet users. Image spam is one of such technique where the spammer varies and changes some portion of the image such that it is indistinguishable from the original image fooling the users. This paper proposes a deep learning based approach for image spam detection using the convolutional neural networks which uses a dataset with 810 natural images and 928 spam images for classification achieving an accuracy of 91.7% outperforming the existing image processing and machine learning techniques
ER  -


TY  - Preprint
T1  - Deep Attentive Tracking via Reciprocative Learning
A1  - Shi Pu
A1  - Yibing Song
A1  - Chao Ma
A1  - Honggang Zhang
A1  - Ming-Hsuan Yang
JO  - ArXiv e-prints
Y1  - 9 October, 2018
UR  - https://arxiv.org/abs/1810.03851
N2  - Visual attention, derived from cognitive neuroscience, facilitates human perception on the most pertinent subset of the sensory data. Recently, significant efforts have been made to exploit attention schemes to advance computer vision systems. For visual tracking, it is often challenging to track target objects undergoing large appearance changes. Attention maps facilitate visual tracking by selectively paying attention to temporal robust features. Existing tracking-by-detection approaches mainly use additional attention modules to generate feature weights as the classifiers are not equipped with such mechanisms. In this paper, we propose a reciprocative learning algorithm to exploit visual attention for training deep classifiers. The proposed algorithm consists of feed-forward and backward operations to generate attention maps, which serve as regularization terms coupled with the original classification loss function for training. The deep classifier learns to attend to the regions of target objects robust to appearance changes. Extensive experiments on large-scale benchmark datasets show that the proposed attentive tracking method performs favorably against the state-of-the-art approaches.
ER  -


TY  - Preprint
T1  - Saliency Prediction in the Deep Learning Era: An Empirical Investigation
A1  - Ali Borji
JO  - ArXiv e-prints
Y1  - 8 October, 2018
UR  - https://arxiv.org/abs/1810.03716
N2  - Visual saliency models have enjoyed a big leap in performance in recent years, thanks to advances in deep learning and large scale annotated data. Despite enormous effort and huge breakthroughs, however, models still fall short in reaching human-level accuracy. In this work, I explore the landscape of the field emphasizing on new deep saliency models, benchmarks, and datasets. A large number of image and video saliency models are reviewed and compared over two image benchmarks and two large scale video datasets. Further, I identify factors that contribute to the gap between models and humans and discuss remaining issues that need to be addressed to build the next generation of more powerful saliency models. Some specific questions that are addressed include: in what ways current models fail, how to remedy them, what can be learned from cognitive studies of attention, how explicit saliency judgments relate to fixations, how to conduct fair model comparison, and what are the emerging applications of saliency models.
ER  -


TY  - Preprint
T1  - Actor-Critic Deep Reinforcement Learning for Dynamic Multichannel Access
A1  - Chen Zhong
A1  - Ziyang Lu
A1  - M. Cenk Gursoy
A1  - Senem Velipasalar
JO  - ArXiv e-prints
Y1  - 8 October, 2018
UR  - https://arxiv.org/abs/1810.03695
N2  - We consider the dynamic multichannel access problem, which can be formulated as a partially observable Markov decision process (POMDP). We first propose a model-free actor-critic deep reinforcement learning based framework to explore the sensing policy. To evaluate the performance of the proposed sensing policy and the framework&#39;s tolerance against uncertainty, we test the framework in scenarios with different channel switching patterns and consider different switching probabilities. Then, we consider a time-varying environment to identify the adaptive ability of the proposed framework. Additionally, we provide comparisons with the Deep-Q network (DQN) based framework proposed in [1], in terms of both average reward and the time efficiency.
ER  -


TY  - Preprint
T1  - Multi-agent Deep Reinforcement Learning for Zero Energy Communities
A1  - Amit Prasad
A1  - Ivana Dusparic
JO  - ArXiv e-prints
Y1  - 8 October, 2018
UR  - https://arxiv.org/abs/1810.03679
N2  - Advances in renewable energy generation and introduction of the government targets to improve energy efficiency gave rise to a concept of a Zero Energy Building (ZEB). A ZEB is a building whose net energy usage over a year is zero, i.e., its energy use is not larger than its overall renewables generation. A collection of ZEBs forms a Zero Energy Community (ZEC). This paper addresses the problem of energy sharing in such a community. This is different from previously addressed energy sharing between buildings as our focus is on the improvement of community energy status, while traditionally research focused on reducing losses due to transmission and storage, or achieving economic gains. We model this problem in a multi-agent environment and propose a Deep Reinforcement Learning (DRL) based solution. Each building is represented by an intelligent agent that learns over time the appropriate behaviour to share energy. We have evaluated the proposed solution in a multi-agent simulation built using osBrain. Results indicate that with time agents learn to collaborate and learn a policy comparable to the optimal policy, which in turn improves the ZEC&#39;s energy status. Buildings with no renewables preferred to request energy from their neighbours rather than from the supply grid.
ER  -


TY  - Preprint
T1  - Wide and Deep Learning for Peer-to-Peer Lending
A1  - Kaveh Bastani
A1  - Elham Asgari
A1  - Hamed Namavari
JO  - ArXiv e-prints
Y1  - 8 October, 2018
UR  - https://arxiv.org/abs/1810.03466
N2  - This paper proposes a two-stage scoring approach to help lenders decide their fund allocations in the peer-to-peer (P2P) lending market. The existing scoring approaches focus on only either probability of default (PD) prediction, known as credit scoring, or profitability prediction, known as profit scoring, to identify the best loans for investment. Credit scoring fails to deliver the main need of lenders on how much profit they may obtain through their investment. On the other hand, profit scoring can satisfy that need by predicting the investment profitability. However, profit scoring completely ignores the class imbalance problem where most of the past loans are non-default. Consequently, ignorance of the class imbalance problem significantly affects the accuracy of profitability prediction. Our proposed two-stage scoring approach is an integration of credit scoring and profit scoring to address the above challenges. More specifically, stage 1 is designed as credit scoring to identify non-default loans while the imbalanced nature of loan status is considered in PD prediction. The loans identified as non-default are then moved to stage 2 for prediction of profitability, measured by internal rate of return. Wide and deep learning is used to build the predictive models in both stages to achieve both memorization and generalization. Extensive numerical studies are conducted based on real-world data to verify the effectiveness of the proposed approach. The numerical studies indicate our two-stage scoring approach outperforms the existing credit scoring and profit scoring approaches.
ER  -


TY  - Preprint
T1  - Deep learning cardiac motion analysis for human survival prediction
A1  - Ghalib A. Bello
A1  - Timothy J. W. Dawes
A1  - Jinming Duan
A1  - Carlo Biffi
A1  - Antonio de Marvao
A1  - Luke S. G. E. Howard
A1  - J. Simon R. Gibbs
A1  - Martin R. Wilkins
A1  - Stuart A. Cook
A1  - Daniel Rueckert
A1  - Declan P. O&#39;Regan
JO  - ArXiv e-prints
Y1  - 8 October, 2018
UR  - https://arxiv.org/abs/1810.03382
N2  - Motion analysis is used in computer vision to understand the behaviour of moving objects in sequences of images. Optimising the interpretation of dynamic biological systems requires accurate and precise motion tracking as well as efficient representations of high-dimensional motion trajectories so that these can be used for prediction tasks. Here we use image sequences of the heart, acquired using cardiac magnetic resonance imaging, to create time-resolved three-dimensional segmentations using a fully convolutional network trained on anatomical shape priors. This dense motion model formed the input to a supervised denoising autoencoder (4Dsurvival), which is a hybrid network consisting of an autoencoder that learns a task-specific latent code representation trained on observed outcome data, yielding a latent representation optimised for survival prediction. To handle right-censored survival outcomes, our network used a Cox partial likelihood loss function. In a study of 302 patients the predictive accuracy (quantified by Harrell&#39;s C-index) was significantly higher (p &lt; .0001) for our model C=0.73 (95$\%$ CI: 0.68 - 0.78) than the human benchmark of C=0.59 (95$\%$ CI: 0.53 - 0.65). This work demonstrates how a complex computer vision task using high-dimensional medical image data can efficiently predict human survival.
ER  -


TY  - Preprint
T1  - Internet Congestion Control via Deep Reinforcement Learning
A1  - Nathan Jay
A1  - Noga H. Rotman
A1  - P. Brighten Godfrey
A1  - Michael Schapira
A1  - Aviv Tamar
JO  - ArXiv e-prints
Y1  - 7 October, 2018
UR  - https://arxiv.org/abs/1810.03259
N2  - We present and investigate a novel and timely application domain for deep reinforcement learning (RL): Internet congestion control. Congestion control is the core networking task of modulating traffic sources&#39; data-transmission rates so as to efficiently and fairly allocate network resources. Congestion control is fundamental to computer networking research and practice, and has recently been the subject of extensive attention in light of the advent of challenging Internet applications such as live video, augmented and virtual reality, Internet-of-Things, and more.
ER  -


TY  - Preprint
T1  - Characterizing Deep-Learning I/O Workloads in TensorFlow
A1  - Steven W. D. Chien
A1  - Stefano Markidis
A1  - Chaitanya Prasad Sishtla
A1  - Luis Santos
A1  - Pawel Herman
A1  - Sai Narasimhamurthy
A1  - Erwin Laure
JO  - ArXiv e-prints
Y1  - 6 October, 2018
UR  - https://arxiv.org/abs/1810.03035
N2  - The performance of Deep-Learning (DL) computing frameworks rely on the performance of data ingestion and checkpointing. In fact, during the training, a considerable high number of relatively small files are first loaded and pre-processed on CPUs and then moved to accelerator for computation. In addition, checkpointing and restart operations are carried out to allow DL computing frameworks to restart quickly from a checkpoint. Because of this, I/O affects the performance of DL applications. In this work, we characterize the I/O performance and scaling of TensorFlow, an open-source programming framework developed by Google and specifically designed for solving DL problems. To measure TensorFlow I/O performance, we first design a micro-benchmark to measure TensorFlow reads, and then use a TensorFlow mini-application based on AlexNet to measure the performance cost of I/O and checkpointing in TensorFlow. To improve the checkpointing performance, we design and implement a burst buffer. We find that increasing the number of threads increases TensorFlow bandwidth by a maximum of 2.3x and 7.8x on our benchmark environments. The use of the tensorFlow prefetcher results in a complete overlap of computation on accelerator and input pipeline on CPU eliminating the effective cost of I/O on the overall performance. The use of a burst buffer to checkpoint to a fast small capacity storage and copy asynchronously the checkpoints to a slower large capacity storage resulted in a performance improvement of 2.6x with respect to checkpointing directly to slower storage on our benchmark environment.
ER  -


TY  - Preprint
T1  - Deep Learning Approaches for Understanding Simple Speech Commands
A1  - Roman A. Solovyev
A1  - Maxim Vakhrushev
A1  - Alexander Radionov
A1  - Vladimir Aliev
A1  - Alexey A. Shvets
JO  - ArXiv e-prints
Y1  - 4 October, 2018
UR  - https://arxiv.org/abs/1810.02364
N2  - Automatic classification of sound commands is becoming increasingly important, especially for mobile and embedded devices. Many of these devices contain both cameras and microphones, and companies that develop them would like to use the same technology for both of these classification tasks. One way of achieving this is to represent sound commands as images, and use convolutional neural networks when classifying images as well as sounds. In this paper we consider several approaches to the problem of sound classification that we applied in TensorFlow Speech Recognition Challenge organized by Google Brain team on the Kaggle platform. Here we show different representation of sounds (Wave frames, Spectrograms, Mel-Spectrograms, MFCCs) and apply several 1D and 2D convolutional neural networks in order to get the best performance. Our experiments show that we found appropriate sound representation and corresponding convolutional neural networks. As a result we achieved good classification accuracy that allowed us to finish the challenge on 8-th place among 1315 teams.
ER  -


TY  - Preprint
T1  - Direct Prediction of Cardiovascular Mortality from Low-dose Chest CT using Deep Learning
A1  - Sanne G. M. van Velzen
A1  - Majd Zreik
A1  - Nikolas Lessmann
A1  - Max A. Viergever
A1  - Pim A. de Jong
A1  - Helena M. Verkooijen
A1  - Ivana IÅ¡gum
JO  - ArXiv e-prints
Y1  - 4 October, 2018
UR  - https://arxiv.org/abs/1810.02277
N2  - Cardiovascular disease (CVD) is a leading cause of death in the lung cancer screening population. Chest CT scans made in lung cancer screening are suitable for identification of participants at risk of CVD. Existing methods analyzing CT images from lung cancer screening for prediction of CVD events or mortality use engineered features extracted from the images combined with patient information. In this work we propose a method that automatically predicts 5-year cardiovascular mortality directly from chest CT scans without the need for hand-crafting image features. A set of 1,583 participants of the National Lung Screening Trial was included (1,188 survivors, 395 non-survivors). Low-dose chest CT images acquired at baseline were analyzed and the follow-up time was 5 years. To limit the analysis to the heart region, the heart was first localized by our previously developed algorithm for organ localization exploiting convolutional neural networks. Thereafter, a convolutional autoencoder was used to encode the identified heart region. Finally, based on the extracted encodings subjects were classified into survivors or non-survivors using a support vector machine classifier. The performance of the method was assessed in eight cross-validation experiments with 1,433 images used for training, 50 for validation and 100 for testing. The method achieved a performance with an area under the ROC curve of 0.72. The results demonstrate that prediction of cardiovascular mortality directly from low-dose screening chest CT scans, without hand-crafted features, is feasible, allowing identification of subjects at risk of fatal CVD events.
ER  -


TY  - Preprint
T1  - Italian Event Detection Goes Deep Learning
A1  - Tommaso Caselli
JO  - ArXiv e-prints
Y1  - 4 October, 2018
UR  - https://arxiv.org/abs/1810.02229
N2  - This paper reports on a set of experiments with different word embeddings to initialize a state-of-the-art Bi-LSTM-CRF network for event detection and classification in Italian, following the EVENTI evaluation exercise. The net- work obtains a new state-of-the-art result by improving the F1 score for detection of 1.3 points, and of 6.5 points for classification, by using a single step approach. The results also provide further evidence that embeddings have a major impact on the performance of such architectures.
ER  -


TY  - Preprint
T1  - Exascale Deep Learning for Climate Analytics
A1  - Thorsten Kurth
A1  - Sean Treichler
A1  - Joshua Romero
A1  - Mayur Mudigonda
A1  - Nathan Luehr
A1  - Everett Phillips
A1  - Ankur Mahesh
A1  - Michael Matheson
A1  - Jack Deslippe
A1  - Massimiliano Fatica
A1  -  Prabhat
A1  - Michael Houston
JO  - ArXiv e-prints
Y1  - 3 October, 2018
UR  - https://arxiv.org/abs/1810.01993
N2  - We extract pixel-level masks of extreme weather patterns using variants of Tiramisu and DeepLabv3+ neural networks. We describe improvements to the software frameworks, input pipeline, and the network training algorithms necessary to efficiently scale deep learning on the Piz Daint and Summit systems. The Tiramisu network scales to 5300 P100 GPUs with a sustained throughput of 21.0 PF/s and parallel efficiency of 79.0%. DeepLabv3+ scales up to 27360 V100 GPUs with a sustained throughput of 325.8 PF/s and a parallel efficiency of 90.7% in single precision. By taking advantage of the FP16 Tensor Cores, a half-precision version of the DeepLabv3+ network achieves a peak and sustained throughput of 1.13 EF/s and 999.0 PF/s respectively.
ER  -


TY  - Preprint
T1  - A deep learning pipeline for product recognition on store shelves
A1  - Alessio Tonioni
A1  - Eugenio Serro
A1  - Luigi Di Stefano
JO  - ArXiv e-prints
Y1  - 4 October, 2018
UR  - https://arxiv.org/abs/1810.01733
N2  - Recognition of grocery products in store shelves poses peculiar challenges. Firstly, the task mandates the recognition of an extremely high number of different items, in the order of several thousands for medium-small shops, with many of them featuring small inter and intra class variability. Then, available product databases usually include just one or a few studio-quality images per product (referred to herein as reference images), whilst at test time recognition is performed on pictures displaying a portion of a shelf containing several products and taken in the store by cheap cameras (referred to as query images). Moreover, as the items on sale in a store as well as their appearance change frequently over time, a practical recognition system should handle seamlessly new products/packages. Inspired by recent advances in object detection and image retrieval, we propose to leverage on state of the art object detectors based on deep learning to obtain an initial productagnostic item detection. Then, we pursue product recognition through a similarity search between global descriptors computed on reference and cropped query images. To maximize performance, we learn an ad-hoc global descriptor by a CNN trained on reference images based on an image embedding loss. Our system is computationally expensive at training time but can perform recognition rapidly and accurately at test time.
ER  -


TY  - Preprint
T1  - Theory of Generative Deep Learning : Probe Landscape of Empirical Error via Norm Based Capacity Control
A1  - Wendi Xu
A1  - Ming Zhang
JO  - ArXiv e-prints
Y1  - 3 October, 2018
UR  - https://arxiv.org/abs/1810.01622
N2  - Despite its remarkable empirical success as a highly competitive branch of artificial intelligence, deep learning is often blamed for its widely known low interpretation and lack of firm and rigorous mathematical foundation. However, most theoretical endeavor is devoted in discriminative deep learning case, whose complementary part is generative deep learning. To the best of our knowledge, we firstly highlight landscape of empirical error in generative case to complete the full picture through exquisite design of image super resolution under norm based capacity control. Our theoretical advance in interpretation of the training dynamic is achieved from both mathematical and biological sides.
ER  -


TY  - Preprint
T1  - Extreme Augmentation : Can deep learning based medical image segmentation be trained using a single manually delineated scan?
A1  - Bilwaj Gaonkar
A1  - Alex Bui
A1  - Matthew Brown
A1  - Luke Macyszyn
JO  - ArXiv e-prints
Y1  - 3 October, 2018
UR  - https://arxiv.org/abs/1810.01621
N2  - Yes, it can. Data augmentation is perhaps the oldest preprocessing step in computer vision literature. Almost every computer vision model trained on imaging data uses some form of augmentation. In this paper, we use the inter-vertebral disk segmentation task alongside a deep residual U-Net as the learning model, to explore the effectiveness of augmentation. In the extreme, we observed that a model trained on patches extracted from just one scan, with each patch augmented 50 times; achieved a Dice score of 0.73 in a validation set of 40 cases. Qualitative evaluation indicated a clinically usable segmentation algorithm, which appropriately segments regions of interest, alongside limited false positive specks. When the initial patches are extracted from nine scans the average Dice coefficient jumps to 0.86 and most of the false positives disappear. While this still falls short of state-of-the-art deep learning based segmentation of discs reported in literature, qualitative examination reveals that it does yield segmentation, which can be amended by expert clinicians with minimal effort to generate additional data for training improved deep models. Extreme augmentation of training data, should thus be construed as a strategy for training deep learning based algorithms, when very little manually annotated data is available to work with. Models trained with extreme augmentation can then be used to accelerate the generation of manually labelled data. Hence, we show that extreme augmentation can be a valuable tool in addressing scaling up small imaging data sets to address medical image segmentation tasks.
ER  -


TY  - Preprint
T1  - A Deep Learning Architecture for De-identification of Patient Notes: Implementation and Evaluation
A1  - Kaung Khin
A1  - Philipp Burckhardt
A1  - Rema Padman
JO  - ArXiv e-prints
Y1  - 2 October, 2018
UR  - https://arxiv.org/abs/1810.01570
N2  - De-identification is the process of removing 18 protected health information (PHI) from clinical notes in order for the text to be considered not individually identifiable. Recent advances in natural language processing (NLP) has allowed for the use of deep learning techniques for the task of de-identification. In this paper, we present a deep learning architecture that builds on the latest NLP advances by incorporating deep contextualized word embeddings and variational drop out Bi-LSTMs. We test this architecture on two gold standard datasets and show that the architecture achieves state-of-the-art performance on both data sets while also converging faster than other systems without the use of dictionaries or other knowledge sources.
ER  -


TY  - Preprint
T1  - Deep Learning Based Caching for Self-Driving Car in Multi-access Edge Computing
A1  - Anselme Ndikumana
A1  - Nguyen H. Tran
A1  - Choong Seon Hong
JO  - ArXiv e-prints
Y1  - 2 October, 2018
UR  - https://arxiv.org/abs/1810.01548
N2  - Once self-driving car becomes a reality and passengers are no longer worry about it, they will need to find new ways of entertainment. However, retrieving entertainment contents at the Data Center (DC) can hinder content delivery service due to high delay of car-to-DC communication. To address these challenges, we propose a deep learning based caching for self-driving car, by using Deep Learning approaches deployed on the Multi-access Edge Computing (MEC) structure. First, at DC, Multi-Layer Perceptron (MLP) is used to predict the probabilities of contents to be requested in specific areas. To reduce the car-DC delay, MLP outputs are logged into MEC servers attached to roadside units. Second, in order to cache entertainment contents stylized for car passengers&#39; features such as age and gender, Convolutional Neural Network (CNN) is used to predict age and gender of passengers. Third, each car requests MLP output from MEC server and compares its CNN and MLP outputs by using k-means and binary classification. Through this, the self-driving car can identify the contents need to be downloaded from the MEC server and cached. Finally, we formulate deep learning based caching in the self-driving car that enhances entertainment services as an optimization problem whose goal is to minimize content downloading delay. To solve the formulated problem, a Block Successive Majorization-Minimization (BS-MM) technique is applied. The simulation results show that the accuracy of our prediction for the contents need to be cached in the areas of the self-driving car is achieved at 98.04% and our approach can minimize delay.
ER  -


TY  - Preprint
T1  - A Practical Approach to Insertion with Variable Socket Position Using Deep Reinforcement Learning
A1  - Mel Vecerik
A1  - Oleg Sushkov
A1  - David Barker
A1  - Thomas RothÃ¶rl
A1  - Todd Hester
A1  - Jon Scholz
JO  - ArXiv e-prints
Y1  - 8 October, 2018
UR  - https://arxiv.org/abs/1810.01531
N2  - Insertion is a challenging haptic and visual control problem with significant practical value for manufacturing. Existing approaches in the model-based robotics community can be highly effective when task geometry is known, but are complex and cumbersome to implement, and must be tailored to each individual problem by a qualified engineer. Within the learning community there is a long history of insertion research, but existing approaches are typically either too sample-inefficient to run on real robots, or assume access to high-level object features, e.g. socket pose. In this paper we show that relatively minor modifications to an off-the-shelf Deep-RL algorithm (DDPG), combined with a small number of human demonstrations, allows the robot to quickly learn to solve these tasks efficiently and robustly. Our approach requires no modeling or simulation, no parameterized search or alignment behaviors, no vision system aside from raw images, and no reward shaping. We evaluate our approach on a narrow-clearance peg-insertion task and a deformable clip-insertion task, both of which include variability in the socket position. Our results show that these tasks can be solved reliably on the real robot in less than 10 minutes of interaction time, and that the resulting policies are robust to variance in the socket position and orientation.
ER  -


TY  - Preprint
T1  - AlphaSeq: Sequence Discovery with Deep Reinforcement Learning
A1  - Yulin Shao
A1  - Soung Chang Liew
A1  - Taotao Wang
JO  - ArXiv e-prints
Y1  - 26 September, 2018
UR  - https://arxiv.org/abs/1810.01218
N2  - Sequences play an important role in many applications and systems. Discovering sequences with desired properties has long been an interesting intellectual pursuit. This paper puts forth a new paradigm, AlphaSeq, to discover desired sequences algorithmically using deep reinforcement learning (DRL) techniques. AlphaSeq treats the sequence discovery problem as an episodic symbol-filling game, in which a player fills symbols in the vacant positions of a sequence set sequentially during an episode of the game. Each episode ends with a completely-filled sequence set, upon which a reward is given based on the desirability of the sequence set. AlphaSeq models the game as a Markov Decision Process (MDP), and adapts the DRL framework of AlphaGo to solve the MDP. Sequences discovered improve progressively as AlphaSeq, starting as a novice, learns to become an expert game player through many episodes of game playing. Compared with traditional sequence construction by mathematical tools, AlphaSeq is particularly suitable for problems with complex objectives intractable to mathematical analysis. We demonstrate the searching capabilities of AlphaSeq in two applications: 1) AlphaSeq successfully rediscovers a set of ideal complementary codes that can zero-force all potential interferences in multi-carrier CDMA systems. 2) AlphaSeq discovers new sequences that triple the signal-to-interference ratio -- benchmarked against the well-known Legendre sequence -- of a mismatched filter estimator in pulse compression radar systems.
ER  -


TY  - Preprint
T1  - An Entropic Optimal Transport Loss for Learning Deep Neural Networks under Label Noise in Remote Sensing Images
A1  - Bharath Bhushan Damodaran
A1  - RÃ©mi Flamary
A1  - Viven Seguy
A1  - Nicolas Courty
JO  - ArXiv e-prints
Y1  - 2 October, 2018
UR  - https://arxiv.org/abs/1810.01163
N2  - Deep neural networks have established as a powerful tool for large scale supervised classification tasks. The state-of-the-art performances of deep neural networks are conditioned to the availability of large number of accurately labeled samples. In practice, collecting large scale accurately labeled datasets is a challenging and tedious task in most scenarios of remote sensing image analysis, thus cheap surrogate procedures are employed to label the dataset. Training deep neural networks on such datasets with inaccurate labels easily overfits to the noisy training labels and degrades the performance of the classification tasks drastically. To mitigate this effect, we propose an original solution with entropic optimal transportation. It allows to learn in an end-to-end fashion deep neural networks that are, to some extent, robust to inaccurately labeled samples. We empirically demonstrate on several remote sensing datasets, where both scene and pixel-based hyperspectral images are considered for classification. Our method proves to be highly tolerant to significant amounts of label noise and achieves favorable results against state-of-the-art methods.
ER  -


TY  - Preprint
T1  - Training compact deep learning models for video classification using circulant matrices
A1  - Alexandre Araujo
A1  - Benjamin Negrevergne
A1  - Yann Chevaleyre
A1  - Jamal Atif
JO  - ArXiv e-prints
Y1  - 8 October, 2018
UR  - https://arxiv.org/abs/1810.01140
N2  - In real world scenarios, model accuracy is hardly the only factor to consider. Large models consume more memory and are computationally more intensive, which makes them difficult to train and to deploy, especially on mobile devices. In this paper, we build on recent results at the crossroads of Linear Algebra and Deep Learning which demonstrate how imposing a structure on large weight matrices can be used to reduce the size of the model. We propose very compact models for video classification based on state-of-the-art network architectures such as Deep Bag-of-Frames, NetVLAD and NetFisherVectors. We then conduct thorough experiments using the large YouTube-8M video classification dataset. As we will show, the circulant DBoF embedding achieves an excellent trade-off between size and accuracy.
ER  -


TY  - Preprint
T1  - Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning
A1  - Charles H. Martin
A1  - Michael W. Mahoney
JO  - ArXiv e-prints
Y1  - 2 October, 2018
UR  - https://arxiv.org/abs/1810.01075
N2  - Random Matrix Theory (RMT) is applied to analyze weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet. Empirical and theoretical results clearly indicate that the DNN training process itself implicitly implements a form of Self-Regularization. The empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of explicit regularization. Building on relatively recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization. These phases can be observed during the training process as well as in the final learned DNNs. For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a &#34;size scale&#34; separating signal from noise. For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems. This results from correlations arising at all size scales, which arises implicitly due to the training process itself. This implicit Self-Regularization can depend strongly on the many knobs of the training process. By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size. This demonstrates that---all else being equal---DNN optimization with larger batch sizes leads to less-well implicitly-regularized models, and it provides an explanation for the generalization gap phenomena.
ER  -


TY  - Preprint
T1  - Cloud Chaser: Real Time Deep Learning Computer Vision on Low Computing Power Devices
A1  - Zhengyi Luo
A1  - Austin Small
A1  - Liam Dugan
A1  - Stephen Lane
JO  - ArXiv e-prints
Y1  - 2 October, 2018
UR  - https://arxiv.org/abs/1810.01069
N2  - Internet of Things(IoT) devices, mobile phones, and robotic systems are often denied the power of deep learning algorithms due to their limited computing power. However, to provide time critical services such as emergency response, home assistance, surveillance, etc, these devices often need real time analysis of their camera data. This paper strives to offer a viable approach to integrate high-performance deep learning based computer vision algorithms with low-resource and low-power devices by leveraging the computing power of the cloud. By offloading the computation work to the cloud, no dedicated hardware is needed to enable deep neural networks on existing low computing power devices. A Raspberry Pi based robot, Cloud Chaser, is built to demonstrate the power of using cloud computing to perform real time vision tasks. Furthermore, to reduce latency and improve real time performance, compression algorithms are proposed and evaluated for streaming real-time video frames to the cloud.
ER  -


TY  - Preprint
T1  - Dynamic Sparse Graph for Efficient Deep Learning
A1  - Liu Liu
A1  - Lei Deng
A1  - Xing Hu
A1  - Maohua Zhu
A1  - Guoqi Li
A1  - Yufei Ding
A1  - Yuan Xie
JO  - ArXiv e-prints
Y1  - 1 October, 2018
UR  - https://arxiv.org/abs/1810.00859
N2  - We propose to execute deep neural networks (DNNs) with dynamic and sparse graph (DSG) structure for compressive memory and accelerative execution during both training and inference. The great success of DNNs motivates the pursuing of lightweight models for the deployment onto embedded devices. However, most of the previous studies optimize for inference while neglect training or even complicate it. Training is far more intractable, since (i) the neurons dominate the memory cost rather than the weights in inference; (ii) the dynamic activation makes previous sparse acceleration via one-off optimization on fixed weight invalid; (iii) batch normalization (BN) is critical for maintaining accuracy while its activation reorganization damages the sparsity. To address these issues, DSG activates only a small amount of neurons with high selectivity at each iteration via a dimension-reduction search (DRS) and obtains the BN compatibility via a double-mask selection (DMS). Experiments show significant memory saving (1.7-4.5x) and operation reduction (2.3-4.4x) with little accuracy loss on various benchmarks.
ER  -


TY  - Preprint
T1  - One Network to Solve All ROIs: Deep Learning CT for Any ROI using Differentiated Backprojection
A1  - Yoseob Han
A1  - Jong Chul Ye
JO  - ArXiv e-prints
Y1  - 30 September, 2018
UR  - https://arxiv.org/abs/1810.00500
N2  - Computed tomography for region-of-interest (ROI) reconstruction has advantages of reducing X-ray radiation dose and using a small detector. However, standard analytic reconstruction methods suffer from severe cupping artifacts, and existing model-based iterative reconstruction methods require extensive computations. Recently, we proposed a deep neural network to learn the cupping artifact, but the network is not well generalized for different ROIs due to the singularities in the corrupted images. Therefore, there is an increasing demand for a neural network that works well for any ROI sizes. In this paper, two types of neural networks are designed. The first type learns ROI size-specific cupping artifacts from the analytic reconstruction images, whereas the second type network is to learn to invert the finite Hilbert transform from the truncated differentiated backprojection (DBP) data. Their generalizability for any ROI sizes is then examined. Experimental results show that the new type of neural network significantly outperforms the existing iterative methods for any ROI size in spite of significantly reduced run-time complexity. Since the proposed method consistently surpasses existing methods for any ROIs, it can be used as a general CT reconstruction engine for many practical applications without compromising possible detector truncation.
ER  -


TY  - Preprint
T1  - Deep Learning for End-to-End Atrial Fibrillation Recurrence Estimation
A1  - Riddhish Bhalodia
A1  - Anupama Goparaju
A1  - Tim Sodergren
A1  - Alan Morris
A1  - Evgueni Kholmovski
A1  - Nassir Marrouche
A1  - Joshua Cates
A1  - Ross Whitaker
A1  - Shireen Elhabian
JO  - ArXiv e-prints
Y1  - 30 September, 2018
UR  - https://arxiv.org/abs/1810.00475
N2  - Left atrium shape has been shown to be an independent predictor of recurrence after atrial fibrillation (AF) ablation. Shape-based representation is imperative to such an estimation process, where correspondence-based representation offers the most flexibility and ease-of-computation for population-level shape statistics. Nonetheless, population-level shape representations in the form of image segmentation and correspondence models derived from cardiac MRI require significant human resources with sufficient anatomy-specific expertise. In this paper, we propose a machine learning approach that uses deep networks to estimate AF recurrence by predicting shape descriptors directly from MRI images, with NO image pre-processing involved. We also propose a novel data augmentation scheme to effectively train a deep network in a limited training data setting. We compare this new method of estimating shape descriptors from images with the state-of-the-art correspondence-based shape modeling that requires image segmentation and correspondence optimization. Results show that the proposed method and the current state-of-the-art produce statistically similar outcomes on AF recurrence, eliminating the need for expensive pre-processing pipelines and associated human labor.
ER  -


TY  - Preprint
T1  - Interactive Learning with Corrective Feedback for Policies based on Deep Neural Networks
A1  - Rodrigo PÃ©rez-Dattari
A1  - Carlos Celemin
A1  - Javier Ruiz-del-Solar
A1  - Jens Kober
JO  - ArXiv e-prints
Y1  - 30 September, 2018
UR  - https://arxiv.org/abs/1810.00466
N2  - Deep Reinforcement Learning (DRL) has become a powerful strategy to solve complex decision making problems based on Deep Neural Networks (DNNs). However, it is highly data demanding, so unfeasible in physical systems for most applications. In this work, we approach an alternative Interactive Machine Learning (IML) strategy for training DNN policies based on human corrective feedback, with a method called Deep COACH (D-COACH). This approach not only takes advantage of the knowledge and insights of human teachers as well as the power of DNNs, but also has no need of a reward function (which sometimes implies the need of external perception for computing rewards). We combine Deep Learning with the COrrective Advice Communicated by Humans (COACH) framework, in which non-expert humans shape policies by correcting the agent&#39;s actions during execution. The D-COACH framework has the potential to solve complex problems without much data or time required. Experimental results validated the efficiency of the framework in three different problems (two simulated, one with a real robot), with state spaces of low and high dimensions, showing the capacity to successfully learn policies for continuous action spaces like in the Car Racing and Cart-Pole problems faster than with DRL.
ER  -


TY  - Preprint
T1  - Using State Predictions for Value Regularization in Curiosity Driven Deep Reinforcement Learning
A1  - Gino Brunner
A1  - Manuel Fritsche
A1  - Oliver Richter
A1  - Roger Wattenhofer
JO  - ArXiv e-prints
Y1  - 30 September, 2018
UR  - https://arxiv.org/abs/1810.00361
N2  - Learning in sparse reward settings remains a challenge in Reinforcement Learning, which is often addressed by using intrinsic rewards. One promising strategy is inspired by human curiosity, requiring the agent to learn to predict the future. In this paper a curiosity-driven agent is extended to use these predictions directly for training. To achieve this, the agent predicts the value function of the next state at any point in time. Subsequently, the consistency of this prediction with the current value function is measured, which is then used as a regularization term in the loss function of the algorithm. Experiments were made on grid-world environments as well as on a 3D navigation task, both with sparse rewards. In the first case the extended agent is able to learn significantly faster than the baselines.
ER  -


TY  - Preprint
T1  - DELMU: A Deep Learning Approach to Maximising the Utility of Virtualised Millimetre-Wave Backhauls
A1  - Rui Li
A1  - Chaoyun Zhang
A1  - Paul Patras
A1  - Pan Cao
A1  - John S. Thompson
JO  - ArXiv e-prints
Y1  - 2 October, 2018
UR  - https://arxiv.org/abs/1810.00356
N2  - Advances in network programmability enable operators to &#39;slice&#39; the physical infrastructure into independent logical networks. By this approach, each network slice aims to accommodate the demands of increasingly diverse services. However, precise allocation of resources to slices across future 5G millimetre-wave backhaul networks, to optimise the total network utility, is challenging. This is because the performance of different services often depends on conflicting requirements, including bandwidth, sensitivity to delay, or the monetary value of the traffic incurred. In this paper, we put forward a general rate utility framework for slicing mm-wave backhaul links, encompassing all known types of service utilities, i.e. logarithmic, sigmoid, polynomial, and linear. We then introduce DELMU, a deep learning solution that tackles the complexity of optimising non-convex objective functions built upon arbitrary combinations of such utilities. Specifically, by employing a stack of convolutional blocks, DELMU can learn correlations between traffic demands and achievable optimal rate assignments. We further regulate the inferences made by the neural network through a simple &#39;sanity check&#39; routine, which guarantees both flow rate admissibility within the network&#39;s capacity region and minimum service levels. The proposed method can be trained within minutes, following which it computes rate allocations that match those obtained with state-of-the-art global optimisation algorithms, yet orders of magnitude faster. This confirms the applicability of DELMU to highly dynamic traffic regimes and we demonstrate up to 62% network utility gains over a baseline greedy approach.
ER  -


TY  - Preprint
T1  - A Deep learning framework for Single sided sound speed inversion in medical ultrasound
A1  - Micha Feigin
A1  - Daniel Freedman
A1  - Brian W. Anthony
JO  - ArXiv e-prints
Y1  - 30 September, 2018
UR  - https://arxiv.org/abs/1810.00322
N2  - Ultrasound elastography is gaining traction as an accessible and useful diagnostic tool for such things as cancer detection and differentiation as well as liver and thyroid disease diagnostics. Unfortunately, state of the art acoustic radiation force techniques are limited to high end ultrasound hardware due to high power requirements, are extremely sensitive to patient and sonographer motion and generally suffer from low frame rates.
ER  -


TY  - Preprint
T1  - Posture recognition using an RGB-D camera : exploring 3D body modeling and deep learning approaches
A1  - Mohamed El Amine Elforaici
A1  - Ismail Chaaraoui
A1  - Wassim Bouachir
A1  - Youssef Ouakrim
A1  - Neila Mezghani
JO  - ArXiv e-prints
Y1  - 29 September, 2018
UR  - https://arxiv.org/abs/1810.00308
N2  - The emergence of RGB-D sensors offered new possibilities for addressing complex artificial vision problems efficiently. Human posture recognition is among these computer vision problems, with a wide range of applications such as ambient assisted living and intelligent health care systems. In this context, our paper presents novel methods and ideas to design automatic posture recognition systems using an RGB-D camera. More specifically, we introduce two supervised methods to learn and recognize human postures using the main types of visual data provided by an RGB-D camera. The first method is based on convolutional features extracted from 2D images. Convolutional Neural Networks (CNNs) are trained to recognize human postures using transfer learning on RGB and depth images. Secondly, we propose to model the posture using the body joint configuration in the 3D space. Posture recognition is then performed through SVM classification of 3D skeleton- based features. To evaluate the proposed methods, we created a challenging posture recognition dataset with a considerable variability regarding the acquisition conditions. The experimental results demonstrated comparable performances and high pre- cision for both methods in recognizing human postures, with a slight superiority for the CNN-based method when applied on depth images. Moreover, the two approaches demonstrated a high robustness to several perturbation factors, such as scale and orientation change.
ER  -


TY  - Preprint
T1  - Directional Analysis of Stochastic Gradient Descent via von Mises-Fisher Distributions in Deep learning
A1  - Cheolhyoung Lee
A1  - Kyunghyun Cho
A1  - Wanmo Kang
JO  - ArXiv e-prints
Y1  - 29 September, 2018
UR  - https://arxiv.org/abs/1810.00150
N2  - Although stochastic gradient descent (SGD) is a driving force behind the recent success of deep learning, our understanding of its dynamics in a high-dimensional parameter space is limited. In recent years, some researchers have used the stochasticity of minibatch gradients, or the signal-to-noise ratio, to better characterize the learning dynamics of SGD. Inspired from these work, we here analyze SGD from a geometrical perspective by inspecting the stochasticity of the norms and directions of minibatch gradients. We propose a model of the directional concentration for minibatch gradients through von Mises-Fisher (VMF) distribution, and show that the directional uniformity of minibatch gradients increases over the course of SGD. We empirically verify our result using deep convolutional networks and observe a higher correlation between the gradient stochasticity and the proposed directional uniformity than that against the gradient norm stochasticity, suggesting that the directional statistics of minibatch gradients is a major factor behind SGD.
ER  -


TY  - Preprint
T1  - DeepSSM: A Deep Learning Framework for Statistical Shape Modeling from Raw Images
A1  - Riddhish Bhalodia
A1  - Shireen Y. Elhabian
A1  - Ladislav Kavan
A1  - Ross T. Whitaker
JO  - ArXiv e-prints
Y1  - 28 September, 2018
UR  - https://arxiv.org/abs/1810.00111
N2  - Statistical shape modeling is an important tool to characterize variation in anatomical morphology. Typical shapes of interest are measured using 3D imaging and a subsequent pipeline of registration, segmentation, and some extraction of shape features or projections onto some lower-dimensional shape space, which facilitates subsequent statistical analysis. Many methods for constructing compact shape representations have been proposed, but are often impractical due to the sequence of image preprocessing operations, which involve significant parameter tuning, manual delineation, and/or quality control by the users. We propose DeepSSM: a deep learning approach to extract a low-dimensional shape representation directly from 3D images, requiring virtually no parameter tuning or user assistance. DeepSSM uses a convolutional neural network (CNN) that simultaneously localizes the biological structure of interest, establishes correspondences, and projects these points onto a low-dimensional shape representation in the form of PCA loadings within a point distribution model. To overcome the challenge of the limited availability of training images, we present a novel data augmentation procedure that uses existing correspondences on a relatively small set of processed images with shape statistics to create plausible training samples with known shape parameters. Hence, we leverage the limited CT/MRI scans (40-50) into thousands of images needed to train a CNN. After the training, the CNN automatically produces accurate low-dimensional shape representations for unseen images. We validate DeepSSM for three different applications pertaining to modeling pediatric cranial CT for characterization of metopic craniosynostosis, femur CT scans identifying morphologic deformities of the hip due to femoroacetabular impingement, and left atrium MRI scans for atrial fibrillation recurrence prediction.
ER  -


TY  - Preprint
T1  - Deep Adaptive Learning for Writer Identification based on Single Handwritten Word Images
A1  - Sheng He
A1  - Lambert Schomaker
JO  - ArXiv e-prints
Y1  - 28 September, 2018
UR  - https://arxiv.org/abs/1809.10954
N2  - There are two types of information in each handwritten word image: explicit information which can be easily read or derived directly, such as lexical content or word length, and implicit attributes such as the author&#39;s identity. Whether features learned by a neural network for one task can be used for another task remains an open question. In this paper, we present a deep adaptive learning method for writer identification based on single-word images using multi-task learning. An auxiliary task is added to the training process to enforce the emergence of reusable features. Our proposed method transfers the benefits of the learned features of a convolutional neural network from an auxiliary task such as explicit content recognition to the main task of writer identification in a single procedure. Specifically, we propose a new adaptive convolutional layer to exploit the learned deep features. A multi-task neural network with one or several adaptive convolutional layers is trained end-to-end, to exploit robust generic features for a specific main task, i.e., writer identification. Three auxiliary tasks, corresponding to three explicit attributes of handwritten word images (lexical content, word length and character attributes), are evaluated. Experimental results on two benchmark datasets show that the proposed deep adaptive learning method can improve the performance of writer identification based on single-word images, compared to non-adaptive and simple linear-adaptive approaches.
ER  -


TY  - Preprint
T1  - Using Deep Reinforcement Learning to Learn High-Level Policies on the ATRIAS Biped
A1  - Tianyu Li
A1  - Akshara Rai
A1  - Hartmut Geyer
A1  - Christopher G. Atkeson
JO  - ArXiv e-prints
Y1  - 27 September, 2018
UR  - https://arxiv.org/abs/1809.10811
N2  - Learning controllers for bipedal robots is a challenging problem, often requiring expert knowledge and extensive tuning of parameters that vary in different situations. Recently, deep reinforcement learning has shown promise at automatically learning controllers for complex systems in simulation. This has been followed by a push towards learning controllers that can be transferred between simulation and hardware, primarily with the use of domain randomization. However, domain randomization can make the problem of finding stable controllers even more challenging, especially for underactuated bipedal robots. In this work, we explore whether policies learned in simulation can be transferred to hardware with the use of high-fidelity simulators and structured controllers. We learn a neural network policy which is a part of a more structured controller. While the neural network is learned in simulation, the rest of the controller stays fixed, and can be tuned by the expert as needed. We show that using this approach can greatly speed up the rate of learning in simulation, as well as enable transfer of policies between simulation and hardware. We present our results on an ATRIAS robot and explore the effect of action spaces and cost functions on the rate of transfer between simulation and hardware. Our results show that structured policies can indeed be learned in simulation and implemented on hardware successfully. This has several advantages, as the structure preserves the intuitive nature of the policy, and the neural network improves the performance of the hand-designed policy. In this way, we propose a way of using neural networks to improve expert designed controllers, while maintaining ease of understanding.
ER  -


TY  - Preprint
T1  - FanStore: Enabling Efficient and Scalable I/O for Distributed Deep Learning
A1  - Zhao Zhang
A1  - Lei Huang
A1  - Uri Manor
A1  - Linjing Fang
A1  - Gabriele Merlo
A1  - Craig Michoski
A1  - John Cazes
A1  - Niall Gaffney
JO  - ArXiv e-prints
Y1  - 27 September, 2018
UR  - https://arxiv.org/abs/1809.10799
N2  - Emerging Deep Learning (DL) applications introduce heavy I/O workloads on computer clusters. The inherent long lasting, repeated, and random file access pattern can easily saturate the metadata and data service and negatively impact other users. In this paper, we present FanStore, a transient runtime file system that optimizes DL I/O on existing hardware/software stacks. FanStore distributes datasets to the local storage of compute nodes, and maintains a global namespace. With the techniques of system call interception, distributed metadata management, and generic data compression, FanStore provides a POSIX-compliant interface with native hardware throughput in an efficient and scalable manner. Users do not have to make intrusive code changes to use FanStore and take advantage of the optimized I/O. Our experiments with benchmarks and real applications show that FanStore can scale DL training to 512 compute nodes with over 90\% scaling efficiency.
ER  -


TY  - Preprint
T1  - A Deep Learning Approach to Denoise Optical Coherence Tomography Images of the Optic Nerve Head
A1  - Sripad Krishna Devalla
A1  - Giridhar Subramanian
A1  - Tan Hung Pham
A1  - Xiaofei Wang
A1  - Shamira Perera
A1  - Tin A. Tun
A1  - Tin Aung
A1  - Leopold Schmetterer
A1  - Alexandre H. Thiery
A1  - Michael J. A. Girard
JO  - ArXiv e-prints
Y1  - 27 September, 2018
UR  - https://arxiv.org/abs/1809.10589
N2  - Purpose: To develop a deep learning approach to de-noise optical coherence tomography (OCT) B-scans of the optic nerve head (ONH).
ER  -


TY  - Preprint
T1  - Real-time 3D Pose Estimation with a Monocular Camera Using Deep Learning and Object Priors On an Autonomous Racecar
A1  - Ankit Dhall
JO  - ArXiv e-prints
Y1  - 27 September, 2018
UR  - https://arxiv.org/abs/1809.10548
N2  - We propose a complete pipeline that allows object detection and simultaneously estimate the pose of these multiple object instances using just a single image. A novel &#34;keypoint regression&#34; scheme with a cross-ratio term is introduced that exploits prior information about the object&#39;s shape and size to regress and find specific feature points. Further, a priori 3D information about the object is used to match 2D-3D correspondences and accurately estimate object positions up to a distance of 15m. A detailed discussion of the results and an in-depth analysis of the pipeline is presented. The pipeline runs efficiently on a low-powered Jetson TX2 and is deployed as part of the perception pipeline on a real-time autonomous vehicle cruising at a top speed of 54 km/hr.
ER  -


TY  - Preprint
T1  - Towards increased trustworthiness of deep learning segmentation methods on cardiac MRI
A1  - JÃ¶rg Sander
A1  - Bob D. de Vos
A1  - Jelmer M. Wolterink
A1  - Ivana IÅ¡gum
JO  - ArXiv e-prints
Y1  - 29 September, 2018
UR  - https://arxiv.org/abs/1809.10430
N2  - Current state-of-the-art deep learning segmentation methods have not yet made a broad entrance into the clinical setting in spite of high demand for such automatic methods. One important reason is the lack of reliability caused by models that fail unnoticed and often locally produce anatomically implausible results that medical experts would not make. This paper presents an automatic image segmentation method based on (Bayesian) dilated convolutional networks (DCNN) that generate segmentation masks and spatial uncertainty maps for the input image at hand. The method was trained and evaluated using segmentation of the left ventricle (LV) cavity, right ventricle (RV) endocardium and myocardium (Myo) at end-diastole (ED) and end-systole (ES) in 100 cardiac 2D MR scans from the MICCAI 2017 Challenge (ACDC). Combining segmentations and uncertainty maps and employing a human-in-the-loop setting, we provide evidence that image areas indicated as highly uncertain regarding the obtained segmentation almost entirely cover regions of incorrect segmentations. The fused information can be harnessed to increase segmentation performance. Our results reveal that we can obtain valuable spatial uncertainty maps with low computational effort using DCNNs.
ER  -


TY  - Preprint
T1  - Image Reconstruction Using Deep Learning
A1  - Po-Yu Liu
A1  - Edmund Y. Lam
JO  - ArXiv e-prints
Y1  - 27 September, 2018
UR  - https://arxiv.org/abs/1809.10410
N2  - This paper proposes a deep learning architecture that attains statistically significant improvements over traditional algorithms in Poisson image denoising espically when the noise is strong. Poisson noise commonly occurs in low-light and photon- limited settings, where the noise can be most accurately modeled by the Poission distribution. Poisson noise traditionally prevails only in specific fields such as astronomical imaging. However, with the booming market of surveillance cameras, which commonly operate in low-light environments, or mobile phones, which produce noisy night scene pictures due to lower-grade sensors, the necessity for an advanced Poisson image denoising algorithm has increased. Deep learning has achieved amazing breakthroughs in other imaging problems, such image segmentation and recognition, and this paper proposes a deep learning denoising network that outperforms traditional algorithms in Poisson denoising especially when the noise is strong. The architecture incorporates a hybrid of convolutional and deconvolutional layers along with symmetric connections. The denoising network achieved statistically significant 0.38dB, 0.68dB, and 1.04dB average PSNR gains over benchmark traditional algorithms in experiments with image peak values 4, 2, and 1. The denoising network can also operate with shorter computational time while still outperforming the benchmark algorithm by tuning the reconstruction stride sizes.
ER  -


TY  - Preprint
T1  - Seeding Deep Learning using Wireless Localization
A1  - Zhujun Xiao
A1  - Yanzi Zhu
A1  - Yuxin Chen
A1  - Ben Y. Zhao
A1  - Junchen Jiang
A1  - Haitao Zheng
JO  - ArXiv e-prints
Y1  - 22 September, 2018
UR  - https://arxiv.org/abs/1809.10242
N2  - Deep learning is often constrained by the lack of large, diverse labeled training datasets, especially images captured in the wild. We believe advances in wireless localization, working in unison with cameras, can produce automated labeling of targets in videos captured in the wild. Using pedestrian detection as a case study, we demonstrate the feasibility, benefits, and challenges of a possible solution using WiFi localization. To enable the automated generation of labeled training data, our work calls for new technical development on passive localization, mobile data analytics, and error-resilient ML models, as well as design issues in user privacy policies.
ER  -


TY  - Preprint
T1  - Towards a Hands-Free Query Optimizer through Deep Learning
A1  - Ryan Marcus
A1  - Olga Papaemmanouil
JO  - ArXiv e-prints
Y1  - 26 September, 2018
UR  - https://arxiv.org/abs/1809.10212
N2  - Query optimization remains one of the most important and well-studied problems in database systems. However, traditional query optimizers are complex heuristically-driven systems, requiring large amounts of time to tune for a particular database and requiring even more time to develop and maintain in the first place. In this vision paper, we argue that a new type of query optimizer, based on deep reinforcement learning, can drastically improve on the state-of-the-art. We identify potential complications for future research that integrates deep learning with query optimization and describe three novel deep learning based approaches that can lead the way to end-to-end learning-based query optimizers.
ER  -


TY  - Preprint
T1  - Morphed Learning: Towards Privacy-Preserving for Deep Learning Based Applications
A1  - Juncheng Shen
A1  - Juzheng Liu
A1  - Yiran Chen
A1  - Hai Li
JO  - ArXiv e-prints
Y1  - 20 September, 2018
UR  - https://arxiv.org/abs/1809.09968
N2  - The concern of potential privacy violation has prevented efficient use of big data for improving deep learning based applications. In this paper, we propose Morphed Learning, a privacy-preserving technique for deep learning based on data morphing that, allows data owners to share their data without leaking sensitive privacy information. Morphed Learning allows the data owners to send securely morphed data and provides the server with an Augmented Convolutional layer to train the network on morphed data without performance loss. Morphed Learning has these three features: (1) Strong protection against reverse-engineering on the morphed data; (2) Acceptable computational and data transmission overhead with no correlation to the depth of the neural network; (3) No degradation of the neural network performance. Theoretical analyses on CIFAR-10 dataset and VGG-16 network show that our method is capable of providing 10^89 morphing possibilities with only 5% computational overhead and 10% transmission overhead under limited knowledge attack scenario. Further analyses also proved that our method can offer same resilience against full knowledge attack if more resources are provided.
ER  -


TY  - Preprint
T1  - Active Learning for Deep Object Detection
A1  - Clemens-Alexander Brust
A1  - Christoph KÃ¤ding
A1  - Joachim Denzler
JO  - ArXiv e-prints
Y1  - 26 September, 2018
UR  - https://arxiv.org/abs/1809.09875
N2  - The great success that deep models have achieved in the past is mainly owed to large amounts of labeled training data. However, the acquisition of labeled data for new tasks aside from existing benchmarks is both challenging and costly. Active learning can make the process of labeling new data more efficient by selecting unlabeled samples which, when labeled, are expected to improve the model the most. In this paper, we combine a novel method of active learning for object detection with an incremental learning scheme to enable continuous exploration of new unlabeled datasets. We propose a set of uncertainty-based active learning metrics suitable for most object detectors. Furthermore, we present an approach to leverage class imbalances during sample selection. All methods are evaluated systematically in a continuous exploration context on the PASCAL VOC 2012 dataset.
ER  -


TY  - Preprint
T1  - Robot Bed-Making: Deep Transfer Learning Using Depth Sensing of Deformable Fabric
A1  - Daniel Seita
A1  - Nawid Jamali
A1  - Michael Laskey
A1  - Ron Berenstein
A1  - Ajay Kumar Tanwani
A1  - Prakash Baskaran
A1  - Soshi Iba
A1  - John Canny
A1  - Ken Goldberg
JO  - ArXiv e-prints
Y1  - 26 September, 2018
UR  - https://arxiv.org/abs/1809.09810
N2  - Bed-making is a common task well-suited for home robots since it is tolerant to error and not time-critical. Bed-making can also be difficult for senior citizens and those with limited mobility due to the bending and reaching movements required. Autonomous bed-making combines multiple challenges in robotics: perception in unstructured environments, deformable object manipulation, transfer learning, and sequential decision making. We formalize the bed-making problem as one of maximizing surface coverage with a blanket, and explore algorithmic approaches that use deep learning on depth images to be invariant to the color and pattern of the blankets. We train two networks: one to identify a corner of the blanket and another to determine when to transition to the other side of the bed. Using the first network, the robot grasps at its estimate of the blanket corner and then pulls it to the appropriate corner of the bed frame. The second network estimates if the robot has sufficiently covered one side and can transition to the other, or if it should attempt another grasp from the same side. We evaluate with two robots, the Toyota HSR and the Fetch, and three blankets. Using 2018 and 654 depth images for training the grasp and transition networks respectively, experiments with a quarter-scale twin bed achieve an average of 91.7% blanket coverage, nearly matching human supervisors with 95.0% coverage. Data is available at https://sites.google.com/view/bed-make.
ER  -


TY  - Preprint
T1  - A Model-Driven Deep Learning Network for MIMO Detection
A1  - Hengtao He
A1  - Chao-Kai Wen
A1  - Shi Jin
A1  - Geoffrey Ye Li
JO  - ArXiv e-prints
Y1  - 25 September, 2018
UR  - https://arxiv.org/abs/1809.09336
N2  - In this paper, we propose a model-driven deep learning network for multiple-input multiple-output (MIMO) detection. The structure of the network is specially designed by unfolding the iterative algorithm. Some trainable parameters are optimized through deep learning techniques to improve the detection performance. Since the number of trainable variables of the network is equal to that of the layers, the network can be easily trained within a very short time. Furthermore, the network can handle time-varying channel with only a single training. Numerical results show that the proposed approach can improve the performance of the iterative algorithm significantly under Rayleigh and correlated MIMO channels.
ER  -


TY  - Preprint
T1  - Hierarchical Deep Multiagent Reinforcement Learning
A1  - Hongyao Tang
A1  - Jianye Hao
A1  - Tangjie Lv
A1  - Yingfeng Chen
A1  - Zongzhang Zhang
A1  - Hangtian Jia
A1  - Chunxu Ren
A1  - Yan Zheng
A1  - Changjie Fan
A1  - Li Wang
JO  - ArXiv e-prints
Y1  - 25 September, 2018
UR  - https://arxiv.org/abs/1809.09332
N2  - Despite deep reinforcement learning has recently achieved great successes, however in multiagent environments, a number of challenges still remain. Multiagent reinforcement learning (MARL) is commonly considered to suffer from the problem of non-stationary environments and exponentially increasing policy space. It would be even more challenging to learn effective policies in circumstances where the rewards are sparse and delayed over long trajectories. In this paper, we study Hierarchical Deep Multiagent Reinforcement Learning (hierarchical deep MARL) in cooperative multiagent problems with sparse and delayed rewards, where efficient multiagent learning methods are desperately needed. We decompose the original MARL problem into hierarchies and investigate how effective policies can be learned hierarchically in synchronous/asynchronous hierarchical MARL frameworks. Several hierarchical deep MARL architectures, i.e., Ind-hDQN, hCom and hQmix, are introduced for different learning paradigms. Moreover, to alleviate the issues of sparse experiences in high-level learning and non-stationarity in multiagent settings, we propose a new experience replay mechanism, named as Augmented Concurrent Experience Replay (ACER). We empirically demonstrate the effects and efficiency of our approaches in several classic Multiagent Trash Collection tasks, as well as in an extremely challenging team sports game, i.e., Fever Basketball Defense.
ER  -


TY  - Preprint
T1  - Attention Mechanism in Speaker Recognition: What Does It Learn in Deep Speaker Embedding?
A1  - Qiongqiong Wang
A1  - Koji Okabe
A1  - Kong Aik Lee
A1  - Hitoshi Yamamoto
A1  - Takafumi Koshinaka
JO  - ArXiv e-prints
Y1  - 25 September, 2018
UR  - https://arxiv.org/abs/1809.09311
N2  - This paper presents an experimental study on deep speaker embedding with an attention mechanism that has been found to be a powerful representation learning technique in speaker recognition. In this framework, an attention model works as a frame selector that computes an attention weight for each frame-level feature vector, in accord with which an utterancelevel representation is produced at the pooling layer in a speaker embedding network. In general, an attention model is trained together with the speaker embedding network on a single objective function, and thus those two components are tightly bound to one another. In this paper, we consider the possibility that the attention model might be decoupled from its parent network and assist other speaker embedding networks and even conventional i-vector extractors. This possibility is demonstrated through a series of experiments on a NIST Speaker Recognition Evaluation (SRE) task, with 9.0% EER reduction and 3.8% min_Cprimary reduction when the attention weights are applied to i-vector extraction. Another experiment shows that DNN-based soft voice activity detection (VAD) can be effectively combined with the attention mechanism to yield further reduction of minCprimary by 6.6% and 1.6% in deep speaker embedding and i-vector systems, respectively.
ER  -


TY  - Preprint
T1  - MedAL: Deep Active Learning Sampling Method for Medical Image Analysis
A1  - Asim Smailagic
A1  - Hae Young Noh
A1  - Pedro Costa
A1  - Devesh Walawalkar
A1  - Kartik Khandelwal
A1  - Mostafa Mirshekari
A1  - Jonathon Fagert
A1  - AdriÃ¡n GaldrÃ¡n
A1  - Susu Xu
JO  - ArXiv e-prints
Y1  - 28 September, 2018
UR  - https://arxiv.org/abs/1809.09287
N2  - Deep learning models have been successfully used in medical image analysis problems but they require a large amount of labeled images to obtain good performance.Deep learning models have been successfully used in medical image analysis problems but they require a large amount of labeled images to obtain good performance. However, such large labeled datasets are costly to acquire. Active learning techniques can be used to minimize the number of required training labels while maximizing the model&#39;s performance.In this work, we propose a novel sampling method that queries the unlabeled examples that maximize the average distance to all training set examples in a learned feature space. We then extend our sampling method to define a better initial training set, without the need for a trained model, by using ORB feature descriptors. We validate MedAL on 3 medical image datasets and show that our method is robust to different dataset properties. MedAL is also efficient, achieving 80% accuracy on the task of Diabetic Retinopathy detection using only 425 labeled images, corresponding to a 32% reduction in the number of required labeled examples compared to the standard uncertainty sampling technique, and a 40% reduction compared to random sampling.
ER  -


TY  - Preprint
T1  - Towards Automated Post-Earthquake Inspections with Deep Learning-based Condition-Aware Models
A1  - Vedhus Hoskere
A1  - Yasutaka Narazaki
A1  - Tu A. Hoang
A1  - Billie F. Spencer Jr
JO  - ArXiv e-prints
Y1  - 24 September, 2018
UR  - https://arxiv.org/abs/1809.09195
N2  - In the aftermath of an earthquake, rapid structural inspections are required to get citizens back in to their homes and offices in a safe and timely manner. These inspections gfare typically conducted by municipal authorities through structural engineer volunteers. As manual inspec-tions can be time consuming, laborious and dangerous, research has been underway to develop methods to help speed up and increase the automation of the entire process. Researchers typi-cally envisage the use of unmanned aerial vehicles (UAV) for data acquisition and computer vision for data processing to extract actionable information. In this work we propose a new framework to generate vision-based condition-aware models that can serve as the basis for speeding up or automating higher level inspection decisions. The condition-aware models are generated by projecting the inference of trained deep-learning models on a set of images of a structure onto a 3D mesh model generated through multi-view stereo from the same image set. Deep fully convolutional residual networks are used for semantic segmentation of images of buildings to provide (i) damage information such as cracks and spalling (ii) contextual infor-mation such as the presence of a building and visually identifiable components like windows and doors. The proposed methodology was implemented on a damaged building that was sur-veyed by the authors after the Central Mexico Earthquake in September 2017 and qualitative-ly evaluated. Results demonstrate the promise of the proposed method towards the ultimate goal of rapid and automated post-earthquake inspections.
ER  -


TY  - Preprint
T1  - Autonomous Deep Learning: Incremental Learning of Denoising Autoencoder for Evolving Data Streams
A1  - Mahardhika Pratama
A1  - Andri Ashfahani
A1  - Yew Soon Ong
A1  - Savitha Ramasamy
A1  - Edwin Lughofer
JO  - ArXiv e-prints
Y1  - 24 September, 2018
UR  - https://arxiv.org/abs/1809.09081
N2  - The generative learning phase of Autoencoder (AE) and its successor Denosing Autoencoder (DAE) enhances the flexibility of data stream method in exploiting unlabelled samples. Nonetheless, the feasibility of DAE for data stream analytic deserves in-depth study because it characterizes a fixed network capacity which cannot adapt to rapidly changing environments. An automated construction of a denoising autoeconder, namely deep evolving denoising autoencoder (DEVDAN), is proposed in this paper. DEVDAN features an open structure both in the generative phase and in the discriminative phase where input features can be automatically added and discarded on the fly. A network significance (NS) method is formulated in this paper and is derived from the bias-variance concept. This method is capable of estimating the statistical contribution of the network structure and its hidden units which precursors an ideal state to add or prune input features. Furthermore, DEVDAN is free of the problem- specific threshold and works fully in the single-pass learning fashion. The efficacy of DEVDAN is numerically validated using nine non-stationary data stream problems simulated under the prequential test-then-train protocol where DEVDAN is capable of delivering an improvement of classification accuracy to recently published online learning works while having flexibility in the automatic extraction of robust input features and in adapting to rapidly changing environments.
ER  -


TY  - Preprint
T1  - Crowd-Robot Interaction: Crowd-aware Robot Navigation with Attention-based Deep Reinforcement Learning
A1  - Changan Chen
A1  - Yuejiang Liu
A1  - Sven Kreiss
A1  - Alexandre Alahi
JO  - ArXiv e-prints
Y1  - 24 September, 2018
UR  - https://arxiv.org/abs/1809.08835
N2  - Mobility in an effective and socially-compliant manner is an essential yet challenging task for robots operating in crowded spaces. Recent works have shown the power of deep reinforcement learning techniques to learn socially cooperative policies. However, their cooperation ability deteriorates as the crowd grows since they typically relax the problem as a one-way Human-Robot interaction problem. In this work, we want to go beyond first-order Human-Robot interaction and more explicitly model Crowd-Robot Interaction (CRI). We propose to (i) rethink pairwise interactions with a self-attention mechanism, and (ii) jointly model Human-Robot as well as Human-Human interactions in the deep reinforcement learning framework. Our model captures the Human-Human interactions occurring in dense crowds that indirectly affects the robot&#39;s anticipation capability. Our proposed attentive pooling mechanism learns the collective importance of neighboring humans with respect to their future states. Various experiments demonstrate that our model can anticipate human dynamics and navigate in crowds with time efficiency, outperforming state-of-the-art methods.
ER  -


TY  - Preprint
T1  - Detecting Features of Tools, Objects, and Actions from Effects in a Robot using Deep Learning
A1  - Namiko Saito
A1  - Kitae Kim
A1  - Shingo Murata
A1  - Tetsuya Ogata
A1  - Shigeki Sugano
JO  - ArXiv e-prints
Y1  - 23 September, 2018
UR  - https://arxiv.org/abs/1809.08613
N2  - We propose a tool-use model that can detect the features of tools, target objects, and actions from the provided effects of object manipulation. We construct a model that enables robots to manipulate objects with tools, using infant learning as a concept. To realize this, we train sensory-motor data recorded during a tool-use task performed by a robot with deep learning. Experiments include four factors: (1) tools, (2) objects, (3) actions, and (4) effects, which the model considers simultaneously. For evaluation, the robot generates predicted images and motions given information of the effects of using unknown tools and objects. We confirm that the robot is capable of detecting features of tools, objects, and actions by learning the effects and executing the task.
ER  -


TY  - Preprint
T1  - DT-LET: Deep Transfer Learning by Exploring where to Transfer
A1  - Jianzhe Lin
A1  - Qi Wang
A1  - Rabab Ward
A1  - Z. Jane Wang
JO  - ArXiv e-prints
Y1  - 23 September, 2018
UR  - https://arxiv.org/abs/1809.08541
N2  - Previous transfer learning methods based on deep network assume the knowledge should be transferred between the same hidden layers of the source domain and the target domains. This assumption doesn&#39;t always hold true, especially when the data from the two domains are heterogeneous with different resolutions. In such case, the most suitable numbers of layers for the source domain data and the target domain data would differ. As a result, the high level knowledge from the source domain would be transferred to the wrong layer of target domain. Based on this observation, &#34;where to transfer&#34; proposed in this paper should be a novel research frontier. We propose a new mathematic model named DT-LET to solve this heterogeneous transfer learning problem. In order to select the best matching of layers to transfer knowledge, we define specific loss function to estimate the corresponding relationship between high-level features of data in the source domain and the target domain. To verify this proposed cross-layer model, experiments for two cross-domain recognition/classification tasks are conducted, and the achieved superior results demonstrate the necessity of layer correspondence searching.
ER  -


TY  - Preprint
T1  - DeepOrigin: End-to-End Deep Learning for Detection of New Malware Families
A1  - Ilay Cordonsky
A1  - Ishai Rosenberg
A1  - Guillaume Sicard
A1  - Eli David
JO  - ArXiv e-prints
Y1  - 22 September, 2018
UR  - https://arxiv.org/abs/1809.08479
N2  - In this paper, we present a novel method of differentiating known from previously unseen malware families. We utilize transfer learning by learning compact file representations that are used for a new classification task between previously seen malware families and novel ones. The learned file representations are composed of static and dynamic features of malware and are invariant to small modifications that do not change their malicious functionality. Using an extensive dataset that consists of thousands of variants of malicious files, we were able to achieve 97.7% accuracy when classifying between seen and unseen malware families. Our method provides an important focalizing tool for cybersecurity researchers and greatly improves the overall ability to adapt to the fast-moving pace of the current threat landscape.
ER  -


TY  - Preprint
T1  - Geometric Multi-Model Fitting by Deep Reinforcement Learning
A1  - Zongliang Zhang
A1  - Hongbin Zeng
A1  - Jonathan Li
A1  - Yiping Chen
A1  - Chenhui Yang
A1  - Cheng Wang
JO  - ArXiv e-prints
Y1  - 22 September, 2018
UR  - https://arxiv.org/abs/1809.08397
N2  - This paper deals with the geometric multi-model fitting from noisy, unstructured point set data (e.g., laser scanned point clouds). We formulate multi-model fitting problem as a sequential decision making process. We then use a deep reinforcement learning algorithm to learn the optimal decisions towards the best fitting result. In this paper, we have compared our method against the state-of-the-art on simulated data. The results demonstrated that our approach significantly reduced the number of fitting iterations.
ER  -


TY  - Preprint
T1  - Comment on All-optical machine learning using diffractive deep neural networks
A1  - Haiqing Wei
A1  - Gang Huang
A1  - Xiuqing Wei
A1  - Yanlong Sun
A1  - Hongbin Wang
JO  - ArXiv e-prints
Y1  - 21 September, 2018
UR  - https://arxiv.org/abs/1809.08360
N2  - Lin et al. (Reports, 7 September 2018, p. 1004) reported a remarkable proposal that employs a passive, strictly linear optical setup to perform pattern classifications. But interpreting the multilayer diffractive setup as a deep neural network and advocating it as an all-optical deep learning framework are not well justified and represent a mischaracterization of the system by overlooking its defining characteristics of perfect linearity and strict passivity.
ER  -


TY  - Preprint
T1  - CPDist: Deep Siamese Networks for Learning Distances Between Structured Preferences
A1  - Andrea Loreggia
A1  - Nicholas Mattei
A1  - Francesca Rossi
A1  - K. Brent Venable
JO  - ArXiv e-prints
Y1  - 21 September, 2018
UR  - https://arxiv.org/abs/1809.08350
N2  - Preference are central to decision making by both machines and humans. Representing, learning, and reasoning with preferences is an important area of study both within computer science and across the sciences. When working with preferences it is necessary to understand and compute the distance between sets of objects, e.g., the preferences of a user and a the descriptions of objects to be recommended. We present CPDist, a novel neural network to address the problem of learning to measure the distance between structured preference representations. We use the popular CP-net formalism to represent preferences and then leverage deep neural networks to learn a recently proposed metric function that is computationally hard to compute directly. CPDist is a novel metric learning approach based on the use of deep siamese networks which learn the Kendal Tau distance between partial orders that are induced by compact preference representations. We find that CPDist is able to learn the distance function with high accuracy and outperform existing approximation algorithms on both the regression and classification task using less computation time. Performance remains good even when CPDist is trained with only a small number of samples compared to the dimension of the solution space, indicating the network generalizes well.
ER  -


TY  - Preprint
T1  - Image Denoising and Super-Resolution using Residual Learning of Deep Convolutional Network
A1  - Rohit Pardasani
A1  - Utkarsh Shreemali
JO  - ArXiv e-prints
Y1  - 21 September, 2018
UR  - https://arxiv.org/abs/1809.08229
N2  - Image super-resolution and denoising are two important tasks in image processing that can lead to improvement in image quality. Image super-resolution is the task of mapping a low resolution image to a high resolution image whereas denoising is the task of learning a clean image from a noisy input. We propose and train a single deep learning network that we term as SuRDCNN (super-resolution and denoising convolutional neural network), to perform these two tasks simultaneously . Our model nearly replicates the architecture of existing state-of-the-art deep learning models for super-resolution and denoising. We use the proven strategy of residual learning, as supported by state-of-the-art networks in this domain. Our trained SuRDCNN is capable of super-resolving image in the presence of Gaussian noise, Poisson noise or any random combination of both of these noises.
ER  -


TY  - Preprint
T1  - Power Market Price Forecasting via Deep Learning
A1  - Yongli Zhu
A1  - Songtao Lu
A1  - Renchang Dai
A1  - Guangyi Liu
A1  - Zhiwei Wang
JO  - ArXiv e-prints
Y1  - 18 September, 2018
UR  - https://arxiv.org/abs/1809.08092
N2  - A study on power market price forecasting by deep learning is presented. As one of the most successful deep learning frameworks, the LSTM (Long short-term memory) neural network is utilized. The hourly prices data from the New England and PJM day-ahead markets are used in this study. First, a LSTM network is formulated and trained. Then the raw input and output data are preprocessed by unit scaling, and the trained network is tested on the real price data under different input lengths, forecasting horizons and data sizes. Its performance is also compared with other existing methods. The forecasted results demonstrate that, the LSTM deep neural network can outperform the others under different application settings in this problem.
ER  -


TY  - Preprint
T1  - On-field player workload exposure and knee injury risk monitoring via deep learning
A1  - William R. Johnson
A1  - Ajmal Mian
A1  - David Lloyd
A1  - Jacqueline Alderson
JO  - ArXiv e-prints
Y1  - 21 September, 2018
UR  - https://arxiv.org/abs/1809.08016
N2  - In sports analytics, an understanding of accurate on-field 3D knee joint moments (KJM) could provide an early warning system for athlete workload exposure and knee injury risk. Traditionally, this analysis has relied on captive laboratory force plates and associated downstream biomechanical modeling, and many researchers have approached the problem of portability by extrapolating models built on linear statistics. An alternative approach would be to capitalize on recent advances in deep learning. In this study, using the pre-trained CaffeNet convolutional neural network (CNN) model, multivariate regression of marker-based motion capture to 3D KJM for three sports-related movement types were compared. The strongest overall mean correlation to source modeling of 0.8895 was achieved over the initial 33 % of stance phase for sidestepping. The accuracy of these mean predictions of the three critical KJM associated with anterior cruciate ligament (ACL) injury demonstrate the feasibility of on-field knee injury assessment using deep learning in lieu of laboratory embedded force plates. This multidisciplinary research approach significantly advances machine representation of real-world physical models with practical application for both community and professional level athletes.
ER  -


TY  - Preprint
T1  - SG-FCN: A Motion and Memory-Based Deep Learning Model for Video Saliency Detection
A1  - Meijun Sun
A1  - Ziqi Zhou
A1  - QinGhua Hu
A1  - Zheng Wang
A1  - Jianmin Jiang
JO  - ArXiv e-prints
Y1  - 21 September, 2018
UR  - https://arxiv.org/abs/1809.07988
N2  - Data-driven saliency detection has attracted strong interest as a result of applying convolutional neural networks to the detection of eye fixations. Although a number of imagebased salient object and fixation detection models have been proposed, video fixation detection still requires more exploration. Different from image analysis, motion and temporal information is a crucial factor affecting human attention when viewing video sequences. Although existing models based on local contrast and low-level features have been extensively researched, they failed to simultaneously consider interframe motion and temporal information across neighboring video frames, leading to unsatisfactory performance when handling complex scenes. To this end, we propose a novel and efficient video eye fixation detection model to improve the saliency detection performance. By simulating the memory mechanism and visual attention mechanism of human beings when watching a video, we propose a step-gained fully convolutional network by combining the memory information on the time axis with the motion information on the space axis while storing the saliency information of the current frame. The model is obtained through hierarchical training, which ensures the accuracy of the detection. Extensive experiments in comparison with 11 state-of-the-art methods are carried out, and the results show that our proposed model outperforms all 11 methods across a number of publicly available datasets.
ER  -


TY  - Preprint
T1  - Dynamic Weights in Multi-Objective Deep Reinforcement Learning
A1  - Axel Abels
A1  - Diederik M. Roijers
A1  - Tom Lenaerts
A1  - Ann NowÃ©
A1  - Denis Steckelmacher
JO  - ArXiv e-prints
Y1  - 20 September, 2018
UR  - https://arxiv.org/abs/1809.07803
N2  - Many real-world decision problems are characterized by multiple objectives which must be balanced based on their relative importance. In the dynamic weights setting this relative importance changes over time, as recognized by Natarajan and Tadepalli (2005) who proposed a tabular Reinforcement Learning algorithm to deal with this problem. However, this earlier work is not feasible for reinforcement learning settings in which the input is high-dimensional, necessitating the use of function approximators, such as neural networks. We propose two novel methods for multi-objective RL with dynamic weights, a multi-network approach and a single-network approach that conditions on the weights. Due to the inherent non-stationarity of the dynamic weights setting, standard experience replay techniques are insufficient. We therefore propose diverse experience replay, a framework to maintain a diverse set of experiences in the replay buffer, and show how it can be applied to make experience replay relevant in multi-objective RL. To evaluate the performance of our algorithms we introduce a new benchmark called the Minecart problem. We show empirically that our algorithms outperform more naive approaches. We also show that, while there are significant differences between many small changes in the weights opposed to sparse larger changes, the conditioned network with diverse experience replay consistently outperforms the other algorithms.
ER  -


TY  - Preprint
T1  - Brain Tumor Segmentation Using Deep Learning by Type Specific Sorting of Images
A1  - Zahra Sobhaninia
A1  - Safiyeh Rezaei
A1  - Alireza Noroozi
A1  - Mehdi Ahmadi
A1  - Hamidreza Zarrabi
A1  - Nader Karimi
A1  - Ali Emami
A1  - Shadrokh Samavi
JO  - ArXiv e-prints
Y1  - 20 September, 2018
UR  - https://arxiv.org/abs/1809.07786
N2  - Recently deep learning has been playing a major role in the field of computer vision. One of its applications is the reduction of human judgment in the diagnosis of diseases. Especially, brain tumor diagnosis requires high accuracy, where minute errors in judgment may lead to disaster. For this reason, brain tumor segmentation is an important challenge for medical purposes. Currently several methods exist for tumor segmentation but they all lack high accuracy. Here we present a solution for brain tumor segmenting by using deep learning. In this work, we studied different angles of brain MR images and applied different networks for segmentation. The effect of using separate networks for segmentation of MR images is evaluated by comparing the results with a single network. Experimental evaluations of the networks show that Dice score of 0.73 is achieved for a single network and 0.79 in obtained for multiple networks.
ER  -


TY  - Preprint
T1  - DuPLO: A DUal view Point deep Learning architecture for time series classificatiOn
A1  - Roberto Interdonato
A1  - Dino Ienco
A1  - Raffaele Gaetano
A1  - Kenji Ose
JO  - ArXiv e-prints
Y1  - 20 September, 2018
UR  - https://arxiv.org/abs/1809.07589
N2  - Nowadays, modern Earth Observation systems continuously generate huge amounts of data. A notable example is represented by the Sentinel-2 mission, which provides images at high spatial resolution (up to 10m) with high temporal revisit period (every 5 days), which can be organized in Satellite Image Time Series (SITS). While the use of SITS has been proved to be beneficial in the context of Land Use/Land Cover (LULC) map generation, unfortunately, machine learning approaches commonly leveraged in remote sensing field fail to take advantage of spatio-temporal dependencies present in such data.
ER  -


TY  - Preprint
T1  - Accelerating Flash Calculation through Deep Learning Methods
A1  - Yu Li
A1  - Tao Zhang
A1  - Shuyu Sun
A1  - Xin Gao
JO  - ArXiv e-prints
Y1  - 29 September, 2018
UR  - https://arxiv.org/abs/1809.07311
N2  - In the past two decades, researchers have made remarkable progress in accelerating flash calculation, which is very useful in a variety of engineering processes. In this paper, general phase splitting problem statements and flash calculation procedures using the Successive Substitution Method are reviewed, while the main shortages are pointed out. Two acceleration methods, Newton&#39;s method and the Sparse Grids Method are presented afterwards as a comparison with the deep learning model proposed in this paper. A detailed introduction from artificial neural networks to deep learning methods is provided here with the authors&#39; own remarks. Factors in the deep learning model are investigated to show their effect on the final result. A selected model based on that has been used in a flash calculation predictor with comparison with other methods mentioned above. It is shown that results from the optimized deep learning model meet the experimental data well with the shortest CPU time. More comparison with experimental data has been conducted to show the robustness of our model.
ER  -


TY  - Preprint
T1  - Deep Learning Based Rib Centerline Extraction and Labeling
A1  - Matthias Lenga
A1  - Tobias Klinder
A1  - Christian BÃ¼rger
A1  - Jens von Berg
A1  - Astrid Franz
A1  - Cristian Lorenz
JO  - ArXiv e-prints
Y1  - 19 September, 2018
UR  - https://arxiv.org/abs/1809.07082
N2  - Automated extraction and labeling of rib centerlines is a typically needed prerequisite for more advanced assisted reading tools that help the radiologist to efficiently inspect all 24 ribs in a CT volume. In this paper, we combine a deep learning-based rib detection with a dedicated centerline extraction algorithm applied to the detection result for the purpose of fast, robust and accurate rib centerline extraction and labeling from CT volumes. More specifically, we first apply a fully convolutional neural network (FCNN) to generate a probability map for detecting the first rib pair, the twelfth rib pair, and the collection of all intermediate ribs. In a second stage, a newly designed centerline extraction algorithm is applied to this multi-label probability map. Finally, the distinct detection of first and twelfth rib separately, allows to derive individual rib labels by simple sorting and counting the detected centerlines. We applied our method to CT volumes from 116 patients which included a variety of different challenges and achieved a centerline accuracy of 0.787 mm with respect to manual centerline annotations.
ER  -


TY  - Preprint
T1  - New approach for solar tracking systems based on computer vision, low cost hardware and deep learning
A1  - Jose A. Carballo
A1  - Javier Bonilla
A1  - Manuel Berenguel
A1  - JesÃºs FernÃ¡ndez-Reche
A1  - GinÃ©s GarcÃ­a
JO  - ArXiv e-prints
Y1  - 19 September, 2018
UR  - https://arxiv.org/abs/1809.07048
N2  - In this work, a new approach for Sun tracking systems is presented. Due to the current system limitations regarding costs and operational problems, a new approach based on low cost, computer vision open hardware and deep learning has been developed. The preliminary tests carried out successfully in Plataforma solar de Almeria (PSA), reveal the great potential and show the new approach as a good alternative to traditional systems. The proposed approach can provide key variables for the Sun tracking system control like cloud movements prediction, block and shadow detection, atmospheric attenuation or measures of concentrated solar radiation, which can improve the control strategies of the system and therefore the system performance.
ER  -


TY  - Preprint
T1  - Deep-learning models improve on community-level diagnosis for common congenital heart disease lesions
A1  - Rima Arnaout
A1  - Lara Curran
A1  - Erin Chinn
A1  - Yili Zhao
A1  - Anita Moon-Grady
JO  - ArXiv e-prints
Y1  - 18 September, 2018
UR  - https://arxiv.org/abs/1809.06993
N2  - Prenatal diagnosis of tetralogy of Fallot (TOF) and hypoplastic left heart syndrome (HLHS), two serious congenital heart defects, improves outcomes and can in some cases facilitate in utero interventions. In practice, however, the fetal diagnosis rate for these lesions is only 30-50 percent in community settings. Improving fetal diagnosis of congenital heart disease is therefore critical. Deep learning is a cutting-edge machine learning technique for finding patterns in images but has not yet been applied to prenatal diagnosis of congenital heart disease. Using 685 retrospectively collected echocardiograms from fetuses 18-24 weeks of gestational age from 2000-2018, we trained convolutional and fully-convolutional deep learning models in a supervised manner to (i) identify the five canonical screening views of the fetal heart and (ii) segment cardiac structures to calculate fetal cardiac biometrics. We then trained models to distinguish by view between normal hearts, TOF, and HLHS. In a holdout test set of images, F-score for identification of the five most important fetal cardiac views was 0.95. Binary classification of unannotated cardiac views of normal heart vs. TOF reached an overall sensitivity of 75% and a specificity of 76%, while normal vs. HLHS reached a sensitivity of 100% and specificity of 90%, both well above average diagnostic rates for these lesions. Furthermore, segmentation-based measurements for cardiothoracic ratio (CTR), cardiac axis (CA), and ventricular fractional area change (FAC) were compatible with clinically measured metrics for normal, TOF, and HLHS hearts. Thus, using guideline-recommended imaging, deep learning models can significantly improve detection of fetal congenital heart disease compared to the common standard of care.
ER  -


TY  - Preprint
T1  - A Study on Deep Learning Based Sauvegrain Method for Measurement of Puberty Bone Age
A1  - Seung Bin Baik
A1  - Keum Gang Cha
JO  - ArXiv e-prints
Y1  - 18 September, 2018
UR  - https://arxiv.org/abs/1809.06965
N2  - This study applies a technique to expand the number of images to a level that allows deep learning. And the applicability of the Sauvegrain method through deep learning with relatively few elbow X-rays is studied. The study was composed of processes similar to the physicians&#39; bone age assessment procedures. The selected reference images were learned without being included in the evaluation data, and at the same time, the data was extended to accommodate the number of cases. In addition, we adjusted the X-ray images to better images using U-Net and selected the ROI with RPN + so as to be able to perform bone age estimation through CNN. The mean absolute error of the Sauvegrain method based on deep learning is 2.8 months and the Mean Absolute Percentage Error (MAPE) is 0.018. This result shows that X - ray analysis using the Sauvegrain method shows higher accuracy than that of the age group of puberty even in the deep learning base. This means that deep learning of the Suvegrain method can be measured at a level similar to that of an expert, based on the extended X-ray image with the image data extension technique. Finally, we applied the Sauvegrain method to deep learning for accurate measurement of bone age at puberty. As a result, the present study is based on deep learning, and compared with the evaluation results of experts, it is possible to overcome limitations of the method of measuring bone age based on machine learning which was in TW3 or Greulich &amp; Pyle due to lack of X- I confirmed the fact. And we also presented the Sauvegrain method, which is applicable to adolescents as well.
ER  -


TY  - Preprint
T1  - On the Learning Dynamics of Deep Neural Networks
A1  - Remi Tachet des Combes
A1  - Mohammad Pezeshki
A1  - Samira Shabanian
A1  - Aaron Courville
A1  - Yoshua Bengio
JO  - ArXiv e-prints
Y1  - 18 September, 2018
UR  - https://arxiv.org/abs/1809.06848
N2  - While a lot of progress has been made in recent years, the dynamics of learning in deep nonlinear neural networks remain to this day largely misunderstood. In this work, we study the case of binary classification and prove various properties of learning in such networks under strong assumptions such as linear separability of the data. Extending existing results from the linear case, we confirm empirical observations by proving that the classification error also follows a sigmoidal shape in nonlinear architectures. We show that given proper initialization, learning expounds parallel independent modes and that certain regions of parameter space might lead to failed training. We also demonstrate that input norm and features&#39; frequency in the dataset lead to distinct convergence speeds which might shed some light on the generalization capabilities of deep neural networks. We provide a comparison between the dynamics of learning with cross-entropy and hinge losses, which could prove useful to understand recent progress in the training of generative adversarial networks. Finally, we identify a phenomenon that we baptize gradient starvation where the most frequent features in a dataset prevent the learning of other less frequent but equally informative features.
ER  -


TY  - Preprint
T1  - Visual Diagnostics for Deep Reinforcement Learning Policy Development
A1  - Jieliang Luo
A1  - Sam Green
A1  - Peter Feghali
A1  - George Legrady
A1  - Ãetin Kaya KoÃ§
JO  - ArXiv e-prints
Y1  - 26 September, 2018
UR  - https://arxiv.org/abs/1809.06781
N2  - Modern vision-based reinforcement learning techniques often use convolutional neural networks (CNN) as universal function approximators to choose which action to take for a given visual input. Until recently, CNNs have been treated like black-box functions, but this mindset is especially dangerous when used for control in safety-critical settings. In this paper, we present our extensions of CNN visualization algorithms to the domain of vision-based reinforcement learning. We use a simulated drone environment as an example scenario. These visualization algorithms are an important tool for behavior introspection and provide insight into the qualities and flaws of trained policies when interacting with the physical world. A video may be seen at https://sites.google.com/view/drlvisual .
ER  -


TY  - Preprint
T1  - Towards Deep and Representation Learning for Talent Search at LinkedIn
A1  - Rohan Ramanath
A1  - Hakan Inan
A1  - Gungor Polatkan
A1  - Bo Hu
A1  - Qi Guo
A1  - Cagri Ozcaglar
A1  - Xianren Wu
A1  - Krishnaram Kenthapadi
A1  - Sahin Cem Geyik
JO  - ArXiv e-prints
Y1  - 17 September, 2018
UR  - https://arxiv.org/abs/1809.06473
N2  - Talent search and recommendation systems at LinkedIn strive to match the potential candidates to the hiring needs of a recruiter or a hiring manager expressed in terms of a search query or a job posting. Recent work in this domain has mainly focused on linear models, which do not take complex relationships between features into account, as well as ensemble tree models, which introduce non-linearity but are still insufficient for exploring all the potential feature interactions, and strictly separate feature generation from modeling. In this paper, we present the results of our application of deep and representation learning models on LinkedIn Recruiter. Our key contributions include: (i) Learning semantic representations of sparse entities within the talent search domain, such as recruiter ids, candidate ids, and skill entity ids, for which we utilize neural network models that take advantage of LinkedIn Economic Graph, and (ii) Deep models for learning recruiter engagement and candidate response in talent search applications. We also explore learning to rank approaches applied to deep models, and show the benefits for the talent search use case. Finally, we present offline and online evaluation results for LinkedIn talent search and recommendation systems, and discuss potential challenges along the path to a fully deep model architecture. The challenges and approaches discussed generalize to any multi-faceted search engine.
ER  -


TY  - Preprint
T1  - DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning
A1  - Kashyap Popat
A1  - Subhabrata Mukherjee
A1  - Andrew Yates
A1  - Gerhard Weikum
JO  - ArXiv e-prints
Y1  - 17 September, 2018
UR  - https://arxiv.org/abs/1809.06416
N2  - Misinformation such as fake news is one of the big challenges of our society. Research on automated fact-checking has proposed methods based on supervised learning, but these approaches do not consider external evidence apart from labeled training instances. Recent approaches counter this deficit by considering external sources related to a claim. However, these methods require substantial feature modeling and rich lexicons. This paper overcomes these limitations of prior work with an end-to-end model for evidence-aware credibility assessment of arbitrary textual claims, without any human intervention. It presents a neural network model that judiciously aggregates signals from external evidence articles, the language of these articles and the trustworthiness of their sources. It also derives informative features for generating user-comprehensible explanations that makes the neural network predictions transparent to the end-user. Experiments with four datasets and ablation studies show the strength of our method.
ER  -


TY  - Preprint
T1  - Generalizing Across Multi-Objective Reward Functions in Deep Reinforcement Learning
A1  - Eli Friedman
A1  - Fred Fontaine
JO  - ArXiv e-prints
Y1  - 17 September, 2018
UR  - https://arxiv.org/abs/1809.06364
N2  - Many reinforcement-learning researchers treat the reward function as a part of the environment, meaning that the agent can only know the reward of a state if it encounters that state in a trial run. However, we argue that this is an unnecessary limitation and instead, the reward function should be provided to the learning algorithm. The advantage is that the algorithm can then use the reward function to check the reward for states that the agent hasn&#39;t even encountered yet. In addition, the algorithm can simultaneously learn policies for multiple reward functions. For each state, the algorithm would calculate the reward using each of the reward functions and add the rewards to its experience replay dataset. The Hindsight Experience Replay algorithm developed by Andrychowicz et al. (2017) does just this, and learns to generalize across a distribution of sparse, goal-based rewards. We extend this algorithm to linearly-weighted, multi-objective rewards and learn a single policy that can generalize across all linear combinations of the multi-objective reward. Whereas other multi-objective algorithms teach the Q-function to generalize across the reward weights, our algorithm enables the policy to generalize, and can thus be used with continuous actions.
ER  -


TY  - Preprint
T1  - Left Ventricle Segmentation and Volume Estimation on Cardiac MRI using Deep Learning
A1  - Ehab Abdelmaguid
A1  - Jolene Huang
A1  - Sanjay Kenchareddy
A1  - Disha Singla
A1  - Laura Wilke
A1  - Mai H. Nguyen
A1  - Ilkay Altintas
JO  - ArXiv e-prints
Y1  - 14 September, 2018
UR  - https://arxiv.org/abs/1809.06247
N2  - In the United States, heart disease is the leading cause of death for males and females, accounting for 610,000 deaths each year [1]. Physicians use Magnetic Resonance Imaging (MRI) scans to take images of the heart in order to non-invasively estimate its structural and functional parameters for cardiovascular diagnosis and disease management. The end-systolic volume (ESV) and end-diastolic volume (EDV) of the left ventricle (LV), and the ejection fraction (EF) are indicators of heart disease. These measures can be derived from the segmented contours of the LV; thus, consistent and accurate segmentation of the LV from MRI images are critical to the accuracy of the ESV, EDV, and EF, and to non-invasive cardiac disease detection.
ER  -


TY  - Preprint
T1  - Binary Classification of Alzheimer Disease using sMRI Imaging modality and Deep Learning
A1  - Ahsan Bin Tufail
A1  - Qiu-Na Zhang
A1  - Yong-Kui Ma
JO  - ArXiv e-prints
Y1  - 8 September, 2018
UR  - https://arxiv.org/abs/1809.06209
N2  - Alzheimer Disease (AD) is the most common form of dementia affecting the elderly population worldwide. Many neuroimaging modalities have been used to check the detection and progression of AD of which structural Magnetic Resonance Imaging (sMRI) is an important one. The recent rise in the popularity of deep learning methods with applications in computer vision, reinforcement learning and artificial intelligence has created a resurgence in the application of these methods to the classification of AD through different imaging modalities. In this study, by utilizing the concept of transfer learning in deep learning, we propose a classification framework to differentiate subjects with Clinical Dementia Rating (CDR) of zero from subjects with CDR greater than zero by using deep learning architectures such as Xception and Inception version 3 in the Keras deep learning library. The attained validation set accuracies are as high as 99.12% for the Inception version 3 network and 97.97% for the Xception network. The presented results suggest that meaningful predictors composed of sMRI and network measures may offer the possibility for early detection of subjects in the early stages of AD.
ER  -


TY  - Preprint
T1  - Curriculum goal masking for continuous deep reinforcement learning
A1  - Manfred Eppe
A1  - Sven Magg
A1  - Stefan Wermter
JO  - ArXiv e-prints
Y1  - 17 September, 2018
UR  - https://arxiv.org/abs/1809.06146
N2  - Deep reinforcement learning has recently gained a focus on problems where policy or value functions are independent of goals. Evidence exists that the sampling of goals has a strong effect on the learning performance, but there is a lack of general mechanisms that focus on optimizing the goal sampling process. In this work, we present a simple and general goal masking method that also allows us to estimate a goal&#39;s difficulty level and thus realize a curriculum learning approach for deep RL. Our results indicate that focusing on goals with a medium difficulty level is appropriate for deep deterministic policy gradient (DDPG) methods, while an &#34;aim for the stars and reach the moon-strategy&#34;, where hard goals are sampled much more often than simple goals, leads to the best learning performance in cases where DDPG is combined with for hindsight experience replay (HER). We demonstrate that the approach significantly outperforms standard goal sampling for different robotic object manipulation problems.
ER  -


TY  - Preprint
T1  - Revisit Multinomial Logistic Regression in Deep Learning: Data Dependent Model Initialization for Image Recognition
A1  - Bowen Cheng
A1  - Rong Xiao
A1  - Yandong Guo
A1  - Yuxiao Hu
A1  - Jianfeng Wang
A1  - Lei Zhang
JO  - ArXiv e-prints
Y1  - 17 September, 2018
UR  - https://arxiv.org/abs/1809.06131
N2  - We study in this paper how to initialize the parameters of multinomial logistic regression (a fully connected layer followed with softmax and cross entropy loss), which is widely used in deep neural network (DNN) models for classification problems. As logistic regression is widely known not having a closed-form solution, it is usually randomly initialized, leading to several deficiencies especially in transfer learning where all the layers except for the last task-specific layer are initialized using a pre-trained model. The deficiencies include slow convergence speed, possibility of stuck in local minimum, and the risk of over-fitting. To address those deficiencies, we first study the properties of logistic regression and propose a closed-form approximate solution named regularized Gaussian classifier (RGC). Then we adopt this approximate solution to initialize the task-specific linear layer and demonstrate superior performance over random initialization in terms of both accuracy and convergence speed on various tasks and datasets. For example, for image classification, our approach can reduce the training time by 10 times and achieve 3.2% gain in accuracy for Flickr-style classification. For object detection, our approach can also be 10 times faster in training for the same accuracy, or 5% better in terms of mAP for VOC 2007 with slightly longer training.
ER  -


TY  - Preprint
T1  - A Deep Learning Framework for Unsupervised Affine and Deformable Image Registration
A1  - Bob D. de Vos
A1  - Floris F. Berendsen
A1  - Max A. Viergever
A1  - Hessam Sokooti
A1  - Marius Staring
A1  - Ivana Isgum
JO  - ArXiv e-prints
Y1  - 17 September, 2018
UR  - https://arxiv.org/abs/1809.06130
N2  - Image registration, the process of aligning two or more images, is the core technique of many (semi-)automatic medical image analysis tasks. Recent studies have shown that deep learning methods, notably convolutional neural networks (ConvNets), can be used for image registration. Thus far training of ConvNets for registration was supervised using predefined example registrations. However, obtaining example registrations is not trivial. To circumvent the need for predefined examples, and thereby to increase convenience of training ConvNets for image registration, we propose the Deep Learning Image Registration (DLIR) framework for \textit{unsupervised} affine and deformable image registration. In the DLIR framework ConvNets are trained for image registration by exploiting image similarity analogous to conventional intensity-based image registration. After a ConvNet has been trained with the DLIR framework, it can be used to register pairs of unseen images in one shot. We propose flexible ConvNets designs for affine image registration and for deformable image registration. By stacking multiple of these ConvNets into a larger architecture, we are able to perform coarse-to-fine image registration. We show for registration of cardiac cine MRI and registration of chest CT that performance of the DLIR framework is comparable to conventional image registration while being several orders of magnitude faster.
ER  -


TY  - Preprint
T1  - Object-sensitive Deep Reinforcement Learning
A1  - Yuezhang Li
A1  - Katia Sycara
A1  - Rahul Iyer
JO  - ArXiv e-prints
Y1  - 17 September, 2018
UR  - https://arxiv.org/abs/1809.06064
N2  - Deep reinforcement learning has become popular over recent years, showing superiority on different visual-input tasks such as playing Atari games and robot navigation. Although objects are important image elements, few work considers enhancing deep reinforcement learning with object characteristics. In this paper, we propose a novel method that can incorporate object recognition processing to deep reinforcement learning models. This approach can be adapted to any existing deep reinforcement learning frameworks. State-of-the-art results are shown in experiments on Atari games. We also propose a new approach called &#34;object saliency maps&#34; to visually explain the actions made by deep reinforcement learning agents.
ER  -


TY  - Preprint
T1  - Transparency and Explanation in Deep Reinforcement Learning Neural Networks
A1  - Rahul Iyer
A1  - Yuezhang Li
A1  - Huao Li
A1  - Michael Lewis
A1  - Ramitha Sundar
A1  - Katia Sycara
JO  - ArXiv e-prints
Y1  - 17 September, 2018
UR  - https://arxiv.org/abs/1809.06061
N2  - Autonomous AI systems will be entering human society in the near future to provide services and work alongside humans. For those systems to be accepted and trusted, the users should be able to understand the reasoning process of the system, i.e. the system should be transparent. System transparency enables humans to form coherent explanations of the system&#39;s decisions and actions. Transparency is important not only for user trust, but also for software debugging and certification. In recent years, Deep Neural Networks have made great advances in multiple application areas. However, deep neural networks are opaque. In this paper, we report on work in transparency in Deep Reinforcement Learning Networks (DRLN). Such networks have been extremely successful in accurately learning action control in image input domains, such as Atari games. In this paper, we propose a novel and general method that (a) incorporates explicit object recognition processing into deep reinforcement learning models, (b) forms the basis for the development of &#34;object saliency maps&#34;, to provide visualization of internal states of DRLNs, thus enabling the formation of explanations and (c) can be incorporated in any existing deep reinforcement learning framework. We present computational results and human experiments to evaluate our approach.
ER  -


TY  - Preprint
T1  - Model-Driven Deep Learning for Physical Layer Communications
A1  - Hengtao He
A1  - Shi Jin
A1  - Chao-Kai Wen
A1  - Feifei Gao
A1  - Geoffrey Ye Li
A1  - Zongben Xu
JO  - ArXiv e-prints
Y1  - 17 September, 2018
UR  - https://arxiv.org/abs/1809.06059
N2  - Intelligent communication is gradually considered as the mainstream direction in future wireless communications. As a major branch of machine learning, deep learning (DL) has been applied in physical layer communications and has demonstrated an impressive performance improvement in recent years. However, most of the existing works related to DL focus on data-driven approaches, which consider the communication system as a black box and train it by using a huge volume of data. Training a network requires sufficient computing resources and extensive time, both of which are rarely found in communication devices. By contrast, model-driven DL approaches combine communication domain knowledge with DL to reduce the demand for computing resources and training time. This article reviews the recent advancements in the application of model-driven DL approaches in physical layer communications, including transmission scheme, receiver design, and channel information recovery. Several open issues for further research are also highlighted after presenting the comprehensive survey.
ER  -


TY  - Preprint
T1  - Autonomous Exploration, Reconstruction, and Surveillance of 3D Environments Aided by Deep Learning
A1  - Louis Ly
A1  - Yen-Hsi Richard Tsai
JO  - ArXiv e-prints
Y1  - 17 September, 2018
UR  - https://arxiv.org/abs/1809.06025
N2  - We study the problem of visibility-based exploration, reconstruction and surveillance in the context of supervised learning. Using a level set representation of data and information, we train a convolutional neural network to determine vantage points that maximize visibility. We show that this method drastically reduces the on-line computational cost and determines a small set of vantage points that solve the problem. This enables us to efficiently produce highly-resolved and topologically accurate maps of complex 3D environments. We present realistic simulations on 2D and 3D urban environments.
ER  -


TY  - Preprint
T1  - Comparison of Deep Learning and the Classical Machine Learning Algorithm for the Malware Detection
A1  - Mohit Sewak
A1  - Sanjay K. Sahay
A1  - Hemant Rathore
JO  - ArXiv e-prints
Y1  - 16 September, 2018
UR  - https://arxiv.org/abs/1809.05889
N2  - Recently, Deep Learning has been showing promising results in various Artificial Intelligence applications like image recognition, natural language processing, language modeling, neural machine translation, etc. Although, in general, it is computationally more expensive as compared to classical machine learning techniques, their results are found to be more effective in some cases. Therefore, in this paper, we investigated and compared one of the Deep Learning Architecture called Deep Neural Network (DNN) with the classical Random Forest (RF) machine learning algorithm for the malware classification. We studied the performance of the classical RF and DNN with 2, 4 &amp; 7 layers architectures with the four different feature sets, and found that irrespective of the features inputs, the classical RF accuracy outperforms the DNN.
ER  -


TY  - Preprint
T1  - An investigation of a deep learning based malware detection system
A1  - Mohit Sewak
A1  - Sanjay K. Sahay
A1  - Hemant Rathore
JO  - ArXiv e-prints
Y1  - 16 September, 2018
UR  - https://arxiv.org/abs/1809.05888
N2  - We investigate a Deep Learning based system for malware detection. In the investigation, we experiment with different combination of Deep Learning architectures including Auto-Encoders, and Deep Neural Networks with varying layers over Malicia malware dataset on which earlier studies have obtained an accuracy of (98%) with an acceptable False Positive Rates (1.07%). But these results were done using extensive man-made custom domain features and investing corresponding feature engineering and design efforts. In our proposed approach, besides improving the previous best results (99.21% accuracy and a False Positive Rate of 0.19%) indicates that Deep Learning based systems could deliver an effective defense against malware. Since it is good in automatically extracting higher conceptual features from the data, Deep Learning based systems could provide an effective, general and scalable mechanism for detection of existing and unknown malware.
ER  -


TY  - Preprint
T1  - An FPGA-Accelerated Design for Deep Learning Pedestrian Detection in Self-Driving Vehicles
A1  - Abdallah Moussawi
A1  - Kamal Haddad
A1  - Anthony Chahine
JO  - ArXiv e-prints
Y1  - 16 September, 2018
UR  - https://arxiv.org/abs/1809.05879
N2  - With the rise of self-driving vehicles comes the risk of accidents and the need for higher safety, and protection for pedestrian detection in the following scenarios: imminent crashes, thus the car should crash into an object and avoid the pedestrian, and in the case of road intersections, where it is important for the car to stop when pedestrians are crossing. Currently, a special topology of deep neural networks called Fused Deep Neural Network (F-DNN) is considered to be the state of the art in pedestrian detection, as it has the lowest miss rate, yet it is very slow. Therefore, acceleration is needed to speed up the performance. This project proposes two contributions to address this problem, by using a deep neural network used for object detection, called Single Shot Multi-Box Detector (SSD). The first contribution is training and tuning the hyperparameters of SSD to improve pedestrian detection. The second contribution is a new FPGA design for accelerating the model on the Altera Arria 10 platform. The final system will be used in self-driving vehicles in real-time. Preliminary results of the improved SSD shows 3% higher miss-rate than F-DNN on Caltech pedestrian detection benchmark, but 4x performance improvement. The acceleration design is expected to achieve an additional performance improvement significantly outweighing the minimal difference in accuracy.
ER  -


TY  - Preprint
T1  - Deep Learning with Experience Ranking Convolutional Neural Network for Robot Manipulator
A1  - Hai Nguyen
A1  - Hung Manh La
A1  - Matthew Deans
JO  - ArXiv e-prints
Y1  - 16 September, 2018
UR  - https://arxiv.org/abs/1809.05819
N2  - Supervised learning, more specifically Convolutional Neural Networks (CNN), has surpassed human ability in some visual recognition tasks such as detection of traffic signs, faces and handwritten numbers. On the other hand, even state-of-the-art reinforcement learning (RL) methods have difficulties in environments with sparse and binary rewards. They requires manually shaping reward functions, which might be challenging to come up with. These tasks, however, are trivial to human. One of the reasons that human are better learners in these tasks is that we are embedded with much prior knowledge of the world. These knowledge might be either embedded in our genes or learned from imitation - a type of supervised learning. For that reason, the best way to narrow the gap between machine and human learning ability should be to mimic how we learn so well in various tasks by a combination of RL and supervised learning. Our method, which integrates Deep Deterministic Policy Gradients and Hindsight Experience Replay (RL method specifically dealing with sparse rewards) with an experience ranking CNN, provides a significant speedup over the learning curve on simulated robotics tasks. Experience ranking allows high-reward transitions to be replayed more frequently, and therefore help learn more efficiently. Our proposed approach can also speed up learning in any other tasks that provide additional information for experience ranking.
ER  -


TY  - Preprint
T1  - Development of deep learning algorithms to categorize free-text notes pertaining to diabetes: convolution neural networks achieve higher accuracy than support vector machines
A1  - Boyi Yang
A1  - Adam Wright
JO  - ArXiv e-prints
Y1  - 16 September, 2018
UR  - https://arxiv.org/abs/1809.05814
N2  - Health professionals can use natural language processing (NLP) technologies when reviewing electronic health records (EHR). Machine learning free-text classifiers can help them identify problems and make critical decisions. We aim to develop deep learning neural network algorithms that identify EHR progress notes pertaining to diabetes and validate the algorithms at two institutions. The data used are 2,000 EHR progress notes retrieved from patients with diabetes and all notes were annotated manually as diabetic or non-diabetic. Several deep learning classifiers were developed, and their performances were evaluated with the area under the ROC curve (AUC). The convolutional neural network (CNN) model with a separable convolution layer accurately identified diabetes-related notes in the Brigham and Womens Hospital testing set with the highest AUC of 0.975. Deep learning classifiers can be used to identify EHR progress notes pertaining to diabetes. In particular, the CNN-based classifier can achieve a higher AUC than an SVM-based classifier.
ER  -


TY  - Preprint
T1  - Deterministic Implementations for Reproducibility in Deep Reinforcement Learning
A1  - Prabhat Nagarajan
A1  - Garrett Warnell
A1  - Peter Stone
JO  - ArXiv e-prints
Y1  - 19 September, 2018
UR  - https://arxiv.org/abs/1809.05676
N2  - While deep reinforcement learning (DRL) has led to numerous successes in recent years, reproducing these successes can be extremely challenging. One reproducibility challenge particularly relevant to DRL is nondeterminism in the training process, which can substantially affect the results. Motivated by this challenge, we study the positive impacts of deterministic implementations in eliminating nondeterminism in training. To do so, we consider the particular case of the deep Q-learning algorithm, for which we produce a deterministic implementation by identifying and controlling all sources of nondeterminism in the training process. One by one, we then allow individual sources of nondeterminism to affect our otherwise deterministic implementation, and measure the impact of each source on the variance in performance. We find that individual sources of nondeterminism can substantially impact the performance of agent, illustrating the benefits of deterministic implementations. In addition, we also discuss the important role of deterministic implementations in achieving exact replicability of results.
ER  -


TY  - Preprint
T1  - OffsetNet: Deep Learning for Localization in the Lung using Rendered Images
A1  - Jake Sganga
A1  - David Eng
A1  - Chauncey Graetzel
A1  - David Camarillo
JO  - ArXiv e-prints
Y1  - 15 September, 2018
UR  - https://arxiv.org/abs/1809.05645
N2  - Navigating surgical tools in the dynamic and tortuous anatomy of the lung&#39;s airways requires accurate, real-time localization of the tools with respect to the preoperative scan of the anatomy. Such localization can inform human operators or enable closed-loop control by autonomous agents, which would require accuracy not yet reported in the literature. In this paper, we introduce a deep learning architecture, called OffsetNet, to accurately localize a bronchoscope in the lung in real-time. After training on only 30 minutes of recorded camera images in conserved regions of a lung phantom, OffsetNet tracks the bronchoscope&#39;s motion on a held-out recording through these same regions at an update rate of 47 Hz and an average position error of 1.4 mm. Because this model performs poorly in less conserved regions, we augment the training dataset with simulated images from these regions. To bridge the gap between camera and simulated domains, we implement domain randomization and a generative adversarial network (GAN). After training on simulated images, OffsetNet tracks the bronchoscope&#39;s motion in less conserved regions at an average position error of 2.4 mm, which meets conservative thresholds required for successful tracking.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning Based Mode Selection and Resource Management for Green Fog Radio Access Networks
A1  - Yaohua Sun
A1  - Mugen Peng
A1  - Shiwen Mao
JO  - ArXiv e-prints
Y1  - 14 September, 2018
UR  - https://arxiv.org/abs/1809.05629
N2  - Fog radio access networks (F-RANs) are seen as potential architectures to support services of internet of things by leveraging edge caching and edge computing. However, current works studying resource management in F-RANs mainly consider a static system with only one communication mode. Given network dynamics, resource diversity, and the coupling of resource management with mode selection, resource management in F-RANs becomes very challenging. Motivated by the recent development of artificial intelligence, a deep reinforcement learning (DRL) based joint mode selection and resource management approach is proposed. Each user equipment (UE) can operate either in cloud RAN (C-RAN) mode or in device-to-device mode, and the resource managed includes both radio resource and computing resource. The core idea is that the network controller makes intelligent decisions on UE communication modes and processors&#39; on-off states with precoding for UEs in C-RAN mode optimized subsequently, aiming at minimizing long-term system power consumption under the dynamics of edge cache states. By simulations, the impacts of several parameters, such as learning rate and edge caching service capability, on system performance are demonstrated, and meanwhile the proposal is compared with other different schemes to show its effectiveness. Moreover, transfer learning is integrated with DRL to accelerate learning process.
ER  -


TY  - Preprint
T1  - Deep CNN Frame Interpolation with Lessons Learned from Natural Language Processing
A1  - Kian Ghodoussi
A1  - Nihar Sheth
A1  - Zane Durante
A1  - Markie Wagner
JO  - ArXiv e-prints
Y1  - 16 September, 2018
UR  - https://arxiv.org/abs/1809.05286
N2  - A major area of growth within deep learning has been the study and implementation of convolutional neural networks. The general explanation within the deep learning community of the robustness of convolutional neural networks (CNNs) within image recognition rests upon the idea that CNNs are able to extract localized features. However, recent developments in fields such as Natural Language Processing are demonstrating that this paradigm may be incorrect. In this paper, we analyze the current state of the field concerning CNN&#39;s and present a hypothesis that provides a novel explanation for the robustness of CNN models. From there, we demonstrate the effectiveness of our approach by presenting novel deep CNN frame interpolation architecture that is comparable to the state of the art interpolation models with a fraction of the complexity.
ER  -


TY  - Preprint
T1  - Macquarie University at BioASQ 6b: Deep learning and deep reinforcement learning for query-based multi-document summarisation
A1  - Diego MollÃ¡
JO  - ArXiv e-prints
Y1  - 14 September, 2018
UR  - https://arxiv.org/abs/1809.05283
N2  - This paper describes Macquarie University&#39;s contribution to the BioASQ Challenge (BioASQ 6b, Phase B). We focused on the extraction of the ideal answers, and the task was approached as an instance of query-based multi-document summarisation. In particular, this paper focuses on the experiments related to the deep learning and reinforcement learning approaches used in the submitted runs. The best run used a deep learning model under a regression-based framework. The deep learning architecture used features derived from the output of LSTM chains on word embeddings, plus features based on similarity with the query, and sentence position. The reinforcement learning approach was a proof-of-concept prototype that trained a global policy using REINFORCE. The global policy was implemented as a neural network that used $tf.idf$ features encoding the candidate sentence, question, and context.
ER  -


TY  - Preprint
T1  - Context2Name: A Deep Learning-Based Approach to Infer Natural Variable Names from Usage Contexts
A1  - Rohan Bavishi
A1  - Michael Pradel
A1  - Koushik Sen
JO  - ArXiv e-prints
Y1  - 31 August, 2018
UR  - https://arxiv.org/abs/1809.05193
N2  - Most of the JavaScript code deployed in the wild has been minified, a process in which identifier names are replaced with short, arbitrary and meaningless names. Minified code occupies less space, but also makes the code extremely difficult to manually inspect and understand. This paper presents Context2Name, a deep learningbased technique that partially reverses the effect of minification by predicting natural identifier names for minified names. The core idea is to predict from the usage context of a variable a name that captures the meaning of the variable. The approach combines a lightweight, token-based static analysis with an auto-encoder neural network that summarizes usage contexts and a recurrent neural network that predict natural names for a given usage context. We evaluate Context2Name with a large corpus of real-world JavaScript code and show that it successfully predicts 47.5% of all minified identifiers while taking only 2.9 milliseconds on average to predict a name. A comparison with the state-of-the-art tools JSNice and JSNaughty shows that our approach performs comparably in terms of accuracy while improving in terms of efficiency. Moreover, Context2Name complements the state-of-the-art by predicting 5.3% additional identifiers that are missed by both existing tools.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Event-Triggered Control
A1  - Dominik Baumann
A1  - Jia-Jie Zhu
A1  - Georg Martius
A1  - Sebastian Trimpe
JO  - ArXiv e-prints
Y1  - 13 September, 2018
UR  - https://arxiv.org/abs/1809.05152
N2  - Event-triggered control (ETC) methods can achieve high-performance control with a significantly lower number of samples compared to usual, time-triggered methods. These frameworks are often based on a mathematical model of the system and specific designs of controller and event trigger. In this paper, we show how deep reinforcement learning (DRL) algorithms can be leveraged to simultaneously learn control and communication behavior from scratch, and present a DRL approach that is particularly suitable for ETC. To our knowledge, this is the first work to apply DRL to ETC. We validate the approach on multiple control tasks and compare it to model-based event-triggering frameworks. In particular, we demonstrate that it can, other than many model-based ETC designs, be straightforwardly applied to nonlinear systems.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Wireless Sensor Scheduling in Cyber-Physical Systems
A1  - Alex S. Leong
A1  - Arunselvan Ramaswamy
A1  - Daniel E. Quevedo
A1  - Holger Karl
A1  - Ling Shi
JO  - ArXiv e-prints
Y1  - 13 September, 2018
UR  - https://arxiv.org/abs/1809.05149
N2  - This paper studies how to schedule wireless transmissions from sensors to estimate the states of multiple remote, dynamic processes. Sensors make observations of each of the processes. Information from the different sensors have to be transmitted to a central gateway over a wireless network for monitoring purposes, where typically fewer wireless channels are available than there are processes to be monitored. Such estimation problems routinely occur in large-scale Cyber-Physical Systems, especially when the dynamic systems (processes) involved are geographically separated. For effective estimation at the gateway, the sensors need to be scheduled appropriately, i.e., at each time instant to decide which sensors have network access and which ones do not. To solve this scheduling problem, we formulate an associated Markov decision process (MDP). Further, we solve this MDP using a Deep Q-Network, a deep reinforcement learning algorithm that is at once scalable and model-free. We compare our scheduling algorithm to popular scheduling algorithms such as round-robin and reduced-waiting-time, among others. Our algorithm is shown to significantly outperform these algorithms for randomly generated example scenarios.
ER  -


TY  - Preprint
T1  - A Deep Learning and Gamification Approach to Energy Conservation at Nanyang Technological University
A1  - Ioannis C. Konstantakopoulos
A1  - Andrew R. Barkan
A1  - Shiying He
A1  - Tanya Veeravalli
A1  - Huihan Liu
A1  - Costas Spanos
JO  - ArXiv e-prints
Y1  - 25 September, 2018
UR  - https://arxiv.org/abs/1809.05142
N2  - The implementation of smart building technology in the form of smart infrastructure applications has great potential to improve sustainability and energy efficiency by leveraging humans-in-the-loop strategy. However, human preference in regard to living conditions is usually unknown and heterogeneous in its manifestation as control inputs to a building. Furthermore, the occupants of a building typically lack the independent motivation necessary to contribute to and play a key role in the control of smart building infrastructure. Moreover, true human actions and their integration with sensing/actuation platforms remains unknown to the decision maker tasked with improving operational efficiency. By modeling user interaction as a sequential discrete game between non-cooperative players, we introduce a gamification approach for supporting user engagement and integration in a human-centric cyber-physical system. We propose the design and implementation of a large-scale network game with the goal of improving the energy efficiency of a building through the utilization of cutting-edge Internet of Things (IoT) sensors and cyber-physical systems sensing/actuation platforms. A benchmark utility learning framework that employs robust estimations for classical discrete choice models provided for the derived high dimensional imbalanced data. To improve forecasting performance, we extend the benchmark utility learning scheme by leveraging Deep Learning end-to-end training with Deep bi-directional Recurrent Neural Networks. We apply the proposed methods to high dimensional data from a social game experiment designed to encourage energy efficient behavior among smart building occupants in Nanyang Technological University (NTU) residential housing. Using occupant-retrieved actions for resources such as lighting and A/C, we simulate the game defined by the estimated utility functions.
ER  -


TY  - Preprint
T1  - Negative Update Intervals in Deep Multi-Agent Reinforcement Learning
A1  - Gregory Palmer
A1  - Rahul Savani
A1  - Karl Tuyls
JO  - ArXiv e-prints
Y1  - 17 September, 2018
UR  - https://arxiv.org/abs/1809.05096
N2  - In Multi-Agent Reinforcement Learning, independent cooperative learners must overcome a number of pathologies in order to learn optimal joint policies. These pathologies include action-shadowing, stochasticity, the moving target and alter-exploration problems (Matignon, Laurent, and Le Fort-Piat 2012; Wei and Luke 2016). Numerous methods have been proposed to address these pathologies, but evaluations are predominately conducted in repeated strategic-form games and stochastic games consisting of only a small number of state transitions. This raises the question of the scalability of the methods to complex, temporally extended, partially observable domains with stochastic transitions and rewards. In this paper we study such complex settings, which require reasoning over long time horizons and confront agents with the curse of dimensionality. To deal with the dimensionality, we adopt a Multi-Agent Deep Reinforcement Learning (MA-DRL) approach. We find that when the agents have to make critical decisions in seclusion, existing methods succumb to a combination of relative overgeneralisation (a type of action shadowing), the alter-exploration problem, and the stochasticity. To address these pathologies we introduce expanding negative update intervals that enable independent learners to establish the near-optimal average utility values for higher-level strategies while largely discarding transitions from episodes that result in mis-coordination. We evaluate Negative Update Intervals Double-DQN (NUI-DDQN) within a temporally extended Climb Game, a normal form game which has frequently been used to study relative over-generalisation and other pathologies. We show that NUI-DDQN can converge towards optimal joint-policies in deterministic and stochastic reward settings, overcoming relative-overgeneralisation and the alter-exploration problem while mitigating the moving target problem.
ER  -


TY  - Preprint
T1  - Full Workspace Generation of Serial-link Manipulators by Deep Learning based Jacobian Estimation
A1  - Peiyuan Liao
A1  - Jiajun Mao
JO  - ArXiv e-prints
Y1  - 13 September, 2018
UR  - https://arxiv.org/abs/1809.05020
N2  - Apart from solving complicated problems that require a certain level of intelligence, fine-tuned deep neural networks can also create fast algorithms for slow, numerical tasks. In this paper, we introduce an improved version of [1]&#39;s work, a fast, deep-learning framework capable of generating the full workspace of serial-link manipulators. The architecture consists of two neural networks: an estimation net that approximates the manipulator Jacobian, and a confidence net that measures the confidence of the approximation. We also introduce M3 (Manipulability Maps of Manipulators), a MATLAB robotics library based on [2](RTB), the datasets generated by which are used by this work. Results have shown that not only are the neural networks significantly faster than numerical inverse kinematics, it also offers superior accuracy when compared to other machine learning alternatives. Implementations of the algorithm (based on Keras[3]), including benchmark evaluation script, are available at https://github.com/liaopeiyuan/Jacobian-Estimation . The M3 Library APIs and datasets are also available at https://github.com/liaopeiyuan/M3 .
ER  -


TY  - Preprint
T1  - Sequential Coordination of Deep Models for Learning Visual Arithmetic
A1  - Eric Crawford
A1  - Guillaume Rabusseau
A1  - Joelle Pineau
JO  - ArXiv e-prints
Y1  - 13 September, 2018
UR  - https://arxiv.org/abs/1809.04988
N2  - Achieving machine intelligence requires a smooth integration of perception and reasoning, yet models developed to date tend to specialize in one or the other; sophisticated manipulation of symbols acquired from rich perceptual spaces has so far proved elusive. Consider a visual arithmetic task, where the goal is to carry out simple arithmetical algorithms on digits presented under natural conditions (e.g. hand-written, placed randomly). We propose a two-tiered architecture for tackling this problem. The lower tier consists of a heterogeneous collection of information processing modules, which can include pre-trained deep neural networks for locating and extracting characters from the image, as well as modules performing symbolic transformations on the representations extracted by perception. The higher tier consists of a controller, trained using reinforcement learning, which coordinates the modules in order to solve the high-level task. For instance, the controller may learn in what contexts to execute the perceptual networks and what symbolic transformations to apply to their outputs. The resulting model is able to solve a variety of tasks in the visual arithmetic domain, and has several advantages over standard, architecturally homogeneous feedforward networks including improved sample efficiency.
ER  -


TY  - Preprint
T1  - Image Captioning based on Deep Reinforcement Learning
A1  - Haichao Shi
A1  - Peng Li
A1  - Bo Wang
A1  - Zhenyu Wang
JO  - ArXiv e-prints
Y1  - 13 September, 2018
UR  - https://arxiv.org/abs/1809.04835
N2  - Recently it has shown that the policy-gradient methods for reinforcement learning have been utilized to train deep end-to-end systems on natural language processing tasks. What&#39;s more, with the complexity of understanding image content and diverse ways of describing image content in natural language, image captioning has been a challenging problem to deal with. To the best of our knowledge, most state-of-the-art methods follow a pattern of sequential model, such as recurrent neural networks (RNN). However, in this paper, we propose a novel architecture for image captioning with deep reinforcement learning to optimize image captioning tasks. We utilize two networks called &#34;policy network&#34; and &#34;value network&#34; to collaboratively generate the captions of images. The experiments are conducted on Microsoft COCO dataset, and the experimental results have verified the effectiveness of the proposed method.
ER  -


TY  - Preprint
T1  - Deep Learning-based Image Super-Resolution Considering Quantitative and Perceptual Quality
A1  - Jun-Ho Choi
A1  - Jun-Hyuk Kim
A1  - Manri Cheon
A1  - Jong-Seok Lee
JO  - ArXiv e-prints
Y1  - 13 September, 2018
UR  - https://arxiv.org/abs/1809.04789
N2  - Recently, it has been shown that in super-resolution, there exists a tradeoff relationship between the quantitative and perceptual quality of super-resolved images, which correspond to the similarity to the ground-truth images and the naturalness, respectively. In this paper, we propose a novel super-resolution method that can improve the perceptual quality of the upscaled images while preserving the conventional quantitative performance. The proposed method employs a deep network for multi-pass upscaling in company with a discriminator network and two quantitative score predictor networks. Experimental results demonstrate that the proposed method achieves a good balance of the quantitative and perceptual quality, showing more satisfactory results than existing methods.
ER  -


TY  - Preprint
T1  - FINN-R: An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks
A1  - Michaela Blott
A1  - Thomas Preusser
A1  - Nicholas Fraser
A1  - Giulio Gambardella
A1  - Kenneth O&#39;Brien
A1  - Yaman Umuroglu
JO  - ArXiv e-prints
Y1  - 12 September, 2018
UR  - https://arxiv.org/abs/1809.04570
N2  - Convolutional Neural Networks have rapidly become the most successful machine learning algorithm, enabling ubiquitous machine vision and intelligent decisions on even embedded computing-systems. While the underlying arithmetic is structurally simple, compute and memory requirements are challenging. One of the promising opportunities is leveraging reduced-precision representations for inputs, activations and model parameters. The resulting scalability in performance, power efficiency and storage footprint provides interesting design compromises in exchange for a small reduction in accuracy. FPGAs are ideal for exploiting low-precision inference engines leveraging custom precisions to achieve the required numerical accuracy for a given application. In this article, we describe the second generation of the FINN framework, an end-to-end tool which enables design space exploration and automates the creation of fully customized inference engines on FPGAs. Given a neural network description, the tool optimizes for given platforms, design targets and a specific precision. We introduce formalizations of resource cost functions and performance predictions, and elaborate on the optimization algorithms. Finally, we evaluate a selection of reduced precision neural networks ranging from CIFAR-10 classifiers to YOLO-based object detection on a range of platforms including PYNQ and AWS\,F1, demonstrating new unprecedented measured throughput at 50TOp/s on AWS-F1 and 5TOp/s on embedded devices.
ER  -


TY  - Preprint
T1  - Multi-task Deep Reinforcement Learning with PopArt
A1  - Matteo Hessel
A1  - Hubert Soyer
A1  - Lasse Espeholt
A1  - Wojciech Czarnecki
A1  - Simon Schmitt
A1  - Hado van Hasselt
JO  - ArXiv e-prints
Y1  - 12 September, 2018
UR  - https://arxiv.org/abs/1809.04474
N2  - The reinforcement learning community has made great strides in designing algorithms capable of exceeding human performance on specific tasks. These algorithms are mostly trained one task at the time, each new task requiring to train a brand new agent instance. This means the learning algorithm is general, but each solution is not; each agent can only solve the one task it was trained on. In this work, we study the problem of learning to master not one but multiple sequential-decision tasks at once. A general issue in multi-task learning is that a balance must be found between the needs of multiple tasks competing for the limited resources of a single learning system. Many learning algorithms can get distracted by certain tasks in the set of tasks to solve. Such tasks appear more salient to the learning process, for instance because of the density or magnitude of the in-task rewards. This causes the algorithm to focus on those salient tasks at the expense of generality. We propose to automatically adapt the contribution of each task to the agent&#39;s updates, so that all tasks have a similar impact on the learning dynamics. This resulted in state of the art performance on learning to play all games in a set of 57 diverse Atari games. Excitingly, our method learned a single trained policy - with a single set of weights - that exceeds median human performance. To our knowledge, this was the first time a single agent surpassed human-level performance on this multi-task domain. The same approach also demonstrated state of the art performance on a set of 30 tasks in the 3D reinforcement learning platform DeepMind Lab.
ER  -


TY  - Preprint
T1  - Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy
A1  - Stanislav Nikolov
A1  - Sam Blackwell
A1  - Ruheena Mendes
A1  - Jeffrey De Fauw
A1  - Clemens Meyer
A1  - CÃ­an Hughes
A1  - Harry Askham
A1  - Bernardino Romera-Paredes
A1  - Alan Karthikesalingam
A1  - Carlton Chu
A1  - Dawn Carnell
A1  - Cheng Boon
A1  - Derek D&#39;Souza
A1  - Syed Ali Moinuddin
A1  - Kevin Sullivan
A1  - DeepMind Radiographer Consortium
A1  - Hugh Montgomery
A1  - Geraint Rees
A1  - Ricky Sharma
A1  - Mustafa Suleyman
A1  - Trevor Back
A1  - Joseph R. Ledsam
A1  - Olaf Ronneberger
JO  - ArXiv e-prints
Y1  - 12 September, 2018
UR  - https://arxiv.org/abs/1809.04430
N2  - Over half a million individuals are diagnosed with head and neck cancer each year worldwide. Radiotherapy is an important curative treatment for this disease, but it requires manually intensive delineation of radiosensitive organs at risk (OARs). This planning process can delay treatment commencement. While auto-segmentation algorithms offer a potentially time-saving solution, the challenges in defining, quantifying and achieving expert performance remain. Adopting a deep learning approach, we demonstrate a 3D U-Net architecture that achieves performance similar to experts in delineating a wide range of head and neck OARs. The model was trained on a dataset of 663 deidentified computed tomography (CT) scans acquired in routine clinical practice and segmented according to consensus OAR definitions. We demonstrate its generalisability through application to an independent test set of 24 CT scans available from The Cancer Imaging Archive collected at multiple international sites previously unseen to the model, each segmented by two independent experts and consisting of 21 OARs commonly segmented in clinical practice. With appropriate validation studies and regulatory approvals, this system could improve the effectiveness of radiotherapy pathways.
ER  -


TY  - Preprint
T1  - Learning Deep Mixtures of Gaussian Process Experts Using Sum-Product Networks
A1  - Martin Trapp
A1  - Robert Peharz
A1  - Carl E. Rasmussen
A1  - Franz Pernkopf
JO  - ArXiv e-prints
Y1  - 12 September, 2018
UR  - https://arxiv.org/abs/1809.04400
N2  - While Gaussian processes (GPs) are the method of choice for regression tasks, they also come with practical difficulties, as inference cost scales cubic in time and quadratic in memory. In this paper, we introduce a natural and expressive way to tackle these problems, by incorporating GPs in sum-product networks (SPNs), a recently proposed tractable probabilistic model allowing exact and efficient inference. In particular, by using GPs as leaves of an SPN we obtain a novel flexible prior over functions, which implicitly represents an exponentially large mixture of local GPs. Exact and efficient posterior inference in this model can be done in a natural interplay of the inference mechanisms in GPs and SPNs. Thereby, each GP is -- similarly as in a mixture of experts approach -- responsible only for a subset of data points, which effectively reduces inference cost in a divide and conquer fashion. We show that integrating GPs into the SPN framework leads to a promising probabilistic regression model which is: (1) computational and memory efficient, (2) allows efficient and exact posterior inference, (3) is flexible enough to mix different kernel functions, and (4) naturally accounts for non-stationarities in time series. In a variate of experiments, we show that the SPN-GP model can learn input dependent parameters and hyper-parameters and is on par with or outperforms the traditional GPs as well as state of the art approximations on real-world data.
ER  -


TY  - Preprint
T1  - NNCP: A citation count prediction methodology based on deep neural network learning techniques
A1  - Ali Abrishami
A1  - Sadegh Aliakbary
JO  - ArXiv e-prints
Y1  - 12 September, 2018
UR  - https://arxiv.org/abs/1809.04365
N2  - With the growing number of published scientific papers world-wide, the need to evaluation and quality assessment methods for research papers is increasing. Scientific fields such as scientometrics, informetrics and bibliometrics establish quantified analysis methods and measurements for scientific papers. In this area, an important problem is to predict the future influence of a published paper. Particularly, early discrimination between influential papers and insignificant papers may find important applications. In this regard, one of the most important metrics is the number of citations to the paper, since this metric is widely utilized in the evaluation of scientific publications and moreover, it serves as the basis for many other metrics such as h-index. In this paper, we propose a novel method for predicting long-term citations of a paper based on the number of its citations in the first few years after publication. In order to train a citations prediction model, we employed artificial neural networks which is a powerful machine learning tool with recently growing applications in many domains including image and text processing. The empirical experiments show that our proposed method out-performs state-of-the-art methods with respect to the prediction accuracy in both yearly and total prediction of the number of citations.
ER  -


TY  - Preprint
T1  - Deep learning for time series classification: a review
A1  - Hassan Ismail Fawaz
A1  - Germain Forestier
A1  - Jonathan Weber
A1  - Lhassane Idoumghar
A1  - Pierre-Alain Muller
JO  - ArXiv e-prints
Y1  - 12 September, 2018
UR  - https://arxiv.org/abs/1809.04356
N2  - Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state of the art performance for document classification and speech recognition. In this article, we study the current state of the art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR archive) and 12 multivariate time series datasets. By training 8,730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.
ER  -


TY  - Preprint
T1  - Deep Learning in Information Security
A1  - Stefan Thaler
A1  - Vlado Menkovski
A1  - Milan Petkovic
JO  - ArXiv e-prints
Y1  - 12 September, 2018
UR  - https://arxiv.org/abs/1809.04332
N2  - Machine learning has a long tradition of helping to solve complex information security problems that are difficult to solve manually. Machine learning techniques learn models from data representations to solve a task. These data representations are hand-crafted by domain experts. Deep Learning is a sub-field of machine learning, which uses models that are composed of multiple layers. Consequently, representations that are used to solve a task are learned from the data instead of being manually designed.
ER  -


TY  - Preprint
T1  - Deep Learning Based Multi-modal Addressee Recognition in Visual Scenes with Utterances
A1  - Thao Minh Le
A1  - Nobuyuki Shimizu
A1  - Takashi Miyazaki
A1  - Koichi Shinoda
JO  - ArXiv e-prints
Y1  - 12 September, 2018
UR  - https://arxiv.org/abs/1809.04288
N2  - With the widespread use of intelligent systems, such as smart speakers, addressee recognition has become a concern in human-computer interaction, as more and more people expect such systems to understand complicated social scenes, including those outdoors, in cafeterias, and hospitals. Because previous studies typically focused only on pre-specified tasks with limited conversational situations such as controlling smart homes, we created a mock dataset called Addressee Recognition in Visual Scenes with Utterances (ARVSU) that contains a vast body of image variations in visual scenes with an annotated utterance and a corresponding addressee for each scenario. We also propose a multi-modal deep-learning-based model that takes different human cues, specifically eye gazes and transcripts of an utterance corpus, into account to predict the conversational addressee from a specific speaker&#39;s view in various real-life conversational scenarios. To the best of our knowledge, we are the first to introduce an end-to-end deep learning model that combines vision and transcripts of utterance for addressee recognition. As a result, our study suggests that future addressee recognition can reach the ability to understand human intention in many social situations previously unexplored, and our modality dataset is a first step in promoting research in this field.
ER  -


TY  - Preprint
T1  - Joint Segmentation and Uncertainty Visualization of Retinal Layers in Optical Coherence Tomography Images using Bayesian Deep Learning
A1  - Suman Sedai
A1  - Bhavna Antony
A1  - Dwarikanath Mahapatra
A1  - Rahil Garnavi
JO  - ArXiv e-prints
Y1  - 12 September, 2018
UR  - https://arxiv.org/abs/1809.04282
N2  - Optical coherence tomography (OCT) is commonly used to analyze retinal layers for assessment of ocular diseases. In this paper, we propose a method for retinal layer segmentation and quantification of uncertainty based on Bayesian deep learning. Our method not only performs end-to-end segmentation of retinal layers, but also gives the pixel wise uncertainty measure of the segmentation output. The generated uncertainty map can be used to identify erroneously segmented image regions which is useful in downstream analysis. We have validated our method on a dataset of 1487 images obtained from 15 subjects (OCT volumes) and compared it against the state-of-the-art segmentation algorithms that does not take uncertainty into account. The proposed uncertainty based segmentation method results in comparable or improved performance, and most importantly is more robust against noise.
ER  -


TY  - Preprint
T1  - Deep Co-investment Network Learning for Financial Assets
A1  - Yue Wang
A1  - Chenwei Zhang
A1  - Shen Wang
A1  - Philip S. Yu
A1  - Lu Bai
A1  - Lixin Cui
JO  - ArXiv e-prints
Y1  - 11 September, 2018
UR  - https://arxiv.org/abs/1809.04227
N2  - Most recent works model the market structure of the stock market as a correlation network of the stocks. They apply pre-defined patterns to extract correlation information from the time series of stocks. Without considering the influences of the evolving market structure to the market index trends, these methods hardly obtain the market structure models which are compatible with the market principles. Advancements in deep learning have shown their incredible modeling capacity on various finance-related tasks. However, the learned inner parameters, which capture the essence of the finance time series, are not further exploited about their representation in the financial fields. In this work, we model the financial market structure as a deep co-investment network and propose a Deep Co-investment Network Learning (DeepCNL) method. DeepCNL automatically learns deep co-investment patterns between any pairwise stocks, where the rise-fall trends of the market index are used for distance supervision. The learned inner parameters of the trained DeepCNL, which encodes the temporal dynamics of deep co-investment patterns, are used to build the co-investment network between the stocks as the investment structure of the corresponding market. We verify the effectiveness of DeepCNL on the real-world stock data and compare it with the existing methods on several financial tasks. The experimental results show that DeepCNL not only has the ability to better reflect the stock market structure that is consistent with widely-acknowledged financial principles but also is more capable to approximate the investment activities which lead to the stock performance reported in the real news or research reports than other alternatives.
ER  -


TY  - Preprint
T1  - Deep Micro-Dictionary Learning and Coding Network
A1  - Hao Tang
A1  - Heng Wei
A1  - Wei Xiao
A1  - Wei Wang
A1  - Dan Xu
A1  - Yan Yan
A1  - Nicu Sebe
JO  - ArXiv e-prints
Y1  - 11 September, 2018
UR  - https://arxiv.org/abs/1809.04185
N2  - In this paper, we propose a novel Deep Micro-Dictionary Learning and Coding Network (DDLCN). DDLCN has most of the standard deep learning layers (pooling, fully, connected, input/output, etc.) but the main difference is that the fundamental convolutional layers are replaced by novel compound dictionary learning and coding layers. The dictionary learning layer learns an over-complete dictionary for the input training data. At the deep coding layer, a locality constraint is added to guarantee that the activated dictionary bases are close to each other. Next, the activated dictionary atoms are assembled together and passed to the next compound dictionary learning and coding layers. In this way, the activated atoms in the first layer can be represented by the deeper atoms in the second dictionary. Intuitively, the second dictionary is designed to learn the fine-grained components which are shared among the input dictionary atoms. In this way, a more informative and discriminative low-level representation of the dictionary atoms can be obtained. We empirically compare the proposed DDLCN with several dictionary learning methods and deep learning architectures. The experimental results on four popular benchmark datasets demonstrate that the proposed DDLCN achieves competitive results compared with state-of-the-art approaches.
ER  -


TY  - Preprint
T1  - What can linguistics and deep learning contribute to each other?
A1  - Tal Linzen
JO  - ArXiv e-prints
Y1  - 14 September, 2018
UR  - https://arxiv.org/abs/1809.04179
N2  - Joe Pater&#39;s target article calls for greater interaction between neural network research and linguistics. I expand on this call and show how such interaction can benefit both fields. Linguists can contribute to research on neural networks for language technologies by clearly delineating the linguistic capabilities that can be expected of such systems, and by constructing controlled experimental paradigms that can determine whether those desiderata have been met. In the other direction, neural networks can benefit the scientific study of language by providing infrastructure for modeling human sentence processing and for evaluating the necessity of particular innate constraints on language acquisition.
ER  -


TY  - Preprint
T1  - Efficient Road Lane Marking Detection with Deep Learning
A1  - Ping-Rong Chen
A1  - Shao-Yuan Lo
A1  - Hsueh-Ming Hang
A1  - Sheng-Wei Chan
A1  - Jing-Jhih Lin
JO  - ArXiv e-prints
Y1  - 11 September, 2018
UR  - https://arxiv.org/abs/1809.03994
N2  - Lane mark detection is an important element in the road scene analysis for Advanced Driver Assistant System (ADAS). Limited by the onboard computing power, it is still a challenge to reduce system complexity and maintain high accuracy at the same time. In this paper, we propose a Lane Marking Detector (LMD) using a deep convolutional neural network to extract robust lane marking features. To improve its performance with a target of lower complexity, the dilated convolution is adopted. A shallower and thinner structure is designed to decrease the computational cost. Moreover, we also design post-processing algorithms to construct 3rd-order polynomial models to fit into the curved lanes. Our system shows promising results on the captured road scenes.
ER  -


TY  - Preprint
T1  - Danger-aware Weighted Advantage Composition of Deep Reinforcement Learning for Robot Navigation
A1  - Wei Zhang
A1  - Yunfeng Zhang
A1  - Ning Liu
JO  - ArXiv e-prints
Y1  - 11 September, 2018
UR  - https://arxiv.org/abs/1809.03847
N2  - Self-navigation, referring to automatically reaching the goal while avoiding collision with obstacles, is a fundamental skill of mobile robots. Currently, Deep Reinforcement Learning (DRL) can enable the robot to navigate in a more complex environment with less computation power compared to conventional methods. However, it is time-consuming and hard to train the robot to learn goal-reaching and obstacle-avoidance skills simultaneously using DRL-based algorithms. In this paper, two Dueling Deep Q Networks (DQN) named Goal Network and Avoidance Network are used to learn the goal-reaching and obstacle-avoidance skills individually. A novel method named danger-aware advantage composition is proposed to fuse the two networks together without any redesigning and retraining. The composed Navigation Network can enable the robot to reach the goal right behind the wall and to navigate in unknown complexed environment safely and quickly.
ER  -


TY  - Preprint
T1  - Does it care what you asked? Understanding Importance of Verbs in Deep Learning QA System
A1  - Barbara Rychalska
A1  - Dominika Basaj
A1  - Przemyslaw Biecek
A1  - Anna Wroblewska
JO  - ArXiv e-prints
Y1  - 11 September, 2018
UR  - https://arxiv.org/abs/1809.03740
N2  - In this paper we present the results of an investigation of the importance of verbs in a deep learning QA system trained on SQuAD dataset. We show that main verbs in questions carry little influence on the decisions made by the system - in over 90% of researched cases swapping verbs for their antonyms did not change system decision. We track this phenomenon down to the insides of the net, analyzing the mechanism of self-attention and values contained in hidden layers of RNN. Finally, we recognize the characteristics of the SQuAD dataset as the source of the problem. Our work refers to the recently popular topic of adversarial examples in NLP, combined with investigating deep net structure.
ER  -


TY  - Preprint
T1  - Comparing Computing Platforms for Deep Learning on a Humanoid Robot
A1  - Alexander Biddulph
A1  - Trent Houlistion
A1  - Alexandre Mendes
A1  - Stephan K. Chalup
JO  - ArXiv e-prints
Y1  - 10 September, 2018
UR  - https://arxiv.org/abs/1809.03668
N2  - The goal of this study is to test two different computing platforms with respect to their suitability for running deep networks as part of a humanoid robot software system. One of the platforms is the CPU-centered Intel NUC7i7BNH and the other is a NVIDIA Jetson TX2 system that puts more emphasis on GPU processing. The experiments addressed a number of benchmarking tasks including pedestrian detection using deep neural networks. Some of the results were unexpected but demonstrate that platforms exhibit both advantages and disadvantages when taking computational performance and electrical power requirements of such a system into account.
ER  -


TY  - Preprint
T1  - URBAN-i: From urban scenes to mapping slums, transport modes, and pedestrians in cities using deep learning and computer vision
A1  - Mohamed R. Ibrahim
A1  - James Haworth
A1  - Tao Cheng
JO  - ArXiv e-prints
Y1  - 10 September, 2018
UR  - https://arxiv.org/abs/1809.03609
N2  - Within the burgeoning expansion of deep learning and computer vision across the different fields of science, when it comes to urban development, deep learning and computer vision applications are still limited towards the notions of smart cities and autonomous vehicles. Indeed, a wide gap of knowledge appears when it comes to cities and urban regions in less developed countries where the chaos of informality is the dominant scheme. How can deep learning and Artificial Intelligence (AI) untangle the complexities of informality to advance urban modelling and our understanding of cities? Various questions and debates can be raised concerning the future of cities of the North and the South in the paradigm of AI and computer vision. In this paper, we introduce a new method for multipurpose realistic-dynamic urban modelling relying on deep learning and computer vision, using deep Convolutional Neural Networks (CNN), to sense and detect informality and slums in urban scenes from aerial and street view images in addition to detection of pedestrian and transport modes. The model has been trained on images of urban scenes in cities across the globe. The model shows a good validation of understanding a wide spectrum of nuances among the planned and the unplanned regions, including informal and slum areas. We attempt to advance urban modelling for better understanding the dynamics of city developments. We also aim to exemplify the significant impacts of AI in cities beyond how smart cities are discussed and perceived in the mainstream. The algorithms of the URBAN-i model are fully-coded in Python programming with the pre-trained deep learning models to be used as a tool for mapping and city modelling in the various corner of the globe, including informal settlements and slum regions.
ER  -


TY  - Preprint
T1  - Deep Learning Towards Mobile Applications
A1  - Ji Wang
A1  - Bokai Cao
A1  - Philip S. Yu
A1  - Lichao Sun
A1  - Weidong Bao
A1  - Xiaomin Zhu
JO  - ArXiv e-prints
Y1  - 10 September, 2018
UR  - https://arxiv.org/abs/1809.03559
N2  - Recent years have witnessed an explosive growth of mobile devices. Mobile devices are permeating every aspect of our daily lives. With the increasing usage of mobile devices and intelligent applications, there is a soaring demand for mobile applications with machine learning services. Inspired by the tremendous success achieved by deep learning in many machine learning tasks, it becomes a natural trend to push deep learning towards mobile applications. However, there exist many challenges to realize deep learning in mobile applications, including the contradiction between the miniature nature of mobile devices and the resource requirement of deep neural networks, the privacy and security concerns about individuals&#39; data, and so on. To resolve these challenges, during the past few years, great leaps have been made in this area. In this paper, we provide an overview of the current challenges and representative achievements about pushing deep learning on mobile devices from three aspects: training with mobile data, efficient inference on mobile devices, and applications of mobile deep learning. The former two aspects cover the primary tasks of deep learning. Then, we go through our two recent applications that apply the data collected by mobile devices to inferring mood disturbance and user identification. Finally, we conclude this paper with the discussion of the future of this area.
ER  -


TY  - Preprint
T1  - Energy Disaggregation via Deep Temporal Dictionary Learning
A1  - Mahdi Khodayar
A1  - Jianhui Wang
A1  - Zhaoyu Wang
JO  - ArXiv e-prints
Y1  - 10 September, 2018
UR  - https://arxiv.org/abs/1809.03534
N2  - This paper addresses the energy disaggregation problem, i.e. decomposing the electricity signal of a whole home to its operating devices. First, we cast the problem as a dictionary learning (DL) problem where the key electricity patterns representing consumption behaviors are extracted for each device and stored in a dictionary matrix. The electricity signal of each device is then modeled by a linear combination of such patterns with sparse coefficients that determine the contribution of each device in the total electricity. Although popular, the classic DL approach is prone to high error in real-world applications including energy disaggregation, as it merely finds linear dictionaries. Moreover, this method lacks a recurrent structure; thus, it is unable to leverage the temporal structure of energy signals. Motivated by such shortcomings, we propose a novel optimization program where the dictionary and its sparse coefficients are optimized simultaneously with a deep neural model extracting powerful nonlinear features from the energy signals. A long short-term memory auto-encoder (LSTM-AE) is proposed with tunable time dependent states to capture the temporal behavior of energy signals for each device. We learn the dictionary in the space of temporal features captured by the LSTM-AE rather than the original space of the energy signals; hence, in contrast to the traditional DL, here, a nonlinear dictionary is learned using powerful temporal features extracted from our deep model. Real experiments on the publicly available Reference Energy Disaggregation Dataset (REDD) show significant improvement compared to the state-of-the-art methodologies in terms of the disaggregation accuracy and F-score metrics.
ER  -


TY  - Preprint
T1  - Not Just Privacy: Improving Performance of Private Deep Learning in Mobile Cloud
A1  - Ji Wang
A1  - Jianguo Zhang
A1  - Weidong Bao
A1  - Xiaomin Zhu
A1  - Bokai Cao
A1  - Philip S. Yu
JO  - ArXiv e-prints
Y1  - 18 September, 2018
UR  - https://arxiv.org/abs/1809.03428
N2  - The increasing demand for on-device deep learning services calls for a highly efficient manner to deploy deep neural networks (DNNs) on mobile devices with limited capacity. The cloud-based solution is a promising approach to enabling deep learning applications on mobile devices where the large portions of a DNN are offloaded to the cloud. However, revealing data to the cloud leads to potential privacy risk. To benefit from the cloud data center without the privacy risk, we design, evaluate, and implement a cloud-based framework ARDEN which partitions the DNN across mobile devices and cloud data centers. A simple data transformation is performed on the mobile device, while the resource-hungry training and the complex inference rely on the cloud data center. To protect the sensitive information, a lightweight privacy-preserving mechanism consisting of arbitrary data nullification and random noise addition is introduced, which provides strong privacy guarantee. A rigorous privacy budget analysis is given. Nonetheless, the private perturbation to the original data inevitably has a negative impact on the performance of further inference on the cloud side. To mitigate this influence, we propose a noisy training method to enhance the cloud-side network robustness to perturbed data. Through the sophisticated design, ARDEN can not only preserve privacy but also improve the inference performance. To validate the proposed ARDEN, a series of experiments based on three image datasets and a real mobile application are conducted. The experimental results demonstrate the effectiveness of ARDEN. Finally, we implement ARDEN on a demo system to verify its practicality.
ER  -


TY  - Preprint
T1  - Improving Optimization Bounds using Machine Learning: Decision Diagrams meet Deep Reinforcement Learning
A1  - Quentin Cappart
A1  - Emmanuel Goutierre
A1  - David Bergman
A1  - Louis-Martin Rousseau
JO  - ArXiv e-prints
Y1  - 10 September, 2018
UR  - https://arxiv.org/abs/1809.03359
N2  - Finding tight bounds on the optimal solution is a critical element of practical solution methods for discrete optimization problems. In the last decade, decision diagrams (DDs) have brought a new perspective on obtaining upper and lower bounds that can be significantly better than classical bounding mechanisms, such as linear relaxations. It is well known that the quality of the bound achieved through this flexible bounding method is highly reliant on the ordering of variables chosen for building the diagram, and finding an ordering that optimizes standard metrics, or even improving one, is an NP-hard problem. In this paper, we propose an innovative and generic approach based on deep reinforcement learning for obtaining an ordering for tightening the bounds obtained with relaxed and restricted DDs. We apply the approach to both the Maximum Independent Set Problem and the Maximum Cut Problem. Experimental results on synthetic instances show that the deep reinforcement learning approach, by achieving tighter objective function bounds, generally outperforms ordering methods commonly used in the literature when the distribution of instances is known. To the best knowledge of the authors, this is the first paper to apply machine learning to directly improve relaxation bounds obtained by general-purpose bounding mechanisms for combinatorial optimization problems.
ER  -


TY  - Preprint
T1  - Guiding the Creation of Deep Learning-based Object Detectors
A1  - Ãngela Casado
A1  - JÃ³nathan Heras
JO  - ArXiv e-prints
Y1  - 6 September, 2018
UR  - https://arxiv.org/abs/1809.03322
N2  - Object detection is a computer vision field that has applications in several contexts ranging from biomedicine and agriculture to security. In the last years, several deep learning techniques have greatly improved object detection models. Among those techniques, we can highlight the YOLO approach, that allows the construction of accurate models that can be employed in real-time applications. However, as most deep learning techniques, YOLO has a steep learning curve and creating models using this approach might be challenging for non-expert users. In this work, we tackle this problem by constructing a suite of Jupyter notebooks that democratizes the construction of object detection models using YOLO. The suitability of our approach has been proven with a dataset of stomata images where we have achieved a mAP of 90.91%.
ER  -


TY  - Preprint
T1  - A Robotic Auto-Focus System based on Deep Reinforcement Learning
A1  - Xiaofan Yu
A1  - Runze Yu
A1  - Jingsong Yang
A1  - Xiaohui Duan
JO  - ArXiv e-prints
Y1  - 4 September, 2018
UR  - https://arxiv.org/abs/1809.03314
N2  - Considering its advantages in dealing with high-dimensional visual input and learning control policies in discrete domain, Deep Q Network (DQN) could be an alternative method of traditional auto-focus means in the future. In this paper, based on Deep Reinforcement Learning, we propose an end-to-end approach that can learn auto-focus policies from visual input and finish at a clear spot automatically. We demonstrate that our method - discretizing the action space with coarse to fine steps and applying DQN is not only a solution to auto-focus but also a general approach towards vision-based control problems. Separate phases of training in virtual and real environments are applied to obtain an effective model. Virtual experiments, which are carried out after the virtual training phase, indicates that our method could achieve 100% accuracy on a certain view with different focus range. Further training on real robots could eliminate the deviation between the simulator and real scenario, leading to reliable performances in real applications.
ER  -


TY  - Preprint
T1  - Privacy-Preserving Deep Learning for any Activation Function
A1  - Le Trieu Phong
A1  - Tran Thi Phuong
JO  - ArXiv e-prints
Y1  - 10 September, 2018
UR  - https://arxiv.org/abs/1809.03272
N2  - This paper considers the scenario that multiple data owners wish to apply a machine learning method over the combined dataset of all owners to obtain the best possible learning output but do not want to share the local datasets owing to privacy concerns. We design systems for the scenario that the stochastic gradient descent (SGD) algorithm is used as the machine learning method because SGD (or its variants) is at the heart of recent deep learning techniques over neural networks. Our systems differ from existing systems in the following features: {\bf (1)} any activation function can be used, meaning that no privacy-preserving-friendly approximation is required; {\bf (2)} gradients computed by SGD are not shared but the weight parameters are shared instead; and {\bf (3)} robustness against colluding parties even in the extreme case that only one honest party exists. We prove that our systems, while privacy-preserving, achieve the same learning accuracy as SGD and hence retain the merit of deep learning with respect to accuracy. Finally, we conduct several experiments using benchmark datasets, and show that our systems outperform previous system in terms of learning accuracies.
ER  -


TY  - Preprint
T1  - Adaptive Behavior Generation for Autonomous Driving using Deep Reinforcement Learning with Compact Semantic States
A1  - Peter Wolf
A1  - Karl Kurzer
A1  - Tobias Wingert
A1  - Florian Kuhnt
A1  - J. Marius ZÃ¶llner
JO  - ArXiv e-prints
Y1  - 10 September, 2018
UR  - https://arxiv.org/abs/1809.03214
N2  - Making the right decision in traffic is a challenging task that is highly dependent on individual preferences as well as the surrounding environment. Therefore it is hard to model solely based on expert knowledge. In this work we use Deep Reinforcement Learning to learn maneuver decisions based on a compact semantic state representation. This ensures a consistent model of the environment across scenarios as well as a behavior adaptation function, enabling on-line changes of desired behaviors without re-training. The input for the neural network is a simulated object list similar to that of Radar or Lidar sensors, superimposed by a relational semantic scene description. The state as well as the reward are extended by a behavior adaptation function and a parameterization respectively. With little expert knowledge and a set of mid-level actions, it can be seen that the agent is capable to adhere to traffic rules and learns to drive safely in a variety of situations.
ER  -


TY  - Preprint
T1  - Shallow vs deep learning architectures for white matter lesion segmentation in the early stages of multiple sclerosis
A1  - Francesco La Rosa
A1  - MÃ¡rio JoÃ£o Fartaria
A1  - Tobias Kober
A1  - Jonas Richiardi
A1  - Cristina Granziera
A1  - Jean-Philippe Thiran
A1  - Meritxell Bach Cuadra
JO  - ArXiv e-prints
Y1  - 10 September, 2018
UR  - https://arxiv.org/abs/1809.03185
N2  - In this work, we present a comparison of a shallow and a deep learning architecture for the automated segmentation of white matter lesions in MR images of multiple sclerosis patients. In particular, we train and test both methods on early stage disease patients, to verify their performance in challenging conditions, more similar to a clinical setting than what is typically provided in multiple sclerosis segmentation challenges. Furthermore, we evaluate a prototype naive combination of the two methods, which refines the final segmentation. All methods were trained on 32 patients, and the evaluation was performed on a pure test set of 73 cases. Results show low lesion-wise false positives (30%) for the deep learning architecture, whereas the shallow architecture yields the best Dice coefficient (63%) and volume difference (19%). Combining both shallow and deep architectures further improves the lesion-wise metrics (69% and 26% lesion-wise true and false positive rate, respectively).
ER  -


TY  - Preprint
T1  - A case for deep learning in semantics
A1  - Christopher Potts
JO  - ArXiv e-prints
Y1  - 9 September, 2018
UR  - https://arxiv.org/abs/1809.03068
N2  - Pater&#39;s target article builds a persuasive case for establishing stronger ties between theoretical linguistics and connectionism (deep learning). This commentary extends his arguments to semantics, focusing in particular on issues of learning, compositionality, and lexical meaning.
ER  -


TY  - Preprint
T1  - PhaseLink: A Deep Learning Approach to Seismic Phase Association
A1  - Zachary E. Ross
A1  - Yisong Yue
A1  - Men-Andrin Meier
A1  - Egill Hauksson
A1  - Thomas H. Heaton
JO  - ArXiv e-prints
Y1  - 8 September, 2018
UR  - https://arxiv.org/abs/1809.02880
N2  - Seismic phase association is a fundamental task in seismology that pertains to linking together phase detections on different sensors that originate from a common earthquake. It is widely employed to detect earthquakes on permanent and temporary seismic networks, and underlies most seismicity catalogs produced around the world. This task can be challenging because the number of sources is unknown, events frequently overlap in time, or can occur simultaneously in different parts of a network. We present PhaseLink, a framework based on recent advances in deep learning for grid-free earthquake phase association. Our approach learns to link phases together that share a common origin, and is trained entirely on tens of millions of synthetic sequences of P- and S-wave arrival times generated using a simple 1D velocity model. Our approach is simple to implement for any tectonic regime, suitable for real-time processing, and can naturally incorporate errors in arrival time picks. Rather than tuning a set of ad hoc hyperparameters to improve performance, PhaseLink can be improved by simply adding examples of problematic cases to the training dataset. We demonstrate the state-of-the-art performance of PhaseLink on a challenging recent sequence from southern California, and synthesized sequences from Japan designed to test the point at which the method fails. These tests show that PhaseLink can precisely associate P- and S-picks to events that are separated by ~12 seconds in origin time. This approach is expected to improve the resolution of seismicity catalogs, add stability to real-time seismic monitoring, and streamline automated processing of large seismic datasets.
ER  -


TY  - Preprint
T1  - Unsupervised Person Re-identification by Deep Learning Tracklet Association
A1  - Minxian Li
A1  - Xiatian Zhu
A1  - Shaogang Gong
JO  - ArXiv e-prints
Y1  - 8 September, 2018
UR  - https://arxiv.org/abs/1809.02874
N2  - Mostexistingpersonre-identification(re-id)methods relyon supervised model learning on per-camera-pair manually labelled pairwise training data. This leads to poor scalability in practical re-id deployment due to the lack of exhaustive identity labelling of image positive and negative pairs for every camera pair. In this work, we address this problem by proposing an unsupervised re-id deep learning approach capable of incrementally discovering and exploiting the underlying re-id discriminative information from automatically generated person tracklet data from videos in an end-to-end model optimisation. We formulate a Tracklet Association Unsupervised Deep Learning (TAUDL) framework characterised by jointly learning per-camera (within-camera) tracklet association (labelling) and cross-camera tracklet correlation by maximising the discovery of most likely tracklet relationships across camera views. Extensive experiments demonstrate the superiority of the proposed TAUDL model over the state-of-the-art unsupervised and domain adaptation re- id methods using six person re-id benchmarking datasets.
ER  -


TY  - Preprint
T1  - Adversarial Learning for Image Forensics Deep Matching with Atrous Convolution
A1  - Yaqi Liu
A1  - Xianfeng Zhao
A1  - Xiaobin Zhu
A1  - Yun Cao
JO  - ArXiv e-prints
Y1  - 8 September, 2018
UR  - https://arxiv.org/abs/1809.02791
N2  - Constrained image splicing detection and localization (CISDL) is a newly proposed challenging task for image forensics, which investigates two input suspected images and identifies whether one image has suspected regions pasted from the other. In this paper, we propose a novel adversarial learning framework to train the deep matching network for CISDL. Our framework mainly consists of three building blocks: 1) the deep matching network based on atrous convolution (DMAC) aims to generate two high-quality candidate masks which indicate the suspected regions of the two input images, 2) the detection network is designed to rectify inconsistencies between the two corresponding candidate masks, 3) the discriminative network drives the DMAC network to produce masks that are hard to distinguish from ground-truth ones. In DMAC, atrous convolution is adopted to extract features with rich spatial information, the correlation layer based on the skip architecture is proposed to capture hierarchical features, and atrous spatial pyramid pooling is constructed to localize tampered regions at multiple scales. The detection network and the discriminative network act as the losses with auxiliary parameters to supervise the training of DMAC in an adversarial way. Extensive experiments, conducted on 21 generated testing sets and two public datasets, demonstrate the effectiveness of the proposed framework and the superior performance of DMAC.
ER  -


TY  - Preprint
T1  - Instance-based Deep Transfer Learning
A1  - Tianyang Wang
A1  - Jun Huan
A1  - Michelle Zhu
JO  - ArXiv e-prints
Y1  - 8 September, 2018
UR  - https://arxiv.org/abs/1809.02776
N2  - Deep transfer learning has acquired significant research interest. It makes use of pre-trained models that are learned from a source domain, and utilizes these models for the tasks in a target domain. Model-based deep transfer learning is arguably the most frequently used method. However, very little work has been devoted to enhancing deep transfer learning by focusing on the influence of data. In this work, we propose an instance-based approach to improve deep transfer learning in target domain. Specifically, we choose a pre-trained model which is learned from a source domain, and utilize this model to estimate the influence of each training sample in a target domain. Then we optimize training data of the target domain by removing the training samples that will lower the performance of the pre-trained model. We then fine-tune the pre-trained model with the optimized training data in the target domain, or build a new model which can be initialized partially based on the pre-trained model, and fine-tune it with the optimized training data in the target domain. Using this approach, transfer learning can help deep learning models to learn more useful features. Extensive experiments demonstrate the effectiveness of our approach on further boosting deep learning models for typical high-level computer vision tasks, such as image classification.
ER  -


TY  - Preprint
T1  - Deep Feature Learning of Multi-Network Topology for Node Classification
A1  - Hansheng Xue
A1  - Jiajie Peng
A1  - Xuequn Shang
JO  - ArXiv e-prints
Y1  - 7 September, 2018
UR  - https://arxiv.org/abs/1809.02394
N2  - Networks are ubiquitous structure that describes complex relationships between different entities in the real world. As a critical component of prediction task over nodes in networks, learning the feature representation of nodes has become one of the most active areas recently. Network Embedding, aiming to learn non-linear and low-dimensional feature representation based on network topology, has been proved to be helpful on tasks of network analysis, especially node classification. For many real-world systems, multiple types of relations are naturally represented by multiple networks. However, existing network embedding methods mainly focus on single network embedding and neglect the information shared among different networks. In this paper, we propose a novel multiple network embedding method based on semisupervised autoencoder, named DeepMNE, which captures complex topological structures of multi-networks and takes the correlation among multi-networks into account. We evaluate DeepMNE on the task of node classification with two real-world datasets. The experimental results demonstrate the superior performance of our method over four state-of-the-art algorithms.
ER  -


TY  - Preprint
T1  - A simple probabilistic deep generative model for learning generalizable disentangled representations from grouped data
A1  - Haruo Hosoya
JO  - ArXiv e-prints
Y1  - 7 September, 2018
UR  - https://arxiv.org/abs/1809.02383
N2  - The disentangling problem is to discover multiple complex factors of variations hidden in data. One recent approach is to take a dataset with grouping structure and separately estimate a factor common within a group (content) and a factor specific to each group member (transformation). Notably, this approach can learn to represent a continuous space of contents, which allows for generalization to data with unseen contents. In this study, we aim at cultivating this approach within probabilistic deep generative models. Motivated by technical complication in existing group-based methods, we propose a simpler probabilistic method, called group-contrastive variational autoencoders. Despite its simplicity, our approach achieves reasonable disentanglement with generalizability for three grouped datasets of 3D object images. In comparison with a previous model, although conventional qualitative evaluation shows little difference, our qualitative evaluation using few-shot classification exhibits superior performances for some datasets. We analyze the content representations from different methods and discuss their transformation-dependency and potential performance impacts.
ER  -


TY  - Preprint
T1  - Deep Learning for Generic Object Detection: A Survey
A1  - Li Liu
A1  - Wanli Ouyang
A1  - Xiaogang Wang
A1  - Paul Fieguth
A1  - Jie Chen
A1  - Xinwang Liu
A1  - Matti PietikÃ¤inen
JO  - ArXiv e-prints
Y1  - 6 September, 2018
UR  - https://arxiv.org/abs/1809.02165
N2  - Generic object detection, aiming at locating object instances from a large number of predefined categories in natural images, is one of the most fundamental and challenging problems in computer vision. Deep learning techniques have emerged in recent years as powerful methods for learning feature representations directly from data, and have led to remarkable breakthroughs in the field of generic object detection. Given this time of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this field brought by deep learning techniques. More than 250 key contributions are included in this survey, covering many aspects of generic object detection research: leading detection frameworks and fundamental subproblems including object feature representation, object proposal generation, context information modeling and training strategies; evaluation issues, specifically benchmark datasets, evaluation metrics, and state of the art performance. We finish by identifying promising directions for future research.
ER  -


TY  - Preprint
T1  - DRAG: Deep Reinforcement Learning Based Base Station Activation in Heterogeneous Networks
A1  - Junhong Ye
A1  - Ying-Jun Angela Zhang
JO  - ArXiv e-prints
Y1  - 6 September, 2018
UR  - https://arxiv.org/abs/1809.02159
N2  - Heterogeneous Network (HetNet), where Small cell Base Stations (SBSs) are densely deployed to offload traffic from macro Base Stations (BSs), is identified as a key solution to meet the unprecedented mobile traffic demand. The high density of SBSs are designed for peak traffic hours and consume an unnecessarily large amount of energy during off-peak time. In this paper, we propose a deep reinforcement-learning based SBS activation strategy that activates the optimal subset of SBSs to significantly lower the energy consumption without compromising the quality of service. In particular, we formulate the SBS on/off switching problem into a Markov Decision Process that can be solved by Actor Critic (AC) reinforcement learning methods. To avoid prohibitively high computational and storage costs of conventional tabular-based approaches, we propose to use deep neural networks to approximate the policy and value functions in the AC approach. Moreover, to expedite the training process, we adopt a Deep Deterministic Policy Gradient (DDPG) approach together with a novel action refinement scheme. Through extensive numerical simulations, we show that the proposed scheme greatly outperforms the existing methods in terms of both energy efficiency and computational efficiency. We also show that the proposed scheme can scale to large system with polynomial complexities in both storage and computation.
ER  -


TY  - Preprint
T1  - Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning
A1  - Tom Zahavy
A1  - Matan Haroush
A1  - Nadav Merlis
A1  - Daniel J. Mankowitz
A1  - Shie Mannor
JO  - ArXiv e-prints
Y1  - 6 September, 2018
UR  - https://arxiv.org/abs/1809.02121
N2  - Learning how to act when there are many available actions in each state is a challenging task for Reinforcement Learning (RL) agents, especially when many of the actions are redundant or irrelevant. In such cases, it is sometimes easier to learn which actions not to take. In this work, we propose the Action-Elimination Deep Q-Network (AE-DQN) architecture that combines a Deep RL algorithm with an Action Elimination Network (AEN) that eliminates sub-optimal actions. The AEN is trained to predict invalid actions, supervised by an external elimination signal provided by the environment. Simulations demonstrate a considerable speedup and added robustness over vanilla DQN in text-based games with over a thousand discrete actions.
ER  -


TY  - Preprint
T1  - ANS: Adaptive Network Scaling for Deep Rectifier Reinforcement Learning Models
A1  - Yueh-Hua Wu
A1  - Fan-Yun Sun
A1  - Yen-Yu Chang
A1  - Should-De Lin
JO  - ArXiv e-prints
Y1  - 6 September, 2018
UR  - https://arxiv.org/abs/1809.02112
N2  - This work provides a thorough study on how reward scaling can affect performance of deep reinforcement learning agents. In particular, we would like to answer the question that how does reward scaling affect non-saturating ReLU networks in RL? This question matters because ReLU is one of the most effective activation functions for deep learning models. We also propose an Adaptive Network Scaling framework to find a suitable scale of the rewards during learning for better performance. We conducted empirical studies to justify the solution.
ER  -


TY  - Preprint
T1  - Emergence of Human-comparable Balancing Behaviors by Deep Reinforcement Learning
A1  - Chuanyu Yang
A1  - Taku Komura
A1  - Zhibin Li
JO  - ArXiv e-prints
Y1  - 6 September, 2018
UR  - https://arxiv.org/abs/1809.02074
N2  - This paper presents a hierarchical framework based on deep reinforcement learning that learns a diversity of policies for humanoid balance control. Conventional zero moment point based controllers perform limited actions during under-actuation, whereas the proposed framework can perform human-like balancing behaviors such as active push-off of ankles. The learning is done through the design of an explainable reward based on physical constraints. The simulated results are presented and analyzed. The successful emergence of human-like behaviors through deep reinforcement learning proves the feasibility of using an AI-based approach for learning humanoid balancing control in a unified framework.
ER  -


TY  - Preprint
T1  - Deep learning for in vitro prediction of pharmaceutical formulations
A1  - Yilong Yang
A1  - Zhuyifan Ye
A1  - Yan Su
A1  - Qianqian Zhao
A1  - Xiaoshan Li
A1  - Defang Ouyang
JO  - ArXiv e-prints
Y1  - 6 September, 2018
UR  - https://arxiv.org/abs/1809.02069
N2  - Current pharmaceutical formulation development still strongly relies on the traditional trial-and-error approach by individual experiences of pharmaceutical scientists, which is laborious, time-consuming and costly. Recently, deep learning has been widely applied in many challenging domains because of its important capability of automatic feature extraction. The aim of this research is to use deep learning to predict pharmaceutical formulations. In this paper, two different types of dosage forms were chosen as model systems. Evaluation criteria suitable for pharmaceutics were applied to assessing the performance of the models. Moreover, an automatic dataset selection algorithm was developed for selecting the representative data as validation and test datasets. Six machine learning methods were compared with deep learning. The result shows the accuracies of both two deep neural networks were above 80% and higher than other machine learning models, which showed good prediction in pharmaceutical formulations. In summary, deep learning with the automatic data splitting algorithm and the evaluation criteria suitable for pharmaceutical formulation data was firstly developed for the prediction of pharmaceutical formulations. The cross-disciplinary integration of pharmaceutics and artificial intelligence may shift the paradigm of pharmaceutical researches from experience-dependent studies to data-driven methodologies.
ER  -


TY  - Preprint
T1  - Model-Based Stabilisation of Deep Reinforcement Learning
A1  - Felix Leibfried
A1  - Rasul Tutunov
A1  - Peter Vrancx
A1  - Haitham Bou-Ammar
JO  - ArXiv e-prints
Y1  - 6 September, 2018
UR  - https://arxiv.org/abs/1809.01906
N2  - Though successful in high-dimensional domains, deep reinforcement learning exhibits high sample complexity and suffers from stability issues as reported by researchers and practitioners in the field. These problems hinder the application of such algorithms in real-world and safety-critical scenarios. In this paper, we take steps towards stable and efficient reinforcement learning by following a model-based approach that is known to reduce agent-environment interactions. Namely, our method augments deep Q-networks (DQNs) with model predictions for transitions, rewards, and termination flags.
ER  -


TY  - Preprint
T1  - Deep Learning-Based Decoding for Constrained Sequence Codes
A1  - Congzhe Cao
A1  - Duanshun Li
A1  - Ivan Fair
JO  - ArXiv e-prints
Y1  - 6 September, 2018
UR  - https://arxiv.org/abs/1809.01859
N2  - Constrained sequence codes have been widely used in modern communication and data storage systems. Sequences encoded with constrained sequence codes satisfy constraints imposed by the physical channel, hence enabling efficient and reliable transmission of coded symbols. Traditional encoding and decoding of constrained sequence codes rely on table look-up, which is prone to errors that occur during transmission. In this paper, we introduce constrained sequence decoding based on deep learning. With multiple layer perception (MLP) networks and convolutional neural networks (CNNs), we are able to achieve low bit error rates that are close to maximum a posteriori probability (MAP) decoding as well as improve the system throughput. Moreover, implementation of capacity-achieving fixed-length codes, where the complexity is prohibitively high with table look-up decoding, becomes practical with deep learning-based decoding.
ER  -


TY  - Preprint
T1  - Connecting Image Denoising and High-Level Vision Tasks via Deep Learning
A1  - Ding Liu
A1  - Bihan Wen
A1  - Jianbo Jiao
A1  - Xianming Liu
A1  - Zhangyang Wang
A1  - Thomas S. Huang
JO  - ArXiv e-prints
Y1  - 6 September, 2018
UR  - https://arxiv.org/abs/1809.01826
N2  - Image denoising and high-level vision tasks are usually handled independently in the conventional practice of computer vision, and their connection is fragile. In this paper, we cope with the two jointly and explore the mutual influence between them with the focus on two questions, namely (1) how image denoising can help improving high-level vision tasks, and (2) how the semantic information from high-level vision tasks can be used to guide image denoising. First for image denoising we propose a convolutional neural network in which convolutions are conducted in various spatial resolutions via downsampling and upsampling operations in order to fuse and exploit contextual information on different scales. Second we propose a deep neural network solution that cascades two modules for image denoising and various high-level tasks, respectively, and use the joint loss for updating only the denoising network via back-propagation. We experimentally show that on one hand, the proposed denoiser has the generality to overcome the performance degradation of different high-level vision tasks. On the other hand, with the guidance of high-level vision information, the denoising network produces more visually appealing results. Extensive experiments demonstrate the benefit of exploiting image semantics simultaneously for image denoising and high-level vision tasks via deep learning. The code is available online: https://github.com/Ding-Liu/DeepDenoising
ER  -


TY  - Preprint
T1  - A deep learning approach for Magnetic Resonance Fingerprinting
A1  - Mohammad Golbabaee
A1  - Dongdong Chen
A1  - Pedro A. GÃ³mez
A1  - Marion I. Menzel
A1  - Mike E. Davies
JO  - ArXiv e-prints
Y1  - 5 September, 2018
UR  - https://arxiv.org/abs/1809.01749
N2  - Current popular methods for Magnetic Resonance Fingerprint (MRF) recovery are bottlenecked by the heavy storage and computation requirements of a matched-filtering step due to the growing size and complexity of the fingerprint dictionaries in multi-parametric quantitative MRI applications. In this abstract we investigate and evaluate advantages of a deep learning approach for embedding the manifold of solutions of the Bloch equations and to address these shortcomings.
ER  -


TY  - Preprint
T1  - Efficient Egocentric Visual Perception Combining Eye-tracking, a Software Retina and Deep Learning
A1  - Nina Hristozova
A1  - Piotr Ozimek
A1  - Jan Paul Siebert
JO  - ArXiv e-prints
Y1  - 5 September, 2018
UR  - https://arxiv.org/abs/1809.01633
N2  - We present ongoing work to harness biological approaches to achieving highly efficient egocentric perception by combining the space-variant imaging architecture of the mammalian retina with Deep Learning methods. By pre-processing images collected by means of eye-tracking glasses to control the fixation locations of a software retina model, we demonstrate that we can reduce the input to a DCNN by a factor of 3, reduce the required number of training epochs and obtain over 98% classification rates when training and validating the system on a database of over 26,000 images of 9 object classes.
ER  -


TY  - Preprint
T1  - Merging datasets through deep learning
A1  - Kavitha Srinivas
A1  - Abraham Gale
A1  - Julian Dolby
JO  - ArXiv e-prints
Y1  - 5 September, 2018
UR  - https://arxiv.org/abs/1809.01604
N2  - Merging datasets is a key operation for data analytics. A frequent requirement for merging is joining across columns that have different surface forms for the same entity (e.g., the name of a person might be represented as &#34;Douglas Adams&#34; or &#34;Adams, Douglas&#34;). Similarly, ontology alignment can require recognizing distinct surface forms of the same entity, especially when ontologies are independently developed. However, data management systems are currently limited to performing merges based on string equality, or at best using string similarity. We propose an approach to performing merges based on deep learning models. Our approach depends on (a) creating a deep learning model that maps surface forms of an entity into a set of vectors such that alternate forms for the same entity are closest in vector space, (b) indexing these vectors using a nearest neighbors algorithm to find the forms that can be potentially joined together. To build these models, we had to adapt techniques from metric learning due to the characteristics of the data; specifically we describe novel sample selection techniques and loss functions that work for this problem. To evaluate our approach, we used Wikidata as ground truth and built models from datasets with approximately 1.1M people&#39;s names (200K identities) and 130K company names (70K identities). We developed models that allow for joins with precision@1 of .75-.81 and recall of .74-.81. We make the models available for aligning people or companies across multiple datasets.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning in High Frequency Trading
A1  - Prakhar Ganesh
A1  - Puneet Rakheja
JO  - ArXiv e-prints
Y1  - 5 September, 2018
UR  - https://arxiv.org/abs/1809.01506
N2  - The ability to give a precise and fast prediction for the price movement of stocks is the key to profitability in High Frequency Trading. The main objective of this paper is to propose a novel way of modeling the high frequency trading problem using Deep Reinforcement Learning and to argue why Deep RL can have a lot of potential in the field of High Frequency Trading. We have analyzed the model&#39;s performance based on it&#39;s prediction accuracy as well as prediction speed across full-day trading simulations.
ER  -


TY  - Preprint
T1  - Deep Bilevel Learning
A1  - Simon Jenni
A1  - Paolo Favaro
JO  - ArXiv e-prints
Y1  - 5 September, 2018
UR  - https://arxiv.org/abs/1809.01465
N2  - We present a novel regularization approach to train neural networks that enjoys better generalization and test error than standard stochastic gradient descent. Our approach is based on the principles of cross-validation, where a validation set is used to limit the model overfitting. We formulate such principles as a bilevel optimization problem. This formulation allows us to define the optimization of a cost on the validation set subject to another optimization on the training set. The overfitting is controlled by introducing weights on each mini-batch in the training set and by choosing their values so that they minimize the error on the validation set. In practice, these weights define mini-batch learning rates in a gradient descent update equation that favor gradients with better generalization capabilities. Because of its simplicity, this approach can be integrated with other regularization methods and training schemes. We evaluate extensively our proposed algorithm on several neural network architectures and datasets, and find that it consistently improves the generalization of the model, especially when labels are noisy.
ER  -


TY  - Preprint
T1  - Zero Shot Learning for Code Education: Rubric Sampling with Deep Learning Inference
A1  - Mike Wu
A1  - Milan Mosse
A1  - Noah Goodman
A1  - Chris Piech
JO  - ArXiv e-prints
Y1  - 5 September, 2018
UR  - https://arxiv.org/abs/1809.01357
N2  - In modern computer science education, massive open online courses (MOOCs) log thousands of hours of data about how students solve coding challenges. Being so rich in data, these platforms have garnered the interest of the machine learning community, with many new algorithms attempting to autonomously provide feedback to help future students learn. But what about those first hundred thousand students? In most educational contexts (i.e. classrooms), assignments do not have enough historical data for supervised learning. In this paper, we introduce a human-in-the-loop &#34;rubric sampling&#34; approach to tackle the &#34;zero shot&#34; feedback challenge. We are able to provide autonomous feedback for the first students working on an introductory programming assignment with accuracy that substantially outperforms data-hungry algorithms and approaches human level fidelity. Rubric sampling requires minimal teacher effort, can associate feedback with specific parts of a student&#39;s solution and can articulate a student&#39;s misconceptions in the language of the instructor. Deep learning inference enables rubric sampling to further improve as more assignment specific student data is acquired. We demonstrate our results on a novel dataset from Code.org, the world&#39;s largest programming education platform.
ER  -


TY  - Preprint
T1  - Automated segmentation on the entire cardiac cycle using a deep learning work-flow
A1  - NicolÃ³ Savioli
A1  - Miguel Silva Vieira
A1  - Pablo Lamata
A1  - Giovanni Montana
JO  - ArXiv e-prints
Y1  - 31 August, 2018
UR  - https://arxiv.org/abs/1809.01015
N2  - The segmentation of the left ventricle (LV) from CINE MRI images is essential to infer important clinical parameters. Typically, machine learning algorithms for automated LV segmentation use annotated contours from only two cardiac phases, diastole, and systole. In this work, we present an analysis work-flow for fully-automated LV segmentation that learns from images acquired through the cardiac cycle. The workflow consists of three components: first, for each image in the sequence, we perform an automated localization and subsequent cropping of the bounding box containing the cardiac silhouette. Second, we identify the LV contours using a Temporal Fully Convolutional Neural Network (T-FCNN), which extends Fully Convolutional Neural Networks (FCNN) through a recurrent mechanism enforcing temporal coherence across consecutive frames. Finally, we further defined the boundaries using either one of two components: fully-connected Conditional Random Fields (CRFs) with Gaussian edge potentials and Semantic Flow. Our initial experiments suggest that significant improvement in performance can potentially be achieved by using a recurrent neural network component that explicitly learns cardiac motion patterns whilst performing LV segmentation.
ER  -


TY  - Preprint
T1  - Deep Learning Based Vehicle Make-Model Classification
A1  - Burak Satar
A1  - Ahmet Emir Dirik
JO  - ArXiv e-prints
Y1  - 23 August, 2018
UR  - https://arxiv.org/abs/1809.00953
N2  - This paper studies the problems of vehicle make &amp; model classification. Some of the main challenges are reaching high classification accuracy and reducing the annotation time of the images. To address these problems, we have created a fine-grained database using online vehicle marketplaces of Turkey. A pipeline is proposed to combine an SSD (Single Shot Multibox Detector) model with a CNN (Convolutional Neural Network) model to train on the database. In the pipeline, we first detect the vehicles by following an algorithm which reduces the time for annotation. Then, we feed them into the CNN model. It is reached approximately 4% better classification accuracy result than using a conventional CNN model. Next, we propose to use the detected vehicles as ground truth bounding box (GTBB) of the images and feed them into an SSD model in another pipeline. At this stage, it is reached reasonable classification accuracy result without using perfectly shaped GTBB. Lastly, an application is implemented in a use case by using our proposed pipelines. It detects the unauthorized vehicles by comparing their license plate numbers and make &amp; models. It is assumed that license plates are readable.
ER  -


TY  - Preprint
T1  - Image Reassembly Combining Deep Learning and Shortest Path Problem
A1  - M. -M. Paumard
A1  - D. Picard
A1  - H. Tabia
JO  - ArXiv e-prints
Y1  - 4 September, 2018
UR  - https://arxiv.org/abs/1809.00898
N2  - This paper addresses the problem of reassembling images from disjointed fragments. More specifically, given an unordered set of fragments, we aim at reassembling one or several possibly incomplete images. The main contributions of this work are: 1) several deep neural architectures to predict the relative position of image fragments that outperform the previous state of the art; 2) casting the reassembly problem into the shortest path in a graph problem for which we provide several construction algorithms depending on available information; 3) a new dataset of images taken from the Metropolitan Museum of Art (MET) dedicated to image reassembly for which we provide a clear setup and a strong baseline.
ER  -


TY  - Preprint
T1  - Improving the Expressiveness of Deep Learning Frameworks with Recursion
A1  - Eunji Jeong
A1  - Joo Seong Jeong
A1  - Soojeong Kim
A1  - Gyeong-In Yu
A1  - Byung-Gon Chun
JO  - ArXiv e-prints
Y1  - 4 September, 2018
UR  - https://arxiv.org/abs/1809.00832
N2  - Recursive neural networks have widely been used by researchers to handle applications with recursively or hierarchically structured data. However, embedded control flow deep learning frameworks such as TensorFlow, Theano, Caffe2, and MXNet fail to efficiently represent and execute such neural networks, due to lack of support for recursion. In this paper, we add recursion to the programming model of existing frameworks by complementing their design with recursive execution of dataflow graphs as well as additional APIs for recursive definitions. Unlike iterative implementations, which can only understand the topological index of each node in recursive data structures, our recursive implementation is able to exploit the recursive relationships between nodes for efficient execution based on parallel computation. We present an implementation on TensorFlow and evaluation results with various recursive neural network models, showing that our recursive implementation not only conveys the recursive nature of recursive neural networks better than other implementations, but also uses given resources more effectively to reduce training and inference time.
ER  -


TY  - Preprint
T1  - A Deep Learning Spatiotemporal Prediction Framework for Mobile Crowdsourced Services
A1  - Ahmed Ben Said
A1  - Abdelkarim Erradi
A1  - Azadeh Ghari Neiat
A1  - Athman Bouguettaya
JO  - ArXiv e-prints
Y1  - 4 September, 2018
UR  - https://arxiv.org/abs/1809.00811
N2  - This papers presents a deep learning-based framework to predict crowdsourced service availability spatially and temporally. A novel two-stage prediction model is introduced based on historical spatio-temporal traces of mobile crowdsourced services. The prediction model first clusters mobile crowdsourced services into regions. The availability prediction of a mobile crowdsourced service at a certain location and time is then formulated as a classification problem. To determine the availability duration of predicted mobile crowdsourced services, we formulate a forecasting task of time series using the Gramian Angular Field. We validated the effectiveness of the proposed framework through multiple experiments.
ER  -


TY  - Preprint
T1  - Transferring Deep Reinforcement Learning with Adversarial Objective and Augmentation
A1  - Shu-Hsuan Hsu
A1  - I-Chao Shen
A1  - Bing-Yu Chen
JO  - ArXiv e-prints
Y1  - 3 September, 2018
UR  - https://arxiv.org/abs/1809.00770
N2  - In the past few years, deep reinforcement learning has been proven to solve problems which have complex states like video games or board games. The next step of intelligent agents would be able to generalize between tasks, and using prior experience to pick up new skills more quickly. However, most reinforcement learning algorithms for now are often suffering from catastrophic forgetting even when facing a very similar target task. Our approach enables the agents to generalize knowledge from a single source task, and boost the learning progress with a semisupervised learning method when facing a new task. We evaluate this approach on Atari games, which is a popular reinforcement learning benchmark, and show that it outperforms common baselines based on pre-training and fine-tuning.
ER  -


TY  - Preprint
T1  - Spatial-Spectral Fusion by Combining Deep Learning and Variation Model
A1  - Huanfeng Shen
A1  - Menghui Jiang
A1  - Jie Li
A1  - Qiangqiang Yuan
A1  - Yanchong Wei
A1  - Liangpei Zhang
JO  - ArXiv e-prints
Y1  - 3 September, 2018
UR  - https://arxiv.org/abs/1809.00764
N2  - In the field of spatial-spectral fusion, the model-based method and the deep learning (DL)-based method are state-of-the-art. This paper presents a fusion method that incorporates the deep neural network into the model-based method for the most common case in the spatial-spectral fusion: PAN/multispectral (MS) fusion. Specifically, we first map the gradient of the high spatial resolution panchromatic image (HR-PAN) and the low spatial resolution multispectral image (LR-MS) to the gradient of the high spatial resolution multispectral image (HR-MS) via a deep residual convolutional neural network (CNN). Then we construct a fusion framework by the LR-MS image, the gradient prior learned from the gradient network, and the ideal fused image. Finally, an iterative optimization algorithm is used to solve the fusion model. Both quantitative and visual assessments on high-quality images from various sources demonstrate that the proposed fusion method is superior to all the mainstream algorithms included in the comparison in terms of overall fusion accuracy.
ER  -


TY  - Preprint
T1  - Learned Cardinalities: Estimating Correlated Joins with Deep Learning
A1  - Andreas Kipf
A1  - Thomas Kipf
A1  - Bernhard Radke
A1  - Viktor Leis
A1  - Peter Boncz
A1  - Alfons Kemper
JO  - ArXiv e-prints
Y1  - 3 September, 2018
UR  - https://arxiv.org/abs/1809.00677
N2  - We describe a new deep learning approach to cardinality estimation. MSCN is a multi-set convolutional network, tailored to representing relational query plans, that employs set semantics to capture query features and true cardinalities. MSCN builds on sampling-based estimation, addressing its weaknesses when no sampled tuples qualify a predicate, and in capturing join-crossing correlations. Our evaluation of MSCN using a real-world dataset shows that deep learning significantly enhances the quality of cardinality estimation, which is the core problem in query optimization.
ER  -


TY  - Preprint
T1  - Deep learning for language understanding of mental health concepts derived from Cognitive Behavioural Therapy
A1  - Lina Rojas-Barahona
A1  - Bo-Hsiang Tseng
A1  - Yinpei Dai
A1  - Clare Mansfield
A1  - Osman Ramadan
A1  - Stefan Ultes
A1  - Michael Crawford
A1  - Milica Gasic
JO  - ArXiv e-prints
Y1  - 3 September, 2018
UR  - https://arxiv.org/abs/1809.00640
N2  - In recent years, we have seen deep learning and distributed representations of words and sentences make impact on a number of natural language processing tasks, such as similarity, entailment and sentiment analysis. Here we introduce a new task: understanding of mental health concepts derived from Cognitive Behavioural Therapy (CBT). We define a mental health ontology based on the CBT principles, annotate a large corpus where this phenomena is exhibited and perform understanding using deep learning and distributed representations. Our results show that the performance of deep learning models combined with word embeddings or sentence embeddings significantly outperform non-deep-learning models in this difficult task. This understanding module will be an essential component of a statistical dialogue system delivering therapy.
ER  -


TY  - Preprint
T1  - Deep Learning of Human Perception in Audio Event Classification
A1  - Yi Yu
A1  - Samuel Beuret
A1  - Donghuo Zeng
A1  - Keizo Oyama
JO  - ArXiv e-prints
Y1  - 3 September, 2018
UR  - https://arxiv.org/abs/1809.00502
N2  - In this paper, we introduce our recent studies on human perception in audio event classification by different deep learning models. In particular, the pre-trained model VGGish is used as feature extractor to process audio data, and DenseNet is trained by and used as feature extractor for our electroencephalography (EEG) data. The correlation between audio stimuli and EEG is learned in a shared space. In the experiments, we record brain activities (EEG signals) of several subjects while they are listening to music events of 8 audio categories selected from Google AudioSet, using a 16-channel EEG headset with active electrodes. Our experimental results demonstrate that i) audio event classification can be improved by exploiting the power of human perception, and ii) the correlation between audio stimuli and EEG can be learned to complement audio event understanding.
ER  -


TY  - Preprint
T1  - Effective Exploration for Deep Reinforcement Learning via Bootstrapped Q-Ensembles under Tsallis Entropy Regularization
A1  - Gang Chen
A1  - Yiming Peng
A1  - Mengjie Zhang
JO  - ArXiv e-prints
Y1  - 4 September, 2018
UR  - https://arxiv.org/abs/1809.00403
N2  - Recently deep reinforcement learning (DRL) has achieved outstanding success on solving many difficult and large-scale RL problems. However the high sample cost required for effective learning often makes DRL unaffordable in resource-limited applications. With the aim of improving sample efficiency and learning performance, we will develop a new DRL algorithm in this paper that seamless integrates entropy-induced and bootstrap-induced techniques for efficient and deep exploration of the learning environment. Specifically, a general form of Tsallis entropy regularizer will be utilized to drive entropy-induced exploration based on efficient approximation of optimal action-selection policies. Different from many existing works that rely on action dithering strategies for exploration, our algorithm is efficient in exploring actions with clear exploration value. Meanwhile, by employing an ensemble of Q-networks under varied Tsallis entropy regularization, the diversity of the ensemble can be further enhanced to enable effective bootstrap-induced exploration. Experiments on Atari game playing tasks clearly demonstrate that our new algorithm can achieve more efficient and effective exploration for DRL, in comparison to recently proposed exploration methods including Bootstrapped Deep Q-Network and UCB Q-Ensemble.
ER  -


TY  - Preprint
T1  - Natural Language Person Search Using Deep Reinforcement Learning
A1  - Ankit Shah
A1  - Tyler Vuong
JO  - ArXiv e-prints
Y1  - 2 September, 2018
UR  - https://arxiv.org/abs/1809.00365
N2  - Recent success in deep reinforcement learning is having an agent learn how to play Go and beat the world champion without any prior knowledge of the game. In that task, the agent has to make a decision on what action to take based on the positions of the pieces. Person Search is recently explored using natural language based text description of images for video surveillance applications (S.Li et.al). We see (Fu.et al) provides an end to end approach for object-based retrieval using deep reinforcement learning without constraints placed on which objects are being detected. However, we believe for real-world applications such as person search defining specific constraints which identify a person as opposed to starting with a general object detection will have benefits in terms of performance and computational resources required. In our task, Deep reinforcement learning would localize the person in an image by reshaping the sizes of the bounding boxes. Deep Reinforcement learning with appropriate constraints would look only for the relevant person in the image as opposed to an unconstrained approach where each individual objects in the image are ranked. For person search, the agent is trying to form a tight bounding box around the person in the image who matches the description. The bounding box is initialized to the full image and at each time step, the agent makes a decision on how to change the current bounding box so that it has a tighter bound around the person based on the description of the person and the pixel values of the current bounding box. After the agent takes an action, it will be given a reward based on the Intersection over Union (IoU) of the current bounding box and the ground truth box. Once the agent believes that the bounding box is covering the person, it will indicate that the person is found.
ER  -


TY  - Preprint
T1  - Identifying Land Patterns from Satellite Imagery in Amazon Rainforest using Deep Learning
A1  - Somnath Rakshit
A1  - Soumyadeep Debnath
A1  - Dhiman Mondal
JO  - ArXiv e-prints
Y1  - 2 September, 2018
UR  - https://arxiv.org/abs/1809.00340
N2  - The Amazon rainforests have been suffering widespread damage, both via natural and artificial means. Every minute, it is estimated that the world loses forest cover the size of 48 football fields. Deforestation in the Amazon rainforest has led to drastically reduced biodiversity, loss of habitat, climate change, and other biological losses. In this respect, it has become essential to track how the nature of these forests change over time. Image classification using deep learning can help speed up this process by removing the manual task of classifying each image. Here, it is shown how convolutional neural networks can be used to track changes in land patterns in the Amazon rainforests. In this work, a testing accuracy of 96.71% was obtained. This can help governments and other agencies to track changes in land patterns more effectively and accurately.
ER  -


TY  - Preprint
T1  - Car Monitoring System in Apartment Garages by Small Autonomous Car using Deep Learning
A1  - Leonardo LeÃ³n
A1  - Felipe Moreno
A1  - Renato Castro
A1  - JosÃ© NavÃ­o
A1  - Marco Capcha
JO  - ArXiv e-prints
Y1  - 28 September, 2018
UR  - https://arxiv.org/abs/1809.00251
N2  - Currently, there is an increase in the number of Peruvian families living in apartments instead of houses for the lots of advantage; however, in some cases there are troubles such as robberies of goods that are usually left at the parking lots or the entrance of strangers that use the tenants parking lots (this last trouble sometimes is related to kidnappings or robberies in building apartments). Due to these problems, the use of a self-driving mini-car is proposed to implement a monitoring system of license plates in an underground garage inside a building using a deep learning model with the aim of recording the vehicles and identifying their owners if they were tenants or not. In addition, the small robot has its own location system using beacons that allow us to identify the position of the parking lot corresponding to each tenant of the building while the mini-car is on its way. Finally, one of the objectives of this work is to build a low cost mini-robot that would replace expensive cameras or work together in order to keep safe the goods of tenants.
ER  -


TY  - Preprint
T1  - Learning Low Precision Deep Neural Networks through Regularization
A1  - Yoojin Choi
A1  - Mostafa El-Khamy
A1  - Jungwon Lee
JO  - ArXiv e-prints
Y1  - 31 August, 2018
UR  - https://arxiv.org/abs/1809.00095
N2  - We consider the quantization of deep neural networks (DNNs) to produce low-precision models for efficient inference of fixed-point operations. Compared to previous approaches to training quantized DNNs directly under the constraints of low-precision weights and activations, we learn the quantization of DNNs with minimal quantization loss through regularization. In particular, we introduce the learnable regularization coefficient to find accurate low-precision models efficiently in training. In our experiments, the proposed scheme yields the state-of-the-art low-precision models of AlexNet and ResNet-18, which have better accuracy than their previously available low-precision models. We also examine our quantization method to produce low-precision DNNs for image super resolution. We observe only $0.5$~dB peak signal-to-noise ratio (PSNR) loss when using binary weights and 8-bit activations. The proposed scheme can be used to train low-precision models from scratch or to fine-tune a well-trained high-precision model to converge to a low-precision model. Finally, we discuss how a similar regularization method can be adopted in DNN weight pruning and compression, and show that $401\times$ compression is achieved for LeNet-5.
ER  -


TY  - Preprint
T1  - A Simplified Approach to Deep Learning for Image Segmentation
A1  - Ishtar Nyawira
A1  - Kristi Bushman
JO  - ArXiv e-prints
Y1  - 31 August, 2018
UR  - https://arxiv.org/abs/1809.00085
N2  - Leaping into the rapidly developing world of deep learning is an exciting and sometimes confusing adventure. All of the advice and tutorials available can be hard to organize and work through, especially when training specific models on specific datasets, different from those originally used to train the network. In this short guide, we aim to walk the reader through the techniques that we have used to successfully train two deep neural networks for pixel-wise classification, including some data management and augmentation approaches for working with image data that may be insufficiently annotated or relatively homogenous.
ER  -


TY  - Preprint
T1  - Understanding Neural Pathways in Zebrafish through Deep Learning and High Resolution Electron Microscope Data
A1  - Ishtar Nyawira
A1  - Kristi Bushman
A1  - Iris Qian
A1  - Annie Zhang
JO  - ArXiv e-prints
Y1  - 31 August, 2018
UR  - https://arxiv.org/abs/1809.00084
N2  - The tracing of neural pathways through large volumes of image data is an incredibly tedious and time-consuming process that significantly encumbers progress in neuroscience. We are exploring deep learning&#39;s potential to automate segmentation of high-resolution scanning electron microscope (SEM) image data to remove that barrier. We have started with neural pathway tracing through 5.1GB of whole-brain serial-section slices from larval zebrafish collected by the Center for Brain Science at Harvard University. This kind of manual image segmentation requires years of careful work to properly trace the neural pathways in an organism as small as a zebrafish larva (approximately 5mm in total body length). In automating this process, we would vastly improve productivity, leading to faster data analysis and breakthroughs in understanding the complexity of the brain. We will build upon prior attempts to employ deep learning for automatic image segmentation extending methods for unconventional deep learning data.
ER  -


TY  - Preprint
T1  - Single-Microphone Speech Enhancement and Separation Using Deep Learning
A1  - Morten KolbÃ¦k
JO  - ArXiv e-prints
Y1  - 31 August, 2018
UR  - https://arxiv.org/abs/1808.10620
N2  - The cocktail party problem comprises the challenging task of understanding a speech signal in a complex acoustic environment, where multiple speakers and background noise signals simultaneously interfere with the speech signal of interest. A signal processing algorithm that can effectively increase the speech intelligibility and quality of speech signals in such complicated acoustic situations is highly desirable. Especially for applications involving mobile communication devices and hearing assistive devices. Due to the re-emergence of machine learning techniques, today, known as deep learning, the challenges involved with such algorithms might be overcome. In this PhD thesis, we study and develop deep learning-based techniques for two sub-disciplines of the cocktail party problem: single-microphone speech enhancement and single-microphone multi-talker speech separation. Specifically, we conduct in-depth empirical analysis of the generalizability capability of modern deep learning-based single-microphone speech enhancement algorithms. We show that performance of such algorithms is closely linked to the training data, and good generalizability can be achieved with carefully designed training data. Furthermore, we propose uPIT, a deep learning-based algorithm for single-microphone speech separation and we report state-of-the-art results on a speaker-independent multi-talker speech separation task. Additionally, we show that uPIT works well for joint speech separation and enhancement without explicit prior knowledge about the noise type or number of speakers. Finally, we show that deep learning-based speech enhancement algorithms designed to minimize the classical short-time spectral amplitude mean squared error leads to enhanced speech signals which are essentially optimal in terms of STOI, a state-of-the-art speech intelligibility estimator.
ER  -


TY  - Preprint
T1  - A Unified Analysis of Stochastic Momentum Methods for Deep Learning
A1  - Yan Yan
A1  - Tianbao Yang
A1  - Zhe Li
A1  - Qihang Lin
A1  - Yi Yang
JO  - ArXiv e-prints
Y1  - 30 August, 2018
UR  - https://arxiv.org/abs/1808.10396
N2  - Stochastic momentum methods have been widely adopted in training deep neural networks. However, their theoretical analysis of convergence of the training objective and the generalization error for prediction is still under-explored. This paper aims to bridge the gap between practice and theory by analyzing the stochastic gradient (SG) method, and the stochastic momentum methods including two famous variants, i.e., the stochastic heavy-ball (SHB) method and the stochastic variant of Nesterov&#39;s accelerated gradient (SNAG) method. We propose a framework that unifies the three variants. We then derive the convergence rates of the norm of gradient for the non-convex optimization problem, and analyze the generalization performance through the uniform stability approach. Particularly, the convergence analysis of the training objective exhibits that SHB and SNAG have no advantage over SG. However, the stability analysis shows that the momentum term can improve the stability of the learned model and hence improve the generalization performance. These theoretical insights verify the common wisdom and are also corroborated by our empirical analysis on deep learning.
ER  -


TY  - Preprint
T1  - Deep Chronnectome Learning via Full Bidirectional Long Short-Term Memory Networks for MCI Diagnosis
A1  - Weizheng Yan
A1  - Han Zhang
A1  - Jing Sui
A1  - Dinggang Shen
JO  - ArXiv e-prints
Y1  - 30 August, 2018
UR  - https://arxiv.org/abs/1808.10383
N2  - Brain functional connectivity (FC) extracted from resting-state fMRI (RS-fMRI) has become a popular approach for disease diagnosis, where discriminating subjects with mild cognitive impairment (MCI) from normal controls (NC) is still one of the most challenging problems. Dynamic functional connectivity (dFC), consisting of time-varying spatiotemporal dynamics, may characterize &#34;chronnectome&#34; diagnostic information for improving MCI classification. However, most of the current dFC studies are based on detecting discrete major brain status via spatial clustering, which ignores rich spatiotemporal dynamics contained in such chronnectome. We propose Deep Chronnectome Learning for exhaustively mining the comprehensive information, especially the hidden higher-level features, i.e., the dFC time series that may add critical diagnostic power for MCI classification. To this end, we devise a new Fully-connected Bidirectional Long Short-Term Memory Network (Full-BiLSTM) to effectively learn the periodic brain status changes using both past and future information for each brief time segment and then fuse them to form the final output. We have applied our method to a rigorously built large-scale multi-site database (i.e., with 164 data from NCs and 330 from MCIs, which can be further augmented by 25 folds). Our method outperforms other state-of-the-art approaches with an accuracy of 73.6% under solid cross-validations. We also made extensive comparisons among multiple variants of LSTM models. The results suggest high feasibility of our method with promising value also for other brain disorder diagnoses.
ER  -


TY  - Preprint
T1  - Towards Effective Deep Embedding for Zero-Shot Learning
A1  - Lei Zhang
A1  - Peng Wang
A1  - Lingqiao Liu
A1  - Chunhua Shen
A1  - Wei Wei
A1  - Yannning Zhang
A1  - Anton Van Den Hengel
JO  - ArXiv e-prints
Y1  - 29 August, 2018
UR  - https://arxiv.org/abs/1808.10075
N2  - Zero-shot learning (ZSL) attempts to recognize visual samples of unseen classes by virtue of the semantic descriptions of those classes. We posit that the key to ZSL is to exploit an effective embedding space where 1) visual samples can be tightly centred around the semantic descriptions of classes that they belong to; 2) visual samples of different classes are separated from each other with a large enough margin. Towards this goal, we present a simple but surprisingly effective deep embedding model. In our model, we separately embed visual samples and semantic descriptions into a latent intermediate space such that visual samples not only coincide with associated semantic descriptions, but also can be correctly discriminated by a trainable linear classifier. By doing this, visual samples can be tightly centred around associated semantic descriptions and more importantly, they can be separated from other semantic descriptions with a large margin, thus leading to a new state-of-the-art for ZSL. Furthermore, due to lacking training samples, the generalization capacity of the learned embedding space to unseen classes can be further improved. To this end, we propose to upgrade our model with a refining strategy which progressively calibrates the embedding space based upon some test samples chosen from unseen classes with high-confidence pseudo labels, and ultimately improves the generalization capacity greatly. Experimental results on five benchmarks demonstrate the great advantage of our model over current state-of-the-art competitors. For example, on AwA1 dataset, our model improves the recognition accuracy on unseen classes by 16.9% in conventional ZSL setting and even by 38.6% in the generalized ZSL setting.
ER  -


TY  - Preprint
T1  - Iterative Deep Learning for Road Topology Extraction
A1  - Carles Ventura
A1  - Jordi Pont-Tuset
A1  - Sergi Caelles
A1  - Kevis-Kokitsi Maninis
A1  - Luc Van Gool
JO  - ArXiv e-prints
Y1  - 28 August, 2018
UR  - https://arxiv.org/abs/1808.09814
N2  - This paper tackles the task of estimating the topology of road networks from aerial images. Building on top of a global model that performs a dense semantical classification of the pixels of the image, we design a Convolutional Neural Network (CNN) that predicts the local connectivity among the central pixel of an input patch and its border points. By iterating this local connectivity we sweep the whole image and infer the global topology of the road network, inspired by a human delineating a complex network with the tip of their finger. We perform an extensive and comprehensive qualitative and quantitative evaluation on the road network estimation task, and show that our method also generalizes well when moving to networks of retinal vessels.
ER  -


TY  - Preprint
T1  - Notes on Deep Learning for NLP
A1  - Antoine J. -P. Tixier
JO  - ArXiv e-prints
Y1  - 30 August, 2018
UR  - https://arxiv.org/abs/1808.09772
N2  - My notes on Deep Learning for NLP.
ER  -


TY  - Preprint
T1  - Towards Semi-Supervised Learning for Deep Semantic Role Labeling
A1  - Sanket Vaibhav Mehta
A1  - Jay Yoon Lee
A1  - Jaime Carbonell
JO  - ArXiv e-prints
Y1  - 28 August, 2018
UR  - https://arxiv.org/abs/1808.09543
N2  - Neural models have shown several state-of-the-art performances on Semantic Role Labeling (SRL). However, the neural models require an immense amount of semantic-role corpora and are thus not well suited for low-resource languages or domains. The paper proposes a semi-supervised semantic role labeling method that outperforms the state-of-the-art in limited SRL training corpora. The method is based on explicitly enforcing syntactic constraints by augmenting the training objective with a syntactic-inconsistency loss component and uses SRL-unlabeled instances to train a joint-objective LSTM. On CoNLL-2012 English section, the proposed semi-supervised training with 1%, 10% SRL-labeled data and varying amounts of SRL-unlabeled data achieves +1.58, +0.78 F1, respectively, over the pre-trained models that were trained on SOTA architecture with ELMo on the same SRL-labeled data. Additionally, by using the syntactic-inconsistency loss on inference time, the proposed model achieves +3.67, +2.1 F1 over pre-trained model on 1%, 10% SRL-labeled data, respectively.
ER  -


TY  - Preprint
T1  - Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning
A1  - Shang-Yu Su
A1  - Xiujun Li
A1  - Jianfeng Gao
A1  - Jingjing Liu
A1  - Yun-Nung Chen
JO  - ArXiv e-prints
Y1  - 6 September, 2018
UR  - https://arxiv.org/abs/1808.09442
N2  - This paper presents a Discriminative Deep Dyna-Q (D3Q) approach to improving the effectiveness and robustness of Deep Dyna-Q (DDQ), a recently proposed framework that extends the Dyna-Q algorithm to integrate planning for task-completion dialogue policy learning. To obviate DDQ&#39;s high dependency on the quality of simulated experiences, we incorporate an RNN-based discriminator in D3Q to differentiate simulated experience from real user experience in order to control the quality of training data. Experiments show that D3Q significantly outperforms DDQ by controlling the quality of simulated experience used for planning. The effectiveness and robustness of D3Q is further demonstrated in a domain extension setting, where the agent&#39;s capability of adapting to a changing environment is tested.
ER  -


TY  - Preprint
T1  - DLFuzz: Differential Fuzzing Testing of Deep Learning Systems
A1  - Jianmin Guo
A1  - Yu Jiang
A1  - Yue Zhao
A1  - Quan Chen
A1  - Jiaguang Sun
JO  - ArXiv e-prints
Y1  - 28 August, 2018
UR  - https://arxiv.org/abs/1808.09413
N2  - Deep learning (DL) systems are increasingly applied to safety-critical domains such as autonomous driving cars. It is of significant importance to ensure the reliability and robustness of DL systems. Existing testing methodologies always fail to include rare inputs in the testing dataset and exhibit low neuron coverage. In this paper, we propose DLFuzz, the frst differential fuzzing testing framework to guide DL systems exposing incorrect behaviors. DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality. We present empirical evaluations on two well-known datasets to demonstrate its efficiency. Compared with DeepXplore, the state-of-the-art DL whitebox testing framework, DLFuzz does not require extra efforts to find similar functional DL systems for cross-referencing check, but could generate 338.59% more adversarial inputs with 89.82% smaller perturbations, averagely obtain 2.86% higher neuron coverage, and save 20.11% time consumption.
ER  -


TY  - Preprint
T1  - Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation
A1  - Chao Chen
A1  - Zhihong Chen
A1  - Boyuan Jiang
A1  - Xinyu Jin
JO  - ArXiv e-prints
Y1  - 28 August, 2018
UR  - https://arxiv.org/abs/1808.09347
N2  - Recently, considerable effort has been devoted to deep domain adaptation in computer vision and machine learning communities. However, most of existing work only concentrates on learning shared feature representation by minimizing the distribution discrepancy across different domains. Due to the fact that all the domain alignment approaches can only reduce, but not remove the domain shift. Target domain samples distributed near the edge of the clusters, or far from their corresponding class centers are easily to be misclassified by the hyperplane learned from the source domain. To alleviate this issue, we propose to joint domain alignment and discriminative feature learning, which could benefit both domain alignment and final classification. Specifically, an instance-based discriminative feature learning method and a center-based discriminative feature learning method are proposed, both of which guarantee the domain invariant features with better intra-class compactness and inter-class separability. Extensive experiments show that learning the discriminative features in the shared feature space can significantly boost the performance of deep domain adaptation methods.
ER  -


TY  - Preprint
T1  - PhaseMAC: A 14 TOPS/W 8bit GRO based Phase Domain MAC Circuit for In-Sensor-Computed Deep Learning Accelerators
A1  - Kentaro Yoshioka
A1  - Yosuke Toyama
A1  - Koichiro Ban
A1  - Daisuke Yashima
A1  - Shigeru Maya
A1  - Akihide Sai
A1  - Kohei Onizuka
JO  - ArXiv e-prints
Y1  - 23 August, 2018
UR  - https://arxiv.org/abs/1808.09335
N2  - PhaseMAC (PMAC), a phase domain Gated-Ring-Oscillator (GRO) based 8bit MAC circuit, is proposed to minimize both area and power consumption of deep learning accelerators. PMAC composes of only digital cells and consumes significantly smaller power than standard digital designs, owing to its efficient analog accumulation nature. It occupies 26.6 times smaller area than conventional analog designs, which is competitive to digital MAC circuits. PMAC achieves a peak efficiency of 14 TOPS/W, which is best reported and 48% higher than conventional arts. Results in anomaly detection tasks are demonstrated, which is the hottest application in the industrial IoT scene.
ER  -


TY  - Preprint
T1  - DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model
A1  - StÃ©phane LathuiliÃ¨re
A1  - Pablo Mesejo
A1  - Xavier Alameda-Pineda
A1  - Radu Horaud
JO  - ArXiv e-prints
Y1  - 28 August, 2018
UR  - https://arxiv.org/abs/1808.09211
N2  - In this paper, we address the problem of how to robustly train a ConvNet for regression, or deep robust regression. Traditionally, deep regression employs the L2 loss function, known to be sensitive to outliers, i.e. samples that either lie at an abnormal distance away from the majority of the training samples, or that correspond to wrongly annotated targets. This means that, during back-propagation, outliers may bias the training process due to the high magnitude of their gradient. In this paper, we propose DeepGUM: a deep regression model that is robust to outliers thanks to the use of a Gaussian-uniform mixture model. We derive an optimization algorithm that alternates between the unsupervised detection of outliers using expectation-maximization, and the supervised training with cleaned samples using stochastic gradient descent. DeepGUM is able to adapt to a continuously evolving outlier distribution, avoiding to manually impose any threshold on the proportion of outliers in the training set. Extensive experimental evaluations on four different tasks (facial and fashion landmark detection, age and head pose estimation) lead us to conclude that our novel robust technique provides reliability in the presence of various types of noise and protection against a high percentage of outliers.
ER  -


TY  - Preprint
T1  - SOLAR: Deep Structured Latent Representations for Model-Based Reinforcement Learning
A1  - Marvin Zhang
A1  - Sharad Vikram
A1  - Laura Smith
A1  - Pieter Abbeel
A1  - Matthew J. Johnson
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 27 August, 2018
UR  - https://arxiv.org/abs/1808.09105
N2  - Model-based reinforcement learning (RL) methods can be broadly categorized as global model methods, which depend on learning models that provide sensible predictions in a wide range of states, or local model methods, which iteratively refit simple models that are used for policy improvement. While predicting future states that will result from the current actions is difficult, local model methods only attempt to understand system dynamics in the neighborhood of the current policy, making it possible to produce local improvements without ever learning to predict accurately far into the future. The main idea in this paper is that we can learn representations that make it easy to retrospectively infer simple dynamics given the data from the current policy, thus enabling local models to be used for policy learning in complex systems. To that end, we focus on learning representations with probabilistic graphical model (PGM) structure, which allows us to devise an efficient local model method that infers dynamics from real-world rollouts with the PGM as a global prior. We compare our method to other model-based and model-free RL methods on a suite of robotics tasks, including manipulation tasks on a real Sawyer robotic arm directly from camera images. Videos of our results are available at https://sites.google.com/view/solar-iclips
ER  -


TY  - Preprint
T1  - Deep Learning for Stress Field Prediction Using Convolutional Neural Networks
A1  - Zhenguo Nie
A1  - Haoliang Jiang
A1  - Levent Burak Kara
JO  - ArXiv e-prints
Y1  - 27 August, 2018
UR  - https://arxiv.org/abs/1808.08914
N2  - This research presents a deep learning based approach to predict stress fields in the solid material elastic deformation using convolutional neural networks (CNN). Two different architectures are proposed to solve the problem. One is Feature Representation embedded Convolutional Neural Network (FR-CNN) with a single input channel, and the other is Squeeze-and-Excitation Residual network modules embedded Fully Convolutional Neural network (SE-Res-FCN) with multiple input channels. Both the tow architectures are stable and converged reliably in training and testing on GPUs. Accuracy analysis shows that SE-Res-FCN has a significantly smaller mean squared error (MSE) and mean absolute error (MAE) than FR-CNN. Mean relative error (MRE) of the SE-Res-FCN model is about 0.25% with respect to the average ground truth. The validation results indicate that the SE-Res-FCN model can accurately predict the stress field. For stress field prediction, the hierarchical architecture becomes deeper within certain limits, and then its prediction becomes more accurate. Fully trained deep learning models have higher computational efficiency over conventional FEM models, so they have great foreground and potential in structural design and topology optimization.
ER  -


TY  - Preprint
T1  - Adaptive Structural Learning of Deep Belief Network for Medical Examination Data and Its Knowledge Extraction by using C4.5
A1  - Shin Kamada
A1  - Takumi Ichimura
A1  - Toshihide Harada
JO  - ArXiv e-prints
Y1  - 27 August, 2018
UR  - https://arxiv.org/abs/1808.08777
N2  - Deep Learning has a hierarchical network architecture to represent the complicated feature of input patterns. The adaptive structural learning method of Deep Belief Network (DBN) has been developed. The method can discover an optimal number of hidden neurons for given input data in a Restricted Boltzmann Machine (RBM) by neuron generation-annihilation algorithm, and generate a new hidden layer in DBN by the extension of the algorithm. In this paper, the proposed adaptive structural learning of DBN was applied to the comprehensive medical examination data for the cancer prediction. The prediction system shows higher classification accuracy (99.8% for training and 95.5% for test) than the traditional DBN. Moreover, the explicit knowledge with respect to the relation between input and output patterns was extracted from the trained DBN network by C4.5. Some characteristics extracted in the form of IF-THEN rules to find an initial cancer at the early stage were reported in this paper.
ER  -


TY  - Preprint
T1  - Deep Learning: Computational Aspects
A1  - Nicholas Polson
A1  - Vadim Sokolov
JO  - ArXiv e-prints
Y1  - 26 August, 2018
UR  - https://arxiv.org/abs/1808.08618
N2  - In this article we review computational aspects of Deep Learning (DL). Deep learning uses network architectures consisting of hierarchical layers of latent variables to construct predictors for high-dimensional input-output models. Training a deep learning architecture is computationally intensive, and efficient linear algebra libraries is the key for training and inference. Stochastic gradient descent (SGD) optimization and batch sampling are used to learn from massive data sets.
ER  -


TY  - Preprint
T1  - Automatic 3D bi-ventricular segmentation of cardiac images by a shape-constrained multi-task deep learning approach
A1  - Jinming Duan
A1  - Ghalib Bello
A1  - Jo Schlemper
A1  - Wenjia Bai
A1  - Timothy J W Dawes
A1  - Carlo Biffi
A1  - Antonio de Marvao
A1  - Georgia Doumou
A1  - Declan P O&#39;Regan
A1  - Daniel Rueckert
JO  - ArXiv e-prints
Y1  - 28 August, 2018
UR  - https://arxiv.org/abs/1808.08578
N2  - Deep learning approaches have achieved state-of-the-art performance in cardiac magnetic resonance (CMR) image segmentation. However, most approaches have focused on learning image intensity features for segmentation, whereas the incorporation of anatomical shape priors has received less attention. In this paper, we combine a multi-task deep learning approach with atlas propagation to develop a shape-constrained bi-ventricular segmentation pipeline for short-axis CMR volumetric images. The pipeline first employs a fully convolutional network (FCN) that learns segmentation and landmark localisation tasks simultaneously. The architecture of the proposed FCN uses a 2.5D representation, thus combining the computational advantage of 2D FCNs networks and the capability of addressing 3D spatial consistency without compromising segmentation accuracy. Moreover, the refinement step is designed to explicitly enforce a shape constraint and improve segmentation quality. This step is effective for overcoming image artefacts (e.g. due to different breath-hold positions and large slice thickness), which preclude the creation of anatomically meaningful 3D cardiac shapes. The proposed pipeline is fully automated, due to network&#39;s ability to infer landmarks, which are then used downstream in the pipeline to initialise atlas propagation. We validate the pipeline on 1831 healthy subjects and 649 subjects with pulmonary hypertension. Extensive numerical experiments on the two datasets demonstrate that our proposed method is robust and capable of producing accurate, high-resolution and anatomically smooth bi-ventricular 3D models, despite the artefacts in input CMR volumes.
ER  -


TY  - Preprint
T1  - Analyzing Learned Representations of a Deep ASR Performance Prediction Model
A1  - Zied Elloumi
A1  - Laurent Besacier
A1  - Olivier Galibert
A1  - Benjamin Lecouteux
JO  - ArXiv e-prints
Y1  - 28 August, 2018
UR  - https://arxiv.org/abs/1808.08573
N2  - This paper addresses a relatively new task: prediction of ASR performance on unseen broadcast programs. In a previous paper, we presented an ASR performance prediction system using CNNs that encode both text (ASR transcript) and speech, in order to predict word error rate. This work is dedicated to the analysis of speech signal embeddings and text embeddings learnt by the CNN while training our prediction model. We try to better understand which information is captured by the deep model and its relation with different conditioning factors. It is shown that hidden layers convey a clear signal about speech style, accent and broadcast type. We then try to leverage these 3 types of information at training time through multi-task learning. Our experiments show that this allows to train slightly more efficient ASR performance prediction systems that - in addition - simultaneously tag the analyzed utterances according to their speech style, accent and broadcast program origin.
ER  -


TY  - Preprint
T1  - An Incremental Construction of Deep Neuro Fuzzy System for Continual Learning of Non-stationary Data Streams
A1  - Mahardhika Pratama
A1  - Witold Pedrycz
A1  - Geoffrey I. Webb
JO  - ArXiv e-prints
Y1  - 26 August, 2018
UR  - https://arxiv.org/abs/1808.08517
N2  - Existing fuzzy neural networks (FNNs) are mostly developed under a shallow network configuration having lower generalization power than those of deep structures. This paper proposes a novel self-organizing deep fuzzy neural network, namely deep evolving fuzzy neural networks (DEVFNN). Fuzzy rules can be automatically extracted from data streams or removed if they play little role during their lifespan. The structure of the network can be deepened on demand by stacking additional layers using a drift detection method which not only detects the covariate drift, variations of input space, but also accurately identifies the real drift, dynamic changes of both feature space and target space. DEVFNN is developed under the stacked generalization principle via the feature augmentation concept where a recently developed algorithm, namely Generic Classifier (gClass), drives the hidden layer. It is equipped by an automatic feature selection method which controls activation and deactivation of input attributes to induce varying subsets of input features. A deep network simplification procedure is put forward using the concept of hidden layer merging to prevent uncontrollable growth of input space dimension due to the nature of feature augmentation approach in building a deep network structure. DEVFNN works in the sample-wise fashion and is compatible for data stream applications. The efficacy of DEVFNN has been thoroughly evaluated using six datasets with non-stationary properties under the prequential test-then-train protocol. It has been compared with four state-of the art data stream methods and its shallow counterpart where DEVFNN demonstrates improvement of classification accuracy.
ER  -


TY  - Preprint
T1  - Deep-Learning Ensembles for Skin-Lesion Segmentation, Analysis, Classification: RECOD Titans at ISIC Challenge 2018
A1  - Alceu Bissoto
A1  - FÃ¡bio Perez
A1  - VinÃ­cius Ribeiro
A1  - Michel Fornaciali
A1  - Sandra Avila
A1  - Eduardo Valle
JO  - ArXiv e-prints
Y1  - 25 August, 2018
UR  - https://arxiv.org/abs/1808.08480
N2  - This extended abstract describes the participation of RECOD Titans in parts 1 to 3 of the ISIC Challenge 2018 &#34;Skin Lesion Analysis Towards Melanoma Detection&#34; (MICCAI 2018). Although our team has a long experience with melanoma classification and moderate experience with lesion segmentation, the ISIC Challenge 2018 was the very first time we worked on lesion attribute detection. For each task we submitted 3 different ensemble approaches, varying combinations of models and datasets. Our best results on the official testing set, regarding the official metric of each task, were: 0.728 (segmentation), 0.344 (attribute detection) and 0.803 (classification). Those submissions reached, respectively, the 56th, 14th and 9th places.
ER  -


TY  - Preprint
T1  - Guiding Deep Learning System Testing using Surprise Adequacy
A1  - Jinhan Kim
A1  - Robert Feldt
A1  - Shin Yoo
JO  - ArXiv e-prints
Y1  - 25 August, 2018
UR  - https://arxiv.org/abs/1808.08444
N2  - Deep Learning (DL) systems are rapidly being adopted in safety and security critical domains, urgently calling for ways to test their correctness and robustness. Testing of DL systems has traditionally relied on manual collection and labelling of data. Recently, a number of coverage criteria based on neuron activation values have been proposed. These criteria essentially count the number of neurons whose activation during the execution of a DL system satisfied certain properties, such as being above predefined thresholds. However, existing coverage criteria are not sufficiently fine grained to capture subtle behaviours exhibited by DL systems. Moreover, evaluations have focused on showing correlation between adversarial examples and proposed criteria rather than evaluating and guiding their use for actual testing of DL systems. We propose a novel test adequacy criterion for testing of DL systems, called Surprise Adequacy for Deep Learning Systems (SADL), which is based on the behaviour of DL systems with respect to their training data. We measure the surprise of an input as the difference in DL system&#39;s behaviour between the input and the training data (i.e., what was learnt during training), and subsequently develop this as an adequacy criterion: a good test input should be sufficiently but not overtly surprising compared to training data. Empirical evaluation using a range of DL systems from simple image classifiers to autonomous driving car platforms shows that systematic sampling of inputs based on their surprise can improve classification accuracy of DL systems against adversarial examples by up to 77.5% via retraining.
ER  -


TY  - Preprint
T1  - Brain Biomarker Interpretation in ASD Using Deep Learning and fMRI
A1  - Xiaoxiao Li
A1  - Nicha C. Dvornek
A1  - Juntang Zhuang
A1  - Pamela Ventola
A1  - James S. Duncan
JO  - ArXiv e-prints
Y1  - 23 August, 2018
UR  - https://arxiv.org/abs/1808.08296
N2  - Autism spectrum disorder (ASD) is a complex neurodevelopmental disorder. Finding the biomarkers associated with ASD is extremely helpful to understand the underlying roots of the disorder and can lead to earlier diagnosis and more targeted treatment. Although Deep Neural Networks (DNNs) have been applied in functional magnetic resonance imaging (fMRI) to identify ASD, understanding the data-driven computational decision making procedure has not been previously explored. Therefore, in this work, we address the problem of interpreting reliable biomarkers associated with identifying ASD; specifically, we propose a 2-stage method that classifies ASD and control subjects using fMRI images and interprets the saliency features activated by the classifier. First, we trained an accurate DNN classifier. Then, for detecting the biomarkers, different from the DNN visualization works in computer vision, we take advantage of the anatomical structure of brain fMRI and develop a frequency-normalized sampling method to corrupt images. Furthermore, in the ASD vs. control subjects classification scenario, we provide a new approach to detect and characterize important brain features into three categories. The biomarkers we found by the proposed method are robust and consistent with previous findings in the literature. We also validate the detected biomarkers by neurological function decoding and comparing with the DNN activation maps.
ER  -


TY  - Preprint
T1  - Deep multiscale convolutional feature learning for weakly supervised localization of chest pathologies in X-ray images
A1  - Suman Sedai
A1  - Dwarikanath Mahapatra
A1  - Zongyuan Ge
A1  - Rajib Chakravorty
A1  - Rahil Garnavi
JO  - ArXiv e-prints
Y1  - 22 August, 2018
UR  - https://arxiv.org/abs/1808.08280
N2  - Localization of chest pathologies in chest X-ray images is a challenging task because of their varying sizes and appearances. We propose a novel weakly supervised method to localize chest pathologies using class aware deep multiscale feature learning. Our method leverages intermediate feature maps from CNN layers at different stages of a deep network during the training of a classification model using image level annotations of pathologies. During the training phase, a set of \emph{layer relevance weights} are learned for each pathology class and the CNN is optimized to perform pathology classification by convex combination of feature maps from both shallow and deep layers using the learned weights. During the test phase, to localize the predicted pathology, the multiscale attention map is obtained by convex combination of class activation maps from each stage using the \emph{layer relevance weights} learned during the training phase. We have validated our method using 112000 X-ray images and compared with the state-of-the-art localization methods. We experimentally demonstrate that the proposed weakly supervised method can improve the localization performance of small pathologies such as nodule and mass while giving comparable performance for bigger pathologies e.g., Cardiomegaly
ER  -


TY  - Preprint
T1  - Improving Breast Cancer Detection using Symmetry Information with Deep Learning
A1  - Yeman Brhane Hagos
A1  - Albert Gubern Merida
A1  - Jonas Teuwen
JO  - ArXiv e-prints
Y1  - 17 August, 2018
UR  - https://arxiv.org/abs/1808.08273
N2  - Convolutional Neural Networks (CNN) have had a huge success in many areas of computer vision and medical image analysis. However, there is still an immense potential for performance improvement in mammogram breast cancer detection Computer-Aided Detection (CAD) systems by integrating all the information that the radiologist utilizes, such as symmetry and temporal data. In this work, we proposed a patch based multi-input CNN that learns symmetrical difference to detect breast masses. The network was trained on a large-scale dataset of 28294 mammogram images. The performance was compared to a baseline architecture without symmetry context using Area Under the ROC Curve (AUC) and Competition Performance Metric (CPM). At candidate level, AUC value of 0.933 with 95% confidence interval of [0.920, 0.954] was obtained when symmetry information is incorporated in comparison with baseline architecture which yielded AUC value of 0.929 with [0.919, 0.947] confidence interval. By incorporating symmetrical information, although there was no a significant candidate level performance again (p = 0.111), we have found a compelling result at exam level with CPM value of 0.733 (p = 0.001). We believe that including temporal data, and adding benign class to the dataset could improve the detection performance.
ER  -


TY  - Preprint
T1  - An Improvement of Data Classification Using Random Multimodel Deep Learning (RMDL)
A1  - Mojtaba Heidarysafa
A1  - Kamran Kowsari
A1  - Donald E. Brown
A1  - Kiana Jafari Meimandi
A1  - Laura E. Barnes
JO  - ArXiv e-prints
Y1  - 22 August, 2018
UR  - https://arxiv.org/abs/1808.08121
N2  - The exponential growth in the number of complex datasets every year requires more enhancement in machine learning methods to provide robust and accurate data classification. Lately, deep learning approaches have achieved surpassing results in comparison to previous machine learning algorithms. However, finding the suitable structure for these models has been a challenge for researchers. This paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble, deep learning approach for classification. RMDL solves the problem of finding the best deep learning structure and architecture while simultaneously improving robustness and accuracy through ensembles of deep learning architectures. In short, RMDL trains multiple randomly generated models of Deep Neural Network (DNN), Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) in parallel and combines their results to produce better result of any of those models individually. In this paper, we describe RMDL model and compare the results for image and text classification as well as face recognition. We used MNIST and CIFAR-10 datasets as ground truth datasets for image classification and WOS, Reuters, IMDB, and 20newsgroup datasets for text classification. Lastly, we used ORL dataset to compare the model performance on face recognition task.
ER  -


TY  - Preprint
T1  - Multi-scenario deep learning for multi-speaker source separation
A1  - Jeroen Zegers
A1  - Hugo Van hamme
JO  - ArXiv e-prints
Y1  - 24 August, 2018
UR  - https://arxiv.org/abs/1808.08095
N2  - Research in deep learning for multi-speaker source separation has received a boost in the last years. However, most studies are restricted to mixtures of a specific number of speakers, called a specific scenario. While some works included experiments for different scenarios, research towards combining data of different scenarios or creating a single model for multiple scenarios have been very rare. In this work it is shown that data of a specific scenario is relevant for solving another scenario. Furthermore, it is concluded that a single model, trained on different scenarios is capable of matching performance of scenario specific models.
ER  -


TY  - Preprint
T1  - Atherosclerotic carotid plaques on panoramic imaging: an automatic detection using deep learning with small dataset
A1  - Lazar Kats
A1  - Marilena Vered
A1  - Ayelet Zlotogorski-Hurvitz
A1  - Itai Harpaz
JO  - ArXiv e-prints
Y1  - 24 August, 2018
UR  - https://arxiv.org/abs/1808.08093
N2  - Stroke is the second most frequent cause of death worldwide with a considerable economic burden on the health systems. In about 15% of strokes, atherosclerotic carotid plaques (ACPs) constitute the main etiological factor. Early detection of ACPs may have a key-role for preventing strokes by managing the patient a-priory to the occurrence of the damage. ACPs can be detected on panoramic images. As these are one of the most common images performed for routine dental practice, they can be used as a source of available data for computerized methods of automatic detection in order to significantly increase timely diagnosis of ACPs. Recently, there has been a definite breakthrough in the field of analysis of medical images due to the use of deep learning based on neural networks. These methods, however have been barely used in dentistry. In this study we used the Faster Region-based Convolutional Network (Faster R-CNN) for deep learning. We aimed to assess the operation of the algorithm on a small database of 65 panoramic images. Due to a small amount of available training data, we had to use data augmentation by changing the brightness and randomly flipping and rotating cropped regions of interest in multiple angles. Receiver Operating Characteristic (ROC) analysis was performed to calculate the accuracy of detection. ACP was detected with a sensitivity of 75%, specificity of 80% and an accuracy of 83%. The ROC analysis showed a significant Area Under Curve (AUC) difference from 0.5. Our novelty lies in that we have showed the efficiency of the Faster R-CNN algorithm in detecting ACPs on routine panoramic images based on a small database. There is a need to further improve the application of the algorithm to the level of introducing this methodology in routine dental practice in order to enable us to prevent stroke events.
ER  -


TY  - Preprint
T1  - From Hand-Crafted to Deep Learning-based Cancer Radiomics: Challenges and Opportunities
A1  - Parnian Afshar
A1  - Arash Mohammadi
A1  - Konstantinos N. Plataniotis
A1  - Anastasia Oikonomou
A1  - Habib Benali
JO  - ArXiv e-prints
Y1  - 27 August, 2018
UR  - https://arxiv.org/abs/1808.07954
N2  - Recent advancements in signal processing and machine learning coupled with developments of electronic medical record keeping in hospitals and the availability of extensive set of medical images through internal/external communication systems, have resulted in a recent surge of significant interest in &#34;Radiomics&#34;. Radiomics is an emerging and relatively new research field, which refers to extracting semi-quantitative and/or quantitative features from medical images with the goal of developing predictive and/or prognostic models, and is expected to become a critical component for integration of image-derived information for personalized treatment in the near future. The conventional Radiomics workflow is typically based on extracting pre-designed features (also referred to as hand-crafted or engineered features) from a segmented region of interest. Nevertheless, recent advancements in deep learning have caused trends towards deep learning-based Radiomics (also referred to as discovery Radiomics). Capitalizing on the advantageous of these two approaches, there are also hybrid solutions developed to exploit the potentials of multiple data sources. Considering the variety of approaches to Radiomics, further improvements require a comprehensive and integrated sketch, which is the goal of this article. This manuscript provides a unique interdisciplinary perspective on Radiomics by discussing state-of-the-art signal processing solutions in the context of cancer Radiomics.
ER  -


TY  - Preprint
T1  - High frame-rate cardiac ultrasound imaging with deep learning
A1  - Ortal Senouf
A1  - Sanketh Vedula
A1  - Grigoriy Zurakhov
A1  - Alex M. Bronstein
A1  - Michael Zibulevsky
A1  - Oleg Michailovich
A1  - Dan Adam
A1  - David Blondheim
JO  - ArXiv e-prints
Y1  - 23 August, 2018
UR  - https://arxiv.org/abs/1808.07823
N2  - Cardiac ultrasound imaging requires a high frame rate in order to capture rapid motion. This can be achieved by multi-line acquisition (MLA), where several narrow-focused received lines are obtained from each wide-focused transmitted line. This shortens the acquisition time at the expense of introducing block artifacts. In this paper, we propose a data-driven learning-based approach to improve the MLA image quality. We train an end-to-end convolutional neural network on pairs of real ultrasound cardiac data, acquired through MLA and the corresponding single-line acquisition (SLA). The network achieves a significant improvement in image quality for both $5-$ and $7-$line MLA resulting in a decorrelation measure similar to that of SLA while having the frame rate of MLA.
ER  -


TY  - Preprint
T1  - High quality ultrasonic multi-line transmission through deep learning
A1  - Sanketh Vedula
A1  - Ortal Senouf
A1  - Grigoriy Zurakhov
A1  - Alex M. Bronstein
A1  - Michael Zibulevsky
A1  - Oleg Michailovich
A1  - Dan Adam
A1  - Diana Gaitini
JO  - ArXiv e-prints
Y1  - 23 August, 2018
UR  - https://arxiv.org/abs/1808.07819
N2  - Frame rate is a crucial consideration in cardiac ultrasound imaging and 3D sonography. Several methods have been proposed in the medical ultrasound literature aiming at accelerating the image acquisition. In this paper, we consider one such method called \textit{multi-line transmission} (MLT), in which several evenly separated focused beams are transmitted simultaneously. While MLT reduces the acquisition time, it comes at the expense of a heavy loss of contrast due to the interactions between the beams (cross-talk artifact). In this paper, we introduce a data-driven method to reduce the artifacts arising in MLT. To this end, we propose to train an end-to-end convolutional neural network consisting of correction layers followed by a constant apodization layer. The network is trained on pairs of raw data obtained through MLT and the corresponding \textit{single-line transmission} (SLT) data. Experimental evaluation demonstrates significant improvement both in the visual image quality and in objective measures such as contrast ratio and contrast-to-noise ratio, while preserving resolution unlike traditional apodization-based methods. We show that the proposed method is able to generalize well across different patients and anatomies on real and phantom data.
ER  -


TY  - Preprint
T1  - Adversarial Attacks on Deep-Learning Based Radio Signal Classification
A1  - Meysam Sadeghi
A1  - Erik G. Larsson
JO  - ArXiv e-prints
Y1  - 23 August, 2018
UR  - https://arxiv.org/abs/1808.07713
N2  - Deep learning (DL), despite its enormous success in many computer vision and language processing applications, is exceedingly vulnerable to adversarial attacks. We consider the use of DL for radio signal (modulation) classification tasks, and present practical methods for the crafting of white-box and universal black-box adversarial attacks in that application. We show that these attacks can considerably reduce the classification performance, with extremely small perturbations of the input. In particular, these attacks are significantly more powerful than classical jamming attacks, which raises significant security and robustness concerns in the use of DL-based algorithms for the wireless physical layer.
ER  -


TY  - Preprint
T1  - Deep multi-task learning for a geographically-regularized semantic segmentation of aerial images
A1  - Michele Volpi
A1  - Devis Tuia
JO  - ArXiv e-prints
Y1  - 23 August, 2018
UR  - https://arxiv.org/abs/1808.07675
N2  - When approaching the semantic segmentation of overhead imagery in the decimeter spatial resolution range, successful strategies usually combine powerful methods to learn the visual appearance of the semantic classes (e.g. convolutional neural networks) with strategies for spatial regularization (e.g. graphical models such as conditional random fields). In this paper, we propose a method to learn evidence in the form of semantic class likelihoods, semantic boundaries across classes and shallow-to-deep visual features, each one modeled by a multi-task convolutional neural network architecture. We combine this bottom-up information with top-down spatial regularization encoded by a conditional random field model optimizing the label space across a hierarchy of segments with constraints related to structural, spatial and data-dependent pairwise relationships between regions. Our results show that such strategy provide better regularization than a series of strong baselines reflecting state-of-the-art technologies. The proposed strategy offers a flexible and principled framework to include several sources of visual and structural information, while allowing for different degrees of spatial regularization accounting for priors about the expected output structures.
ER  -


TY  - Preprint
T1  - Generating Magnetic Resonance Spectroscopy Imaging Data of Brain Tumours from Linear, Non-Linear and Deep Learning Models
A1  - Nathan J Olliverre
A1  - Guang Yang
A1  - Gregory Slabaugh
A1  - Constantino Carlos Reyes-Aldasoro
A1  - Eduardo Alonso
JO  - ArXiv e-prints
Y1  - 22 August, 2018
UR  - https://arxiv.org/abs/1808.07592
N2  - Magnetic Resonance Spectroscopy (MRS) provides valuable information to help with the identification and understanding of brain tumors, yet MRS is not a widely available medical imaging modality. Aiming to counter this issue, this research draws on the advancements in machine learning techniques in other fields for the generation of artificial data. The generated methods were tested through the evaluation of their output against that of a real-world labelled MRS brain tumor data-set. Furthermore the resultant output from the generative techniques were each used to train separate traditional classifiers which were tested on a subset of the real MRS brain tumor dataset. The results suggest that there exist methods capable of producing accurate, ground truth based MRS voxels. These findings indicate that through generative techniques, large datasets can be made available for training deep, learning models for the use in brain tumor diagnosis.
ER  -


TY  - Preprint
T1  - Deep Association Learning for Unsupervised Video Person Re-identification
A1  - Yanbei Chen
A1  - Xiatian Zhu
A1  - Shaogang Gong
JO  - ArXiv e-prints
Y1  - 22 August, 2018
UR  - https://arxiv.org/abs/1808.07301
N2  - Deep learning methods have started to dominate the research progress of video-based person re-identification (re-id). However, existing methods mostly consider supervised learning, which requires exhaustive manual efforts for labelling cross-view pairwise data. Therefore, they severely lack scalability and practicality in real-world video surveillance applications. In this work, to address the video person re-id task, we formulate a novel Deep Association Learning (DAL) scheme, the first end-to-end deep learning method using none of the identity labels in model initialisation and training. DAL learns a deep re-id matching model by jointly optimising two margin-based association losses in an end-to-end manner, which effectively constrains the association of each frame to the best-matched intra-camera representation and cross-camera representation. Existing standard CNNs can be readily employed within our DAL scheme. Experiment results demonstrate that our proposed DAL significantly outperforms current state-of-the-art unsupervised video person re-id methods on three benchmarks: PRID 2011, iLIDS-VID and MARS.
ER  -


TY  - Preprint
T1  - DeepCorr: Strong Flow Correlation Attacks on Tor Using Deep Learning
A1  - Milad Nasr
A1  - Alireza Bahramali
A1  - Amir Houmansadr
JO  - ArXiv e-prints
Y1  - 22 August, 2018
UR  - https://arxiv.org/abs/1808.07285
N2  - Flow correlation is the core technique used in a multitude of deanonymization attacks on Tor. Despite the importance of flow correlation attacks on Tor, existing flow correlation techniques are considered to be ineffective and unreliable in linking Tor flows when applied at a large scale, i.e., they impose high rates of false positive error rates or require impractically long flow observations to be able to make reliable correlations. In this paper, we show that, unfortunately, flow correlation attacks can be conducted on Tor traffic with drastically higher accuracies than before by leveraging emerging learning mechanisms. We particularly design a system, called DeepCorr, that outperforms the state-of-the-art by significant margins in correlating Tor connections. DeepCorr leverages an advanced deep learning architecture to learn a flow correlation function tailored to Tor&#39;s complex network this is in contrast to previous works&#39; use of generic statistical correlation metrics to correlated Tor flows. We show that with moderate learning, DeepCorr can correlate Tor connections (and therefore break its anonymity) with accuracies significantly higher than existing algorithms, and using substantially shorter lengths of flow observations. For instance, by collecting only about 900 packets of each target Tor flow (roughly 900KB of Tor data), DeepCorr provides a flow correlation accuracy of 96% compared to 4% by the state-of-the-art system of RAPTOR using the same exact setting.
ER  -


TY  - Preprint
T1  - A Survey of Modern Object Detection Literature using Deep Learning
A1  - Karanbir Singh Chahal
A1  - Kuntal Dey
JO  - ArXiv e-prints
Y1  - 22 August, 2018
UR  - https://arxiv.org/abs/1808.07256
N2  - Object detection is the identification of an object in the image along with its localisation and classification. It has wide spread applications and is a critical component for vision based software systems. This paper seeks to perform a rigorous survey of modern object detection algorithms that use deep learning. As part of the survey, the topics explored include various algorithms, quality metrics, speed/size trade offs and training methodologies. This paper focuses on the two types of object detection algorithms- the SSD class of single step detectors and the Faster R-CNN class of two step detectors. Techniques to construct detectors that are portable and fast on low powered devices are also addressed by exploring new lightweight convolutional base architectures. Ultimately, a rigorous review of the strengths and weaknesses of each detector leads us to the present state of the art.
ER  -


TY  - Preprint
T1  - Approximating Poker Probabilities with Deep Learning
A1  - Brandon Da Silva
JO  - ArXiv e-prints
Y1  - 22 August, 2018
UR  - https://arxiv.org/abs/1808.07220
N2  - Many poker systems, whether created with heuristics or machine learning, rely on the probability of winning as a key input. However calculating the precise probability using combinatorics is an intractable problem, so instead we approximate it. Monte Carlo simulation is an effective technique that can be used to approximate the probability that a player will win and/or tie a hand. However, without the use of a memory-intensive lookup table or a supercomputer, it becomes infeasible to run millions of times when training an agent with self-play. To combat the space-time tradeoff, we use deep learning to approximate the probabilities obtained from the Monte Carlo simulation with high accuracy. The learned model proves to be a lightweight alternative to Monte Carlo simulation, which ultimately allows us to use the probabilities as inputs during self-play efficiently. The source code and optimized neural network can be found at https://github.com/brandinho/Poker-Probability-Approximation
ER  -


TY  - Preprint
T1  - Fisher Information and Natural Gradient Learning of Random Deep Networks
A1  - Shun-ichi Amari
A1  - Ryo Karakida
A1  - Masafumi Oizumi
JO  - ArXiv e-prints
Y1  - 21 August, 2018
UR  - https://arxiv.org/abs/1808.07172
N2  - A deep neural network is a hierarchical nonlinear model transforming input signals to output signals. Its input-output relation is considered to be stochastic, being described for a given input by a parameterized conditional probability distribution of outputs. The space of parameters consisting of weights and biases is a Riemannian manifold, where the metric is defined by the Fisher information matrix. The natural gradient method uses the steepest descent direction in a Riemannian manifold, so it is effective in learning, avoiding plateaus. It requires inversion of the Fisher information matrix, however, which is practically impossible when the matrix has a huge number of dimensions. Many methods for approximating the natural gradient have therefore been introduced. The present paper uses statistical neurodynamical method to reveal the properties of the Fisher information matrix in a net of random connections under the mean field approximation. We prove that the Fisher information matrix is unit-wise block diagonal supplemented by small order terms of off-block-diagonal elements, which provides a justification for the quasi-diagonal natural gradient method by Y. Ollivier. A unitwise block-diagonal Fisher metrix reduces to the tensor product of the Fisher information matrices of single units. We further prove that the Fisher information matrix of a single unit has a simple reduced form, a sum of a diagonal matrix and a rank 2 matrix of weight-bias correlations. We obtain the inverse of Fisher information explicitly. We then have an explicit form of the natural gradient, without relying on the numerical matrix inversion, which drastically speeds up stochastic gradient learning.
ER  -


TY  - Preprint
T1  - Deep Learned Full-3D Object Completion from Single View
A1  - Dario Rethage
A1  - Federico Tombari
A1  - Felix Achilles
A1  - Nassir Navab
JO  - ArXiv e-prints
Y1  - 21 August, 2018
UR  - https://arxiv.org/abs/1808.06843
N2  - 3D geometry is a very informative cue when interacting with and navigating an environment. This writing proposes a new approach to 3D reconstruction and scene understanding, which implicitly learns 3D geometry from depth maps pairing a deep convolutional neural network architecture with an auto-encoder. A data set of synthetic depth views and voxelized 3D representations is built based on ModelNet, a large-scale collection of CAD models, to train networks. The proposed method offers a significant advantage over current, explicit reconstruction methods in that it learns key geometric features offline and makes use of those to predict the most probable reconstruction of an unseen object. The relatively small network, consisting of roughly 4 million weights, achieves a 92.9% reconstruction accuracy at a 30x30x30 resolution through the use of a pre-trained decompression layer. This is roughly 1/4 the weights of the current leading network. The fast execution time of the model makes it suitable for real-time applications.
ER  -


TY  - Preprint
T1  - Synthetic Patient Generation: A Deep Learning Approach Using Variational Autoencoders
A1  - Ally Salim Jr
JO  - ArXiv e-prints
Y1  - 20 August, 2018
UR  - https://arxiv.org/abs/1808.06444
N2  - Artificial Intelligence in healthcare is a new and exciting frontier and the possibilities are endless. With deep learning approaches beating human performances in many areas, the logical next step is to attempt their application in the health space. For these and other Machine Learning approaches to produce good results and have their potential realized, the need for, and importance of, large amounts of accurate data is second to none. This is a challenge faced by many industries and more so in the healthcare space. We present an approach of using Variational Autoencoders (VAE&#39;s) as an approach to generating more data for training deeper networks, as well as uncovering underlying patterns in diagnoses and the patients suffering from them. By training a VAE, on available data, it was able to learn the latent distribution of the patient features given the diagnosis. It is then possible, after training, to sample from the learnt latent distribution to generate new accurate patient records given the patient diagnosis.
ER  -


TY  - Preprint
T1  - DeeSIL: Deep-Shallow Incremental Learning
A1  - Eden Belouadah
A1  - Adrian Popescu
JO  - ArXiv e-prints
Y1  - 20 August, 2018
UR  - https://arxiv.org/abs/1808.06396
N2  - Incremental Learning (IL) is an interesting AI problem when the algorithm is assumed to work on a budget. This is especially true when IL is modeled using a deep learning approach, where two com- plex challenges arise due to limited memory, which induces catastrophic forgetting and delays related to the retraining needed in order to incorpo- rate new classes. Here we introduce DeeSIL, an adaptation of a known transfer learning scheme that combines a fixed deep representation used as feature extractor and learning independent shallow classifiers to in- crease recognition capacity. This scheme tackles the two aforementioned challenges since it works well with a limited memory budget and each new concept can be added within a minute. Moreover, since no deep re- training is needed when the model is incremented, DeeSIL can integrate larger amounts of initial data that provide more transferable features. Performance is evaluated on ImageNet LSVRC 2012 against three state of the art algorithms. Results show that, at scale, DeeSIL performance is 23 and 33 points higher than the best baseline when using the same and more initial data respectively.
ER  -


TY  - Preprint
T1  - Learning to Learn from Web Data through Deep Semantic Embeddings
A1  - Raul Gomez
A1  - Lluis Gomez
A1  - Jaume Gibert
A1  - Dimosthenis Karatzas
JO  - ArXiv e-prints
Y1  - 20 August, 2018
UR  - https://arxiv.org/abs/1808.06368
N2  - In this paper we propose to learn a multimodal image and text embedding from Web and Social Media data, aiming to leverage the semantic knowledge learnt in the text domain and transfer it to a visual model for semantic image retrieval. We demonstrate that the pipeline can learn from images with associated text without supervision and perform a thourough analysis of five different text embeddings in three different benchmarks. We show that the embeddings learnt with Web and Social Media data have competitive performances over supervised methods in the text based image retrieval task, and we clearly outperform state of the art in the MIRFlickr dataset when training in the target data. Further we demonstrate how semantic multimodal image retrieval can be performed using the learnt embeddings, going beyond classical instance-level retrieval problems. Finally, we present a new dataset, InstaCities1M, composed by Instagram images and their associated texts that can be used for fair comparison of image-text embeddings.
ER  -


TY  - Preprint
T1  - Deep learning, deep change? Mapping the development of the Artificial Intelligence General Purpose Technology
A1  - J. Klinger
A1  - J. Mateos-Garcia
A1  - K. Stathoulopoulos
JO  - ArXiv e-prints
Y1  - 20 August, 2018
UR  - https://arxiv.org/abs/1808.06355
N2  - General Purpose Technologies (GPTs) that can be applied in many industries are an important driver of economic growth and national and regional competitiveness. In spite of this, the geography of their development and diffusion has not received significant attention in the literature. We address this with an analysis of Deep Learning (DL), a core technique in Artificial Intelligence (AI) increasingly being recognized as the latest GPT. We identify DL papers in a novel dataset from ArXiv, a popular preprints website, and use CrunchBase, a technology business directory to measure industrial capabilities related to it. After showing that DL conforms with the definition of a GPT, having experienced rapid growth and diffusion into new fields where it has generated an impact, we describe changes in its geography. Our analysis shows China&#39;s rise in AI rankings and relative decline in several European countries. We also find that initial volatility in the geography of DL has been followed by consolidation, suggesting that the window of opportunity for new entrants might be closing down as new DL research hubs become dominant. Finally, we study the regional drivers of DL clustering. We find that competitive DL clusters tend to be based in regions combining research and industrial activities related to it. This could be because GPT developers and adopters located close to each other can collaborate and share knowledge more easily, thus overcoming coordination failures in GPT deployment. Our analysis also reveals a Chinese comparative advantage in DL after we control for other explanatory factors, perhaps underscoring the importance of access to data and supportive policies for the successful development of this complex, `omni-use&#39; technology.
ER  -


TY  - Preprint
T1  - Deep Multiple Instance Learning for Airplane Detection in High Resolution Imagery
A1  - Mohammad Reza Mohammadi
JO  - ArXiv e-prints
Y1  - 19 August, 2018
UR  - https://arxiv.org/abs/1808.06178
N2  - Automatic airplane detection in aerial imagery has a variety of applications. Two of the major challenges in this area are variations in scale and direction of the airplanes. In order to solve these challenges, we present a rotation-and-scale invariant airplane proposal generator. This proposal generator is developed based on the symmetric and regular boundaries of airplanes from the top view called symmetric line segments (SLS). Then, the generated proposals are used to train a deep convolutional neural network for removing non-airplane proposals. Since each airplane can have multiple SLS proposals, where some of them are not in the direction of the fuselage, we collect all proposals correspond to one ground truth as a positive bag and the others as the negative instances. To have multiple instance deep learning, we modify the training approach of the network to learn from each positive bag at least one instance as well as all negative instances. Finally, we employ non-maximum suppression to remove duplicate detections. Our experiments on NWPU VHR-10 dataset show that our method is a promising approach for automatic airplane detection in very high resolution images. Moreover, the proposed algorithm can estimate the direction of the airplanes using box-level annotations as an extra achievement.
ER  -


TY  - Preprint
T1  - Neural Body Fitting: Unifying Deep Learning and Model-Based Human Pose and Shape Estimation
A1  - Mohamed Omran
A1  - Christoph Lassner
A1  - Gerard Pons-Moll
A1  - Peter V. Gehler
A1  - Bernt Schiele
JO  - ArXiv e-prints
Y1  - 17 August, 2018
UR  - https://arxiv.org/abs/1808.05942
N2  - Direct prediction of 3D body pose and shape remains a challenge even for highly parameterized deep learning models. Mapping from the 2D image space to the prediction space is difficult: perspective ambiguities make the loss function noisy and training data is scarce. In this paper, we propose a novel approach (Neural Body Fitting (NBF)). It integrates a statistical body model within a CNN, leveraging reliable bottom-up semantic body part segmentation and robust top-down body model constraints. NBF is fully differentiable and can be trained using 2D and 3D annotations. In detailed experiments, we analyze how the components of our model affect performance, especially the use of part segmentations as an explicit intermediate representation, and present a robust, efficiently trainable framework for 3D human pose estimation from 2D images with competitive results on standard benchmarks. Code will be made available at http://github.com/mohomran/neural_body_fitting
ER  -


TY  - Preprint
T1  - Epithelium segmentation using deep learning in H&amp;E-stained prostate specimens with immunohistochemistry as reference standard
A1  - Wouter Bulten
A1  - PÃ©ter BÃ¡ndi
A1  - Jeffrey Hoven
A1  - Rob van de Loo
A1  - Johannes Lotz
A1  - Nick Weiss
A1  - Jeroen van der Laak
A1  - Bram van Ginneken
A1  - Christina Hulsbergen-van de Kaa
A1  - Geert Litjens
JO  - ArXiv e-prints
Y1  - 17 August, 2018
UR  - https://arxiv.org/abs/1808.05883
N2  - Prostate cancer (PCa) is graded by pathologists by examining the architectural pattern of cancerous epithelial tissue on hematoxylin and eosin (H&amp;E) stained slides. Given the importance of gland morphology, automatically differentiating between glandular epithelial tissue and other tissues is an important prerequisite for the development of automated methods for detecting PCa. We propose a new method, using deep learning, for automatically segmenting epithelial tissue in digitized prostatectomy slides. We employed immunohistochemistry (IHC) to render the ground truth less subjective and more precise compared to manual outlining on H&amp;E slides, especially in areas with high-grade and poorly differentiated PCa. Our dataset consisted of 102 tissue blocks, including both low and high grade PCa. From each block a single new section was cut, stained with H&amp;E, scanned, restained using P63 and CK8/18 to highlight the epithelial structure, and scanned again. The H&amp;E slides were co-registered to the IHC slides. On a subset of the IHC slides we applied color deconvolution, corrected stain errors manually, and trained a U-Net to perform segmentation of epithelial structures. Whole-slide segmentation masks generated by the IHC U-Net were used to train a second U-Net on H&amp;E. Our system makes precise cell-level segmentations and segments both intact glands as well as individual (tumor) epithelial cells. We achieved an F1-score of 0.895 on a hold-out test set and 0.827 on an external reference set from a different center. We envision this segmentation as being the first part of a fully automated prostate cancer detection and grading pipeline.
ER  -


TY  - Preprint
T1  - Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study
A1  - Aditya Siddhant
A1  - Zachary C. Lipton
JO  - ArXiv e-prints
Y1  - 24 September, 2018
UR  - https://arxiv.org/abs/1808.05697
N2  - Several recent papers investigate Active Learning (AL) for mitigating the data dependence of deep learning for natural language processing. However, the applicability of AL to real-world problems remains an open question. While in supervised learning, practitioners can try many different methods, evaluating each against a validation set before selecting a model, AL affords no such luxury. Over the course of one AL run, an agent annotates its dataset exhausting its labeling budget. Thus, given a new task, an active learner has no opportunity to compare models and acquisition functions. This paper provides a large scale empirical study of deep active learning, addressing multiple tasks and, for each, multiple datasets, multiple models, and a full suite of acquisition functions. We find that across all settings, Bayesian active learning by disagreement, using uncertainty estimates provided either by Dropout or Bayes-by Backprop significantly improves over i.i.d. baselines and usually outperforms classic uncertainty sampling.
ER  -


TY  - Preprint
T1  - Statistical Analysis Driven Optimized Deep Learning System for Intrusion Detection
A1  - Cosimo Ieracitano
A1  - Ahsan Adeel
A1  - Mandar Gogate
A1  - Kia Dashtipour
A1  - Francesco Carlo Morabito
A1  - Hadi Larijani
A1  - Ali Raza
A1  - Amir Hussain
JO  - ArXiv e-prints
Y1  - 16 August, 2018
UR  - https://arxiv.org/abs/1808.05633
N2  - Attackers have developed ever more sophisticated and intelligent ways to hack information and communication technology systems. The extent of damage an individual hacker can carry out upon infiltrating a system is well understood. A potentially catastrophic scenario can be envisaged where a nation-state intercepting encrypted financial data gets hacked. Thus, intelligent cybersecurity systems have become inevitably important for improved protection against malicious threats. However, as malware attacks continue to dramatically increase in volume and complexity, it has become ever more challenging for traditional analytic tools to detect and mitigate threat. Furthermore, a huge amount of data produced by large networks has made the recognition task even more complicated and challenging. In this work, we propose an innovative statistical analysis driven optimized deep learning system for intrusion detection. The proposed intrusion detection system (IDS) extracts optimized and more correlated features using big data visualization and statistical analysis methods (human-in-the-loop), followed by a deep autoencoder for potential threat detection. Specifically, a pre-processing module eliminates the outliers and converts categorical variables into one-hot-encoded vectors. The feature extraction module discard features with null values and selects the most significant features as input to the deep autoencoder model (trained in a greedy-wise manner). The NSL-KDD dataset from the Canadian Institute for Cybersecurity is used as a benchmark to evaluate the feasibility and effectiveness of the proposed architecture. Simulation results demonstrate the potential of our proposed system and its outperformance as compared to existing state-of-the-art methods and recently published novel approaches. Ongoing work includes further optimization and real-time evaluation of our proposed IDS.
ER  -


TY  - Preprint
T1  - Anatomy Of High-Performance Deep Learning Convolutions On SIMD Architectures
A1  - Evangelos Georganas
A1  - Sasikanth Avancha
A1  - Kunal Banerjee
A1  - Dhiraj Kalamkar
A1  - Greg Henry
A1  - Hans Pabst
A1  - Alexander Heinecke
JO  - ArXiv e-prints
Y1  - 20 August, 2018
UR  - https://arxiv.org/abs/1808.05567
N2  - Convolution layers are prevalent in many classes of deep neural networks, including Convolutional Neural Networks (CNNs) which provide state-of-the-art results for tasks like image recognition, neural machine translation and speech recognition. The computationally expensive nature of a convolution operation has led to the proliferation of implementations including matrix-matrix multiplication formulation, and direct convolution primarily targeting GPUs. In this paper, we introduce direct convolution kernels for x86 architectures, in particular for Xeon and XeonPhi systems, which are implemented via a dynamic compilation approach. Our JIT-based implementation shows close to theoretical peak performance, depending on the setting and the CPU architecture at hand. We additionally demonstrate how these JIT-optimized kernels can be integrated into a lightweight multi-node graph execution model. This illustrates that single- and multi-node runs yield high efficiencies and high image-throughputs when executing state-of-the-art image recognition tasks on CPUs.
ER  -


TY  - Preprint
T1  - DRLGENCERT: Deep Learning-based Automated Testing of Certificate Verification in SSL/TLS Implementations
A1  - Chao Chen
A1  - Wenrui Diao
A1  - Yingpei Zeng
A1  - Shanqing Guo
A1  - Chengyu Hu
JO  - ArXiv e-prints
Y1  - 16 August, 2018
UR  - https://arxiv.org/abs/1808.05444
N2  - The Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols are the foundation of network security. The certificate verification in SSL/TLS implementations is vital and may become the weak link in the whole network ecosystem. In previous works, some research focused on the automated testing of certificate verification, and the main approaches rely on generating massive certificates through randomly combining parts of seed certificates for fuzzing. Although the generated certificates could meet the semantic constraints, the cost is quite heavy, and the performance is limited due to the randomness. To fill this gap, in this paper, we propose DRLGENCERT, the first framework of applying deep reinforcement learning to the automated testing of certificate verification in SSL/TLS implementations. DRLGENCERT accepts ordinary certificates as input and outputs newly generated certificates which could trigger discrepancies with high efficiency. Benefited by the deep reinforcement learning, when generating certificates, our framework could choose the best next action according to the result of a previous modification, instead of simple random combinations. At the same time, we developed a set of new techniques to support the overall design, like new feature extraction method for X.509 certificates, fine-grained differential testing, and so forth. Also, we implemented a prototype of DRLGENCERT and carried out a series of real-world experiments. The results show DRLGENCERT is quite efficient, and we obtained 84,661 discrepancy-triggering certificates from 181,900 certificate seeds, say around 46.5% effectiveness. Also, we evaluated six popular SSL/TLS implementations, including GnuTLS, MatrixSSL, MbedTLS, NSS, OpenSSL, and wolfSSL. DRLGENCERT successfully discovered 23 serious certificate verification flaws, and most of them were previously unknown.
ER  -


TY  - Preprint
T1  - Conceptual Domain Adaptation Using Deep Learning
A1  - Behrang Mehrparvar
A1  - Ricardo Vilalta
JO  - ArXiv e-prints
Y1  - 16 August, 2018
UR  - https://arxiv.org/abs/1808.05355
N2  - Deep learning has recently been shown to be instrumental in the problem of domain adaptation, where the goal is to learn a model on a target domain using a similar --but not identical-- source domain. The rationale for coupling both techniques is the possibility of extracting common concepts across domains. Considering (strictly) local representations, traditional deep learning assumes common concepts must be captured in the same hidden units. We contend that jointly training a model with source and target data using a single deep network is prone to failure when there is inherently lower-level representational discrepancy between the two domains; such discrepancy leads to a misalignment of corresponding concepts in separate hidden units. We introduce a search framework to correctly align high-level representations when training deep networks; such framework leads to the notion of conceptual --as opposed to representational-- domain adaptation.
ER  -


TY  - Preprint
T1  - Tool Breakage Detection using Deep Learning
A1  - Guang Li
A1  - Xin Yang
A1  - Duanbing Chen
A1  - Anxing Song
A1  - Yuke Fang
A1  - Junlin Zhou
JO  - ArXiv e-prints
Y1  - 16 August, 2018
UR  - https://arxiv.org/abs/1808.05347
N2  - In manufacture, steel and other metals are mainly cut and shaped during the fabrication process by computer numerical control (CNC) machines. To keep high productivity and efficiency of the fabrication process, engineers need to monitor the real-time process of CNC machines, and the lifetime management of machine tools. In a real manufacturing process, breakage of machine tools usually happens without any indication, this problem seriously affects the fabrication process for many years. Previous studies suggested many different approaches for monitoring and detecting the breakage of machine tools. However, there still exists a big gap between academic experiments and the complex real fabrication processes such as the high demands of real-time detections, the difficulty in data acquisition and transmission. In this work, we use the spindle current approach to detect the breakage of machine tools, which has the high performance of real-time monitoring, low cost, and easy to install. We analyze the features of the current of a milling machine spindle through tools wearing processes, and then we predict the status of tool breakage by a convolutional neural network(CNN). In addition, we use a BP neural network to understand the reliability of the CNN. The results show that our CNN approach can detect tool breakage with an accuracy of 93%, while the best performance of BP is 80%.
ER  -


TY  - Preprint
T1  - Sequential Behavioral Data Processing Using Deep Learning and the Markov Transition Field in Online Fraud Detection
A1  - Ruinan Zhang
A1  - Fanglan Zheng
A1  - Wei Min
JO  - ArXiv e-prints
Y1  - 15 August, 2018
UR  - https://arxiv.org/abs/1808.05329
N2  - Due to the popularity of the Internet and smart mobile devices, more and more financial transactions and activities have been digitalized. Compared to traditional financial fraud detection strategies using credit-related features, customers are generating a large amount of unstructured behavioral data every second. In this paper, we propose an Recurrent Neural Netword (RNN) based deep-learning structure integrated with Markov Transition Field (MTF) for predicting online fraud behaviors using customer&#39;s interactions with websites or smart-phone apps as a series of states. In practice, we tested and proved that the proposed network structure for processing sequential behavioral data could significantly boost fraud predictive ability comparing with the multilayer perceptron network and distance based classifier with Dynamic Time Warping(DTW) as distance metric.
ER  -


TY  - Preprint
T1  - DeepDownscale: a Deep Learning Strategy for High-Resolution Weather Forecast
A1  - Eduardo R. Rodrigues
A1  - Igor Oliveira
A1  - Renato L. F. Cunha
A1  - Marco A. S. Netto
JO  - ArXiv e-prints
Y1  - 15 August, 2018
UR  - https://arxiv.org/abs/1808.05264
N2  - Running high-resolution physical models is computationally expensive and essential for many disciplines. Agriculture, transportation, and energy are sectors that depend on high-resolution weather models, which typically consume many hours of large High Performance Computing (HPC) systems to deliver timely results. Many users cannot afford to run the desired resolution and are forced to use low resolution output. One simple solution is to interpolate results for visualization. It is also possible to combine an ensemble of low resolution models to obtain a better prediction. However, these approaches fail to capture the redundant information and patterns in the low-resolution input that could help improve the quality of prediction. In this paper, we propose and evaluate a strategy based on a deep neural network to learn a high-resolution representation from low-resolution predictions using weather forecast as a practical use case. We take a supervised learning approach, since obtaining labeled data can be done automatically. Our results show significant improvement when compared with standard practices and the strategy is still lightweight enough to run on modest computer systems.
ER  -


TY  - Preprint
T1  - Deep Learning using K-space Based Data Augmentation for Automated Cardiac MR Motion Artefact Detection
A1  - Ilkay Oksuz
A1  - Bram Ruijsink
A1  - Esther Puyol-Anton
A1  - Aurelien Bustin
A1  - Gastao Cruz
A1  - Claudia Prieto
A1  - Daniel Rueckert
A1  - Julia A. Schnabel
A1  - Andrew P. King
JO  - ArXiv e-prints
Y1  - 31 August, 2018
UR  - https://arxiv.org/abs/1808.05130
N2  - Quality assessment of medical images is essential for complete automation of image processing pipelines. For large population studies such as the UK Biobank, artefacts such as those caused by heart motion are problematic and manual identification is tedious and time-consuming. Therefore, there is an urgent need for automatic image quality assessment techniques. In this paper, we propose a method to automatically detect the presence of motion-related artefacts in cardiac magnetic resonance (CMR) images. As this is a highly imbalanced classification problem (due to the high number of good quality images compared to the low number of images with motion artefacts), we propose a novel k-space based training data augmentation approach in order to address this problem. Our method is based on 3D spatio-temporal Convolutional Neural Networks, and is able to detect 2D+time short axis images with motion artefacts in less than 1ms. We test our algorithm on a subset of the UK Biobank dataset consisting of 3465 CMR images and achieve not only high accuracy in detection of motion artefacts, but also high precision and recall. We compare our approach to a range of state-of-the-art quality assessment methods.
ER  -


TY  - Preprint
T1  - Exploiting Deep Learning for Persian Sentiment Analysis
A1  - Kia Dashtipour
A1  - Mandar Gogate
A1  - Ahsan Adeel
A1  - Cosimo Ieracitano
A1  - Hadi Larijani
A1  - Amir Hussain
JO  - ArXiv e-prints
Y1  - 15 August, 2018
UR  - https://arxiv.org/abs/1808.05077
N2  - The rise of social media is enabling people to freely express their opinions about products and services. The aim of sentiment analysis is to automatically determine subject&#39;s sentiment (e.g., positive, negative, or neutral) towards a particular aspect such as topic, product, movie, news etc. Deep learning has recently emerged as a powerful machine learning technique to tackle a growing demand of accurate sentiment analysis. However, limited work has been conducted to apply deep learning algorithms to languages other than English, such as Persian. In this work, two deep learning models (deep autoencoders and deep convolutional neural networks (CNNs)) are developed and applied to a novel Persian movie reviews dataset. The proposed deep learning models are analyzed and compared with the state-of-the-art shallow multilayer perceptron (MLP) based machine learning model. Simulation results demonstrate the enhanced performance of deep learning over state-of-the-art MLP.
ER  -


TY  - Preprint
T1  - Deep RTS: A Game Environment for Deep Reinforcement Learning in Real-Time Strategy Games
A1  - Per-Arne Andersen
A1  - Morten Goodwin
A1  - Ole-Christoffer Granmo
JO  - ArXiv e-prints
Y1  - 15 August, 2018
UR  - https://arxiv.org/abs/1808.05032
N2  - Reinforcement learning (RL) is an area of research that has blossomed tremendously in recent years and has shown remarkable potential for artificial intelligence based opponents in computer games. This success is primarily due to the vast capabilities of convolutional neural networks, that can extract useful features from noisy and complex data. Games are excellent tools to test and push the boundaries of novel RL algorithms because they give valuable insight into how well an algorithm can perform in isolated environments without the real-life consequences. Real-time strategy games (RTS) is a genre that has tremendous complexity and challenges the player in short and long-term planning. There is much research that focuses on applied RL in RTS games, and novel advances are therefore anticipated in the not too distant future. However, there are to date few environments for testing RTS AIs. Environments in the literature are often either overly simplistic, such as microRTS, or complex and without the possibility for accelerated learning on consumer hardware like StarCraft II. This paper introduces the Deep RTS game environment for testing cutting-edge artificial intelligence algorithms for RTS games. Deep RTS is a high-performance RTS game made specifically for artificial intelligence research. It supports accelerated learning, meaning that it can learn at a magnitude of 50 000 times faster compared to existing RTS games. Deep RTS has a flexible configuration, enabling research in several different RTS scenarios, including partially observable state-spaces and map complexity. We show that Deep RTS lives up to our promises by comparing its performance with microRTS, ELF, and StarCraft II on high-end consumer hardware. Using Deep RTS, we show that a Deep Q-Network agent beats random-play agents over 70% of the time. Deep RTS is publicly available at https://github.com/cair/DeepRTS.
ER  -


TY  - Preprint
T1  - A framework for automatic question generation from text using deep reinforcement learning
A1  - Vishwajeet Kumar
A1  - Ganesh Ramakrishnan
A1  - Yuan-Fang Li
JO  - ArXiv e-prints
Y1  - 15 August, 2018
UR  - https://arxiv.org/abs/1808.04961
N2  - Automatic question generation (QG) is a useful yet challenging task in NLP. Recent neural network-based approaches represent the state-of-the-art in this task, but they are not without shortcomings. Firstly, these models lack the ability to handle rare words and the word repetition problem. Moreover, all previous works optimize the cross-entropy loss, which can induce inconsistencies between training (objective) and testing (evaluation measure). In this paper, we present a novel deep reinforcement learning based framework for automatic question generation. The generator of the framework is a sequence-to-sequence model, enhanced with the copy mechanism to handle the rare-words problem and the coverage mechanism to solve the word repetition problem. The evaluator model of the framework evaluates and assigns a reward to each predicted question. The overall model is trained by learning the parameters of the generator network which maximizes the reward. Our framework allows us to directly optimize any task-specific score including evaluation measures such as BLEU, GLEU, ROUGE-L, {\em etc.}, suitable for sequence to sequence tasks such as QG. Our comprehensive evaluation shows that our approach significantly outperforms state-of-the-art systems on the widely-used SQuAD benchmark in both automatic and human evaluation.
ER  -


TY  - Preprint
T1  - Treepedia 2.0: Applying Deep Learning for Large-scale Quantification of Urban Tree Cover
A1  - Bill Yang Cai
A1  - Xiaojiang Li
A1  - Ian Seiferling
A1  - Carlo Ratti
JO  - ArXiv e-prints
Y1  - 14 August, 2018
UR  - https://arxiv.org/abs/1808.04754
N2  - Recent advances in deep learning have made it possible to quantify urban metrics at fine resolution, and over large extents using street-level images. Here, we focus on measuring urban tree cover using Google Street View (GSV) images. First, we provide a small-scale labelled validation dataset and propose standard metrics to compare the performance of automated estimations of street tree cover using GSV. We apply state-of-the-art deep learning models, and compare their performance to a previously established benchmark of an unsupervised method. Our training procedure for deep learning models is novel; we utilize the abundance of openly available and similarly labelled street-level image datasets to pre-train our model. We then perform additional training on a small training dataset consisting of GSV images. We find that deep learning models significantly outperform the unsupervised benchmark method. Our semantic segmentation model increased mean intersection-over-union (IoU) from 44.10% to 60.42% relative to the unsupervised method and our end-to-end model decreased Mean Absolute Error from 10.04% to 4.67%. We also employ a recently developed method called gradient-weighted class activation map (Grad-CAM) to interpret the features learned by the end-to-end model. This technique confirms that the end-to-end model has accurately learned to identify tree cover area as key features for predicting percentage tree cover. Our paper provides an example of applying advanced deep learning techniques on a large-scale, geo-tagged and image-based dataset to efficiently estimate important urban metrics. The results demonstrate that deep learning models are highly accurate, can be interpretable, and can also be efficient in terms of data-labelling effort and computational resources.
ER  -


TY  - Preprint
T1  - DeepNeuro: an open-source deep learning toolbox for neuroimaging
A1  - Andrew Beers
A1  - James Brown
A1  - Ken Chang
A1  - Katharina Hoebel
A1  - Elizabeth Gerstner
A1  - Bruce Rosen
A1  - Jayashree Kalpathy-Cramer
JO  - ArXiv e-prints
Y1  - 14 August, 2018
UR  - https://arxiv.org/abs/1808.04589
N2  - Translating neural networks from theory to clinical practice has unique challenges, specifically in the field of neuroimaging. In this paper, we present DeepNeuro, a deep learning framework that is best-suited to putting deep learning algorithms for neuroimaging in practical usage with a minimum of friction. We show how this framework can be used to both design and train neural network architectures, as well as modify state-of-the-art architectures in a flexible and intuitive way. We display the pre- and postprocessing functions common in the medical imaging community that DeepNeuro offers to ensure consistent performance of networks across variable users, institutions, and scanners. And we show how pipelines created in DeepNeuro can be concisely packaged into shareable Docker containers and command-line interfaces using DeepNeuro&#39;s pipeline resources.
ER  -


TY  - Preprint
T1  - Deep Randomized Ensembles for Metric Learning
A1  - Hong Xuan
A1  - Richard Souvenir
A1  - Robert Pless
JO  - ArXiv e-prints
Y1  - 4 September, 2018
UR  - https://arxiv.org/abs/1808.04469
N2  - Learning embedding functions, which map semantically related inputs to nearby locations in a feature space supports a variety of classification and information retrieval tasks. In this work, we propose a novel, generalizable and fast method to define a family of embedding functions that can be used as an ensemble to give improved results. Each embedding function is learned by randomly bagging the training labels into small subsets. We show experimentally that these embedding ensembles create effective embedding functions. The ensemble output defines a metric space that improves state of the art performance for image retrieval on CUB-200-2011, Cars-196, In-Shop Clothes Retrieval and VehicleID.
ER  -


TY  - Preprint
T1  - Deep Learning Based Natural Language Processing for End to End Speech Translation
A1  - Sarvesh Patil
JO  - ArXiv e-prints
Y1  - 9 August, 2018
UR  - https://arxiv.org/abs/1808.04459
N2  - Deep Learning methods employ multiple processing layers to learn hierarchial representations of data. They have already been deployed in a humongous number of applications and have produced state-of-the-art results. Recently with the growth in processing power of computers to be able to do high dimensional tensor calculations, Natural Language Processing (NLP) applications have been given a significant boost in terms of efficiency as well as accuracy. In this paper, we will take a look at various signal processing techniques and then application of them to produce a speech-to-text system using Deep Recurrent Neural Networks.
ER  -


TY  - Preprint
T1  - Multimodal Deep Neural Networks using Both Engineered and Learned Representations for Biodegradability Prediction
A1  - Garrett B. Goh
A1  - Khushmeen Sakloth
A1  - Charles Siegel
A1  - Abhinav Vishnu
A1  - Jim Pfaendtner
JO  - ArXiv e-prints
Y1  - 13 September, 2018
UR  - https://arxiv.org/abs/1808.04456
N2  - Deep learning algorithms excel at extracting patterns from raw data, and with large datasets, they have been very successful in computer vision and natural language applications. However, in other domains, large datasets on which to learn representations from may not exist. In this work, we develop a novel multimodal CNN-MLP neural network architecture that utilizes both domain-specific feature engineering as well as learned representations from raw data. We illustrate the effectiveness of such network designs in the chemical sciences, for predicting biodegradability. DeepBioD, a multimodal CNN-MLP network is more accurate than either standalone network designs, and achieves an error classification rate of 0.125 that is 27% lower than the current state-of-the-art. Thus, our work indicates that combining traditional feature engineering with representation learning can be effective, particularly in situations where labeled data is limited.
ER  -


TY  - Preprint
T1  - Deep Learning Super-Resolution Enables Rapid Simultaneous Morphological and Quantitative Magnetic Resonance Imaging
A1  - Akshay Chaudhari
A1  - Zhongnan Fang
A1  - Jin Hyung Lee
A1  - Garry Gold
A1  - Brian Hargreaves
JO  - ArXiv e-prints
Y1  - 7 August, 2018
UR  - https://arxiv.org/abs/1808.04447
N2  - Obtaining magnetic resonance images (MRI) with high resolution and generating quantitative image-based biomarkers for assessing tissue biochemistry is crucial in clinical and research applications. How- ever, acquiring quantitative biomarkers requires high signal-to-noise ratio (SNR), which is at odds with high-resolution in MRI, especially in a single rapid sequence. In this paper, we demonstrate how super-resolution can be utilized to maintain adequate SNR for accurate quantification of the T2 relaxation time biomarker, while simultaneously generating high- resolution images. We compare the efficacy of resolution enhancement using metrics such as peak SNR and structural similarity. We assess accuracy of cartilage T2 relaxation times by comparing against a standard reference method. Our evaluation suggests that SR can successfully maintain high-resolution and generate accurate biomarkers for accelerating MRI scans and enhancing the value of clinical and research MRI.
ER  -


TY  - Preprint
T1  - RedSync : Reducing Synchronization Traffic for Distributed Deep Learning
A1  - Jiarui Fang
A1  - Haohuan Fu
A1  - Guangwen Yang
A1  - Cho-Jui Hsieh
JO  - ArXiv e-prints
Y1  - 13 August, 2018
UR  - https://arxiv.org/abs/1808.04357
N2  - Data parallelism has already become a dominant method to scale Deep Neural Network (DNN) training to multiple computation nodes. Considering that the synchronization of local model or gradient between iterations can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently. Among several recent proposed compression algorithms, Residual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the message size (0.1% of the original size) and still preserve accuracy. However, the literature on compressing deep networks focuses almost exclusively on finding good compression rate, while the efficiency of RGC in real implementation has been less investigated. In this paper, we explore the potential of application RGC method in the real distributed system. Targeting the widely adopted multi-GPU system, we proposed an RGC system design call RedSync, which includes a set of optimizations to reduce communication bandwidth while introducing limited overhead. We examine the performance of RedSync on two different multiple GPU platforms, including a supercomputer and a multi-card server. Our test cases include image classification and language modeling tasks on Cifar10, ImageNet, Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to computation ratio, which have long been considered with poor scalability, RedSync shows significant performance improvement.
ER  -


TY  - Preprint
T1  - Hidden Fluid Mechanics: A Navier-Stokes Informed Deep Learning Framework for Assimilating Flow Visualization Data
A1  - Maziar Raissi
A1  - Alireza Yazdani
A1  - George Em Karniadakis
JO  - ArXiv e-prints
Y1  - 13 August, 2018
UR  - https://arxiv.org/abs/1808.04327
N2  - We present hidden fluid mechanics (HFM), a physics informed deep learning framework capable of encoding an important class of physical laws governing fluid motions, namely the Navier-Stokes equations. In particular, we seek to leverage the underlying conservation laws (i.e., for mass, momentum, and energy) to infer hidden quantities of interest such as velocity and pressure fields merely from spatio-temporal visualizations of a passive scaler (e.g., dye or smoke), transported in arbitrarily complex domains (e.g., in human arteries or brain aneurysms). Our approach towards solving the aforementioned data assimilation problem is unique as we design an algorithm that is agnostic to the geometry or the initial and boundary conditions. This makes HFM highly flexible in choosing the spatio-temporal domain of interest for data acquisition as well as subsequent training and predictions. Consequently, the predictions made by HFM are among those cases where a pure machine learning strategy or a mere scientific computing approach simply cannot reproduce. The proposed algorithm achieves accurate predictions of the pressure and velocity fields in both two and three dimensional flows for several benchmark problems motivated by real-world applications. Our results demonstrate that this relatively simple methodology can be used in physical and biomedical problems to extract valuable quantitative information (e.g., lift and drag forces or wall shear stresses in arteries) for which direct measurements may not be possible.
ER  -


TY  - Preprint
T1  - What is Unique in Individual Gait Patterns? Understanding and Interpreting Deep Learning in Gait Analysis
A1  - Fabian Horst
A1  - Sebastian Lapuschkin
A1  - Wojciech Samek
A1  - Klaus-Robert MÃ¼ller
A1  - Wolfgang I. SchÃ¶llhorn
JO  - ArXiv e-prints
Y1  - 13 August, 2018
UR  - https://arxiv.org/abs/1808.04308
N2  - Machine learning (ML) techniques such as (deep) artificial neural networks (DNN) are solving very successfully a plethora of tasks and provide new predictive models for complex physical, chemical, biological and social systems. However, in most cases this comes with the disadvantage of acting as a black box, rarely providing information about what made them arrive at a particular prediction. This black box aspect of ML techniques can be problematic especially in medical diagnoses, so far hampering a clinical acceptance. The present paper studies the uniqueness of individual gait patterns in clinical biomechanics using DNNs. By attributing portions of the model predictions back to the input variables (ground reaction forces and full-body joint angles), the Layer-Wise Relevance Propagation (LRP) technique reliably demonstrates which variables at what time windows of the gait cycle are most relevant for the characterisation of gait patterns from a certain individual. By measuring the timeresolved contribution of each input variable to the prediction of ML techniques such as DNNs, our method describes the first general framework that enables to understand and interpret non-linear ML methods in (biomechanical) gait analysis and thereby supplies a powerful tool for analysis, diagnosis and treatment of human gait.
ER  -


TY  - Preprint
T1  - Understanding training and generalization in deep learning by Fourier analysis
A1  - Zhiqin John Xu
JO  - ArXiv e-prints
Y1  - 17 September, 2018
UR  - https://arxiv.org/abs/1808.04295
N2  - Background: It is still an open research area to theoretically understand why Deep Neural Networks (DNNs)---equipped with many more parameters than training data and trained by (stochastic) gradient-based methods---often achieve remarkably low generalization error. Contribution: We study DNN training by Fourier analysis. Our theoretical framework explains: i) DNN with (stochastic) gradient-based methods endows low-frequency components of the target function with a higher priority during the training; ii) Small initialization leads to good generalization ability of DNN while preserving the DNN&#39;s ability of fitting any function. These results are further confirmed by experiments of DNNs fitting the following datasets, i.e., natural images, one-dimensional functions and MNIST dataset.
ER  -


TY  - Preprint
T1  - Visual Sensor Network Reconfiguration with Deep Reinforcement Learning
A1  - Paul Jasek
A1  - Bernard Abayowa
JO  - ArXiv e-prints
Y1  - 13 August, 2018
UR  - https://arxiv.org/abs/1808.04287
N2  - We present an approach for reconfiguration of dynamic visual sensor networks with deep reinforcement learning (RL). Our RL agent uses a modified asynchronous advantage actor-critic framework and the recently proposed Relational Network module at the foundation of its network architecture. To address the issue of sample inefficiency in current approaches to model-free reinforcement learning, we train our system in an abstract simulation environment that represents inputs from a dynamic scene. Our system is validated using inputs from a real-world scenario and preexisting object detection and tracking algorithms.
ER  -


TY  - Preprint
T1  - Fully Distributed Multi-Robot Collision Avoidance via Deep Reinforcement Learning for Safe and Efficient Navigation in Complex Scenarios
A1  - Tingxiang Fan
A1  - Pinxin Long
A1  - Wenxi Liu
A1  - Jia Pan
JO  - ArXiv e-prints
Y1  - 11 August, 2018
UR  - https://arxiv.org/abs/1808.03841
N2  - In this paper, we present a decentralized sensor-level collision avoidance policy for multi-robot systems, which shows promising results in practical applications. In particular, our policy directly maps raw sensor measurements to an agent&#39;s steering commands in terms of the movement velocity. As a first step toward reducing the performance gap between decentralized and centralized methods, we present a multi-scenario multi-stage training framework to learn an optimal policy. The policy is trained over a large number of robots in rich, complex environments simultaneously using a policy gradient based reinforcement learning algorithm. The learning algorithm is also integrated into a hybrid control framework to further improve the policy&#39;s robustness and effectiveness.
ER  -


TY  - Preprint
T1  - Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning
A1  - Noah Frazier-Logue
A1  - Stephen JosÃ© Hanson
JO  - ArXiv e-prints
Y1  - 10 August, 2018
UR  - https://arxiv.org/abs/1808.03578
N2  - Multi-layer neural networks have lead to remarkable performance on many kinds of benchmark tasks in text, speech and image processing. Nonlinear parameter estimation in hierarchical models is known to be subject to overfitting. One approach to this overfitting and related problems (local minima, colinearity, feature discovery etc.) is called dropout (Srivastava, et al 2014, Baldi et al 2016). This method removes hidden units with a Bernoulli random variable with probability $p$ over updates. In this paper we will show that Dropout is a special case of a more general model published originally in 1990 called the stochastic delta rule ( SDR, Hanson, 1990). SDR parameterizes each weight in the network as a random variable with mean $Î¼_{w_{ij}}$ and standard deviation $Ï_{w_{ij}}$. These random variables are sampled on each forward activation, consequently creating an exponential number of potential networks with shared weights. Both parameters are updated according to prediction error, thus implementing weight noise injections that reflect a local history of prediction error and efficient model averaging. SDR therefore implements a local gradient-dependent simulated annealing per weight converging to a bayes optimal network. Tests on standard benchmarks (CIFAR) using a modified version of DenseNet shows the SDR outperforms standard dropout in error by over 50% and in loss by over 50%. Furthermore, the SDR implementation converges on a solution much faster, reaching a training error of 5 in just 15 epochs with DenseNet-40 compared to standard DenseNet-40&#39;s 94 epochs.
ER  -


TY  - Preprint
T1  - Deep Learning Based Speed Estimation for Constraining Strapdown Inertial Navigation on Smartphones
A1  - Santiago CortÃ©s
A1  - Arno Solin
A1  - Juho Kannala
JO  - ArXiv e-prints
Y1  - 10 August, 2018
UR  - https://arxiv.org/abs/1808.03485
N2  - Strapdown inertial navigation systems are sensitive to the quality of the data provided by the accelerometer and gyroscope. Low-grade IMUs in handheld smart-devices pose a problem for inertial odometry on these devices. We propose a scheme for constraining the inertial odometry problem by complementing non-linear state estimation by a CNN-based deep-learning model for inferring the momentary speed based on a window of IMU samples. We show the feasibility of the model using a wide range of data from an iPhone, and present proof-of-concept results for how the model can be combined with an inertial navigation system for three-dimensional inertial navigation.
ER  -


TY  - Preprint
T1  - Error Forward-Propagation: Reusing Feedforward Connections to Propagate Errors in Deep Learning
A1  - Adam A. Kohan
A1  - Edward A. Rietman
A1  - Hava T. Siegelmann
JO  - ArXiv e-prints
Y1  - 9 August, 2018
UR  - https://arxiv.org/abs/1808.03357
N2  - We introduce Error Forward-Propagation, a biologically plausible mechanism to propagate error feedback forward through the network. Architectural constraints on connectivity are virtually eliminated for error feedback in the brain; systematic backward connectivity is not used or needed to deliver error feedback. Feedback as a means of assigning credit to neurons earlier in the forward pathway for their contribution to the final output is thought to be used in learning in the brain. How the brain solves the credit assignment problem is unclear. In machine learning, error backpropagation is a highly successful mechanism for credit assignment in deep multilayered networks. Backpropagation requires symmetric reciprocal connectivity for every neuron. From a biological perspective, there is no evidence of such an architectural constraint, which makes backpropagation implausible for learning in the brain. This architectural constraint is reduced with the use of random feedback weights. Models using random feedback weights require backward connectivity patterns for every neuron, but avoid symmetric weights and reciprocal connections. In this paper, we practically remove this architectural constraint, requiring only a backward loop connection for effective error feedback. We propose reusing the forward connections to deliver the error feedback by feeding the outputs into the input receiving layer. This mechanism, Error Forward-Propagation, is a plausible basis for how error feedback occurs deep in the brain independent of and yet in support of the functionality underlying intricate network architectures. We show experimentally that recurrent neural networks with two and three hidden layers can be trained using Error Forward-Propagation on the MNIST and Fashion MNIST datasets, achieving $1.90\%$ and $11\%$ generalization errors respectively.
ER  -


TY  - Preprint
T1  - Deep Learning for Single Image Super-Resolution: A Brief Review
A1  - Wenming Yang
A1  - Xuechen Zhang
A1  - Yapeng Tian
A1  - Wei Wang
A1  - Jing-Hao Xue
JO  - ArXiv e-prints
Y1  - 9 August, 2018
UR  - https://arxiv.org/abs/1808.03344
N2  - Single image super-resolution (SISR) is a notoriously challenging ill-posed problem, which aims to obtain a high- resolution (HR) output from one of its low-resolution (LR) versions. To solve the SISR problem, recently powerful deep learning algorithms have been employed and achieved the state- of-the-art performance. In this survey, we review representative deep learning-based SISR methods, and group them into two categories according to their major contributions to two essential aspects of SISR: the exploration of efficient neural network archi- tectures for SISR, and the development of effective optimization objectives for deep SISR learning. For each category, a baseline is firstly established and several critical limitations of the baseline are summarized. Then representative works on overcoming these limitations are presented based on their original contents as well as our critical understandings and analyses, and relevant comparisons are conducted from a variety of perspectives. Finally we conclude this review with some vital current challenges and future trends in SISR leveraging deep learning algorithms.
ER  -


TY  - Preprint
T1  - Learning to Optimize Join Queries With Deep Reinforcement Learning
A1  - Sanjay Krishnan
A1  - Zongheng Yang
A1  - Ken Goldberg
A1  - Joseph Hellerstein
A1  - Ion Stoica
JO  - ArXiv e-prints
Y1  - 9 August, 2018
UR  - https://arxiv.org/abs/1808.03196
N2  - Exhaustive enumeration of all possible join orders is often avoided, and most optimizers leverage heuristics to prune the search space. The design and implementation of heuristics are well-understood when the cost model is roughly linear, and we find that these heuristics can be significantly suboptimal when there are non-linearities in cost. Ideally, instead of a fixed heuristic, we would want a strategy to guide the search space in a more data-driven way---tailoring the search to a specific dataset and query workload. Recent work in deep reinforcement learning (Deep RL) may provide a new perspective on this problem. Deep RL poses sequential problems, like join optimization, as a series of 1-step prediction problems that can be learned from data. We present our deep RL-based DQ optimizer, which currently optimizes select-project-join blocks, and we evaluate DQ on the Join Order Benchmark. We found that DQ achieves plan costs within a factor of 2 of the optimal solution on all cost models and improves on the next best heuristic by up to $3\times$. Furthermore, DQ executes 10,000$\times$ faster than exhaustive enumeration and more than 10$\times$ faster than left/right-deep enumeration on the largest queries in the benchmark.
ER  -


TY  - Preprint
T1  - Radon Inversion via Deep Learning
A1  - Ji He
A1  - Jianhua Ma
JO  - ArXiv e-prints
Y1  - 9 August, 2018
UR  - https://arxiv.org/abs/1808.03015
N2  - Radon transform is widely used in physical and life sciences and one of its major applications is the X-ray computed tomography (X-ray CT), which is significant in modern health examination. The Radon inversion or image reconstruction is challenging due to the potentially defective radon projections. Conventionally, the reconstruction process contains several ad hoc stages to approximate the corresponding Radon inversion. Each of the stages is highly dependent on the results of the previous stage. In this paper, we propose a novel unified framework for Radon inversion via deep learning (DL). The Radon inversion can be approximated by the proposed framework with an end-to-end fashion instead of processing step-by-step with multiple stages. For simplicity, the proposed framework is short as iRadonMap (inverse Radon transform approximation). Specifically, we implement the iRadonMap as an appropriative neural network, of which the architecture can be divided into two segments. In the first segment, a learnable fully-connected filtering layer is used to filter the radon projections along the view-angle direction, which is followed by a learnable sinusoidal back-projection layer to transfer the filtered radon projections into an image. The second segment is a common neural network architecture to further improve the reconstruction performance in the image domain. The iRadonMap is overall optimized by training a large number of generic images from ImageNet database. To evaluate the performance of the iRadonMap, clinical patient data is used. Qualitative results show promising reconstruction performance of the iRadonMap.
ER  -


TY  - Preprint
T1  - Parkinson&#39;s Disease Assessment from a Wrist-Worn Wearable Sensor in Free-Living Conditions: Deep Ensemble Learning and Visualization
A1  - Terry Taewoong Um
A1  - Franz Michael Josef Pfister
A1  - Daniel Christian Pichler
A1  - Satoshi Endo
A1  - Muriel Lang
A1  - Sandra Hirche
A1  - Urban Fietzek
A1  - Dana KuliÄ
JO  - ArXiv e-prints
Y1  - 8 August, 2018
UR  - https://arxiv.org/abs/1808.02870
N2  - Parkinson&#39;s Disease (PD) is characterized by disorders in motor function such as freezing of gait, rest tremor, rigidity, and slowed and hyposcaled movements. Medication with dopaminergic medication may alleviate those motor symptoms, however, side-effects may include uncontrolled movements, known as dyskinesia. In this paper, an automatic PD motor-state assessment in free-living conditions is proposed using an accelerometer in a wrist-worn wearable sensor. In particular, an ensemble of convolutional neural networks (CNNs) is applied to capture the large variability of daily-living activities and overcome the dissimilarity between training and test patients due to the inter-patient variability. In addition, class activation map (CAM), a visualization technique for CNNs, is applied for providing an interpretation of the results.
ER  -


TY  - Preprint
T1  - Unsupervised/Semi-supervised Deep Learning for Low-dose CT Enhancement
A1  - Mingrui Geng
A1  - Yun Deng
A1  - Qian Zhao
A1  - Qi Xie
A1  - Dong Zeng
A1  - Dong Zeng
A1  - Wangmeng Zuo
A1  - Deyu Meng
JO  - ArXiv e-prints
Y1  - 7 August, 2018
UR  - https://arxiv.org/abs/1808.02603
N2  - Recently, deep learning(DL) methods have been proposed for the low-dose computed tomography(LdCT) enhancement, and obtain good trade-off between computational efficiency and image quality. Most of them need large number of pre-collected ground-truth/high-dose sinograms with less noise, and train the network in a supervised end-to-end manner. This may bring major limitations on these methods because the number of such low-dose/high-dose training sinogram pairs would affect the network&#39;s capability and sometimes the ground-truth sinograms are hard to be obtained in large scale. Since large number of low-dose ones are relatively easy to obtain, it should be critical to make these sources play roles in network training in an unsupervised learning manner. To address this issue, we propose an unsupervised DL method for LdCT enhancement that incorporates unlabeled LdCT sinograms directly into the network training. The proposed method effectively considers the structure characteristics and noise distribution in the measured LdCT sinogram, and then learns the proper gradient of the LdCT sinogram in a pure unsupervised manner. Similar to the labeled ground-truth, the gradient information in an unlabeled LdCT sinogram can be used for sufficient network training. The experiments on the patient data show effectiveness of the proposed method.
ER  -


TY  - Preprint
T1  - Application of End-to-End Deep Learning in Wireless Communications Systems
A1  - Woongsup Lee
A1  - Ohyun Jo
A1  - Minhoe Kim
JO  - ArXiv e-prints
Y1  - 7 August, 2018
UR  - https://arxiv.org/abs/1808.02394
N2  - Deep learning is a potential paradigm changer for the design of wireless communications systems (WCS), from conventional handcrafted schemes based on sophisticated mathematical models with assumptions to autonomous schemes based on the end-to-end deep learning using a large number of data. In this article, we present a basic concept of the deep learning and its application to WCS by investigating the resource allocation (RA) scheme based on a deep neural network (DNN) where multiple goals with various constraints can be satisfied through the end-to-end deep learning. Especially, the optimality and feasibility of the DNN based RA are verified through simulation. Then, we discuss the technical challenges regarding the application of deep learning in WCS.
ER  -


TY  - Preprint
T1  - Deep Learning for Domain Adaption: Engagement Recognition
A1  - Omid Mohamad Nezami
A1  - Len Hamey
A1  - Deborah Richards
A1  - Mark Dras
JO  - ArXiv e-prints
Y1  - 7 August, 2018
UR  - https://arxiv.org/abs/1808.02324
N2  - Engagement is a key indicator of the quality of learning experience, and one that plays a major role in developing intelligent educational interfaces. Any such interface requires the ability to recognise the level of engagement in order to respond appropriately; however, there is very little existing data to learn from, and new data is expensive and difficult to acquire. This paper presents a deep learning model to improve engagement recognition from face images captured `in the wild&#39; that overcomes the data sparsity challenge by pre-training on readily available basic facial expression data, before training on specialised engagement data. In the first of two steps, a state-of-the-art facial expression recognition model is trained to provide a rich face representation using deep learning. In the second step, we use the model&#39;s weights to initialize our deep learning based model to recognize engagement; we term this the Transfer model. We train the model on our new engagement recognition (ER) dataset with 4627 engaged and disengaged samples. We find that our Transfer architecture outperforms standard deep learning architectures that we apply for the first time to engagement recognition, as well as approaches using HOG features and SVMs. The model achieves a classification accuracy of 72.38%, which is 6.1% better than the best baseline model on the test set of the ER dataset. Using the F1 measure and the area under the ROC curve, our Transfer model achieves 73.90% and 73.74%, exceeding the best baseline model by 3.49% and 5.33% respectively.
ER  -


TY  - Preprint
T1  - Grassmannian Learning: Embedding Geometry Awareness in Shallow and Deep Learning
A1  - Jiayao Zhang
A1  - Guangxu Zhu
A1  - Robert W. Heath Jr.
A1  - Kaibin Huang
JO  - ArXiv e-prints
Y1  - 12 August, 2018
UR  - https://arxiv.org/abs/1808.02229
N2  - Modern machine learning algorithms have been adopted in a range of signal-processing applications spanning computer vision, natural language processing, and artificial intelligence. Many relevant problems involve subspace-structured features, orthogonality constrained or low-rank constrained objective functions, or subspace distances. These mathematical characteristics are expressed naturally using the Grassmann manifold. Unfortunately, this fact is not yet explored in many traditional learning algorithms. In the last few years, there have been growing interests in studying Grassmann manifold to tackle new learning problems. Such attempts have been reassured by substantial performance improvements in both classic learning and learning using deep neural networks. We term the former as shallow and the latter deep Grassmannian learning. The aim of this paper is to introduce the emerging area of Grassmannian learning by surveying common mathematical problems and primary solution approaches, and overviewing various applications. We hope to inspire practitioners in different fields to adopt the powerful tool of Grassmannian learning in their research.
ER  -


TY  - Preprint
T1  - Deep Learning with Predictive Control for Human Motion Tracking
A1  - Don Joven Agravante
A1  - Giovanni De Magistris
A1  - Asim Munawar
A1  - Phongtharin Vinayavekhin
A1  - Ryuki Tachibana
JO  - ArXiv e-prints
Y1  - 6 August, 2018
UR  - https://arxiv.org/abs/1808.02200
N2  - We propose to combine model predictive control with deep learning for the task of accurate human motion tracking with a robot. We design the MPC to allow switching between the learned and a conservative prediction. We also explored online learning with a DyBM model. We applied this method to human handwriting motion tracking with a UR-5 robot. The results show that the framework significantly improves tracking performance.
ER  -


TY  - Preprint
T1  - Non-Learning based Deep Parallel MRI Reconstruction (NLDpMRI)
A1  - Ali Pour Yazdanpanah
A1  - Onur Afacan
A1  - Simon K. Warfield
JO  - ArXiv e-prints
Y1  - 6 August, 2018
UR  - https://arxiv.org/abs/1808.02122
N2  - Fast data acquisition in Magnetic Resonance Imaging (MRI) is vastly in demand and scan time directly depends on the number of acquired k-space samples. The most common issues in any deep learning-based MRI reconstruction approaches are generalizability and transferability. For different MRI scanner configurations using these approaches, the network must be trained from scratch every time with new training dataset, acquired under new configurations, to be able to provide good reconstruction performance. Here, we propose a new parallel imaging method based on deep neural networks called NLDpMRI to reduce any structured aliasing ambiguities related to the different k-space undersampling patterns for accelerated data acquisition. Two loss functions including non-regularized and regularized are proposed for parallel MRI reconstruction using deep network optimization and we reconstruct MR images by optimizing the proposed loss functions over the network parameters. Unlike any deep learning-based MRI reconstruction approaches, our method doesn&#39;t include any training step that the network learns from a large number of training samples and it only needs the single undersampled multi-coil k-space data for reconstruction. Also, the proposed method can handle k-space data with different undersampling patterns, and different number of coils. Unlike most deep learning-based MRI reconstruction methods, our method operates on real-world acquisitions with the complex data format, not on simulated data, real-valued data, or data with added simulated-phase. Experimental results show that the proposed method outperforms the current state-of-the-art GRAPPA reconstruction method.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Online Offloading in Wireless Powered Mobile-Edge Computing Networks
A1  - Liang Huang
A1  - Suzhi Bi
A1  - Ying-Jun Angela Zhang
JO  - ArXiv e-prints
Y1  - 6 August, 2018
UR  - https://arxiv.org/abs/1808.01977
N2  - In this paper, we consider a wireless powered mobile-edge computing (MEC) network following the binary offloading policy, such that each computation task of wireless devices (WDs) is either executed locally or fully offloaded to an MEC server. Our goal is to acquire an online algorithm under time-varying wireless channels that jointly optimizes task offloading decisions and resource allocation to maximize the data processing capability of the network. In practice, this requires successively solving hard combinatorial optimization problems to address the multi-user correlation in the offloading decisions. The existing approaches are mainly based on either branch-and-bound algorithm or relaxation heuristics, which are limited by the tradeoff between optimality and efficiency. To tackle this problem, we propose in this paper a Deep Reinforcement learning-based Online Offloading (DROO) framework that implements a deep neural network to generate offloading decisions. In particular, the proposed DROO framework does not require any manually labeled training data as the input, and thus completely removes the need of solving combinatorial optimization problems. Besides, it avoids the curse of dimensionality problem encountered by some existing reinforcement learning approaches and is computationally efficient in large-size networks. To further reduce the computational complexity, we propose an adaptive procedure that automatically adjusts the parameters of the DROO algorithm on the fly. Numerical results show that the proposed algorithm can achieve near-optimal performance while significantly decreasing the computation time by more than an order of magnitude compared with existing methods. For example, the complexity is reduced from several seconds to less than $0.1$ second in a $30$-user network, making real-time and optimal offloading design truly viable even in a fast fading environment.
ER  -


TY  - Preprint
T1  - A Survey on Deep Transfer Learning
A1  - Chuanqi Tan
A1  - Fuchun Sun
A1  - Tao Kong
A1  - Wenchang Zhang
A1  - Chao Yang
A1  - Chunfang Liu
JO  - ArXiv e-prints
Y1  - 6 August, 2018
UR  - https://arxiv.org/abs/1808.01974
N2  - As a new classification platform, deep learning has recently received increasing attention from researchers and has been successfully applied to many domains. In some domains, like bioinformatics and robotics, it is very difficult to construct a large-scale well-annotated dataset due to the expense of data acquisition and costly annotation, which limits its development. Transfer learning relaxes the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insufficient training data. This survey focuses on reviewing the current researches of transfer learning by using deep neural network and its applications. We defined deep transfer learning, category and review the recent research works based on the techniques used in deep transfer learning.
ER  -


TY  - Preprint
T1  - An Efficient Deep Reinforcement Learning Model for Urban Traffic Control
A1  - Yilun Lin
A1  - Xingyuan Dai
A1  - Li Li
A1  - Fei-Yue Wang
JO  - ArXiv e-prints
Y1  - 24 August, 2018
UR  - https://arxiv.org/abs/1808.01876
N2  - Urban Traffic Control (UTC) plays an essential role in Intelligent Transportation System (ITS) but remains difficult. Since model-based UTC methods may not accurately describe the complex nature of traffic dynamics in all situations, model-free data-driven UTC methods, especially reinforcement learning (RL) based UTC methods, received increasing interests in the last decade. However, existing DL approaches did not propose an efficient algorithm to solve the complicated multiple intersections control problems whose state-action spaces are vast. To solve this problem, we propose a Deep Reinforcement Learning (DRL) algorithm that combines several tricks to master an appropriate control strategy within an acceptable time. This new algorithm relaxes the fixed traffic demand pattern assumption and reduces human invention in parameter tuning. Simulation experiments have shown that our method outperforms traditional rule-based approaches and has the potential to handle more complex traffic problems in the real world.
ER  -


TY  - Preprint
T1  - Deep Transfer Learning for EEG-based Brain Computer Interface
A1  - Chuanqi Tan
A1  - Fuchun Sun
A1  - Wenchang Zhang
JO  - ArXiv e-prints
Y1  - 6 August, 2018
UR  - https://arxiv.org/abs/1808.01752
N2  - The electroencephalography classifier is the most important component of brain-computer interface based systems. There are two major problems hindering the improvement of it. First, traditional methods do not fully exploit multimodal information. Second, large-scale annotated EEG datasets are almost impossible to acquire because biological data acquisition is challenging and quality annotation is costly. Herein, we propose a novel deep transfer learning approach to solve these two problems. First, we model cognitive events based on EEG data by characterizing the data using EEG optical flow, which is designed to preserve multimodal EEG information in a uniform representation. Second, we design a deep transfer learning framework which is suitable for transferring knowledge by joint training, which contains a adversarial network and a special loss function. The experiments demonstrate that our approach, when applied to EEG classification tasks, has many advantages, such as robustness and accuracy.
ER  -


TY  - Preprint
T1  - 3D Conceptual Design Using Deep Learning
A1  - Zhangsihao Yang
A1  - Haoliang Jiang
A1  - Zou Lan
JO  - ArXiv e-prints
Y1  - 5 August, 2018
UR  - https://arxiv.org/abs/1808.01675
N2  - This article proposes a data-driven methodology to achieve a fast design support, in order to generate or develop novel designs covering multiple object categories. This methodology implements two state-of-the-art Variational Autoencoder dealing with 3D model data. Our methodology constructs a self-defined loss function. The loss function, containing the outputs of certain layers in the autoencoder, obtains combination of different latent features from different 3D model categories.
ER  -


TY  - Preprint
T1  - A Review of Learning with Deep Generative Models from perspective of graphical modeling
A1  - Zhijian Ou
JO  - ArXiv e-prints
Y1  - 1 September, 2018
UR  - https://arxiv.org/abs/1808.01630
N2  - This document aims to provide a review on learning with deep generative models (DGMs), which is an highly-active area in machine learning and more generally, artificial intelligence. This review is not meant to be a tutorial, but when necessary, we provide self-contained derivations for completeness. This review has two features. First, though there are different perspectives to classify DGMs, we choose to organize this review from the perspective of graphical modeling, because the learning methods for directed DGMs and undirected DGMs are fundamentally different. Second, we differentiate model definitions from model learning algorithms, since different learning algorithms can be applied to solve the learning problem on the same model, and an algorithm can be applied to learn different models. We thus separate model definition and model learning, with more emphasis on reviewing, differentiating and connecting different learning algorithms. We also discuss promising future research directions.
ER  -


TY  - Preprint
T1  - Classification of Dermoscopy Images using Deep Learning
A1  - Nithin D Reddy
JO  - ArXiv e-prints
Y1  - 5 August, 2018
UR  - https://arxiv.org/abs/1808.01607
N2  - Skin cancer is one of the most common forms of cancer and its incidence is projected to rise over the next decade. Artificial intelligence is a viable solution to the issue of providing quality care to patients in areas lacking access to trained dermatologists. Considerable progress has been made in the use of automated applications for accurate classification of skin lesions from digital images. In this manuscript, we discuss the design and implementation of a deep learning algorithm for classification of dermoscopy images from the HAM10000 Dataset. We trained a convolutional neural network based on the ResNet50 architecture to accurately classify dermoscopy images of skin lesions into one of seven disease categories. Using our custom model, we obtained a balanced accuracy of 91% on the validation dataset.
ER  -


TY  - Preprint
T1  - Deep Multi-Center Learning for Face Alignment
A1  - Zhiwen Shao
A1  - Hengliang Zhu
A1  - Xin Tan
A1  - Yangyang Hao
A1  - Lizhuang Ma
JO  - ArXiv e-prints
Y1  - 5 August, 2018
UR  - https://arxiv.org/abs/1808.01558
N2  - Facial landmarks are highly correlated with each other since a certain landmark can be estimated by its neighboring landmarks. Most of the existing deep learning methods only use one fully-connected layer called shape prediction layer to estimate the location of facial landmarks. In this paper, we propose a novel deep learning framework named Multi-Center Learning with multiple shape prediction layers for face alignment. In particular, each shape prediction layer emphasizes on the detection of a certain cluster of semantically relevant landmarks respectively. Challenging landmarks are focused firstly, and each cluster of landmarks is further optimized respectively. Moreover, to reduce the model complexity, we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer. Extensive experiments demonstrate that our method is effective for handling complex occlusions and appearance variations with real-time performance. The code for our method is available at https://github.com/ZhiwenShao/MCNet-Extension.
ER  -


TY  - Preprint
T1  - Deep Reinforcement One-Shot Learning for Artificially Intelligent Classification Systems
A1  - Anton Puzanov
A1  - Kobi Cohen
JO  - ArXiv e-prints
Y1  - 4 August, 2018
UR  - https://arxiv.org/abs/1808.01527
N2  - In recent years there has been a sharp rise in networking applications, in which significant events need to be classified but only a few training instances are available. These are known as cases of one-shot learning. Examples include analyzing network traffic under zero-day attacks, and computer vision tasks by sensor networks deployed in the field. To handle this challenging task, organizations often use human analysts to classify events under high uncertainty. Existing algorithms use a threshold-based mechanism to decide whether to classify an object automatically or send it to an analyst for deeper inspection. However, this approach leads to a significant waste of resources since it does not take the practical temporal constraints of system resources into account. Our contribution is threefold. First, we develop a novel Deep Reinforcement One-shot Learning (DeROL) framework to address this challenge. The basic idea of the DeROL algorithm is to train a deep-Q network to obtain a policy which is oblivious to the unseen classes in the testing data. Then, in real-time, DeROL maps the current state of the one-shot learning process to operational actions based on the trained deep-Q network, to maximize the objective function. Second, we develop the first open-source software for practical artificially intelligent one-shot classification systems with limited resources for the benefit of researchers in related fields. Third, we present an extensive experimental study using the OMNIGLOT dataset for computer vision tasks and the UNSW-NB15 dataset for intrusion detection tasks that demonstrates the versatility and efficiency of the DeROL framework.
ER  -


TY  - Preprint
T1  - DELIMIT PyTorch - An extension for Deep Learning in Diffusion Imaging
A1  - Simon Koppers
A1  - Dorit Merhof
JO  - ArXiv e-prints
Y1  - 4 August, 2018
UR  - https://arxiv.org/abs/1808.01517
N2  - DELIMIT is a framework extension for deep learning in diffusion imaging, which extends the basic framework PyTorch towards spherical signals. Based on several novel layers, deep learning can be applied to spherical diffusion imaging data in a very convenient way. First, two spherical harmonic interpolation layers are added to the extension, which allow to transform the signal from spherical surface space into the spherical harmonic space, and vice versa. In addition, a local spherical convolution layer is introduced that adds the possibility to include gradient neighborhood information within the network. Furthermore, these extensions can also be utilized for the preprocessing of diffusion signals.
ER  -


TY  - Preprint
T1  - Deep Learning Advances on Different 3D Data Representations: A Survey
A1  - Eman Ahmed
A1  - Alexandre Saint
A1  - Abd El Rahman Shabayek
A1  - Kseniya Cherenkova
A1  - Rig Das
A1  - Gleb Gusev
A1  - Djamila Aouada
A1  - Bjorn Ottersten
JO  - ArXiv e-prints
Y1  - 4 August, 2018
UR  - https://arxiv.org/abs/1808.01462
N2  - 3D data is a valuable asset in the field of computer vision as it provides rich information about the full geometry of sensed objects and scenes. With the recent availability of large 3D datasets and the increase in computational power, it is today possible to consider applying deep learning to learn specific tasks on 3D data such as segmentation, recognition and correspondence. Depending on the considered 3D data representation, different challenges may be foreseen in using existent deep learning architectures. In this paper, we provide a comprehensive overview of various 3D data representations highlighting the difference between Euclidean and non-Euclidean ones. We also discuss how deep learning methods are applied on each representation, analyzing the challenges to overcome.
ER  -


TY  - Preprint
T1  - Deep Learning-Based Multiple Object Visual Tracking on Embedded System for IoT and Mobile Edge Computing Applications
A1  - Beatriz Blanco-Filgueira
A1  - Daniel GarcÃ­a-Lesta
A1  - Mauro FernÃ¡ndez-Sanjurjo
A1  - VÃ­ctor M. Brea
A1  - Paula LÃ³pez
JO  - ArXiv e-prints
Y1  - 31 July, 2018
UR  - https://arxiv.org/abs/1808.01356
N2  - Compute and memory demands of state-of-the-art deep learning methods are still a shortcoming that must be addressed to make them useful at IoT end-nodes. In particular, recent results depict a hopeful prospect for image processing using Convolutional Neural Netwoks, CNNs, but the gap between software and hardware implementations is already considerable for IoT and mobile edge computing applications due to their high power consumption. This proposal performs low-power and real time deep learning-based multiple object visual tracking implemented on an NVIDIA Jetson TX2 development kit. It includes a camera and wireless connection capability and it is battery powered for mobile and outdoor applications. A collection of representative sequences captured with the on-board camera, dETRUSC video dataset, is used to exemplify the performance of the proposed algorithm and to facilitate benchmarking. The results in terms of power consumption and frame rate demonstrate the feasibility of deep learning algorithms on embedded platforms although more effort to joint algorithm and hardware design of CNNs is needed.
ER  -


TY  - Preprint
T1  - A Deep Learning based Joint Segmentation and Classification Framework for Glaucoma Assesment in Retinal Color Fundus Images
A1  - Arunava Chakravarty
A1  - Jayanthi Sivswamy
JO  - ArXiv e-prints
Y1  - 29 July, 2018
UR  - https://arxiv.org/abs/1808.01355
N2  - Automated Computer Aided diagnostic tools can be used for the early detection of glaucoma to prevent irreversible vision loss. In this work, we present a Multi-task Convolutional Neural Network (CNN) that jointly segments the Optic Disc (OD), Optic Cup (OC) and predicts the presence of glaucoma in color fundus images. The CNN utilizes a combination of image appearance features and structural features obtained from the OD-OC segmentation to obtain a robust prediction. The use of fewer network parameters and the sharing of the CNN features for multiple related tasks ensures the good generalizability of the architecture, allowing it to be trained on small training sets. The cross-testing performance of the proposed method on an independent validation set acquired using a different camera and image resolution was found to be good with an average dice score of 0.92 for OD, 0.84 for OC and AUC of 0.95 on the task of glaucoma classification illustrating its potential as a mass screening tool for the early detection of glaucoma.
ER  -


TY  - Preprint
T1  - Enabling Trust in Deep Learning Models: A Digital Forensics Case Study
A1  - Aditya K
A1  - Slawomir Grzonkowski
A1  - Nhien An Lekhac
JO  - ArXiv e-prints
Y1  - 3 August, 2018
UR  - https://arxiv.org/abs/1808.01196
N2  - Today, the volume of evidence collected per case is growing exponentially, to address this problem forensics investigators are looking for investigation process with tools built on new technologies like big data, cloud services, and Deep Learning (DL) techniques. Consequently, the accuracy of artifacts found also relies on the performance of techniques used, especially DL models. Recently, \textbf{D}eep \textbf{N}eural \textbf{N}ets (\textbf{DNN}) have achieved state of the art performance in the tasks of classification and recognition. In the context of digital forensics, DNN has been applied to the domains of cybercrime investigation such as child abuse investigations, malware classification, steganalysis and image forensics. However, the robustness of DNN models in the context of digital forensics is never studied before. Hence, in this research, we design and implement a domain-independent Adversary Testing Framework (ATF) to test the security robustness of black-box DNN&#39;s. By using ATF, we also methodically test a commercially available DNN service used in forensic investigations and bypass the detection, where published methods fail in control settings.
ER  -


TY  - Preprint
T1  - Generalization Error in Deep Learning
A1  - Daniel Jakubovitz
A1  - Raja Giryes
A1  - Miguel R. D. Rodrigues
JO  - ArXiv e-prints
Y1  - 3 August, 2018
UR  - https://arxiv.org/abs/1808.01174
N2  - Deep learning models have lately shown great performance in various fields such as computer vision, speech recognition, speech translation, and natural language processing. However, alongside their state-of-the-art performance, it is still generally unclear what is the source of their generalization ability. Thus, an important question is what makes deep neural networks able to generalize well from the training set to new data. In this article, we provide an overview of the existing theory and bounds for the characterization of the generalization error of deep neural networks, combining both classical and more recent theoretical and empirical results.
ER  -


TY  - Preprint
T1  - PHI Scrubber: A Deep Learning Approach
A1  - Abhai Kollara Dilip
A1  - Kamal Raj K
A1  - Malaikannan Sankarasubbu
JO  - ArXiv e-prints
Y1  - 3 August, 2018
UR  - https://arxiv.org/abs/1808.01128
N2  - Confidentiality of patient information is an essential part of Electronic Health Record System. Patient information, if exposed, can cause a serious damage to the privacy of individuals receiving healthcare. Hence it is important to remove such details from physician notes. A system is proposed which consists of a deep learning model where a de-convolutional neural network and bi-directional LSTM-CNN is used along with regular expressions to recognize and eliminate the individually identifiable information. This information is then removed from a medical practitioner&#39;s data which further allows the fair usage of such information among researchers and in clinical trials.
ER  -


TY  - Preprint
T1  - Dynamic Detection of False Data Injection Attack in Smart Grid using Deep Learning
A1  - Xiangyu Niu Jiangnan Li
A1  - Jinyuan Sun
JO  - ArXiv e-prints
Y1  - 14 September, 2018
UR  - https://arxiv.org/abs/1808.01094
N2  - Modern advances in sensor, computing, and communication technologies enable various smart grid applications. The heavy dependence on communication technology has highlighted the vulnerability of the electricity grid to false data injection (FDI) attacks that can bypass bad data detection mechanisms. Existing mitigation in the power system either focus on redundant measurements or protect a set of basic measurements. These methods make specific assumptions about FDI attacks, which are often restrictive and inadequate to deal with modern cyber threats. In the proposed approach, a deep learning based framework is used to detect injected data measurement. Our time-series anomaly detector adopts a Convolutional Neural Network (CNN) and a Long Short Term Memory (LSTM) network. To effectively estimate system variables, our approach observes both data measurements and network level features to jointly learn system states. The proposed system is tested on IEEE 39-bus system. Experimental analysis shows that the deep learning algorithm can identify anomalies which cannot be detected by traditional state estimation bad data detection.
ER  -


TY  - Preprint
T1  - Evaluating the Readability of Force Directed Graph Layouts: A Deep Learning Approach
A1  - Hammad Haleem
A1  - Yong Wang
A1  - Abishek Puri
A1  - Sahil Wadhwa
A1  - Huamin Qu
JO  - ArXiv e-prints
Y1  - 2 August, 2018
UR  - https://arxiv.org/abs/1808.00703
N2  - Existing graph layout algorithms are usually not able to optimize all the aesthetic properties desired in a graph layout. To evaluate how well the desired visual features are reflected in a graph layout, many readability metrics have been proposed in the past decades. However, the calculation of these readability metrics often requires access to the node and edge coordinates and is usually computationally inefficient, especially for dense graphs. Importantly, when the node and edge coordinates are not accessible, it becomes impossible to evaluate the graph layouts quantitatively. In this paper, we present a novel deep learning-based approach to evaluate the readability of graph layouts by directly using graph images. A convolutional neural network architecture is proposed and trained on a benchmark dataset of graph images, which is composed of synthetically-generated graphs and graphs created by sampling from real large networks. Multiple representative readability metrics (including edge crossing, node spread, and group overlap) are considered in the proposed approach. We quantitatively compare our approach to traditional methods and qualitatively evaluate our approach using a case study and visualizing convolutional layers. This work is a first step towards using deep learning based methods to evaluate images from the visualization field quantitatively.
ER  -


TY  - Preprint
T1  - Linguistic Search Optimization for Deep Learning Based LVCSR
A1  - Zhehuai Chen
JO  - ArXiv e-prints
Y1  - 2 August, 2018
UR  - https://arxiv.org/abs/1808.00687
N2  - Recent advances in deep learning based large vocabulary con- tinuous speech recognition (LVCSR) invoke growing demands in large scale speech transcription. The inference process of a speech recognizer is to find a sequence of labels whose corresponding acoustic and language models best match the input feature [1]. The main computation includes two stages: acoustic model (AM) inference and linguistic search (weighted finite-state transducer, WFST). Large computational overheads of both stages hamper the wide application of LVCSR. Benefit from stronger classifiers, deep learning, and more powerful computing devices, we propose general ideas and some initial trials to solve these fundamental problems.
ER  -


TY  - Preprint
T1  - Deep Learning for Radio Resource Allocation in Multi-Cell Networks
A1  - K. I. Ahmed
A1  - H. Tabassum
A1  - E. Hossain
JO  - ArXiv e-prints
Y1  - 2 August, 2018
UR  - https://arxiv.org/abs/1808.00667
N2  - Increased complexity and heterogeneity of emerging 5G and beyond 5G (B5G) wireless networks will require a paradigm shift from traditional resource allocation mechanisms. Deep learning (DL) is a powerful tool where a multi-layer neural network can be trained to model a resource management algorithm using network data.Therefore, resource allocation decisions can be obtained without intensive online computations which would be required otherwise for the solution of resource allocation problems. In this context, this article focuses on the application of DL to obtain solutions for the radio resource allocation problems in multi-cell networks. Starting with a brief overview of a deep neural network (DNN) as a DL model, relevant DNN architectures and the data training procedure, we provide an overview of existing state-of-the-art applying DL in the context of radio resource allocation. A qualitative comparison is provided in terms of their objectives, inputs/outputs, learning and data training methods. Then, we present a supervised DL model to solve the sub-band and power allocation problem in a multi-cell network. Using the data generated by a genetic algorithm, we first train the model and then test the accuracy of the proposed model in predicting the resource allocation solutions. Simulation results show that the trained DL model is able to provide the desired optimal solution 86.3% of time.
ER  -


TY  - Preprint
T1  - Sequence Discriminative Training for Deep Learning based Acoustic Keyword Spotting
A1  - Zhehuai Chen
A1  - Yanmin Qian
A1  - Kai Yu
JO  - ArXiv e-prints
Y1  - 1 August, 2018
UR  - https://arxiv.org/abs/1808.00639
N2  - Speech recognition is a sequence prediction problem. Besides employing various deep learning approaches for framelevel classification, sequence-level discriminative training has been proved to be indispensable to achieve the state-of-the-art performance in large vocabulary continuous speech recognition (LVCSR). However, keyword spotting (KWS), as one of the most common speech recognition tasks, almost only benefits from frame-level deep learning due to the difficulty of getting competing sequence hypotheses. The few studies on sequence discriminative training for KWS are limited for fixed vocabulary or LVCSR based methods and have not been compared to the state-of-the-art deep learning based KWS approaches. In this paper, a sequence discriminative training framework is proposed for both fixed vocabulary and unrestricted acoustic KWS. Sequence discriminative training for both sequence-level generative and discriminative models are systematically investigated. By introducing word-independent phone lattices or non-keyword blank symbols to construct competing hypotheses, feasible and efficient sequence discriminative training approaches are proposed for acoustic KWS. Experiments showed that the proposed approaches obtained consistent and significant improvement in both fixed vocabulary and unrestricted KWS tasks, compared to previous frame-level deep learning based acoustic KWS methods.
ER  -


TY  - Preprint
T1  - Classification of Building Information Model (BIM) Structures with Deep Learning
A1  - Francesco Lomio
A1  - Ricardo Farinha
A1  - Mauri Laasonen
A1  - Heikki Huttunen
JO  - ArXiv e-prints
Y1  - 1 August, 2018
UR  - https://arxiv.org/abs/1808.00601
N2  - In this work we study an application of machine learning to the construction industry and we use classical and modern machine learning methods to categorize images of building designs into three classes: Apartment building, Industrial building or Other. No real images are used, but only images extracted from Building Information Model (BIM) software, as these are used by the construction industry to store building designs. For this task, we compared four different methods: the first is based on classical machine learning, where Histogram of Oriented Gradients (HOG) was used for feature extraction and a Support Vector Machine (SVM) for classification; the other three methods are based on deep learning, covering common pre-trained networks as well as ones designed from scratch. To validate the accuracy of the models, a database of 240 images was used. The accuracy achieved is 57% for the HOG + SVM model, and above 89% for the neural networks.
ER  -


TY  - Preprint
T1  - Stock Chart Pattern recognition with Deep Learning
A1  - Marc Velay
A1  - Fabrice Daniel
JO  - ArXiv e-prints
Y1  - 1 August, 2018
UR  - https://arxiv.org/abs/1808.00418
N2  - This study evaluates the performances of CNN and LSTM for recognizing common charts patterns in a stock historical data. It presents two common patterns, the method used to build the training set, the neural networks architectures and the accuracies obtained.
ER  -


TY  - Preprint
T1  - Lip-Reading Driven Deep Learning Approach for Speech Enhancement
A1  - Ahsan Adeel
A1  - Mandar Gogate
A1  - Amir Hussain
A1  - William M. Whitmer
JO  - ArXiv e-prints
Y1  - 31 July, 2018
UR  - https://arxiv.org/abs/1808.00046
N2  - This paper proposes a novel lip-reading driven deep learning framework for speech enhancement. The proposed approach leverages the complementary strengths of both deep learning and analytical acoustic modelling (filtering based approach) as compared to recently published, comparatively simpler benchmark approaches that rely only on deep learning. The proposed audio-visual (AV) speech enhancement framework operates at two levels. In the first level, a novel deep learning-based lip-reading regression model is employed. In the second level, lip-reading approximated clean-audio features are exploited, using an enhanced, visually-derived Wiener filter (EVWF), for the clean audio power spectrum estimation. Specifically, a stacked long-short-term memory (LSTM) based lip-reading regression model is designed for clean audio features estimation using only temporal visual features considering different number of prior visual frames. For clean speech spectrum estimation, a new filterbank-domain EVWF is formulated, which exploits estimated speech features. The proposed EVWF is compared with conventional Spectral Subtraction and Log-Minimum Mean-Square Error methods using both ideal AV mapping and LSTM driven AV mapping. The potential of the proposed speech enhancement framework is evaluated under different dynamic real-world commercially-motivated scenarios (e.g. cafe, public transport, pedestrian area) at different SNR levels (ranging from low to high SNRs) using benchmark Grid and ChiME3 corpora. For objective testing, perceptual evaluation of speech quality is used to evaluate the quality of restored speech. For subjective testing, the standard mean-opinion-score method is used with inferential statistics. Comparative simulation results demonstrate significant lip-reading and speech enhancement improvement in terms of both speech quality and speech intelligibility.
ER  -


TY  - Preprint
T1  - Fast Sketch Segmentation and Labeling with Deep Learning
A1  - Lei Li
A1  - Hongbo Fu
A1  - Chiew-Lan Tai
JO  - ArXiv e-prints
Y1  - 31 July, 2018
UR  - https://arxiv.org/abs/1807.11847
N2  - We present a simple and efficient method based on deep learning to automatically decompose sketched objects into semantically valid parts. We train a deep neural network to transfer existing segmentations and labelings from 3D models to freehand sketches without requiring numerous well-annotated sketches as training data. The network takes the binary image of a sketched object as input and produces a corresponding segmentation map with per-pixel labelings as output. A subsequent post-process procedure with multi-label graph cuts further refines the segmentation and labeling result. We validate our proposed method on two sketch datasets. Experiments show that our method outperforms the state-of-the-art method in terms of segmentation and labeling accuracy and is significantly faster, enabling further integration in interactive drawing systems. We demonstrate the efficiency of our method in a sketch-based modeling application that automatically transforms input sketches into 3D models by part assembly.
ER  -


TY  - Preprint
T1  - Deep learning in agriculture: A survey
A1  - Andreas Kamilaris
A1  - Francesc X. Prenafeta-Boldu
JO  - ArXiv e-prints
Y1  - 31 July, 2018
UR  - https://arxiv.org/abs/1807.11809
N2  - Deep learning constitutes a recent, modern technique for image processing and data analysis, with promising results and large potential. As deep learning has been successfully applied in various domains, it has recently entered also the domain of agriculture. In this paper, we perform a survey of 40 research efforts that employ deep learning techniques, applied to various agricultural and food production challenges. We examine the particular agricultural problems under study, the specific models and frameworks employed, the sources, nature and pre-processing of data used, and the overall performance achieved according to the metrics used at each work under study. Moreover, we study comparisons of deep learning with other existing popular techniques, in respect to differences in classification or regression performance. Our findings indicate that deep learning provides high accuracy, outperforming existing commonly used image processing techniques.
ER  -


TY  - Preprint
T1  - Disaster Monitoring using Unmanned Aerial Vehicles and Deep Learning
A1  - Andreas Kamilaris
A1  - Francesc X. Prenafeta-BoldÃº
JO  - ArXiv e-prints
Y1  - 8 August, 2018
UR  - https://arxiv.org/abs/1807.11805
N2  - Monitoring of disasters is crucial for mitigating their effects on the environment and human population, and can be facilitated by the use of unmanned aerial vehicles (UAV), equipped with camera sensors that produce aerial photos of the areas of interest. A modern technique for recognition of events based on aerial photos is deep learning. In this paper, we present the state of the art work related to the use of deep learning techniques for disaster identification. We demonstrate the potential of this technique in identifying disasters with high accuracy, by means of a relatively simple deep learning model. Based on a dataset of 544 images (containing disaster images such as fires, earthquakes, collapsed buildings, tsunami and flooding, as well as non-disaster scenes), our results show an accuracy of 91% achieved, indicating that deep learning, combined with UAV equipped with camera sensors, have the potential to predict disasters with high accuracy.
ER  -


TY  - Preprint
T1  - Deep Learning in Physical Layer Communications
A1  - Zhijin Qin
A1  - Hao Ye
A1  - Geoffrey Ye Li
A1  - Biing-Hwang Fred Juang
JO  - ArXiv e-prints
Y1  - 24 September, 2018
UR  - https://arxiv.org/abs/1807.11713
N2  - Deep learning (DL) has shown the great potentials to break the bottleneck of communication systems. This article provides an overview on the recent advancements in DL-based physical layer communications. DL can improve the performance of each individual block in communication systems or optimize the whole transmitter/receiver. Therefore, we categorize the applications of DL in physical layer communications into systems with and without block structures. For DL-based communication systems with block structures, we demonstrate the power of DL in signal compression and signal detection. We also discuss the recent endeavors in developing end-to-end communication systems. Finally, the potential research directions are identified to boost the intelligent physical layer communications with DL.
ER  -


TY  - Preprint
T1  - Spectrum concentration in deep residual learning: a free probability appproach
A1  - Zenan Ling
A1  - Robert C. Qiu
JO  - ArXiv e-prints
Y1  - 31 July, 2018
UR  - https://arxiv.org/abs/1807.11694
N2  - We revisit the initialization of deep residual networks (ResNets) by introducing a novel analytical tool in free probability to the community of deep learning. This tool deals with non-Hermitian random matrices, rather than their conventional Hermitian counterparts in the literature. As a consequence, this new tool enables us to evaluate the singular value spectrum of the input-output Jacobian of a fully- connected deep ResNet for both linear and nonlinear cases. With the powerful tool of free probability, we conduct an asymptotic analysis of the spectrum on the single-layer case, and then extend this analysis to the multi-layer case of an arbitrary number of layers. In particular, we propose to rescale the classical random initialization by the number of residual units, so that the spectrum has the order of $O(1)$, when compared with the large width and depth of the network. We empirically demonstrate that the proposed initialization scheme learns at a speed of orders of magnitudes faster than the classical ones, and thus attests a strong practical relevance of this investigation.
ER  -


TY  - Preprint
T1  - Deep Cross Modal Learning for Caricature Verification and Identification(CaVINet)
A1  - Jatin Garg
A1  - Skand Vishwanath Peri
A1  - Himanshu Tolani
A1  - Narayanan C Krishnan
JO  - ArXiv e-prints
Y1  - 31 July, 2018
UR  - https://arxiv.org/abs/1807.11688
N2  - Learning from different modalities is a challenging task. In this paper, we look at the challenging problem of cross modal face verification and recognition between caricature and visual image modalities. Caricature have exaggerations of facial features of a person. Due to the significant variations in the caricatures, building vision models for recognizing and verifying data from this modality is an extremely challenging task. Visual images with significantly lesser amount of distortions can act as a bridge for the analysis of caricature modality. We introduce a publicly available large Caricature-VIsual dataset [CaVI] with images from both the modalities that captures the rich variations in the caricature of an identity. This paper presents the first cross modal architecture that handles extreme distortions of caricatures using a deep learning network that learns similar representations across the modalities. We use two convolutional networks along with transformations that are subjected to orthogonality constraints to capture the shared and modality specific representations. In contrast to prior research, our approach neither depends on manually extracted facial landmarks for learning the representations, nor on the identities of the person for performing verification. The learned shared representation achieves 91% accuracy for verifying unseen images and 75% accuracy on unseen identities. Further, recognizing the identity in the image by knowledge transfer using a combination of shared and modality specific representations, resulted in an unprecedented performance of 85% rank-1 accuracy for caricatures and 95% rank-1 accuracy for visual images.
ER  -


TY  - Preprint
T1  - Deep Learning-based CSI Feedback Approach for Time-varying Massive MIMO Channels
A1  - Tianqi Wang
A1  - Chao-Kai Wen
A1  - Shi Jin
A1  - Geoffrey Ye Li
JO  - ArXiv e-prints
Y1  - 31 July, 2018
UR  - https://arxiv.org/abs/1807.11673
N2  - Massive multiple-input multiple-output (MIMO) systems rely on channel state information (CSI) feedback to perform precoding and achieve performance gain in frequency division duplex (FDD) networks. However, the huge number of antennas poses a challenge to conventional CSI feedback reduction methods and leads to excessive feedback overhead. In this article, we develop a real-time CSI feedback architecture, called CsiNet-long short-term memory (LSTM), by extending a novel deep learning (DL)-based CSI sensing and recovery network. CsiNet-LSTM considerably enhances recovery quality and improves trade-off between compression ratio (CR) and complexity by directly learning spatial structures combined with time correlation from training samples of time-varying massive MIMO channels. Simulation results demonstrate that CsiNet- LSTM outperforms existing compressive sensing-based and DLbased methods and is remarkably robust to CR reduction.
ER  -


TY  - Preprint
T1  - Security and Privacy Issues in Deep Learning
A1  - Ho Bae
A1  - Jaehee Jang
A1  - Dahuin Jung
A1  - Hyemi Jang
A1  - Heonseok Ha
A1  - Sungroh Yoon
JO  - ArXiv e-prints
Y1  - 31 July, 2018
UR  - https://arxiv.org/abs/1807.11655
N2  - With the development of machine learning, expectations for artificial intelligence (AI) technology are increasing day by day. In particular, deep learning has shown enriched performance results in a variety of fields. There are many applications that are closely related to our daily life, such as making significant decisions in application area based on predictions or classifications, in which a deep learning (DL) model could be relevant. Hence, if a DL model causes mispredictions or misclassifications due to malicious external influences, it can cause very large difficulties in real life. Moreover, training deep learning models involves relying on an enormous amount of data and the training data often includes sensitive information. Therefore, deep learning models should not expose the privacy of such data. In this paper, we reviewed the threats and developed defense methods on the security of the models and the data privacy under the notion of SPAI: Secure and Private AI. We also discuss current challenges and open issues.
ER  -


TY  - Preprint
T1  - State-of-the-art and gaps for deep learning on limited training data in remote sensing
A1  - John E. Ball
A1  - Derek T. Anderson
A1  - Pan Wei
JO  - ArXiv e-prints
Y1  - 11 July, 2018
UR  - https://arxiv.org/abs/1807.11573
N2  - Deep learning usually requires big data, with respect to both volume and variety. However, most remote sensing applications only have limited training data, of which a small subset is labeled. Herein, we review three state-of-the-art approaches in deep learning to combat this challenge. The first topic is transfer learning, in which some aspects of one domain, e.g., features, are transferred to another domain. The next is unsupervised learning, e.g., autoencoders, which operate on unlabeled data. The last is generative adversarial networks, which can generate realistic looking data that can fool the likes of both a deep learning network and human. The aim of this article is to raise awareness of this dilemma, to direct the reader to existing work and to highlight current gaps that need solving.
ER  -


TY  - Preprint
T1  - Improving Spatiotemporal Self-Supervision by Deep Reinforcement Learning
A1  - Uta BÃ¼chler
A1  - Biagio Brattoli
A1  - BjÃ¶rn Ommer
JO  - ArXiv e-prints
Y1  - 30 July, 2018
UR  - https://arxiv.org/abs/1807.11293
N2  - Self-supervised learning of convolutional neural networks can harness large amounts of cheap unlabeled data to train powerful feature representations. As surrogate task, we jointly address ordering of visual data in the spatial and temporal domain. The permutations of training samples, which are at the core of self-supervision by ordering, have so far been sampled randomly from a fixed preselected set. Based on deep reinforcement learning we propose a sampling policy that adapts to the state of the network, which is being trained. Therefore, new permutations are sampled according to their expected utility for updating the convolutional feature representation. Experimental evaluation on unsupervised and transfer learning tasks demonstrates competitive performance on standard benchmarks for image and video classification and nearest neighbor retrieval.
ER  -


TY  - Preprint
T1  - Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes
A1  - Xianyan Jia
A1  - Shutao Song
A1  - Wei He
A1  - Yangzihao Wang
A1  - Haidong Rong
A1  - Feihu Zhou
A1  - Liqiang Xie
A1  - Zhenyu Guo
A1  - Yuanzhou Yang
A1  - Liwei Yu
A1  - Tiegang Chen
A1  - Guangxiao Hu
A1  - Shaohuai Shi
A1  - Xiaowen Chu
JO  - ArXiv e-prints
Y1  - 30 July, 2018
UR  - https://arxiv.org/abs/1807.11205
N2  - Synchronized stochastic gradient descent (SGD) optimizers with data parallelism are widely used in training large-scale deep neural networks. Although using larger mini-batch sizes can improve the system scalability by reducing the communication-to-computation ratio, it may hurt the generalization ability of the models. To this end, we build a highly scalable deep learning training system for dense GPU clusters with three main contributions: (1) We propose a mixed-precision training method that significantly improves the training throughput of a single GPU without losing accuracy. (2) We propose an optimization approach for extremely large mini-batch size (up to 64k) that can train CNN models on the ImageNet dataset without losing accuracy. (3) We propose highly optimized all-reduce algorithms that achieve up to 3x and 11x speedup on AlexNet and ResNet-50 respectively than NCCL-based training on a cluster with 1024 Tesla P40 GPUs. On training ResNet-50 with 90 epochs, the state-of-the-art GPU-based system with 1024 Tesla P100 GPUs spent 15 minutes and achieved 74.9\% top-1 test accuracy, and another KNL-based system with 2048 Intel KNLs spent 20 minutes and achieved 75.4\% accuracy. Our training system can achieve 75.8\% top-1 test accuracy in only 6.6 minutes using 2048 Tesla P40 GPUs. When training AlexNet with 95 epochs, our system can achieve 58.7\% top-1 test accuracy within 4 minutes, which also outperforms all other existing systems.
ER  -


TY  - Preprint
T1  - Human Motion Analysis with Deep Metric Learning
A1  - Huseyin Coskun
A1  - David Joseph Tan
A1  - Sailesh Conjeti
A1  - Nassir Navab
A1  - Federico Tombari
JO  - ArXiv e-prints
Y1  - 5 August, 2018
UR  - https://arxiv.org/abs/1807.11176
N2  - Effectively measuring the similarity between two human motions is necessary for several computer vision tasks such as gait analysis, person identi- fication and action retrieval. Nevertheless, we believe that traditional approaches such as L2 distance or Dynamic Time Warping based on hand-crafted local pose metrics fail to appropriately capture the semantic relationship across motions and, as such, are not suitable for being employed as metrics within these tasks. This work addresses this limitation by means of a triplet-based deep metric learning specifically tailored to deal with human motion data, in particular with the prob- lem of varying input size and computationally expensive hard negative mining due to motion pair alignment. Specifically, we propose (1) a novel metric learn- ing objective based on a triplet architecture and Maximum Mean Discrepancy; as well as, (2) a novel deep architecture based on attentive recurrent neural networks. One benefit of our objective function is that it enforces a better separation within the learned embedding space of the different motion categories by means of the associated distribution moments. At the same time, our attentive recurrent neural network allows processing varying input sizes to a fixed size of embedding while learning to focus on those motion parts that are semantically distinctive. Our ex- periments on two different datasets demonstrate significant improvements over conventional human motion metrics.
ER  -


TY  - Preprint
T1  - Learning to Interrupt: A Hierarchical Deep Reinforcement Learning Framework for Efficient Exploration
A1  - Tingguang Li
A1  - Jin Pan
A1  - Delong Zhu
A1  - Max Q. -H. Meng
JO  - ArXiv e-prints
Y1  - 29 July, 2018
UR  - https://arxiv.org/abs/1807.11150
N2  - To achieve scenario intelligence, humans must transfer knowledge to robots by developing goal-oriented algorithms, which are sometimes insensitive to dynamically changing environments. While deep reinforcement learning achieves significant success recently, it is still extremely difficult to be deployed in real robots directly. In this paper, we propose a hybrid structure named Option-Interruption in which human knowledge is embedded into a hierarchical reinforcement learning framework. Our architecture has two key components: options, represented by existing human-designed methods, can significantly speed up the training process and interruption mechanism, based on learnable termination functions, enables our system to quickly respond to the external environment. To implement this architecture, we derive a set of update rules based on policy gradient methods and present a complete training process. In the experiment part, our method is evaluated in Four-room navigation and exploration task, which shows the efficiency and flexibility of our framework.
ER  -


TY  - Preprint
T1  - Towards End-to-End Acoustic Localization using Deep Learning: from Audio Signal to Source Position Coordinates
A1  - Juan Manuel Vera-Diaz
A1  - Daniel Pizarro
A1  - Javier Macias-Guarasa
JO  - ArXiv e-prints
Y1  - 29 July, 2018
UR  - https://arxiv.org/abs/1807.11094
N2  - This paper presents a novel approach for indoor acoustic source localization using microphone arrays and based on a Convolutional Neural Network (CNN). The proposed solution is, to the best of our knowledge, the first published work in which the CNN is designed to directly estimate the three dimensional position of an acoustic source, using the raw audio signal as the input information avoiding the use of hand crafted audio features. Given the limited amount of available localization data, we propose in this paper a training strategy based on two steps. We first train our network using semi-synthetic data, generated from close talk speech recordings, and where we simulate the time delays and distortion suffered in the signal that propagates from the source to the array of microphones. We then fine tune this network using a small amount of real data. Our experimental results show that this strategy is able to produce networks that significantly improve existing localization methods based on \textit{SRP-PHAT} strategies. In addition, our experiments show that our CNN method exhibits better resistance against varying gender of the speaker and different window sizes compared with the other methods.
ER  -


TY  - Preprint
T1  - A Survey of Machine and Deep Learning Methods for Internet of Things (IoT) Security
A1  - Mohammed Ali Al-Garadi
A1  - Amr Mohamed
A1  - Abdulla Al-Ali
A1  - Xiaojiang Du
A1  - Mohsen Guizani
JO  - ArXiv e-prints
Y1  - 29 July, 2018
UR  - https://arxiv.org/abs/1807.11023
N2  - The Internet of Things (IoT) integrates billions of smart devices that can communicate with one another with minimal human intervention. It is one of the fastest developing fields in the history of computing, with an estimated 50 billion devices by the end of 2020. On the one hand, IoT play a crucial role in enhancing several real-life smart applications that can improve life quality. On the other hand, the crosscutting nature of IoT systems and the multidisciplinary components involved in the deployment of such systems introduced new security challenges. Implementing security measures, such as encryption, authentication, access control, network security and application security, for IoT devices and their inherent vulnerabilities is ineffective. Therefore, existing security methods should be enhanced to secure the IoT system effectively. Machine learning and deep learning (ML/DL) have advanced considerably over the last few years, and machine intelligence has transitioned from laboratory curiosity to practical machinery in several important applications. Consequently, ML/DL methods are important in transforming the security of IoT systems from merely facilitating secure communication between devices to security-based intelligence systems. The goal of this work is to provide a comprehensive survey of ML /DL methods that can be used to develop enhanced security methods for IoT systems. IoT security threats that are related to inherent or newly introduced threats are presented, and various potential IoT system attack surfaces and the possible threats related to each surface are discussed. We then thoroughly review ML/DL methods for IoT security and present the opportunities, advantages and shortcomings of each method. We discuss the opportunities and challenges involved in applying ML/DL to IoT security. These opportunities and challenges can serve as potential future research directions.
ER  -


TY  - Preprint
T1  - Bridge the Gap Between VQA and Human Behavior on Omnidirectional Video: A Large-Scale Dataset and a Deep Learning Model
A1  - Chen Li
A1  - Mai Xu
A1  - Xinzhe Du
A1  - Zulin Wang
JO  - ArXiv e-prints
Y1  - 28 July, 2018
UR  - https://arxiv.org/abs/1807.10990
N2  - Omnidirectional video enables spherical stimuli with the $360 \times 180^ \circ$ viewing range. Meanwhile, only the viewport region of omnidirectional video can be seen by the observer through head movement (HM), and an even smaller region within the viewport can be clearly perceived through eye movement (EM). Thus, the subjective quality of omnidirectional video may be correlated with HM and EM of human behavior. To fill in the gap between subjective quality and human behavior, this paper proposes a large-scale visual quality assessment (VQA) dataset of omnidirectional video, called VQA-OV, which collects 60 reference sequences and 540 impaired sequences. Our VQA-OV dataset provides not only the subjective quality scores of sequences but also the HM and EM data of subjects. By mining our dataset, we find that the subjective quality of omnidirectional video is indeed related to HM and EM. Hence, we develop a deep learning model, which embeds HM and EM, for objective VQA on omnidirectional video. Experimental results show that our model significantly improves the state-of-the-art performance of VQA on omnidirectional video.
ER  -


TY  - Preprint
T1  - A Survey of the Usages of Deep Learning in Natural Language Processing
A1  - Daniel W. Otter
A1  - Julian R. Medina
A1  - Jugal K. Kalita
JO  - ArXiv e-prints
Y1  - 27 July, 2018
UR  - https://arxiv.org/abs/1807.10854
N2  - Over the last several years, the field of natural language processing has been propelled forward by an explosion in the use of deep learning models. This survey provides a brief introduction to the field and a quick overview of deep learning architectures and methods. It then sifts through the plethora of recent studies and summarizes a large assortment of relevant contributions. Analyzed research areas include several core linguistic processing issues in addition to a number of applications of computational linguistics. A discussion of the current state of the art is then provided along with recommendations for future research in the field.
ER  -


TY  - Preprint
T1  - On the overfly algorithm in deep learning of neural networks
A1  - Alexei Tsygvintsev
JO  - ArXiv e-prints
Y1  - 27 September, 2018
UR  - https://arxiv.org/abs/1807.10668
N2  - In this paper we investigate the supervised backpropagation training of multilayer neural networks from a dynamical systems point of view. We discuss some links with the qualitative theory of differential equations and introduce the overfly algorithm to tackle the local minima problem. Our approach is based on the existence of first integrals of the generalised gradient system with build-in dissipation.
ER  -


TY  - Preprint
T1  - Deep Learning Hyperspectral Image Classification Using Multiple Class-based Denoising Autoencoders, Mixed Pixel Training Augmentation, and Morphological Operations
A1  - John E. Ball
A1  - Pan Wei
JO  - ArXiv e-prints
Y1  - 11 July, 2018
UR  - https://arxiv.org/abs/1807.10574
N2  - Herein, we present a system for hyperspectral image segmentation that utilizes multiple class--based denoising autoencoders which are efficiently trained. Moreover, we present a novel hyperspectral data augmentation method for labelled HSI data using linear mixtures of pixels from each class, which helps the system with edge pixels which are almost always mixed pixels. Finally, we utilize a deep neural network and morphological hole-filling to provide robust image classification. Results run on the Salinas dataset verify the high performance of the proposed algorithm.
ER  -


TY  - Preprint
T1  - Embedded Implementation of a Deep Learning Smile Detector
A1  - Pedram Ghazi
A1  - Antti P. Happonen
A1  - Jani Boutellier
A1  - Heikki Huttunen
JO  - ArXiv e-prints
Y1  - 10 July, 2018
UR  - https://arxiv.org/abs/1807.10570
N2  - In this paper we study the real time deployment of deep learning algorithms in low resource computational environments. As the use case, we compare the accuracy and speed of neural networks for smile detection using different neural network architectures and their system level implementation on NVidia Jetson embedded platform. We also propose an asynchronous multithreading scheme for parallelizing the pipeline. Within this framework, we experimentally compare thirteen widely used network topologies. The experiments show that low complexity architectures can achieve almost equal performance as larger ones, with a fraction of computation required.
ER  -


TY  - Preprint
T1  - DeepLink: A Novel Link Prediction Framework based on Deep Learning
A1  - Mohammad Mehdi Keikha
A1  - Maseud Rahgozar
A1  - Masoud Asadpour
JO  - ArXiv e-prints
Y1  - 27 July, 2018
UR  - https://arxiv.org/abs/1807.10494
N2  - Recently, link prediction has attracted more attentions from various disciplines such as computer science, bioinformatics and economics. In this problem, unknown links between nodes are discovered based on numerous information such as network topology, profile information and user generated contents. Most of the previous researchers have focused on the structural features of the networks. While the recent researches indicate that contextual information can change the network topology. Although, there are number of valuable researches which combine structural and content information, but they face with the scalability issue due to feature engineering. Because, majority of the extracted features are obtained by a supervised or semi supervised algorithm. Moreover, the existing features are not general enough to indicate good performance on different networks with heterogeneous structures. Besides, most of the previous researches are presented for undirected and unweighted networks. In this paper, a novel link prediction framework called &#34;DeepLink&#34; is presented based on deep learning techniques. In contrast to the previous researches which fail to automatically extract best features for the link prediction, deep learning reduces the manual feature engineering. In this framework, both the structural and content information of the nodes are employed. The framework can use different structural feature vectors, which are prepared by various link prediction methods. It considers all proximity orders that are presented in a network during the structural feature learning. We have evaluated the performance of DeepLink on two real social network datasets including Telegram and irBlogs. On both datasets, the proposed framework outperforms several structural and hybrid approaches for link prediction problem.
ER  -


TY  - Preprint
T1  - A Deep Learning Framework for Automatic Diagnosis in Lung Cancer
A1  - Nikolay Burlutskiy
A1  - Feng Gu
A1  - Lena Kajland Wilen
A1  - Max Backman
A1  - Patrick Micke
JO  - ArXiv e-prints
Y1  - 27 July, 2018
UR  - https://arxiv.org/abs/1807.10466
N2  - We developed a deep learning framework that helps to automatically identify and segment lung cancer areas in patients&#39; tissue specimens. The study was based on a cohort of lung cancer patients operated at the Uppsala University Hospital. The tissues were reviewed by lung pathologists and then the cores were compiled to tissue micro-arrays (TMAs). For experiments, hematoxylin-eosin stained slides from 712 patients were scanned and then manually annotated. Then these scans and annotations were used to train segmentation models of the developed framework. The performance of the developed deep learning framework was evaluated on fully annotated TMA cores from 178 patients reaching pixel-wise precision of 0.80 and recall of 0.86. Finally, publicly available Stanford TMA cores were used to demonstrate high performance of the framework qualitatively.
ER  -


TY  - Preprint
T1  - DeepSPINE: Automated Lumbar Vertebral Segmentation, Disc-level Designation, and Spinal Stenosis Grading Using Deep Learning
A1  - Jen-Tang Lu
A1  - Stefano Pedemonte
A1  - Bernardo Bizzo
A1  - Sean Doyle
A1  - Katherine P. Andriole
A1  - Mark H. Michalski
A1  - R. Gilberto Gonzalez
A1  - Stuart R. Pomerantz
JO  - ArXiv e-prints
Y1  - 26 July, 2018
UR  - https://arxiv.org/abs/1807.10215
N2  - The high prevalence of spinal stenosis results in a large volume of MRI imaging, yet interpretation can be time-consuming with high inter-reader variability even among the most specialized radiologists. In this paper, we develop an efficient methodology to leverage the subject-matter-expertise stored in large-scale archival reporting and image data for a deep-learning approach to fully-automated lumbar spinal stenosis grading. Specifically, we introduce three major contributions: (1) a natural-language-processing scheme to extract level-by-level ground-truth labels from free-text radiology reports for the various types and grades of spinal stenosis (2) accurate vertebral segmentation and disc-level localization using a U-Net architecture combined with a spine-curve fitting method, and (3) a multi-input, multi-task, and multi-class convolutional neural network to perform central canal and foraminal stenosis grading on both axial and sagittal imaging series inputs with the extracted report-derived labels applied to corresponding imaging level segments. This study uses a large dataset of 22796 disc-levels extracted from 4075 patients. We achieve state-of-the-art performance on lumbar spinal stenosis classification and expect the technique will increase both radiology workflow efficiency and the perceived value of radiology reports for referring clinicians and patients.
ER  -


TY  - Preprint
T1  - LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks
A1  - Dongqing Zhang
A1  - Jiaolong Yang
A1  - Dongqiangzi Ye
A1  - Gang Hua
JO  - ArXiv e-prints
Y1  - 26 July, 2018
UR  - https://arxiv.org/abs/1807.10029
N2  - Although weight and activation quantization is an effective approach for Deep Neural Network (DNN) compression and has a lot of potentials to increase inference speed leveraging bit-operations, there is still a noticeable gap in terms of prediction accuracy between the quantized model and the full-precision model. To address this gap, we propose to jointly train a quantized, bit-operation-compatible DNN and its associated quantizers, as opposed to using fixed, handcrafted quantization schemes such as uniform or logarithmic quantization. Our method for learning the quantizers applies to both network weights and activations with arbitrary-bit precision, and our quantizers are easy to train. The comprehensive experiments on CIFAR-10 and ImageNet datasets show that our method works consistently well for various network structures such as AlexNet, VGG-Net, GoogLeNet, ResNet, and DenseNet, surpassing previous quantization methods in terms of accuracy by an appreciable margin. Code available at https://github.com/Microsoft/LQ-Nets
ER  -


TY  - Preprint
T1  - A Deep Material Network for Multiscale Topology Learning and Accelerated Nonlinear Modeling of Heterogeneous Materials
A1  - Zeliang Liu
A1  - C. T. Wu
A1  - M. Koishi
JO  - ArXiv e-prints
Y1  - 25 September, 2018
UR  - https://arxiv.org/abs/1807.09829
N2  - In this paper, a new data-driven multiscale material modeling method, which we refer to as deep material network, is developed based on mechanistic homogenization theory of representative volume element (RVE) and advanced machine learning techniques. We propose to use a collection of connected mechanistic building blocks with analytical homogenization solutions which avoids the loss of essential physics in generic neural networks, and this concept is demonstrated for 2-dimensional RVE problems and network depth up to 7. Based on linear elastic RVE data from offline direct numerical simulations, the material network can be effectively trained using stochastic gradient descent with backpropagation algorithm, enhanced by model compression methods. Importantly, the trained network is valid for any local material laws without the need for additional calibration or micromechanics assumption. Its extrapolations to unknown material and loading spaces for a wide range of problems are validated through numerical experiments, including linear elasticity with high contrast of phase properties, nonlinear history-dependent plasticity and finite-strain hyperelasticity under large deformations.
ER  -


TY  - Preprint
T1  - PADME: A Deep Learning-based Framework for Drug-Target Interaction Prediction
A1  - Qingyuan Feng
A1  - Evgenia Dueva
A1  - Artem Cherkasov
A1  - Martin Ester
JO  - ArXiv e-prints
Y1  - 25 July, 2018
UR  - https://arxiv.org/abs/1807.09741
N2  - In silico Drug-target Interaction (DTI) prediction is an important and challenging problem in medicinal chemistry with a huge potential benefit to the pharmaceutical industry and patients. Most existing methods for DTI prediction generally have binary endpoints, which could be an oversimplification of the problem. With the advent of deep learning, some deep learning models were devised to solve the DTI prediction problem, but most of them still use binary endpoints, and they are generally unable to handle cold-target problems, i.e., problems involving target protein that never appeared in the training set. We contrived PADME (Protein And Drug Molecule interaction prEdiction), a framework based on Deep Neural Networks, to predict real-valued interaction strength between compounds and proteins. PADME inputs both compound and protein information into the model, so it is applicable to cold-target problems. To our knowledge, we are also the first to incorporate Molecular Graph Convolution (MGC) into the model for compound featurization. We used different Cross-Validation split schemes and different metrics to measure the performance of PADME on multiple datasets (in which we are the first to use ToxCast for such problems), and PADME consistently dominates baseline methods. We also conducted a case study, predicting the interaction between compounds and androgen receptor (AR) and compared the prediction results with growth inhibition activity of the compounds in NCI60, which also gave us satisfactory results, suggesting PADME&#39;s potential in drug development. We expect different variants of PADME to be proposed and experimented on in the future, and we believe Deep Learning will transform the field of cheminformatics.
ER  -


TY  - Preprint
T1  - Scheduling Computation Graphs of Deep Learning Models on Manycore CPUs
A1  - Linpeng Tang
A1  - Yida Wang
A1  - Theodore L. Willke
A1  - Kai Li
JO  - ArXiv e-prints
Y1  - 16 July, 2018
UR  - https://arxiv.org/abs/1807.09667
N2  - For a deep learning model, efficient execution of its computation graph is key to achieving high performance. Previous work has focused on improving the performance for individual nodes of the computation graph, while ignoring the parallelization of the graph as a whole. However, we observe that running multiple operations simultaneously without interference is critical to efficiently perform parallelizable small operations. The attempt of executing the computation graph in parallel in deep learning frameworks usually involves much resource contention among concurrent operations, leading to inferior performance on manycore CPUs. To address these issues, in this paper, we propose Graphi, a generic and high-performance execution engine to efficiently execute a computation graph in parallel on manycore CPUs. Specifically, Graphi minimizes the interference on both software/hardware resources, discovers the best parallel setting with a profiler, and further optimizes graph execution with the critical-path first scheduling. Our experiments show that the parallel execution consistently outperforms the sequential one. The training times on four different neural networks with Graphi are 2.1x to 9.5x faster than those with TensorFlow on a 68-core Intel Xeon Phi processor.
ER  -


TY  - Preprint
T1  - Multi-Tenant Cross-Slice Resource Orchestration: A Deep Reinforcement Learning Approach
A1  - Xianfu Chen
A1  - Zhifeng Zhao
A1  - Celimuge Wu
A1  - Mehdi Bennis
A1  - Hang Liu
A1  - Yusheng Ji
A1  - Honggang Zhang
JO  - ArXiv e-prints
Y1  - 17 July, 2018
UR  - https://arxiv.org/abs/1807.09350
N2  - In a software-defined radio access network (RAN), a major challenge lies in how to support diverse services for mobile users (MUs) over a common physical network infrastructure. Network slicing is a promising solution to tailor the network to match such service requests. This paper considers a software-defined RAN, where a limited number of channels are auctioned across scheduling slots to MUs of multiple service providers (SPs) (i.e., the tenants). Each SP behaves selfishly to maximize the expected long-term payoff from competition with other SPs for the orchestration of channel access opportunities over its MUs, which request both mobile-edge computing and traditional cellular services in the slices. This problem is modelled as a stochastic game, in which the decision makings of a SP depend on the network dynamics as well as the control policies of its competitors. We propose an abstract stochastic game to approximate the Nash equilibrium. The selfish behaviours of a SP can then be characterized by a single-agent Markov decision process (MDP). To simplify decision makings, we linearly decompose the per-SP MDP and derive an online scheme based on deep reinforcement learning to approach the optimal abstract control policies. Numerical experiments show significant performance gains from our scheme.
ER  -


TY  - Preprint
T1  - Deep Learning on Retina Images as Screening Tool for Diagnostic Decision Support
A1  - Maria Camila Alvarez Trivino
A1  - Jeremie Despraz
A1  - Jesus Alfonso Lopez Sotelo
A1  - Carlos Andres Pena
JO  - ArXiv e-prints
Y1  - 24 July, 2018
UR  - https://arxiv.org/abs/1807.09232
N2  - In this project, we developed a deep learning system applied to human retina images for medical diagnostic decision support. The retina images were provided by EyePACS. These images were used in the framework of a Kaggle contest, whose purpose to identify diabetic retinopathy signs through an automatic detection system. Using as inspiration one of the solutions proposed in the contest, we implemented a model that successfully detects diabetic retinopathy from retina images. After a carefully designed preprocessing, the images were used as input to a deep convolutional neural network (CNN). The CNN performed a feature extraction process followed by a classification stage, which allowed the system to differentiate between healthy and ill patients using five categories. Our model was able to identify diabetic retinopathy in the patients with an agreement rate of 76.73% with respect to the medical expert&#39;s labels for the test data.
ER  -


TY  - Preprint
T1  - End-to-End Deep Imitation Learning: Robot Soccer Case Study
A1  - Okan AÅÄ±k
A1  - Binnur GÃ¶rer
A1  - H. Levent AkÄ±n
JO  - ArXiv e-prints
Y1  - 28 June, 2018
UR  - https://arxiv.org/abs/1807.09205
N2  - In imitation learning, behavior learning is generally done using the features extracted from the demonstration data. Recent deep learning algorithms enable the development of machine learning methods that can get high dimensional data as an input. In this work, we use imitation learning to teach the robot to dribble the ball to the goal. We use B-Human robot software to collect demonstration data and a deep convolutional network to represent the policies. We use top and bottom camera images of the robot as input and speed commands as outputs. The CNN policy learns the mapping between the series of images and speed commands. In 3D realistic robotics simulator experiments, we show that the robot is able to learn to search the ball and dribble the ball, but it struggles to align to the goal. The best-proposed policy model learns to score 4 goals out of 20 test episodes.
ER  -


TY  - Preprint
T1  - Self-Paced Learning with Adaptive Deep Visual Embeddings
A1  - Vithursan Thangarasa
A1  - Graham W. Taylor
JO  - ArXiv e-prints
Y1  - 24 July, 2018
UR  - https://arxiv.org/abs/1807.09200
N2  - Selecting the most appropriate data examples to present a deep neural network (DNN) at different stages of training is an unsolved challenge. Though practitioners typically ignore this problem, a non-trivial data scheduling method may result in a significant improvement in both convergence and generalization performance. In this paper, we introduce Self-Paced Learning with Adaptive Deep Visual Embeddings (SPL-ADVisE), a novel end-to-end training protocol that unites self-paced learning (SPL) and deep metric learning (DML). We leverage the Magnet Loss to train an embedding convolutional neural network (CNN) to learn a salient representation space. The student CNN classifier dynamically selects similar instance-level training examples to form a mini-batch, where the easiness from the cross-entropy loss and the true diverseness of examples from the learned metric space serve as sample importance priors. To demonstrate the effectiveness of SPL-ADVisE, we use deep CNN architectures for the task of supervised image classification on several coarse- and fine-grained visual recognition datasets. Results show that, across all datasets, the proposed method converges faster and reaches a higher final accuracy than other SPL variants, particularly on fine-grained classes.
ER  -


TY  - Preprint
T1  - Deep-CLASS at ISIC Machine Learning Challenge 2018
A1  - Sara Nasiri
A1  - Matthias Jung
A1  - Julien Helsper
A1  - Madjid Fathi
JO  - ArXiv e-prints
Y1  - 24 July, 2018
UR  - https://arxiv.org/abs/1807.08993
N2  - This paper reports the method and evaluation results of MedAusbild team for ISIC challenge task. Since early 2017, our team has worked on melanoma classification [1][6], and has employed deep learning since beginning of 2018 [7]. Deep learning helps researchers absolutely to treat and detect diseases by analyzing medical data (e.g., medical images). One of the representative models among the various deep-learning models is a convolutional neural network (CNN). Although our team has an experience with segmentation and classification of benign and malignant skin-lesions, we have participated in the task 3 of ISIC Challenge 2018 for classification of seven skin diseases, explained in this paper.
ER  -


TY  - Preprint
T1  - Deep Learning from Label Proportions for Emphysema Quantification
A1  - Gerda Bortsova
A1  - Florian Dubost
A1  - Silas Ãrting
A1  - Ioannis Katramados
A1  - Laurens Hogeweg
A1  - Laura Thomsen
A1  - Mathilde Wille
A1  - Marleen de Bruijne
JO  - ArXiv e-prints
Y1  - 23 July, 2018
UR  - https://arxiv.org/abs/1807.08601
N2  - We propose an end-to-end deep learning method that learns to estimate emphysema extent from proportions of the diseased tissue. These proportions were visually estimated by experts using a standard grading system, in which grades correspond to intervals (label example: 1-5% of diseased tissue). The proposed architecture encodes the knowledge that the labels represent a volumetric proportion. A custom loss is designed to learn with intervals. Thus, during training, our network learns to segment the diseased tissue such that its proportions fit the ground truth intervals. Our architecture and loss combined improve the performance substantially (8% ICC) compared to a more conventional regression network. We outperform traditional lung densitometry and two recently published methods for emphysema quantification by a large margin (at least 7% AUC and 15% ICC), and achieve near-human-level performance. Moreover, our method generates emphysema segmentations that predict the spatial distribution of emphysema at human level.
ER  -


TY  - Preprint
T1  - Real-Time Patient-Specific Lung Radiotherapy Targeting using Deep Learning
A1  - Markus D. Foote
A1  - Blake Zimmerman
A1  - Amit Sawant
A1  - Sarang Joshi
JO  - ArXiv e-prints
Y1  - 22 July, 2018
UR  - https://arxiv.org/abs/1807.08388
N2  - Radiation therapy has presented a need for dynamic tracking of a target tumor volume. Fiducial markers such as implanted gold seeds have been used to gate radiation delivery but the markers are invasive and gating significantly increases treatment time. Pretreatment acquisition of a 4DCT allows for the development of accurate motion estimation for treatment planning. A deep convolutional neural network and subspace motion tracking is used to recover anatomical positions from a single radiograph projection in real-time. We approximate the nonlinear inverse of a diffeomorphic transformation composed with radiographic projection as a deep network that produces subspace coordinates to define the patient-specific deformation of the lungs from a baseline anatomic position. The geometric accuracy of the subspace projections on real patient data is similar to accuracy attained by original image registration between individual respiratory-phase image volumes.
ER  -


TY  - Preprint
T1  - Skin Lesion Analysis Towards Melanoma Detection via End-to-end Deep Learning of Convolutional Neural Networks
A1  - Katherine M. Li
A1  - Evelyn C. Li
JO  - ArXiv e-prints
Y1  - 22 July, 2018
UR  - https://arxiv.org/abs/1807.08332
N2  - This article presents the design, experiments and results of our solution submitted to the 2018 ISIC challenge: Skin Lesion Analysis Towards Melanoma Detection. We design a pipeline using state-of-the-art Convolutional Neural Network (CNN) models for a Lesion Boundary Segmentation task and a Lesion Diagnosis task.
ER  -


TY  - Preprint
T1  - Deep Learning Parametrization for B-Spline Curve Approximation
A1  - Pascal Laube
A1  - Matthias O. Franz
A1  - Georg Umlauf
JO  - ArXiv e-prints
Y1  - 22 July, 2018
UR  - https://arxiv.org/abs/1807.08304
N2  - In this paper we present a method using deep learning to compute parametrizations for B-spline curve approximation. Existing methods consider the computation of parametric values and a knot vector as separate problems. We propose to train interdependent deep neural networks to predict parametric values and knots. We show that it is possible to include B-spline curve approximation directly into the neural network architecture. The resulting parametrizations yield tight approximations and are able to outperform state-of-the-art methods.
ER  -


TY  - Preprint
T1  - Correlation Net : spatio temporal multimodal deep learning
A1  - Novanto Yudistira
A1  - Takio Kurita
JO  - ArXiv e-prints
Y1  - 6 October, 2018
UR  - https://arxiv.org/abs/1807.08291
N2  - This letter describes a network that is able to capture spatiotemporal correlations over arbitrary timestamps. The proposed scheme operates as a complementary, extended network over spatiotemporal regions. Recently, multimodal fusion has been extensively researched in deep learning. For action recognition, the spatial and temporal streams are vital components of deep Convolutional Neural Network (CNNs), but reducing the occurrence of overfitting and fusing these two streams remain open problems. The existing fusion approach is to average the two streams. To this end, we propose a correlation network with a Shannon fusion to learn a CNN that has already been trained. Long-range video may consist of spatiotemporal correlation over arbitrary times. This correlation can be captured using simple fully connected layers to form the correlation network. This is found to be complementary to the existing network fusion methods. We evaluate our approach on the UCF-101 and HMDB-51 datasets, and the resulting improvement in accuracy demonstrates the importance of multimodal correlation.
ER  -


TY  - Preprint
T1  - Implementation of Q Learning and Deep Q Network For Controlling a Self Balancing Robot Model
A1  - MD Muhaimin Rahman
A1  - SM Hasanur Rashid
A1  - M. M Hossain
JO  - ArXiv e-prints
Y1  - 22 July, 2018
UR  - https://arxiv.org/abs/1807.08272
N2  - In this paper, the implementation of two Reinforcement learnings namely, Q Learning and Deep Q Network(DQN) on a Self Balancing Robot Gazebo model has been discussed. The goal of the experiments is to make the robot model learn the best actions for staying balanced in an environment. The more time it can stay within a specified limit , the more reward it accumulates and hence more balanced it is. Different experiments with different learning parameters on Q Learning and DQN are conducted and the plots of the experiments are shown.
ER  -


TY  - Preprint
T1  - Deep learning at the shallow end: Malware classification for non-domain experts
A1  - Quan Le
A1  - OisÃ­n Boydell
A1  - Brian Mac Namee
A1  - Mark Scanlon
JO  - ArXiv e-prints
Y1  - 22 July, 2018
UR  - https://arxiv.org/abs/1807.08265
N2  - Current malware detection and classification approaches generally rely on time consuming and knowledge intensive processes to extract patterns (signatures) and behaviors from malware, which are then used for identification. Moreover, these signatures are often limited to local, contiguous sequences within the data whilst ignoring their context in relation to each other and throughout the malware file as a whole. We present a Deep Learning based malware classification approach that requires no expert domain knowledge and is based on a purely data driven approach for complex pattern and feature identification.
ER  -


TY  - Preprint
T1  - NAVREN-RL: Learning to fly in real environment via end-to-end deep reinforcement learning using monocular images
A1  - Malik Aqeel Anwar
A1  - Arijit Raychowdhury
JO  - ArXiv e-prints
Y1  - 22 July, 2018
UR  - https://arxiv.org/abs/1807.08241
N2  - We present NAVREN-RL, an approach to NAVigate an unmanned aerial vehicle in an indoor Real ENvironment via end-to-end reinforcement learning RL. A suitable reward function is designed keeping in mind the cost and weight constraints for micro drone with minimum number of sensing modalities. Collection of small number of expert data and knowledge based data aggregation is integrated into the RL process to aid convergence. Experimentation is carried out on a Parrot AR drone in different indoor arenas and the results are compared with other baseline technologies. We demonstrate how the drone successfully avoids obstacles and navigates across different arenas.
ER  -


TY  - Preprint
T1  - Learning Deep Hidden Nonlinear Dynamics from Aggregate Data
A1  - Yisen Wang
A1  - Bo Dai
A1  - Lingkai Kong
A1  - Sarah Monazam Erfani
A1  - James Bailey
A1  - Hongyuan Zha
JO  - ArXiv e-prints
Y1  - 29 July, 2018
UR  - https://arxiv.org/abs/1807.08237
N2  - Learning nonlinear dynamics from diffusion data is a challenging problem since the individuals observed may be different at different time points, generally following an aggregate behaviour. Existing work cannot handle the tasks well since they model such dynamics either directly on observations or enforce the availability of complete longitudinal individual-level trajectories. However, in most of the practical applications, these requirements are unrealistic: the evolving dynamics may be too complex to be modeled directly on observations, and individual-level trajectories may not be available due to technical limitations, experimental costs and/or privacy issues. To address these challenges, we formulate a model of diffusion dynamics as the {\em hidden stochastic process} via the introduction of hidden variables for flexibility, and learn the hidden dynamics directly on {\em aggregate observations} without any requirement for individual-level trajectories. We propose a dynamic generative model with Wasserstein distance for LEarninG dEep hidden Nonlinear Dynamics (LEGEND) and prove its theoretical guarantees as well. Experiments on a range of synthetic and real-world datasets illustrate that LEGEND has very strong performance compared to state-of-the-art baselines.
ER  -


TY  - Preprint
T1  - Recent Advances in Deep Learning: An Overview
A1  - Matiur Rahman Minar
A1  - Jibon Naher
JO  - ArXiv e-prints
Y1  - 21 July, 2018
UR  - https://arxiv.org/abs/1807.08169
N2  - Deep Learning is one of the newest trends in Machine Learning and Artificial Intelligence research. It is also one of the most popular scientific research trends now-a-days. Deep learning methods have brought revolutionary advances in computer vision and machine learning. Every now and then, new and new deep learning techniques are being born, outperforming state-of-the-art machine learning and even existing deep learning techniques. In recent years, the world has seen many major breakthroughs in this field. Since deep learning is evolving at a huge speed, its kind of hard to keep track of the regular advances especially for new researchers. In this paper, we are going to briefly discuss about recent advances in Deep Learning for past few years.
ER  -


TY  - Preprint
T1  - What is not where: the challenge of integrating spatial representations into deep learning architectures
A1  - John D. Kelleher
A1  - Simon Dobnik
JO  - ArXiv e-prints
Y1  - 21 July, 2018
UR  - https://arxiv.org/abs/1807.08133
N2  - This paper examines to what degree current deep learning architectures for image caption generation capture spatial language. On the basis of the evaluation of examples of generated captions from the literature we argue that systems capture what objects are in the image data but not where these objects are located: the captions generated by these systems are the output of a language model conditioned on the output of an object detector that cannot capture fine-grained location information. Although language models provide useful knowledge for image captions, we argue that deep learning image captioning architectures should also model geometric relations between objects.
ER  -


TY  - Preprint
T1  - Learning Heuristics for Automated Reasoning through Deep Reinforcement Learning
A1  - Gil Lederman
A1  - Markus N. Rabe
A1  - Sanjit A. Seshia
JO  - ArXiv e-prints
Y1  - 20 July, 2018
UR  - https://arxiv.org/abs/1807.08058
N2  - We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning. We consider search algorithms for quantified Boolean logics, that already can solve formulas of impressive size - up to 100s of thousands of variables. The main challenge is to find a representation which lends to making predictions in a scalable way. The heuristics learned through our approach significantly improve over the handwritten heuristics for several sets of formulas.
ER  -


TY  - Preprint
T1  - Ensemble of Deep Learned Features for Melanoma Classification
A1  - Loris Nanni
A1  - Alessandra Lumini
A1  - Stefano Ghidoni
JO  - ArXiv e-prints
Y1  - 20 July, 2018
UR  - https://arxiv.org/abs/1807.08008
N2  - The aim of this work is to propose an ensemble of descriptors for Melanoma Classification, whose performance has been evaluated on validation and test datasets of the melanoma challenge 2018. The system proposed here achieves a strong discriminative power thanks to the combination of multiple descriptors. The proposed system represents a very simple yet effective way of boosting the performance of trained CNNs by composing multiple CNNs into an ensemble and combining scores by sum rule. Several types of ensembles are considered, with different CNN architectures along with different learning parameter sets. Moreover CNN are used as feature extractors: an input image is processed by a trained CNN and the response of a particular layer (usually the classification layer, but also internal layers can be employed) is treated as a descriptor for the image and used for training a set of Support Vector Machines (SVM).
ER  -


TY  - Preprint
T1  - Data-Efficient Weakly Supervised Learning for Low-Resource Audio Event Detection Using Deep Learning
A1  - Veronica Morfi
A1  - Dan Stowell
JO  - ArXiv e-prints
Y1  - 17 July, 2018
UR  - https://arxiv.org/abs/1807.06972
N2  - We propose a method to perform audio event detection under the common constraint that only limited training data are available. In training a deep learning system to perform audio event detection, two practical problems arise. Firstly, most datasets are &#39;weakly labelled&#39; having only a list of events present in each recording without any temporal information for training. Secondly, deep neural networks need a very large amount of labelled training data to achieve good quality performance, yet in practice it is difficult to collect enough samples for most classes of interest. In this paper, we propose a data-efficient training of a stacked convolutional and recurrent neural network. This neural network is trained in a multi instance learning setting for which we introduce a new loss function that leads to improved training compared to the usual approaches for weakly supervised learning. We successfully test our approach on a low-resource dataset that lacks temporal labels, for bird vocalisation detection.
ER  -


TY  - Preprint
T1  - Towards Automated Deep Learning: Efficient Joint Neural Architecture and Hyperparameter Search
A1  - Arber Zela
A1  - Aaron Klein
A1  - Stefan Falkner
A1  - Frank Hutter
JO  - ArXiv e-prints
Y1  - 18 July, 2018
UR  - https://arxiv.org/abs/1807.06906
N2  - While existing work on neural architecture search (NAS) tunes hyperparameters in a separate post-processing step, we demonstrate that architectural choices and other hyperparameter settings interact in a way that can render this separation suboptimal. Likewise, we demonstrate that the common practice of using very few epochs during the main NAS and much larger numbers of epochs during a post-processing step is inefficient due to little correlation in the relative rankings for these two training regimes. To combat both of these problems, we propose to use a recent combination of Bayesian optimization and Hyperband for efficient joint neural architecture and hyperparameter search.
ER  -


TY  - Preprint
T1  - Learning Interpretable Anatomical Features Through Deep Generative Models: Application to Cardiac Remodeling
A1  - Carlo Biffi
A1  - Ozan Oktay
A1  - Giacomo Tarroni
A1  - Wenjia Bai
A1  - Antonio De Marvao
A1  - Georgia Doumou
A1  - Martin Rajchl
A1  - Reem Bedair
A1  - Sanjay Prasad
A1  - Stuart Cook
A1  - Declan O&#39;Regan
A1  - Daniel Rueckert
JO  - ArXiv e-prints
Y1  - 18 July, 2018
UR  - https://arxiv.org/abs/1807.06843
N2  - Alterations in the geometry and function of the heart define well-established causes of cardiovascular disease. However, current approaches to the diagnosis of cardiovascular diseases often rely on subjective human assessment as well as manual analysis of medical images. Both factors limit the sensitivity in quantifying complex structural and functional phenotypes. Deep learning approaches have recently achieved success for tasks such as classification or segmentation of medical images, but lack interpretability in the feature extraction and decision processes, limiting their value in clinical diagnosis. In this work, we propose a 3D convolutional generative model for automatic classification of images from patients with cardiac diseases associated with structural remodeling. The model leverages interpretable task-specific anatomic patterns learned from 3D segmentations. It further allows to visualise and quantify the learned pathology-specific remodeling patterns in the original input space of the images. This approach yields high accuracy in the categorization of healthy and hypertrophic cardiomyopathy subjects when tested on unseen MR images from our own multi-centre dataset (100%) as well on the ACDC MICCAI 2017 dataset (90%). We believe that the proposed deep learning approach is a promising step towards the development of interpretable classifiers for the medical imaging domain, which may help clinicians to improve diagnostic accuracy and enhance patient risk-stratification.
ER  -


TY  - Preprint
T1  - SySeVR: A Framework for Using Deep Learning to Detect Software Vulnerabilities
A1  - Zhen Li
A1  - Deqing Zou
A1  - Shouhuai Xu
A1  - Hai Jin
A1  - Yawei Zhu
A1  - Zhaoxuan Chen
JO  - ArXiv e-prints
Y1  - 20 September, 2018
UR  - https://arxiv.org/abs/1807.06756
N2  - The detection of software vulnerabilities (or vulnerabilities for short) is an important problem that has yet to be tackled, as manifested by many vulnerabilities reported on a daily basis. This calls for machine learning methods to automate vulnerability detection. Deep learning is attractive for this purpose because it does not require human experts to manually define features. Despite the tremendous success of deep learning in other domains, its applicability to vulnerability detection is not systematically understood. In order to fill this void, we propose the first systematic framework for using deep learning to detect vulnerabilities. The framework, dubbed Syntax-based, Semantics-based, and Vector Representations (SySeVR), focuses on obtaining program representations that can accommodate syntax and semantic information pertinent to vulnerabilities. Our experiments with 4 software products demonstrate the usefulness of the framework: we detect 15 vulnerabilities that are not reported in the National Vulnerability Database. Among these 15 vulnerabilities, 7 are unknown and have been reported to the vendors, and the other 8 have been &#34;silently&#34; patched by the vendors when releasing newer versions of the products.
ER  -


TY  - Preprint
T1  - Integrating Algorithmic Planning and Deep Learning for Partially Observable Navigation
A1  - Peter Karkus
A1  - David Hsu
A1  - Wee Sun Lee
JO  - ArXiv e-prints
Y1  - 17 July, 2018
UR  - https://arxiv.org/abs/1807.06696
N2  - We propose to take a novel approach to robot system design where each building block of a larger system is represented as a differentiable program, i.e. a deep neural network. This representation allows for integrating algorithmic planning and deep learning in a principled manner, and thus combine the benefits of model-free and model-based methods. We apply the proposed approach to a challenging partially observable robot navigation task. The robot must navigate to a goal in a previously unseen 3-D environment without knowing its initial location, and instead relying on a 2-D floor map and visual observations from an onboard camera. We introduce the Navigation Networks (NavNets) that encode state estimation, planning and acting in a single, end-to-end trainable recurrent neural network. In preliminary simulation experiments we successfully trained navigation networks to solve the challenging partially observable navigation task.
ER  -


TY  - Preprint
T1  - Efficient Deep Learning on Multi-Source Private Data
A1  - Nick Hynes
A1  - Raymond Cheng
A1  - Dawn Song
JO  - ArXiv e-prints
Y1  - 17 July, 2018
UR  - https://arxiv.org/abs/1807.06689
N2  - Machine learning models benefit from large and diverse datasets. Using such datasets, however, often requires trusting a centralized data aggregator. For sensitive applications like healthcare and finance this is undesirable as it could compromise patient privacy or divulge trade secrets. Recent advances in secure and privacy-preserving computation, including trusted hardware enclaves and differential privacy, offer a way for mutually distrusting parties to efficiently train a machine learning model without revealing the training data. In this work, we introduce Myelin, a deep learning framework which combines these privacy-preservation primitives, and use it to establish a baseline level of performance for fully private machine learning.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Swarm Systems
A1  - Maximilian HÃ¼ttenrauch
A1  - Adrian Å oÅ¡iÄ
A1  - Gerhard Neumann
JO  - ArXiv e-prints
Y1  - 17 July, 2018
UR  - https://arxiv.org/abs/1807.06613
N2  - Recently, deep reinforcement learning (RL) methods have been applied successfully to multi-agent scenarios. Typically, these methods rely on a concatenation of agent states to represent the information content required for decentralized decision making. However, concatenation scales poorly to swarm systems with a large number of homogeneous agents as it does not exploit the fundamental properties inherent to these systems: (i) the agents in the swarm are interchangeable and (ii) the exact number of agents in the swarm is irrelevant. Therefore, we propose a new state representation for deep multi-agent RL based on mean embeddings of distributions. We treat the agents as samples of a distribution and use the empirical mean embedding as input for a decentralized policy. We define different feature spaces of the mean embedding using histograms, radial basis functions and a neural network learned end-to-end. We evaluate the representation on two well known problems from the swarm literature (rendezvous and pursuit evasion), in a globally and locally observable setup. For the local setup we furthermore introduce simple communication protocols. Of all approaches, the mean embedding representation using neural network features enables the richest information exchange between neighboring agents facilitating the development of more complex collective strategies.
ER  -


TY  - Preprint
T1  - A Deep Learning Driven Active Framework for Segmentation of Large 3D Shape Collections
A1  - David George
A1  - Xianguha Xie
A1  - Yu-Kun Lai
A1  - Gary KL Tam
JO  - ArXiv e-prints
Y1  - 17 July, 2018
UR  - https://arxiv.org/abs/1807.06551
N2  - High-level shape understanding and technique evaluation on large repositories of 3D shapes often benefit from additional information known about the shapes. One example of such information is the semantic segmentation of a shape into functional or meaningful parts. Generating accurate segmentations with meaningful segment boundaries is, however, a costly process, typically requiring large amounts of user time to achieve high quality results. In this paper we present an active learning framework for large dataset segmentation, which iteratively provides the user with new predictions by training new models based on already segmented shapes. Our proposed pipeline consists of three novel components. First, we a propose a fast and relatively accurate feature-based deep learning model to provide dataset-wide segmentation predictions. Second, we propose an information theory measure to estimate the prediction quality and for ordering subsequent fast and meaningful shape selection. Our experiments show that such suggestive ordering helps reduce users time and effort, produce high quality predictions, and construct a model that generalizes well. Finally, we provide effective segmentation refinement features to help the user quickly correct any incorrect predictions. We show that our framework is more accurate and in general more efficient than state-of-the-art, for massive dataset segmentation with while also providing consistent segment boundaries.
ER  -


TY  - Preprint
T1  - Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try After Deep Learning
A1  - Tomohiko Konno
A1  - Michiaki Iwazume
JO  - ArXiv e-prints
Y1  - 17 July, 2018
UR  - https://arxiv.org/abs/1807.06540
N2  - We found an easy and quick post-learning method named &#34;Icing on the Cake&#34; to enhance a classification performance in deep learning. The method is that we train only the final classifier again after an ordinary training is done.
ER  -


TY  - Preprint
T1  - Pseudo-Feature Generation for Imbalanced Data Analysis in Deep Learning
A1  - Tomohiko Konno
A1  - Michiaki Iwazume
JO  - ArXiv e-prints
Y1  - 11 September, 2018
UR  - https://arxiv.org/abs/1807.06538
N2  - We generate pseudo-features by multivariate probability distributions obtained from feature maps in a low layer of trained deep neural networks. Then, we virtually augment the data of minor classes by the pseudo-features in order to overcome imbalanced data problems. Because all the wild data are imbalanced, the proposed method has the possibility to improve the ability of DNN in a broad range of problems
ER  -


TY  - Preprint
T1  - A framework for remote sensing images processing using deep learning technique
A1  - RÃ©mi Cresson
JO  - ArXiv e-prints
Y1  - 5 September, 2018
UR  - https://arxiv.org/abs/1807.06535
N2  - Deep learning techniques are becoming increasingly important to solve a number of image processing tasks. Among common algorithms, Convolutional Neural Networks and Recurrent Neural Networks based systems achieve state of the art results on satellite and aerial imagery in many applications. While these approaches are subject to scientific interest, there is currently no operational and generic implementation available at user-level for the remote sensing community. In this paper, we presents a framework enabling the use of deep learning techniques with remote sensing images and geospatial data. Our solution takes roots in two extensively used open-source libraries, the remote sensing image processing library Orfeo ToolBox, and the high performance numerical computation library TensorFlow. It can apply deep nets without restriction on images size and is computationally efficient, regardless hardware configuration.
ER  -


TY  - Preprint
T1  - Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks
A1  - Giuseppe Marra
A1  - Dario Zanca
A1  - Alessandro Betti
A1  - Marco Gori
JO  - ArXiv e-prints
Y1  - 5 October, 2018
UR  - https://arxiv.org/abs/1807.06302
N2  - The effectiveness of deep neural architectures has been widely supported in terms of both experimental and foundational principles. There is also clear evidence that the activation function (e.g. the rectifier and the LSTM units) plays a crucial role in the complexity of learning. Based on this remark, this paper discusses an optimal selection of the neuron non-linearity in a functional framework that is inspired from classic regularization arguments. It is shown that the best activation function is represented by a kernel expansion in the training set, that can be effectively approximated over an opportune set of points modeling 1-D clusters. The idea can be naturally extended to recurrent networks, where the expressiveness of kernel-based activation functions turns out to be a crucial ingredient to capture long-term dependencies. We give experimental evidence of this property by a set of challenging experiments, where we compare the results with neural architectures based on state of the art LSTM cells.
ER  -


TY  - Preprint
T1  - Robust Deep Multi-modal Learning Based on Gated Information Fusion Network
A1  - Jaekyum Kim
A1  - Junho Koh
A1  - Yecheol Kim
A1  - Jaehyung Choi
A1  - Youngbae Hwang
A1  - Jun Won Choi
JO  - ArXiv e-prints
Y1  - 17 July, 2018
UR  - https://arxiv.org/abs/1807.06233
N2  - The goal of multi-modal learning is to use complimentary information on the relevant task provided by the multiple modalities to achieve reliable and robust performance. Recently, deep learning has led significant improvement in multi-modal learning by allowing for the information fusion in the intermediate feature levels. This paper addresses a problem of designing robust deep multi-modal learning architecture in the presence of imperfect modalities. We introduce deep fusion architecture for object detection which processes each modality using the separate convolutional neural network (CNN) and constructs the joint feature map by combining the intermediate features from the CNNs. In order to facilitate the robustness to the degraded modalities, we employ the gated information fusion (GIF) network which weights the contribution from each modality according to the input feature maps to be fused. The weights are determined through the convolutional layers followed by a sigmoid function and trained along with the information fusion network in an end-to-end fashion. Our experiments show that the proposed GIF network offers the additional architectural flexibility to achieve robust performance in handling some degraded modalities, and show a significant performance improvement based on Single Shot Detector (SSD) for KITTI dataset using the proposed fusion network and data augmentation schemes.
ER  -


TY  - Preprint
T1  - Weakly Supervised Deep Learning for Thoracic Disease Classification and Localization on Chest X-rays
A1  - Chaochao Yan
A1  - Jiawen Yao
A1  - Ruoyu Li
A1  - Zheng Xu
A1  - Junzhou Huang
JO  - ArXiv e-prints
Y1  - 16 July, 2018
UR  - https://arxiv.org/abs/1807.06067
N2  - Chest X-rays is one of the most commonly available and affordable radiological examinations in clinical practice. While detecting thoracic diseases on chest X-rays is still a challenging task for machine intelligence, due to 1) the highly varied appearance of lesion areas on X-rays from patients of different thoracic disease and 2) the shortage of accurate pixel-level annotations by radiologists for model training. Existing machine learning methods are unable to deal with the challenge that thoracic diseases usually happen in localized disease-specific areas. In this article, we propose a weakly supervised deep learning framework equipped with squeeze-and-excitation blocks, multi-map transfer, and max-min pooling for classifying thoracic diseases as well as localizing suspicious lesion regions. The comprehensive experiments and discussions are performed on the ChestX-ray14 dataset. Both numerical and visual results have demonstrated the effectiveness of the proposed model and its better performance against the state-of-the-art pipelines.
ER  -


TY  - Preprint
T1  - Toward Interpretable Deep Reinforcement Learning with Linear Model U-Trees
A1  - Guiliang Liu
A1  - Oliver Schulte
A1  - Wang Zhu
A1  - Qingcan Li
JO  - ArXiv e-prints
Y1  - 16 July, 2018
UR  - https://arxiv.org/abs/1807.05887
N2  - Deep Reinforcement Learning (DRL) has achieved impressive success in many applications. A key component of many DRL models is a neural network representing a Q function, to estimate the expected cumulative reward following a state-action pair. The Q function neural network contains a lot of implicit knowledge about the RL problems, but often remains unexamined and uninterpreted. To our knowledge, this work develops the first mimic learning framework for Q functions in DRL. We introduce Linear Model U-trees (LMUTs) to approximate neural network predictions. An LMUT is learned using a novel on-line algorithm that is well-suited for an active play setting, where the mimic learner observes an ongoing interaction between the neural net and the environment. Empirical evaluation shows that an LMUT mimics a Q function substantially better than five baseline methods. The transparent tree structure of an LMUT facilitates understanding the network&#39;s learned knowledge by analyzing feature influence, extracting rules, and highlighting the super-pixels in image inputs.
ER  -


TY  - Preprint
T1  - Automatic acoustic detection of birds through deep learning: the first Bird Audio Detection challenge
A1  - Dan Stowell
A1  - Yannis Stylianou
A1  - Mike Wood
A1  - Hanna PamuÅa
A1  - HervÃ© Glotin
JO  - ArXiv e-prints
Y1  - 16 July, 2018
UR  - https://arxiv.org/abs/1807.05812
N2  - Assessing the presence and abundance of birds is important for monitoring specific species as well as overall ecosystem health. Many birds are most readily detected by their sounds, and thus passive acoustic monitoring is highly appropriate. Yet acoustic monitoring is often held back by practical limitations such as the need for manual configuration, reliance on example sound libraries, low accuracy, low robustness, and limited ability to generalise to novel acoustic conditions. Here we report outcomes from a collaborative data challenge showing that with modern machine learning including deep learning, general-purpose acoustic bird detection can achieve very high retrieval rates in remote monitoring data --- with no manual recalibration, and no pre-training of the detector for the target species or the acoustic conditions in the target environment. Multiple methods were able to attain performance of around 88% AUC (area under the ROC curve), much higher performance than previous general-purpose methods. We present new acoustic monitoring datasets, summarise the machine learning techniques proposed by challenge teams, conduct detailed performance evaluation, and discuss how such approaches to detection can be integrated into remote monitoring projects.
ER  -


TY  - Preprint
T1  - Learning Transferable Deep Models for Land-Use Classification with High-Resolution Remote Sensing Images
A1  - Xin-Yi Tong
A1  - Gui-Song Xia
A1  - Qikai Lu
A1  - Huanfeng Shen
A1  - Shengyang Li
A1  - Shucheng You
A1  - Liangpei Zhang
JO  - ArXiv e-prints
Y1  - 16 July, 2018
UR  - https://arxiv.org/abs/1807.05713
N2  - In recent years, large amount of high spatial-resolution remote sensing (HRRS) images are available for land-use mapping. However, due to the complex information brought by the increased spatial resolution and the data disturbances caused by different conditions of image acquisition, it is often difficult to find an efficient method for achieving accurate land-use classification with heterogeneous and high-resolution remote sensing images. In this paper, we propose a scheme to learn transferable deep models for land-use classification with HRRS images. The main idea is to rely on deep neural networks for presenting the semantic information contained in different types of land-uses and propose a pseudo-labeling and sample selection scheme for improving the transferability of deep models. More precisely, a deep Convolutional Neural Networks (CNNs) is first pre-trained with a well-annotated land-use dataset, referred to as the source data. Then, given a target image with no labels, the pre-trained CNN model is utilized to classify the image in a patch-wise manner. The patches with high classification probability are assigned with pseudo-labels and employed as the queries to retrieve related samples from the source data. The pseudo-labels confirmed with the retrieved results are regarded as supervised information for fine-tuning the pre-trained deep model. In order to obtain a pixel-wise land-use classification with the target image, we rely on the fine-tuned CNN and develop a hybrid classification by combining patch-wise classification and hierarchical segmentation. In addition, we create a large-scale land-use dataset containing $150$ Gaofen-2 satellite images for CNN pre-training. Experiments on multi-source HRRS images, including Gaofen-2, Gaofen-1, Jilin-1, Ziyuan-3, and Google Earth images, show encouraging results and demonstrate the efficiency of the proposed scheme.
ER  -


TY  - Preprint
T1  - Scene Learning: Deep Convolutional Networks For Wind Power Prediction by Embedding Turbines into Grid Space
A1  - Ruiguo Yu
A1  - Zhiqiang Liu
A1  - Xuewei Li
A1  - Wenhuan Lu
A1  - Mei Yu
A1  - Jianrong Wang
A1  - Bin Li
JO  - ArXiv e-prints
Y1  - 17 July, 2018
UR  - https://arxiv.org/abs/1807.05666
N2  - Wind power prediction is of vital importance in wind power utilization. There have been a lot of researches based on the time series of the wind power or speed, but In fact, these time series cannot express the temporal and spatial changes of wind, which fundamentally hinders the advance of wind power prediction. In this paper, a new kind of feature that can describe the process of temporal and spatial variation is proposed, namely, Spatio-Temporal Features. We first map the data collected at each moment from the wind turbine to the plane to form the state map, namely, the scene, according to the relative positions. The scene time series over a period of time is a multi-channel image, i.e. the Spatio-Temporal Features. Based on the Spatio-Temporal Features, the deep convolutional network is applied to predict the wind power, achieving a far better accuracy than the existing methods. Compared with the starge-of-the-art method, the mean-square error (MSE) in our method is reduced by 49.83%, and the average time cost for training models can be shortened by a factor of more than 150.
ER  -


TY  - Preprint
T1  - Deep Learning for Semantic Segmentation on Minimal Hardware
A1  - Sander G. van Dijk
A1  - Marcus M. Scheunemann
JO  - ArXiv e-prints
Y1  - 15 July, 2018
UR  - https://arxiv.org/abs/1807.05597
N2  - Deep learning has revolutionised many fields, but it is still challenging to transfer its success to small mobile robots with minimal hardware. Specifically, some work has been done to this effect in the RoboCup humanoid football domain, but results that are performant and efficient and still generally applicable outside of this domain are lacking. We propose an approach conceptually different from those taken previously. It is based on semantic segmentation and does achieve these desired properties. In detail, it is being able to process full VGA images in real-time on a low-power mobile processor. It can further handle multiple image dimensions without retraining, it does not require specific domain knowledge for achieving a high frame rate and it is applicable on a minimal mobile hardware.
ER  -


TY  - Preprint
T1  - DeepInf: Social Influence Prediction with Deep Learning
A1  - Jiezhong Qiu
A1  - Jian Tang
A1  - Hao Ma
A1  - Yuxiao Dong
A1  - Kuansan Wang
A1  - Jie Tang
JO  - ArXiv e-prints
Y1  - 15 July, 2018
UR  - https://arxiv.org/abs/1807.05560
N2  - Social and information networking activities such as on Facebook, Twitter, WeChat, and Weibo have become an indispensable part of our everyday life, where we can easily access friends&#39; behaviors and are in turn influenced by them. Consequently, an effective social influence prediction for each user is critical for a variety of applications such as online recommendation and advertising.
ER  -


TY  - Preprint
T1  - Deep Clustering for Unsupervised Learning of Visual Features
A1  - Mathilde Caron
A1  - Piotr Bojanowski
A1  - Armand Joulin
A1  - Matthijs Douze
JO  - ArXiv e-prints
Y1  - 15 July, 2018
UR  - https://arxiv.org/abs/1807.05520
N2  - Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks.
ER  -


TY  - Preprint
T1  - Object Detection with Deep Learning: A Review
A1  - Zhong-Qiu Zhao
A1  - Peng Zheng
A1  - Shou-tao Xu
A1  - Xindong Wu
JO  - ArXiv e-prints
Y1  - 15 July, 2018
UR  - https://arxiv.org/abs/1807.05511
N2  - Due to object detection&#39;s close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles which combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy and optimization function, etc. In this paper, we provide a review on deep learning based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely Convolutional Neural Network (CNN). Then we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network based learning systems.
ER  -


TY  - Preprint
T1  - Survey on Deep Learning Techniques for Person Re-Identification Task
A1  - Bahram Lavi
A1  - Mehdi Fatan Serj
A1  - Ihsan Ullah
JO  - ArXiv e-prints
Y1  - 19 July, 2018
UR  - https://arxiv.org/abs/1807.05284
N2  - Intelligent video-surveillance is currently an active research field in computer vision and machine learning techniques. It provides useful tools for surveillance operators and forensic video investigators. Person re-identification (PReID) is one among these tools. It consists of recognizing whether an individual has already been observed over a camera in a network or not. This tool can also be employed in various possible applications such as off-line retrieval of all the video-sequences showing an individual of interest whose image is given a query, and online pedestrian tracking over multiple camera views. To this aim, many techniques have been proposed to increase the performance of PReID. Among the systems, many researchers utilized deep neural networks (DNNs) because of their better performance and fast execution at test time. Our objective is to provide for future researchers the work being done on PReID to date. Therefore, we summarized state-of-the-art DNN models being used for this task. A brief description of each model along with their evaluation on a set of benchmark datasets is given. Finally, a detailed comparison is provided among these models followed by some limitations that can work as guidelines for future research.
ER  -


TY  - Preprint
T1  - Deep Learning in the Wild
A1  - Thilo Stadelmann
A1  - Mohammadreza Amirian
A1  - Ismail Arabaci
A1  - Marek Arnold
A1  - Gilbert FranÃ§ois Duivesteijn
A1  - Ismail Elezi
A1  - Melanie Geiger
A1  - Stefan LÃ¶rwald
A1  - Benjamin Bruno Meier
A1  - Katharina Rombach
A1  - Lukas Tuggener
JO  - ArXiv e-prints
Y1  - 13 July, 2018
UR  - https://arxiv.org/abs/1807.04950
N2  - Deep learning with neural networks is applied by an increasing number of people outside of classic research environments, due to the vast success of the methodology on a wide range of machine perception tasks. While this interest is fueled by beautiful success stories, practical work in deep learning on novel tasks without existing baselines remains challenging. This paper explores the specific challenges arising in the realm of real world tasks, based on case studies from research \&amp; development in conjunction with industry, and extracts lessons learned from them. It thus fills a gap between the publication of latest algorithmic and methodical developments, and the usually omitted nitty-gritty of how to make them work. Specifically, we give insight into deep learning projects on face matching, print media monitoring, industrial quality control, music scanning, strategy game playing, and automated machine learning, thereby providing best practices for deep learning in practice.
ER  -


TY  - Preprint
T1  - Automatic segmentation of skin lesions using deep learning
A1  - Joshua Peter Ebenezer
A1  - Jagath C. Rajapakse
JO  - ArXiv e-prints
Y1  - 12 July, 2018
UR  - https://arxiv.org/abs/1807.04893
N2  - This paper summarizes the method used in our submission to Task 1 of the International Skin Imaging Collaboration&#39;s (ISIC) Skin Lesion Analysis Towards Melanoma Detection challenge held in 2018. We used a fully automated method to accurately segment lesion boundaries from dermoscopic images. A U-net deep learning network is trained on publicly available data from ISIC. We introduce the use of intensity, color, and texture enhancement operations as pre-processing steps and morphological operations and contour identification as post-processing steps.
ER  -


TY  - Preprint
T1  - When deep learning meets security
A1  - Majd Latah
JO  - ArXiv e-prints
Y1  - 12 July, 2018
UR  - https://arxiv.org/abs/1807.04739
N2  - Deep learning is an emerging research field that has proven its effectiveness towards deploying more efficient intelligent systems. Security, on the other hand, is one of the most essential issues in modern communication systems. Recently many papers have shown that using deep learning models can achieve promising results when applied to the security domain. In this work, we provide an overview for the recent studies that apply deep learning techniques to the field of security.
ER  -


TY  - Preprint
T1  - The Bottleneck Simulator: A Model-based Deep Reinforcement Learning Approach
A1  - Iulian Vlad Serban
A1  - Chinnadhurai Sankar
A1  - Michael Pieper
A1  - Joelle Pineau
A1  - Yoshua Bengio
JO  - ArXiv e-prints
Y1  - 12 July, 2018
UR  - https://arxiv.org/abs/1807.04723
N2  - Deep reinforcement learning has recently shown many impressive successes. However, one major obstacle towards applying such methods to real-world problems is their lack of data-efficiency. To this end, we propose the Bottleneck Simulator: a model-based reinforcement learning method which combines a learned, factorized transition model of the environment with rollout simulations to learn an effective policy from few examples. The learned transition model employs an abstract, discrete (bottleneck) state, which increases sample efficiency by reducing the number of model parameters and by exploiting structural properties of the environment. We provide a mathematical analysis of the Bottleneck Simulator in terms of fixed points of the learned policy, which reveals how performance is affected by four distinct sources of error: an error related to the abstract space structure, an error related to the transition model estimation variance, an error related to the transition model estimation bias, and an error related to the transition model class bias. Finally, we evaluate the Bottleneck Simulator on two natural language processing tasks: a text adventure game and a real-world, complex dialogue response selection task. On both tasks, the Bottleneck Simulator yields excellent performance beating competing approaches.
ER  -


TY  - Preprint
T1  - Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures
A1  - Sergey Bartunov
A1  - Adam Santoro
A1  - Blake A. Richards
A1  - Geoffrey E. Hinton
A1  - Timothy Lillicrap
JO  - ArXiv e-prints
Y1  - 12 July, 2018
UR  - https://arxiv.org/abs/1807.04587
N2  - The backpropagation of error algorithm (BP) is often said to be impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might implement or approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present the first results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.
ER  -


TY  - Preprint
T1  - Deep Learning for Imbalance Data Classification using Class Expert Generative Adversarial Network
A1  -  Fanny
A1  - Tjeng Wawan Cenggoro
JO  - ArXiv e-prints
Y1  - 12 July, 2018
UR  - https://arxiv.org/abs/1807.04585
N2  - Without any specific way for imbalance data classification, artificial intelligence algorithm cannot recognize data from minority classes easily. In general, modifying the existing algorithm by assuming that the training data is imbalanced, is the only way to handle imbalance data. However, for a normal data handling, this way mostly produces a deficient result. In this research, we propose a class expert generative adversarial network (CE-GAN) as the solution for imbalance data classification. CE-GAN is a modification in deep learning algorithm architecture that does not have an assumption that the training data is imbalance data. Moreover, CE-GAN is designed to identify more detail about the character of each class before classification step. CE-GAN has been proved in this research to give a good performance for imbalance data classification.
ER  -


TY  - Preprint
T1  - A Generic Approach to Lung Field Segmentation from Chest Radiographs using Deep Space and Shape Learning
A1  - Awais Mansoor
A1  - Juan J. Cerrolaza
A1  - Geovanny Perez
A1  - Elijah Biggs
A1  - Kazunori Okada
A1  - Gustavo Nino
A1  - Marius George Linguraru
JO  - ArXiv e-prints
Y1  - 11 July, 2018
UR  - https://arxiv.org/abs/1807.04339
N2  - Computer-aided diagnosis (CAD) techniques for lung field segmentation from chest radiographs (CXR) have been proposed for adult cohorts, but rarely for pediatric subjects. Statistical shape models (SSMs), the workhorse of most state-of-the-art CXR-based lung field segmentation methods, do not efficiently accommodate shape variation of the lung field during the pediatric developmental stages. The main contributions of our work are: (1) a generic lung field segmentation framework from CXR accommodating large shape variation for adult and pediatric cohorts; (2) a deep representation learning detection mechanism, \emph{ensemble space learning}, for robust object localization; and (3) \emph{marginal shape deep learning} for the shape deformation parameter estimation. Unlike the iterative approach of conventional SSMs, the proposed shape learning mechanism transforms the parameter space into marginal subspaces that are solvable efficiently using the recursive representation learning mechanism. Furthermore, our method is the first to include the challenging retro-cardiac region in the CXR-based lung segmentation for accurate lung capacity estimation. The framework is evaluated on 668 CXRs of patients between 3 month to 89 year of age. We obtain a mean Dice similarity coefficient of $0.96\pm0.03$ (including the retro-cardiac region). For a given accuracy, the proposed approach is also found to be faster than conventional SSM-based iterative segmentation methods. The computational simplicity of the proposed generic framework could be similarly applied to the fast segmentation of other deformable objects.
ER  -


TY  - Preprint
T1  - Automated Vulnerability Detection in Source Code Using Deep Representation Learning
A1  - Rebecca L. Russell
A1  - Louis Kim
A1  - Lei H. Hamilton
A1  - Tomo Lazovich
A1  - Jacob A. Harer
A1  - Onur Ozdemir
A1  - Paul M. Ellingwood
A1  - Marc W. McConley
JO  - ArXiv e-prints
Y1  - 11 July, 2018
UR  - https://arxiv.org/abs/1807.04320
N2  - Increasing numbers of software vulnerabilities are discovered every year whether they are reported publicly or discovered internally in proprietary code. These vulnerabilities can pose serious risk of exploit and result in system compromise, information leaks, or denial of service. We leveraged the wealth of C and C++ open-source code available to develop a large-scale function-level vulnerability detection system using machine learning. To supplement existing labeled vulnerability datasets, we compiled a vast dataset of millions of open-source functions and labeled it with carefully-selected findings from three different static analyzers that indicate potential exploits. Using these datasets, we developed a fast and scalable vulnerability detection tool based on deep feature representation learning that directly interprets lexed source code. We evaluated our tool on code from both real software packages and the NIST SATE IV benchmark dataset. Our results demonstrate that deep feature representation learning on source code is a promising approach for automated software vulnerability detection.
ER  -


TY  - Preprint
T1  - VTA: An Open Hardware-Software Stack for Deep Learning
A1  - Thierry Moreau
A1  - Tianqi Chen
A1  - Ziheng Jiang
A1  - Luis Ceze
A1  - Carlos Guestrin
A1  - Arvind Krishnamurthy
JO  - ArXiv e-prints
Y1  - 11 July, 2018
UR  - https://arxiv.org/abs/1807.04188
N2  - Hardware acceleration is an enabler for ubiquitous and efficient deep learning. With hardware accelerators being introduced in datacenter and edge devices, it is time to acknowledge that hardware specialization is central to the deep learning system stack.
ER  -


TY  - Preprint
T1  - Adaptive Learning Method of Recurrent Temporal Deep Belief Network to Analyze Time Series Data
A1  - Takumi Ichimura
A1  - Shin Kamada
JO  - ArXiv e-prints
Y1  - 11 July, 2018
UR  - https://arxiv.org/abs/1807.03953
N2  - Deep Learning has the hierarchical network architecture to represent the complicated features of input patterns. Such architecture is well known to represent higher learning capability compared with some conventional models if the best set of parameters in the optimal network structure is found. We have been developing the adaptive learning method that can discover the optimal network structure in Deep Belief Network (DBN). The learning method can construct the network structure with the optimal number of hidden neurons in each Restricted Boltzmann Machine and with the optimal number of layers in the DBN during learning phase. The network structure of the learning method can be self-organized according to given input patterns of big data set. In this paper, we embed the adaptive learning method into the recurrent temporal RBM and the self-generated layer into DBN. In order to verify the effectiveness of our proposed method, the experimental results are higher classification capability than the conventional methods in this paper.
ER  -


TY  - Preprint
T1  - Shortening Time Required for Adaptive Structural Learning Method of Deep Belief Network with Multi-Modal Data Arrangement
A1  - Shin Kamada
A1  - Takumi Ichimura
JO  - ArXiv e-prints
Y1  - 11 July, 2018
UR  - https://arxiv.org/abs/1807.03952
N2  - Recently, Deep Learning has been applied in the techniques of artificial intelligence. Especially, Deep Learning performed good results in the field of image recognition. Most new Deep Learning architectures are naturally developed in image recognition. For this reason, not only the numerical data and text data but also the time-series data are transformed to the image data format. Multi-modal data consists of two or more kinds of data such as picture and text. The arrangement in a general method is formed in the squared array with no specific aim. In this paper, the data arrangement are modified according to the similarity of input-output pattern in Adaptive Structural Learning method of Deep Belief Network. The similarity of output signals of hidden neurons is made by the order rearrangement of hidden neurons. The experimental results for the data rearrangement in squared array showed the shortening time required for DBN learning.
ER  -


TY  - Preprint
T1  - DeepDiff: Deep-learning for predicting Differential gene expression from histone modifications
A1  - Arshdeep Sekhon
A1  - Ritambhara Singh
A1  - Yanjun Qi
JO  - ArXiv e-prints
Y1  - 10 July, 2018
UR  - https://arxiv.org/abs/1807.03878
N2  - Computational methods that predict differential gene expression from histone modification signals are highly desirable for understanding how histone modifications control the functional heterogeneity of cells through influencing differential gene regulation. Recent studies either failed to capture combinatorial effects on differential prediction or primarily only focused on cell type-specific analysis. In this paper, we develop a novel attention-based deep learning architecture, DeepDiff, that provides a unified and end-to-end solution to model and to interpret how dependencies among histone modifications control the differential patterns of gene regulation. DeepDiff uses a hierarchy of multiple Long short-term memory (LSTM) modules to encode the spatial structure of input signals and to model how various histone modifications cooperate automatically. We introduce and train two levels of attention jointly with the target prediction, enabling DeepDiff to attend differentially to relevant modifications and to locate important genome positions for each modification. Additionally, DeepDiff introduces a novel deep-learning based multi-task formulation to use the cell-type-specific gene expression predictions as auxiliary tasks, encouraging richer feature embeddings in our primary task of differential expression prediction. Using data from Roadmap Epigenomics Project (REMC) for ten different pairs of cell types, we show that DeepDiff significantly outperforms the state-of-the-art baselines for differential gene expression prediction. The learned attention weights are validated by observations from previous studies about how epigenetic mechanisms connect to differential gene expression. Codes and results are available at \url{deepchrome.org}
ER  -


TY  - Preprint
T1  - Using deep learning for comprehensive, personalized forecasting of Alzheimer&#39;s Disease progression
A1  - Charles K. Fisher
A1  - Aaron M. Smith
A1  - Jonathan R. Walsh
A1  - the Coalition Against Major Diseases
JO  - ArXiv e-prints
Y1  - 10 July, 2018
UR  - https://arxiv.org/abs/1807.03876
N2  - A patient is more than one number, yet most approaches to machine learning from electronic health data can only predict a single endpoint. Here, we present an alternative -- using unsupervised deep learning to simulate detailed patient trajectories. We use data comprising 18-month longitudinal trajectories of 42 clinical variables from 1908 patients with Mild Cognitive Impairment (MCI) or Alzheimer&#39;s Disease (AD) to train a model for personalized forecasting of disease progression. Our model simulates the evolution of each sub-component of cognitive exams, laboratory tests, and their associations with baseline clinical characteristics, generating both predictions and their confidence intervals. Even though it is not trained to predict changes in disease severity, our unsupervised model predicts changes in total ADAS-Cog scores with the same accuracy as specifically trained supervised models. We show how simulations can be used to interpret our model and demonstrate how to create synthetic control arm data for AD clinical trials. Our model&#39;s ability to simultaneously predict dozens of characteristics of a patient at any point in the future is a crucial step forward in computational precision medicine.
ER  -


TY  - Preprint
T1  - Model-based free-breathing cardiac MRI reconstruction using deep learned \&amp; STORM priors: MoDL-STORM
A1  - Sampurna Biswas
A1  - Hemant K. Aggarwal
A1  - Sunrita Poddar
A1  - Mathews Jacob
JO  - ArXiv e-prints
Y1  - 10 July, 2018
UR  - https://arxiv.org/abs/1807.03845
N2  - We introduce a model-based reconstruction framework with deep learned (DL) and smoothness regularization on manifolds (STORM) priors to recover free breathing and ungated (FBU) cardiac MRI from highly undersampled measurements. The DL priors enable us to exploit the local correlations, while the STORM prior enables us to make use of the extensive non-local similarities that are subject dependent. We introduce a novel model-based formulation that allows the seamless integration of deep learning methods with available prior information, which current deep learning algorithms are not capable of. The experimental results demonstrate the preliminary potential of this work in accelerating FBU cardiac MRI.
ER  -


TY  - Preprint
T1  - Deep Learning for Audio Transcription on Low-Resource Datasets
A1  - Veronica Morfi
A1  - Dan Stowell
JO  - ArXiv e-prints
Y1  - 11 July, 2018
UR  - https://arxiv.org/abs/1807.03697
N2  - In training a deep learning system to perform audio transcription, two practical problems may arise. Firstly, most datasets are weakly labelled, having only a list of events present in each recording without any temporal information for training. Secondly, deep neural networks need a very large amount of labelled training data to achieve good quality performance, yet in practice it is difficult to collect enough samples for most classes of interest. In this paper, we propose factorising the final task of audio transcription into multiple intermediate tasks in order to improve the training performance when dealing with this kind of low-resource datasets. We evaluate three data-efficient approaches of training a stacked convolutional and recurrent neural network for the intermediate tasks. Our results show that different methods of training have different advantages and disadvantages.
ER  -


TY  - Preprint
T1  - Deep-Reinforcement-Learning for Gliding and Perching Bodies
A1  - Guido Novati
A1  - Lakshminarayanan Mahadevan
A1  - Petros Koumoutsakos
JO  - ArXiv e-prints
Y1  - 7 July, 2018
UR  - https://arxiv.org/abs/1807.03671
N2  - Controlled gliding is one of the most energetically efficient modes of transportation for natural and human powered fliers. Here we demonstrate that gliding and landing strategies with different optimality criteria can be identified through deep reinforcement learning without explicit knowledge of the underlying physics. We combine a two dimensional model of a controlled elliptical body with deep reinforcement learning (D-RL) to achieve gliding with either minimum energy expenditure, or fastest time of arrival, at a predetermined location. In both cases the gliding trajectories are smooth, although energy/time optimal strategies are distinguished by small/high frequency actuations. We examine the effects of the ellipse&#39;s shape and weight on the optimal policies for controlled gliding. Surprisingly, we find that the model-free reinforcement learning leads to more robust gliding than model-based optimal control strategies with a modest additional computational cost. We also demonstrate that the gliders with D-RL can generalize their strategies to reach the target location from previously unseen starting positions. The model-free character and robustness of D-RL suggests a promising framework for developing mechanical devices capable of exploiting complex flow environments.
ER  -


TY  - Preprint
T1  - Window Opening Model using Deep Learning Methods
A1  - Romana Markovic
A1  - Eva Grintal
A1  - Daniel WÃ¶lki
A1  - JÃ©rÃ´me Frisch
A1  - Christoph van Treeck
JO  - ArXiv e-prints
Y1  - 20 September, 2018
UR  - https://arxiv.org/abs/1807.03610
N2  - Occupant behavior (OB) and in particular window openings need to be considered in building performance simulation (BPS), in order to realistically model the indoor climate and energy consumption for heating ventilation and air conditioning (HVAC). However, the proposed OB window opening models are often biased towards the over-represented class where windows remained closed. In addition, they require tuning for each occupant which can not be efficiently scaled to the increased number of occupants. This paper presents a window opening model for commercial buildings using deep learning methods. The model is trained using data from occupants from an office building in Germany. In total the model is evaluated using almost 20 mio. data points from 3 independent buildings, located in Aachen, Frankfurt and Philadelphia. Eventually, the results of 3100 core hours of model development are summarized, which makes this study the largest of its kind in window states modeling. Additionally, the practical potential of the proposed model was tested by incorporating it in the Modelica-based thermal building simulation. The resulting evaluation accuracy and F1 scores on the office buildings ranged between 86-89 % and 0.53-0.65 respectively. The performance dropped around 15 % points in case of sparse input data, while the F1 score remained high.
ER  -


TY  - Preprint
T1  - DLOPT: Deep Learning Optimization Library
A1  - AndrÃ©s Camero
A1  - Jamal Toutouh
A1  - Enrique Alba
JO  - ArXiv e-prints
Y1  - 10 July, 2018
UR  - https://arxiv.org/abs/1807.03523
N2  - Deep learning hyper-parameter optimization is a tough task. Finding an appropriate network configuration is a key to success, however most of the times this labor is roughly done. In this work we introduce a novel library to tackle this problem, the Deep Learning Optimization Library: DLOPT. We briefly describe its architecture and present a set of use examples. This is an open source project developed under the GNU GPL v3 license and it is freely available at https://github.com/acamero/dlopt
ER  -


TY  - Preprint
T1  - An Adaptive Learning Method of Deep Belief Network by Layer Generation Algorithm
A1  - Shin Kamada
A1  - Takumi Ichimura
JO  - ArXiv e-prints
Y1  - 11 July, 2018
UR  - https://arxiv.org/abs/1807.03486
N2  - Deep Belief Network (DBN) has a deep architecture that represents multiple features of input patterns hierarchically with the pre-trained Restricted Boltzmann Machines (RBM). A traditional RBM or DBN model cannot change its network structure during the learning phase. Our proposed adaptive learning method can discover the optimal number of hidden neurons and weights and/or layers according to the input space. The model is an important method to take account of the computational cost and the model stability. The regularities to hold the sparse structure of network is considerable problem, since the extraction of explicit knowledge from the trained network should be required. In our previous research, we have developed the hybrid method of adaptive structural learning method of RBM and Learning Forgetting method to the trained RBM. In this paper, we propose the adaptive learning method of DBN that can determine the optimal number of layers during the learning. We evaluated our proposed model on some benchmark data sets.
ER  -


TY  - Preprint
T1  - SceneEDNet: A Deep Learning Approach for Scene Flow Estimation
A1  - Ravi Kumar Thakur
A1  - Snehasis Mukherjee
JO  - ArXiv e-prints
Y1  - 9 July, 2018
UR  - https://arxiv.org/abs/1807.03464
N2  - Estimating scene flow in RGB-D videos is attracting much interest of the computer vision researchers, due to its potential applications in robotics. The state-of-the-art techniques for scene flow estimation, typically rely on the knowledge of scene structure of the frame and the correspondence between frames. However, with the increasing amount of RGB-D data captured from sophisticated sensors like Microsoft Kinect, and the recent advances in the area of sophisticated deep learning techniques, introduction of an efficient deep learning technique for scene flow estimation, is becoming important. This paper introduces a first effort to apply a deep learning method for direct estimation of scene flow by presenting a fully convolutional neural network with an encoder-decoder (ED) architecture. The proposed network SceneEDNet involves estimation of three dimensional motion vectors of all the scene points from sequence of stereo images. The training for direct estimation of scene flow is done using consecutive pairs of stereo images and corresponding scene flow ground truth. The proposed architecture is applied on a huge dataset and provides meaningful results.
ER  -


TY  - Preprint
T1  - Developing Brain Atlas through Deep Learning
A1  - Asim Iqbal
A1  - Romesa Khan
A1  - Theofanis Karayannis
JO  - ArXiv e-prints
Y1  - 9 July, 2018
UR  - https://arxiv.org/abs/1807.03440
N2  - To uncover the organizational principles governing the human brain, neuroscientists are in need of developing high-throughput methods that can explore the structure and function of distinct brain regions using animal models. The first step towards this goal is to accurately register the regions of interest in a mouse brain, against a standard reference atlas, with minimum human supervision. The second step is to scale this approach to different animal ages, so as to also allow insights into normal and pathological brain development and aging. We introduce here a fully automated convolutional neural network-based method (SeBRe) for registration through Segmenting Brain Regions of interest in mice at different ages. We demonstrate the validity of our method on different mouse brain post-natal (P) developmental time points, across a range of neuronal markers. Our method outperforms the existing brain registration methods, and provides the minimum mean squared error (MSE) score on a mouse brain dataset. We propose that our deep learning-based registration method can (i) accelerate brain-wide exploration of region-specific changes in brain development and (ii) replace the existing complex brain registration methodology, by simply segmenting brain regions of interest for high-throughput brain-wide analysis.
ER  -


TY  - Preprint
T1  - Exploring Brain-wide Development of Inhibition through Deep Learning
A1  - Asim Iqbal
A1  - Asfandyar Sheikh
A1  - Theofanis Karayannis
JO  - ArXiv e-prints
Y1  - 9 July, 2018
UR  - https://arxiv.org/abs/1807.03238
N2  - We introduce here a fully automated convolutional neural network-based method for brain image processing to Detect Neurons in different brain Regions during Development (DeNeRD). Our method takes a developing mouse brain as input and i) registers the brain sections against a developing mouse reference atlas, ii) detects various types of neurons, and iii) quantifies the neural density in many unique brain regions at different postnatal (P) time points. Our method is invariant to the shape, size and expression of neurons and by using DeNeRD, we compare the brain-wide neural density of all GABAergic neurons in developing brains of ages P4, P14 and P56. We discover and report 6 different clusters of regions in the mouse brain in which GABAergic neurons develop in a differential manner from early age (P4) to adulthood (P56). These clusters reveal key steps of GABAergic cell development that seem to track with the functional development of diverse brain regions as the mouse transitions from a passive receiver of sensory information (&lt;P14) to an active seeker (&gt;P14).
ER  -


TY  - Preprint
T1  - Efficient Decentralized Deep Learning by Dynamic Model Averaging
A1  - Michael Kamp
A1  - Linara Adilova
A1  - Joachim Sicking
A1  - Fabian HÃ¼ger
A1  - Peter Schlicht
A1  - Tim Wirtz
A1  - Stefan Wrobel
JO  - ArXiv e-prints
Y1  - 9 July, 2018
UR  - https://arxiv.org/abs/1807.03210
N2  - We propose an efficient protocol for decentralized training of deep neural networks from distributed data sources. The proposed protocol allows to handle different phases of model training equally well and to quickly adapt to concept drifts. This leads to a reduction of communication by an order of magnitude compared to periodically communicating state-of-the-art approaches. Moreover, we derive a communication bound that scales well with the hardness of the serialized learning problem. The reduction in communication comes at almost no cost, as the predictive performance remains virtually unchanged. Indeed, the proposed protocol retains loss bounds of periodically averaging schemes. An extensive empirical evaluation validates major improvement of the trade-off between model performance and communication which could be beneficial for numerous decentralized learning applications, such as autonomous driving, or voice recognition and image classification on mobile phones.
ER  -


TY  - Preprint
T1  - Approximate k-space models and Deep Learning for fast photoacoustic reconstruction
A1  - Andreas Hauptmann
A1  - Ben Cox
A1  - Felix Lucka
A1  - Nam Huynh
A1  - Marta Betcke
A1  - Paul Beard
A1  - Simon Arridge
JO  - ArXiv e-prints
Y1  - 9 July, 2018
UR  - https://arxiv.org/abs/1807.03191
N2  - We present a framework for accelerated iterative reconstructions using a fast and approximate forward model that is based on k-space methods for photoacoustic tomography. The approximate model introduces aliasing artefacts in the gradient information for the iterative reconstruction, but these artefacts are highly structured and we can train a CNN that can use the approximate information to perform an iterative reconstruction. We show feasibility of the method for human in-vivo measurements in a limited-view geometry. The proposed method is able to produce superior results to total variation reconstructions with a speed-up of 32 times.
ER  -


TY  - Preprint
T1  - YouTube for Patient Education: A Deep Learning Approach for Understanding Medical Knowledge from User-Generated Videos
A1  - Xiao Liu
A1  - Bin Zhang
A1  - Anjana Susarla
A1  - Rema Padman
JO  - ArXiv e-prints
Y1  - 6 July, 2018
UR  - https://arxiv.org/abs/1807.03179
N2  - YouTube presents an unprecedented opportunity to explore how machine learning methods can improve healthcare information dissemination. We propose an interdisciplinary lens that synthesizes machine learning methods with healthcare informatics themes to address the critical issue of developing a scalable algorithmic solution to evaluate videos from a health literacy and patient education perspective. We develop a deep learning method to understand the level of medical knowledge encoded in YouTube videos. Preliminary results suggest that we can extract medical knowledge from YouTube videos and classify videos according to the embedded knowledge with satisfying performance. Deep learning methods show great promise in knowledge extraction, natural language understanding, and image classification, especially in an era of patient-centric care and precision medicine.
ER  -


TY  - Preprint
T1  - Data Augmentation for Detection of Architectural Distortion in Digital Mammography using Deep Learning Approach
A1  - Arthur C. Costa
A1  - Helder C. R. Oliveira
A1  - Juliana H. Catani
A1  - Nestor de Barros
A1  - Carlos F. E. Melo
A1  - Marcelo A. C. Vieira
JO  - ArXiv e-prints
Y1  - 5 July, 2018
UR  - https://arxiv.org/abs/1807.03167
N2  - Early detection of breast cancer can increase treatment efficiency. Architectural Distortion (AD) is a very subtle contraction of the breast tissue and may represent the earliest sign of cancer. Since it is very likely to be unnoticed by radiologists, several approaches have been proposed over the years but none using deep learning techniques. To train a Convolutional Neural Network (CNN), which is a deep neural architecture, is necessary a huge amount of data. To overcome this problem, this paper proposes a data augmentation approach applied to clinical image dataset to properly train a CNN. Results using receiver operating characteristic analysis showed that with a very limited dataset we could train a CNN to detect AD in digital mammography with area under the curve (AUC = 0.74).
ER  -


TY  - Preprint
T1  - Towards Radiologist-Level Accurate Deep Learning System for Pulmonary Screening
A1  - Mrinal Haloi
A1  - K. Raja Rajalakshmi
A1  - Pradeep Walia
JO  - ArXiv e-prints
Y1  - 25 June, 2018
UR  - https://arxiv.org/abs/1807.03120
N2  - In this work, we propose advanced pneumonia and Tuberculosis grading system for X-ray images. The proposed system is a very deep fully convolutional classification network with online augmentation that outputs confidence values for diseases prevalence. Its a fully automated system capable of disease feature understanding without any offline preprocessing step or manual feature extraction. We have achieved state- of-the- art performance on the public databases such as ChestXray-14, Mendeley, Shenzhen Hospital X-ray and Belarus X-ray set.
ER  -


TY  - Preprint
T1  - Deep Global-Connected Net With The Generalized Multi-Piecewise ReLU Activation in Deep Learning
A1  - Zhi Chen
A1  - Pin-han Ho
JO  - ArXiv e-prints
Y1  - 19 June, 2018
UR  - https://arxiv.org/abs/1807.03116
N2  - Recent Progress has shown that exploitation of hidden layer neurons in convolution neural networks incorporating with a carefully designed activation function can yield better classification results in the field of computer vision. The paper firstly introduces a novel deep learning architecture aiming to mitigate the gradient-vanishing problem, in which the earlier hidden layer neurons could be directly connected with the last hidden layer and feed into the last layer for classification. We then design a generalized linear rectifier function as the activation function that can approximate arbitrary complex functions via training of the parameters. We will show that our design can achieve similar performance in a number of object recognition and video action benchmark tasks, under significantly less number of parameters and shallower network infrastructure, which is not only promising in training in terms of computation burden and memory usage, but is also applicable to low-computation, low-memory mobile scenarios.
ER  -


TY  - Preprint
T1  - Deep Co-Clustering for Unsupervised Audiovisual Learning
A1  - Di Hu
A1  - Feiping Nie
A1  - Xuelong Li
JO  - ArXiv e-prints
Y1  - 10 July, 2018
UR  - https://arxiv.org/abs/1807.03094
N2  - The seen birds twitter, the running cars accompany with noise, people talks by face-to-face, etc. These naturally audiovisual correspondences provide the possibilities to explore and understand the outside world. However, the mixed multiple objects and sounds make it intractable to perform efficient matching in the unconstrained environment. To settle this problem, we propose to adequately excavate audio and visual components and perform elaborate correspondence learning among them. Concretely, a novel unsupervised audiovisual learning model is proposed, named as Deep Co-Clustering (DCC), that synchronously performs sets of clustering with multimodal vectors of convolutional maps in different shared spaces for capturing multiple audiovisual correspondences. And such integrated multimodal clustering network can be effectively trained with max-margin loss in the end-to-end fashion. Amounts of experiments in feature evaluation and audiovisual tasks are performed. The results demonstrate that DCC can learn effective unimodal representation, with which the classifier can even outperform human. Further, DCC shows noticeable performance in the task of sound localization, multisource detection, and audiovisual understanding.
ER  -


TY  - Preprint
T1  - Video Summarisation by Classification with Deep Reinforcement Learning
A1  - Kaiyang Zhou
A1  - Tao Xiang
A1  - Andrea Cavallaro
JO  - ArXiv e-prints
Y1  - 3 September, 2018
UR  - https://arxiv.org/abs/1807.03089
N2  - Most existing video summarisation methods are based on either supervised or unsupervised learning. In this paper, we propose a reinforcement learning-based weakly supervised method that exploits easy-to-obtain, video-level category labels and encourages summaries to contain category-related information and maintain category recognisability. Specifically, We formulate video summarisation as a sequential decision-making process and train a summarisation network with deep Q-learning (DQSN). A companion classification network is also trained to provide rewards for training the DQSN. With the classification network, we develop a global recognisability reward based on the classification result. Critically, a novel dense ranking-based reward is also proposed in order to cope with the temporally delayed and sparse reward problems for long sequence reinforcement learning. Extensive experiments on two benchmark datasets show that the proposed approach achieves state-of-the-art performance.
ER  -


TY  - Preprint
T1  - A deep learning approach for understanding natural language commands for mobile service robots
A1  - Pedro Henrique Martins
A1  - LuÃ­s CustÃ³dio
A1  - Rodrigo Ventura
JO  - ArXiv e-prints
Y1  - 9 July, 2018
UR  - https://arxiv.org/abs/1807.03053
N2  - Using natural language to give instructions to robots is challenging, since natural language understanding is still largely an open problem. In this paper we address this problem by restricting our attention to commands modeled as one action, plus arguments (also known as slots). For action detection (also called intent detection) and slot filling various architectures of Recurrent Neural Networks and Long Short Term Memory (LSTM) networks were evaluated, having LSTMs achieved a superior accuracy. As the action requested may not fall within the robots capabilities, a Support Vector Machine(SVM) is used to determine whether it is or not. For the input of the neural networks, several word embedding algorithms were compared. Finally, to implement the system in a robot, a ROS package is created using a SMACH state machine. The proposed system is then evaluated both using well-known datasets and benchmarks in the context of domestic service robots.
ER  -


TY  - Preprint
T1  - Deep Learning for Singing Processing: Achievements, Challenges and Impact on Singers and Listeners
A1  - Emilia GÃ³mez
A1  - Merlijn Blaauw
A1  - Jordi Bonada
A1  - Pritish Chandna
A1  - Helena Cuesta
JO  - ArXiv e-prints
Y1  - 9 July, 2018
UR  - https://arxiv.org/abs/1807.03046
N2  - This paper summarizes some recent advances on a set of tasks related to the processing of singing using state-of-the-art deep learning techniques. We discuss their achievements in terms of accuracy and sound quality, and the current challenges, such as availability of data and computing resources. We also discuss the impact that these advances do and will have on listeners and singers when they are integrated in commercial applications.
ER  -


TY  - Preprint
T1  - Improving Deep Learning through Automatic Programming
A1  - The-Hien Dang-Ha
JO  - ArXiv e-prints
Y1  - 8 July, 2018
UR  - https://arxiv.org/abs/1807.02816
N2  - Deep learning and deep architectures are emerging as the best machine learning methods so far in many practical applications such as reducing the dimensionality of data, image classification, speech recognition or object segmentation. In fact, many leading technology companies such as Google, Microsoft or IBM are researching and using deep architectures in their systems to replace other traditional models. Therefore, improving the performance of these models could make a strong impact in the area of machine learning. However, deep learning is a very fast-growing research domain with many core methodologies and paradigms just discovered over the last few years. This thesis will first serve as a short summary of deep learning, which tries to include all of the most important ideas in this research area. Based on this knowledge, we suggested, and conducted some experiments to investigate the possibility of improving the deep learning based on automatic programming (ADATE). Although our experiments did produce good results, there are still many more possibilities that we could not try due to limited time as well as some limitations of the current ADATE version. I hope that this thesis can promote future work on this topic, especially when the next version of ADATE comes out. This thesis also includes a short analysis of the power of ADATE system, which could be useful for other researchers who want to know what it is capable of.
ER  -


TY  - Preprint
T1  - Deep Learning for Launching and Mitigating Wireless Jamming Attacks
A1  - Tugba Erpek
A1  - Yalin E. Sagduyu
A1  - Yi Shi
JO  - ArXiv e-prints
Y1  - 3 July, 2018
UR  - https://arxiv.org/abs/1807.02567
N2  - An adversarial machine learning approach is introduced to launch jamming attacks on wireless communications and a defense strategy is provided. A cognitive transmitter uses a pre-trained classifier to predict current channel status based on recent sensing results and decides whether to transmit or not, whereas a jammer collects channel status and ACKs to build a deep learning classifier that reliably predicts whether there will be a successful transmission next and effectively jams these transmissions. This jamming approach is shown to reduce the performance of the transmitter much more severely compared with randomized or sensing-based jamming. Next, a generative adversarial network (GAN) is developed for the jammer to reduce the time to collect the training dataset by augmenting it with synthetic samples. Then, a defense scheme is introduced for the transmitter that prevents the jammer from building a reliable classifier by deliberately taking a small number of wrong actions (in form of a causative attack launched against the jammer) when it accesses the spectrum. The transmitter systematically selects when to take wrong actions and adapts the level of defense to machine learning-based or conventional jamming behavior in order to mislead the jammer into making prediction errors and consequently increase its throughput.
ER  -


TY  - Preprint
T1  - M-ADDA: Unsupervised Domain Adaptation with Deep Metric Learning
A1  - Issam Laradji
A1  - Reza Babanezhad
JO  - ArXiv e-prints
Y1  - 6 July, 2018
UR  - https://arxiv.org/abs/1807.02552
N2  - Unsupervised domain adaptation techniques have been successful for a wide range of problems where supervised labels are limited. The task is to classify an unlabeled `target&#39; dataset by leveraging a labeled `source&#39; dataset that comes from a slightly similar distribution. We propose metric-based adversarial discriminative domain adaptation (M-ADDA) which performs two main steps. First, it uses a metric learning approach to train the source model on the source dataset by optimizing the triplet loss function. This results in clusters where embeddings of the same label are close to each other and those with different labels are far from one another. Next, it uses the adversarial approach (as that used in ADDA \cite{2017arXiv170205464T}) to make the extracted features from the source and target datasets indistinguishable. Simultaneously, we optimize a novel loss function that encourages the target dataset&#39;s embeddings to form clusters. While ADDA and M-ADDA use similar architectures, we show that M-ADDA performs significantly better on the digits adaptation datasets of MNIST and USPS. This suggests that using metric-learning for domain adaptation can lead to large improvements in classification accuracy for the domain adaptation task. The code is available at \url{https://github.com/IssamLaradji/M-ADDA}.
ER  -


TY  - Preprint
T1  - Blockchain as a Service: An Autonomous, Privacy Preserving, Decentralized Architecture for Deep Learning
A1  - Gihan J. Mendis
A1  - Moein Sabounchi
A1  - Jin Wei
A1  - Rigoberto Roche&#39;
JO  - ArXiv e-prints
Y1  - 5 July, 2018
UR  - https://arxiv.org/abs/1807.02515
N2  - Deep learning algorithms have recently gained attention due to their inherent capabilities and the application opportunities that they provide. Two of the main reasons for the success of deep learning methods are the availability of processing power and big data. Both of these two are expensive and rare commodities that present limitations to the usage and implementation of deep learning. Decentralization of the processing and data is one of the most prevalent solutions for these issues. This paper proposes a cooperative decentralized deep learning architecture. The contributors can train deep learning models with private data and share them to the cooperative data-driven applications initiated elsewhere. Shared models are fused together to obtain a better model. In this work, the contributors can both design their own models or train the models provided by the initiator. In order to utilize an efficient decentralized learning algorithm, blockchain technology is incorporated as a method of creating an incentive-compatible market. In the proposed method, Ethereum blockchain&#39;s scripting capabilities are employed to devise a decentralized deep learning mechanism, which provides much higher, collective processing power and grants access to large amounts of data, which would be otherwise inaccessible. The technical description of the mechanism is described and the simulation results are presented.
ER  -


TY  - Preprint
T1  - Deep Multiple Instance Feature Learning via Variational Autoencoder
A1  - Shabnam Ghaffarzadegan
JO  - ArXiv e-prints
Y1  - 6 July, 2018
UR  - https://arxiv.org/abs/1807.02490
N2  - We describe a novel weakly supervised deep learning framework that combines both the discriminative and generative models to learn meaningful representation in the multiple instance learning (MIL) setting. MIL is a weakly supervised learning problem where labels are associated with groups of instances (referred as bags) instead of individual instances. To address the essential challenge in MIL problems raised from the uncertainty of positive instances label, we use a discriminative model regularized by variational autoencoders (VAEs) to maximize the differences between latent representations of all instances and negative instances. As a result, the hidden layer of the variational autoencoder learns meaningful representation. This representation can effectively be used for MIL problems as illustrated by better performance on the standard benchmark datasets comparing to the state-of-the-art approaches. More importantly, unlike most related studies, the proposed framework can be easily scaled to large dataset problems, as illustrated by the audio event detection and segmentation task. Visualization also confirms the effectiveness of the latent representation in discriminating positive and negative classes.
ER  -


TY  - Preprint
T1  - A Review of Different Word Embeddings for Sentiment Classification using Deep Learning
A1  - Debadri Dutta
JO  - ArXiv e-prints
Y1  - 5 July, 2018
UR  - https://arxiv.org/abs/1807.02471
N2  - The web is loaded with textual content, and Natural Language Processing is a standout amongst the most vital fields in Machine Learning. But when data is huge simple Machine Learning algorithms are not able to handle it and that is when Deep Learning comes into play which based on Neural Networks. However since neural networks cannot process raw text, we have to change over them through some diverse strategies of word embedding. This paper demonstrates those distinctive word embedding strategies implemented on an Amazon Review Dataset, which has two sentiments to be classified: Happy and Unhappy based on numerous customer reviews. Moreover we demonstrate the distinction in accuracy with a discourse about which word embedding to apply when.
ER  -


TY  - Preprint
T1  - Learning a Representation Map for Robot Navigation using Deep Variational Autoencoder
A1  - Kaixin Hu
A1  - Peter O&#39;Connor
JO  - ArXiv e-prints
Y1  - 13 September, 2018
UR  - https://arxiv.org/abs/1807.02401
N2  - The aim of this work is to use Variational Autoencoder (VAE) to learn a representation of an indoor environment that can be used for robot navigation. We use images extracted from a video, in which a camera takes a tour around a house, for training the VAE model with a 4 dimensional latent space. After the model is trained, each real frame has a corresponding representation point on manifold in the latent space, and each representation point has corresponding reconstructed image. For the navigation problem, we map the starting image and destination image to the latent space, then optimize a path on the learned manifold connecting the two points, and finally map the path back through decoder to a sequence of images. The ideal sequence of images should correspond to a route that is spatially continuous - i.e. neighbor images in the route should correspond to neighbor locations in physical space. Such a route could be used for navigation with computer vision techniques, i.e. a robot could follow the image sequence from starting location to destination in the environment step by step. We implement this algorithm, but find in our experimental results that the resulting route is not satisfactory. The route consist of several discontinuous image frames along the ideal routes, so that the route could not be followed by a robot with computer vision techniques in practice. In our evaluation, we propose two reasons for our failure to automatically find continuous routes: (1) The VAE tends to capture global structures, but discard the details; (2) the Euclidean similarity metric used for measuring continuity between house images is sub-optimal. For further work, we propose: trying other generative models like VAE-GANs which may be better at reconstructing the details to learn the representation map, and adjusting the similarity metric in the path selecting algorithm.
ER  -


TY  - Preprint
T1  - End-to-End Race Driving with Deep Reinforcement Learning
A1  - Maximilian Jaritz
A1  - Raoul de Charette
A1  - Marin Toromanoff
A1  - Etienne Perot
A1  - Fawzi Nashashibi
JO  - ArXiv e-prints
Y1  - 31 August, 2018
UR  - https://arxiv.org/abs/1807.02371
N2  - We present research using the latest reinforcement learning algorithm for end-to-end driving without any mediated perception (object recognition, scene understanding). The newly proposed reward and learning strategies lead together to faster convergence and more robust driving using only RGB image from a forward facing camera. An Asynchronous Actor Critic (A3C) framework is used to learn the car control in a physically and graphically realistic rally game, with the agents evolving simultaneously on tracks with a variety of road structures (turns, hills), graphics (seasons, location) and physics (road adherence). A thorough evaluation is conducted and generalization is proven on unseen tracks and using legal speed limits. Open loop tests on real sequences of images show some domain adaption capability of our method.
ER  -


TY  - Preprint
T1  - Automatic deep learning-based normalization of breast dynamic contrast-enhanced magnetic resonance images
A1  - Jun Zhang
A1  - Ashirbani Saha
A1  - Brian J. Soher
A1  - Maciej A. Mazurowski
JO  - ArXiv e-prints
Y1  - 5 July, 2018
UR  - https://arxiv.org/abs/1807.02152
N2  - Objective: To develop an automatic image normalization algorithm for intensity correction of images from breast dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) acquired by different MRI scanners with various imaging parameters, using only image information. Methods: DCE-MR images of 460 subjects with breast cancer acquired by different scanners were used in this study. Each subject had one T1-weighted pre-contrast image and three T1-weighted post-contrast images available. Our normalization algorithm operated under the assumption that the same type of tissue in different patients should be represented by the same voxel value. We used four tissue/material types as the anchors for the normalization: 1) air, 2) fat tissue, 3) dense tissue, and 4) heart. The algorithm proceeded in the following two steps: First, a state-of-the-art deep learning-based algorithm was applied to perform tissue segmentation accurately and efficiently. Then, based on the segmentation results, a subject-specific piecewise linear mapping function was applied between the anchor points to normalize the same type of tissue in different patients into the same intensity ranges. We evaluated the algorithm with 300 subjects used for training and the rest used for testing. Results: The application of our algorithm to images with different scanning parameters resulted in highly improved consistency in pixel values and extracted radiomics features. Conclusion: The proposed image normalization strategy based on tissue segmentation can perform intensity correction fully automatically, without the knowledge of the scanner parameters. Significance: We have thoroughly tested our algorithm and showed that it successfully normalizes the intensity of DCE-MR images. We made our software publicly available for others to apply in their analyses.
ER  -


TY  - Preprint
T1  - 3D Human Action Recognition with Siamese-LSTM Based Deep Metric Learning
A1  - Seyma Yucer
A1  - Yusuf Sinan Akgul
JO  - ArXiv e-prints
Y1  - 5 July, 2018
UR  - https://arxiv.org/abs/1807.02131
N2  - This paper proposes a new 3D Human Action Recognition system as a two-phase system: (1) Deep Metric Learning Module which learns a similarity metric between two 3D joint sequences using Siamese-LSTM networks; (2) A Multiclass Classification Module that uses the output of the first module to produce the final recognition output. This model has several advantages: the first module is trained with a larger set of data because it uses many combinations of sequence pairs.Our deep metric learning module can also be trained independently of the datasets, which makes our system modular and generalizable. We tested the proposed system on standard and newly introduced datasets that showed us that initial results are promising. We will continue developing this system by adding more sophisticated LSTM blocks and by cross-training between different datasets.
ER  -


TY  - Preprint
T1  - Calamari - A High-Performance Tensorflow-based Deep Learning Package for Optical Character Recognition
A1  - Christoph Wick
A1  - Christian Reul
A1  - Frank Puppe
JO  - ArXiv e-prints
Y1  - 6 August, 2018
UR  - https://arxiv.org/abs/1807.02004
N2  - Optical Character Recognition (OCR) on contemporary and historical data is still in the focus of many researchers. Especially historical prints require book specific trained OCR models to achieve applicable results (Springmann and LÃ¼deling, 2016, Reul et al., 2017a). To reduce the human effort for manually annotating ground truth (GT) various techniques such as voting and pretraining have shown to be very efficient (Reul et al., 2018a, Reul et al., 2018b). Calamari is a new open source OCR line recognition software that both uses state-of-the art Deep Neural Networks (DNNs) implemented in Tensorflow and giving native support for techniques such as pretraining and voting. The customizable network architectures constructed of Convolutional Neural Networks (CNNS) and Long-ShortTerm-Memory (LSTM) layers are trained by the so-called Connectionist Temporal Classification (CTC) algorithm of Graves et al. (2006). Optional usage of a GPU drastically reduces the computation times for both training and prediction. We use two different datasets to compare the performance of Calamari to OCRopy, OCRopus3, and Tesseract 4. Calamari reaches a Character Error Rate (CER) of 0.11% on the UW3 dataset written in modern English and 0.18% on the DTA19 dataset written in German Fraktur, which considerably outperforms the results of the existing softwares.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Doom using Unsupervised Auxiliary Tasks
A1  - Georgios Papoudakis
A1  - Kyriakos C. Chatzidimitriou
A1  - Pericles A. Mitkas
JO  - ArXiv e-prints
Y1  - 5 July, 2018
UR  - https://arxiv.org/abs/1807.01960
N2  - Recent developments in deep reinforcement learning have enabled the creation of agents for solving a large variety of games given a visual input. These methods have been proven successful for 2D games, like the Atari games, or for simple tasks, like navigating in mazes. It is still an open question, how to address more complex environments, in which the reward is sparse and the state space is huge. In this paper we propose a divide and conquer deep reinforcement learning solution and we test our agent in the first person shooter (FPS) game of Doom. Our work is based on previous works in deep reinforcement learning and in Doom agents. We also present how our agent is able to perform better in unknown environments compared to a state of the art reinforcement learning algorithm.
ER  -


TY  - Preprint
T1  - Deep Cross-modality Adaptation via Semantics Preserving Adversarial Learning for Sketch-based 3D Shape Retrieval
A1  - Jiaxin Chen
A1  - Yi Fang
JO  - ArXiv e-prints
Y1  - 4 July, 2018
UR  - https://arxiv.org/abs/1807.01806
N2  - Due to the large cross-modality discrepancy between 2D sketches and 3D shapes, retrieving 3D shapes by sketches is a significantly challenging task. To address this problem, we propose a novel framework to learn a discriminative deep cross-modality adaptation model in this paper. Specifically, we first separately adopt two metric networks, following two deep convolutional neural networks (CNNs), to learn modality-specific discriminative features based on an importance-aware metric learning method. Subsequently, we explicitly introduce a cross-modality transformation network to compensate for the divergence between two modalities, which can transfer features of 2D sketches to the feature space of 3D shapes. We develop an adversarial learning based method to train the transformation model, by simultaneously enhancing the holistic correlations between data distributions of two modalities, and mitigating the local semantic divergences through minimizing a cross-modality mean discrepancy term. Experimental results on the SHREC 2013 and SHREC 2014 datasets clearly show the superior retrieval performance of our proposed model, compared to the state-of-the-art approaches.
ER  -


TY  - Preprint
T1  - Synthetic contrast enhancement in cardiac CT with Deep Learning
A1  - Gianmarco Santini
A1  - Lorena M. Zumbo
A1  - Nicola Martini
A1  - Gabriele Valvano
A1  - Andrea Leo
A1  - Andrea Ripoli
A1  - Francesco Avogliero
A1  - Dante Chiappino
A1  - Daniele Della Latta
JO  - ArXiv e-prints
Y1  - 2 July, 2018
UR  - https://arxiv.org/abs/1807.01779
N2  - In Europe the 20% of the CT scans cover the thoracic region. The acquired images contain information about the cardiovascular system that often remains latent due to the lack of contrast in the cardiac area. On the other hand, the contrast enhanced computed tomography (CECT) represents an imaging technique that allows to easily assess the cardiac chambers volumes and the contrast dynamics. With this work we aim to face the problem of extraction and presentation of these latent information, using a deep learning approach with convolutional neural networks. Starting from the extraction of relevant features from the image without contrast medium, we try to re-map them on features typical of CECT, to synthesize an image characterized by an attenuation in the cardiac chambers as if a virtually iodine contrast medium was injected. The purposes are to guarantee an estimation of the left cardiac chambers volume and to perform an evaluation of the contrast dynamics. Our approach is based on a deconvolutional network trained on a set of 120 patients who underwent both CT acquisitions in the same contrastographic arterial phase and the same cardiac phase. To ensure a reliable predicted CECT image, in terms of values and morphology, a custom loss function is defined by combining an error function to find a pixel-wise correspondence, which takes into account the similarity in term of Hounsfield units between the input and output images and by a cross-entropy computed on the binarized versions of the synthesized and of the real CECT image. The proposed method is finally tested on 20 subjects.
ER  -


TY  - Preprint
T1  - Learning Personalized Representation for Inverse Problems in Medical Imaging Using Deep Neural Network
A1  - Kuang Gong
A1  - Kyungsang Kim
A1  - Jianan Cui
A1  - Ning Guo
A1  - Ciprian Catana
A1  - Jinyi Qi
A1  - Quanzheng Li
JO  - ArXiv e-prints
Y1  - 4 July, 2018
UR  - https://arxiv.org/abs/1807.01759
N2  - Recently deep neural networks have been widely and successfully applied in computer vision tasks and attracted growing interests in medical imaging. One barrier for the application of deep neural networks to medical imaging is the need of large amounts of prior training pairs, which is not always feasible in clinical practice. In this work we propose a personalized representation learning framework where no prior training pairs are needed, but only the patient&#39;s own prior images. The representation is expressed using a deep neural network with the patient&#39;s prior images as network input. We then applied this novel image representation to inverse problems in medical imaging in which the original inverse problem was formulated as a constraint optimization problem and solved using the alternating direction method of multipliers (ADMM) algorithm. Anatomically guided brain positron emission tomography (PET) image reconstruction and image denoising were employed as examples to demonstrate the effectiveness of the proposed framework. Quantification results based on simulation and real datasets show that the proposed personalized representation framework outperform other widely adopted methods.
ER  -


TY  - Preprint
T1  - Deep Learning Based Damage Detection on Post-Hurricane Satellite Imagery
A1  - Quoc Dung Cao
A1  - Youngjun Choe
JO  - ArXiv e-prints
Y1  - 4 July, 2018
UR  - https://arxiv.org/abs/1807.01688
N2  - After a hurricane, damage assessment is critical to emergency managers and first responders. To improve the efficiency and accuracy of damage assessment, instead of using windshield survey, we propose to automatically detect damaged buildings using image classification algorithms. The method is applied to the case study of 2017 Hurricane Harvey.
ER  -


TY  - Preprint
T1  - The SEN1-2 Dataset for Deep Learning in SAR-Optical Data Fusion
A1  - Michael Schmitt
A1  - Lloyd Haydn Hughes
A1  - Xiao Xiang Zhu
JO  - ArXiv e-prints
Y1  - 4 July, 2018
UR  - https://arxiv.org/abs/1807.01569
N2  - While deep learning techniques have an increasing impact on many technical fields, gathering sufficient amounts of training data is a challenging problem in remote sensing. In particular, this holds for applications involving data from multiple sensors with heterogeneous characteristics. One example for that is the fusion of synthetic aperture radar (SAR) data and optical imagery. With this paper, we publish the SEN1-2 dataset to foster deep learning research in SAR-optical data fusion. SEN1-2 comprises 282,384 pairs of corresponding image patches, collected from across the globe and throughout all meteorological seasons. Besides a detailed description of the dataset, we show exemplary results for several possible applications, such as SAR image colorization, SAR-optical image matching, and creation of artificial optical images from SAR input data. Since SEN1-2 is the first large open dataset of this kind, we believe it will support further developments in the field of deep learning for remote sensing as well as multi-sensor data fusion.
ER  -


TY  - Preprint
T1  - Wideband Time-Domain Digital Backpropagation via Subband Processing and Deep Learning
A1  - Christian HÃ¤ger
A1  - Henry D. Pfister
JO  - ArXiv e-prints
Y1  - 4 July, 2018
UR  - https://arxiv.org/abs/1807.01545
N2  - We propose a low-complexity sub-banded DSP architecture for digital backpropagation where the walk-off effect is compensated using simple delay elements. For a simulated 96-Gbaud signal and 2500 km optical link, our method achieves a 2.8 dB SNR improvement over linear equalization.
ER  -


TY  - Preprint
T1  - EmbNum: Semantic labeling for numerical values with deep metric learning
A1  - Phuc Nguyen
A1  - Khai Nguyen
A1  - Ryutaro Ichise
A1  - Hideaki Takeda
JO  - ArXiv e-prints
Y1  - 16 August, 2018
UR  - https://arxiv.org/abs/1807.01367
N2  - Semantic labeling for numerical values is a task of assigning semantic labels to unknown numerical attributes. The semantic labels could be numerical properties in ontologies, instances in knowledge bases, or labeled data that are manually annotated by domain experts. In this paper, we refer to semantic labeling as a retrieval setting where the label of an unknown attribute is assigned by the label of the most relevant attribute in labeled data. One of the greatest challenges is that an unknown attribute rarely has the same set of values with the similar one in the labeled data. To overcome the issue, statistical interpretation of value distribution is taken into account. However, the existing studies assume a specific form of distribution. It is not appropriate in particular to apply open data where there is no knowledge of data in advance. To address these problems, we propose a neural numerical embedding model (EmbNum) to learn useful representation vectors for numerical attributes without prior assumptions on the distribution of data. Then, the &#34;semantic similarities&#34; between the attributes are measured on these representation vectors by the Euclidean distance. Our empirical experiments on City Data and Open Data show that EmbNum significantly outperforms state-of-the-art methods for the task of numerical attribute semantic labeling regarding effectiveness and efficiency.
ER  -


TY  - Preprint
T1  - Human-level performance in first-person multiplayer games with population-based deep reinforcement learning
A1  - Max Jaderberg
A1  - Wojciech M. Czarnecki
A1  - Iain Dunning
A1  - Luke Marris
A1  - Guy Lever
A1  - Antonio Garcia Castaneda
A1  - Charles Beattie
A1  - Neil C. Rabinowitz
A1  - Ari S. Morcos
A1  - Avraham Ruderman
A1  - Nicolas Sonnerat
A1  - Tim Green
A1  - Louise Deason
A1  - Joel Z. Leibo
A1  - David Silver
A1  - Demis Hassabis
A1  - Koray Kavukcuoglu
A1  - Thore Graepel
JO  - ArXiv e-prints
Y1  - 3 July, 2018
UR  - https://arxiv.org/abs/1807.01281
N2  - Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.
ER  -


TY  - Preprint
T1  - Securing Input Data of Deep Learning Inference Systems via Partitioned Enclave Execution
A1  - Zhongshu Gu
A1  - Heqing Huang
A1  - Jialong Zhang
A1  - Dong Su
A1  - Ankita Lamba
A1  - Dimitrios Pendarakis
A1  - Ian Molloy
JO  - ArXiv e-prints
Y1  - 3 July, 2018
UR  - https://arxiv.org/abs/1807.00969
N2  - Deep learning systems have been widely deployed as backend engines of artificial intelligence (AI) services for their approaching-human performance in cognitive tasks. However, end users always have some concerns about the confidentiality of their provisioned input data, even for those reputable AI service providers. Accidental disclosures of sensitive user data might unexpectedly happen due to security breaches, exploited vulnerabilities, neglect, or insiders. In this paper, we systematically investigate the potential information exposure in deep learning based AI inference systems. Based on our observation, we develop DeepEnclave, a privacy-enhancing system to mitigate sensitive information disclosure in deep learning inference pipelines. The key innovation is to partition deep learning models and leverage secure enclave techniques on cloud infrastructures to cryptographically protect the confidentiality and integrity of user inputs. We formulate the information exposure problem as a reconstruction privacy attack and quantify the adversary&#39;s capabilities with different attack strategies. Our comprehensive security analysis and performance measurement can act as a guideline for end users to determine their principle of partitioning deep neural networks, thus to achieve maximum privacy guarantee with acceptable performance overhead.
ER  -


TY  - Preprint
T1  - Deepcode: Feedback Codes via Deep Learning
A1  - Hyeji Kim
A1  - Yihan Jiang
A1  - Sreeram Kannan
A1  - Sewoong Oh
A1  - Pramod Viswanath
JO  - ArXiv e-prints
Y1  - 2 July, 2018
UR  - https://arxiv.org/abs/1807.00801
N2  - The design of codes for communicating reliably over a statistically well defined channel is an important endeavor involving deep mathematical research and wide-ranging practical applications. In this work, we present the first family of codes obtained via deep learning, which significantly beats state-of-the-art codes designed over several decades of research. The communication channel under consideration is the Gaussian noise channel with feedback, whose study was initiated by Shannon; feedback is known theoretically to improve reliability of communication, but no practical codes that do so have ever been successfully constructed.
ER  -


TY  - Preprint
T1  - Online Label Recovery for Deep Learning-based Communication through Error Correcting Codes
A1  - Stefan Schibisch
A1  - Sebastian Cammerer
A1  - Sebastian DÃ¶rner
A1  - Jakob Hoydis
A1  - Stephan ten Brink
JO  - ArXiv e-prints
Y1  - 2 July, 2018
UR  - https://arxiv.org/abs/1807.00747
N2  - We demonstrate that error correcting codes (ECCs) can be used to construct a labeled data set for finetuning of &#34;trainable&#34; communication systems without sacrificing resources for the transmission of known symbols. This enables adaptive systems, which can be trained on-the-fly to compensate for slow fluctuations in channel conditions or varying hardware impairments. We examine the influence of corrupted training data and show that it is crucial to train based on correct labels. The proposed method can be applied to fully end-to-end trained communication systems (autoencoders) as well as systems with only some trainable components. This is exemplified by extending a conventional OFDM system with a trainable pre-equalizer neural network (NN) that can be optimized at run time.
ER  -


TY  - Preprint
T1  - Classifying neuromorphic data using a deep learning framework for image classification
A1  - Roshan Gopalakrishnan
A1  - Yansong Chua
A1  - Laxmi R Iyer
JO  - ArXiv e-prints
Y1  - 2 July, 2018
UR  - https://arxiv.org/abs/1807.00578
N2  - In the field of artificial intelligence, neuromorphic computing has been around for several decades. Deep learning has however made much recent progress such that it consistently outperforms neuromorphic learning algorithms in classification tasks in terms of accuracy. Specifically in the field of image classification, neuromorphic computing has been traditionally using either the temporal or rate code for encoding static images in datasets into spike trains. It is only till recently, that neuromorphic vision sensors are widely used by the neuromorphic research community, and provides an alternative to such encoding methods. Since then, several neuromorphic datasets as obtained by applying such sensors on image datasets (e.g. the neuromorphic CALTECH 101) have been introduced. These data are encoded in spike trains and hence seem ideal for benchmarking of neuromorphic learning algorithms. Specifically, we train a deep learning framework used for image classification on the CALTECH 101 and a collapsed version of the neuromorphic CALTECH 101 datasets. We obtained an accuracy of 91.66% and 78.01% for the CALTECH 101 and neuromorphic CALTECH 101 datasets respectively. For CALTECH 101, our accuracy is close to the best reported accuracy, while for neuromorphic CALTECH 101, it outperforms the last best reported accuracy by over 10%. This raises the question of the suitability of such datasets as benchmarks for neuromorphic learning algorithms.
ER  -


TY  - Preprint
T1  - Confounding variables can degrade generalization performance of radiological deep learning models
A1  - John R. Zech
A1  - Marcus A. Badgeley
A1  - Manway Liu
A1  - Anthony B. Costa
A1  - Joseph J. Titano
A1  - Eric K. Oermann
JO  - ArXiv e-prints
Y1  - 12 July, 2018
UR  - https://arxiv.org/abs/1807.00431
N2  - Early results in using convolutional neural networks (CNNs) on x-rays to diagnose disease have been promising, but it has not yet been shown that models trained on x-rays from one hospital or one group of hospitals will work equally well at different hospitals. Before these tools are used for computer-aided diagnosis in real-world clinical settings, we must verify their ability to generalize across a variety of hospital systems. A cross-sectional design was used to train and evaluate pneumonia screening CNNs on 158,323 chest x-rays from NIH (n=112,120 from 30,805 patients), Mount Sinai (42,396 from 12,904 patients), and Indiana (n=3,807 from 3,683 patients). In 3 / 5 natural comparisons, performance on chest x-rays from outside hospitals was significantly lower than on held-out x-rays from the original hospital systems. CNNs were able to detect where an x-ray was acquired (hospital system, hospital department) with extremely high accuracy and calibrate predictions accordingly. The performance of CNNs in diagnosing diseases on x-rays may reflect not only their ability to identify disease-specific imaging findings on x-rays, but also their ability to exploit confounding information. Estimates of CNN performance based on test data from hospital systems used for model training may overstate their likely real-world performance.
ER  -


TY  - Preprint
T1  - SYQ: Learning Symmetric Quantization For Efficient Deep Neural Networks
A1  - Julian Faraone
A1  - Nicholas Fraser
A1  - Michaela Blott
A1  - Philip H. W. Leong
JO  - ArXiv e-prints
Y1  - 1 July, 2018
UR  - https://arxiv.org/abs/1807.00301
N2  - Inference for state-of-the-art deep neural networks is computationally expensive, making them difficult to deploy on constrained hardware environments. An efficient way to reduce this complexity is to quantize the weight parameters and/or activations during training by approximating their distributions with a limited entry codebook. For very low-precisions, such as binary or ternary networks with 1-8-bit activations, the information loss from quantization leads to significant accuracy degradation due to large gradient mismatches between the forward and backward functions. In this paper, we introduce a quantization method to reduce this loss by learning a symmetric codebook for particular weight subgroups. These subgroups are determined based on their locality in the weight matrix, such that the hardware simplicity of the low-precision representations is preserved. Empirically, we show that symmetric quantization can substantially improve accuracy for networks with extremely low-precision weights and activations. We also demonstrate that this representation imposes minimal or no hardware implications to more coarse-grained approaches. Source code is available at https://www.github.com/julianfaraone/SYQ.
ER  -


TY  - Preprint
T1  - Autonomous Deep Learning: A Genetic DCNN Designer for Image Classification
A1  - Benteng Ma
A1  - Yong Xia
JO  - ArXiv e-prints
Y1  - 1 July, 2018
UR  - https://arxiv.org/abs/1807.00284
N2  - Recent years have witnessed the breakthrough success of deep convolutional neural networks (DCNNs) in image classification and other vision applications. Although freeing users from the troublesome handcrafted feature extraction by providing a uniform feature extraction-classification framework, DCNNs still require a handcrafted design of their architectures. In this paper, we propose the genetic DCNN designer, an autonomous learning algorithm can generate a DCNN architecture automatically based on the data available for a specific image classification problem. We first partition a DCNN into multiple stacked meta convolutional blocks and fully connected blocks, each containing the operations of convolution, pooling, fully connection, batch normalization, activation and drop out, and thus convert the architecture into an integer vector. Then, we use refined evolutionary operations, including selection, mutation and crossover to evolve a population of DCNN architectures. Our results on the MNIST, Fashion-MNIST, EMNISTDigit, EMNIST-Letter, CIFAR10 and CIFAR100 datasets suggest that the proposed genetic DCNN designer is able to produce automatically DCNN architectures, whose performance is comparable to, if not better than, that of stateof- the-art DCNN models
ER  -


TY  - Preprint
T1  - Accurate Uncertainties for Deep Learning Using Calibrated Regression
A1  - Volodymyr Kuleshov
A1  - Nathan Fenner
A1  - Stefano Ermon
JO  - ArXiv e-prints
Y1  - 30 June, 2018
UR  - https://arxiv.org/abs/1807.00263
N2  - Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate -- for example, a 90% credible interval may not contain the true outcome 90% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.
ER  -


TY  - Preprint
T1  - Adversarial Examples in Deep Learning: Characterization and Divergence
A1  - Wenqi Wei
A1  - Ling Liu
A1  - Stacey Truex
A1  - Lei Yu
A1  - Mehmet Emre Gursoy
JO  - ArXiv e-prints
Y1  - 29 June, 2018
UR  - https://arxiv.org/abs/1807.00051
N2  - The burgeoning success of deep learning has raised the security and privacy concerns as more and more tasks are accompanied with sensitive data. Adversarial attacks in deep learning have emerged as one of the dominating security threat to a range of mission-critical deep learning systems and applications. This paper takes a holistic and principled approach to perform statistical characterization of adversarial examples in deep learning. We provide a general formulation of adversarial examples and elaborate on the basic principle for adversarial attack algorithm design. We introduce easy and hard categorization of adversarial attacks to analyze the effectiveness of adversarial examples in terms of attack success rate, degree of change in adversarial perturbation, average entropy of prediction qualities, and fraction of adversarial examples that lead to successful attacks. We conduct extensive experimental study on adversarial behavior in easy and hard attacks under deep learning models with different hyperparameters and different deep learning frameworks. We show that the same adversarial attack behaves differently under different hyperparameters and across different frameworks due to the different features learned under different deep learning model training process. Our statistical characterization with strong empirical evidence provides a transformative enlightenment on mitigation strategies towards effective countermeasures against present and future adversarial attacks.
ER  -


TY  - Preprint
T1  - Exploration of Low Numeric Precision Deep Learning Inference Using Intel FPGAs
A1  - Philip Colangelo
A1  - Nasibeh Nasiri
A1  - Asit Mishra
A1  - Eriko Nurvitadhi
A1  - Martin Margala
A1  - Kevin Nealis
JO  - ArXiv e-prints
Y1  - 12 June, 2018
UR  - https://arxiv.org/abs/1806.11547
N2  - CNNs have been shown to maintain reasonable classification accuracy when quantized to lower precisions. Quantizing to sub 8-bit activations and weights can result in accuracy falling below an acceptable threshold. Techniques exist for closing the accuracy gap of limited numeric precision typically by increasing computation. This results in a trade-off between throughput and accuracy and can be tailored for different networks through various combinations of activation and weight data widths. Hardware architectures like FPGAs provide the opportunity for data width specific computation through unique logic configurations leading to highly optimized processing that is unattainable by full precision networks. Ternary and binary weighted networks offer an efficient method of inference for 2-bit and 1-bit data respectively. Most hardware architectures can take advantage of the memory storage and bandwidth savings that come along with smaller datapaths, but very few architectures can take advantage of limited numeric precision at the computation level. In this paper, we present a hardware design for FPGAs that takes advantage of bandwidth, memory, power, and computation savings of limited numerical precision data. We provide insights into the trade-offs between throughput and accuracy for various networks and how they map to our framework. Further, we show how limited numeric precision computation can be efficiently mapped onto FPGAs for both ternary and binary cases. Starting with Arria 10, we show a 2-bit activation and ternary weighted AlexNet running in hardware that achieves 3,700 images per second on the ImageNet dataset with a top-1 accuracy of 0.49. Using a hardware modeler designed for our low numeric precision framework we project performance most notably for a 55.5 TOPS Stratix 10 device running a modified ResNet-34 with only 3.7% accuracy degradation compared with single precision.
ER  -


TY  - Preprint
T1  - MRFusion: A Deep Learning architecture to fuse PAN and MS imagery for land cover mapping
A1  - Raffaele Gaetano
A1  - Dino Ienco
A1  - Kenji Ose
A1  - Remi Cresson
JO  - ArXiv e-prints
Y1  - 29 June, 2018
UR  - https://arxiv.org/abs/1806.11452
N2  - Nowadays, Earth Observation systems provide a multitude of heterogeneous remote sensing data. How to manage such richness leveraging its complementarity is a crucial chal- lenge in modern remote sensing analysis. Data Fusion techniques deal with this point proposing method to combine and exploit complementarity among the different data sensors. Considering optical Very High Spatial Resolution (VHSR) images, satellites obtain both Multi Spectral (MS) and panchro- matic (PAN) images at different spatial resolution. VHSR images are extensively exploited to produce land cover maps to deal with agricultural, ecological, and socioeconomic issues as well as assessing ecosystem status, monitoring biodiversity and provid- ing inputs to conceive food risk monitoring systems. Common techniques to produce land cover maps from such VHSR images typically opt for a prior pansharpening of the multi-resolution source for a full resolution processing. Here, we propose a new deep learning architecture to jointly use PAN and MS imagery for a direct classification without any prior image fusion or resampling process. By managing the spectral information at its native spatial resolution, our method, named MRFusion, aims at avoiding the possible infor- mation loss induced by pansharpening or any other hand-crafted preprocessing. Moreover, the proposed architecture is suitably designed to learn non-linear transformations of the sources with the explicit aim of taking as much as possible advantage of the complementarity of PAN and MS imagery. Experiments are carried out on two-real world scenarios depicting large areas with different land cover characteristics. The characteristics of the proposed scenarios underline the applicability and the generality of our method in operational settings.
ER  -


TY  - Preprint
T1  - Detecting Mammals in UAV Images: Best Practices to address a substantially Imbalanced Dataset with Deep Learning
A1  - Benjamin Kellenberger
A1  - Diego Marcos
A1  - Devis Tuia
JO  - ArXiv e-prints
Y1  - 29 June, 2018
UR  - https://arxiv.org/abs/1806.11368
N2  - Knowledge over the number of animals in large wildlife reserves is a vital necessity for park rangers in their efforts to protect endangered species. Manual animal censuses are dangerous and expensive, hence Unmanned Aerial Vehicles (UAVs) with consumer level digital cameras are becoming a popular alternative tool to estimate livestock. Several works have been proposed that semi-automatically process UAV images to detect animals, of which some employ Convolutional Neural Networks (CNNs), a recent family of deep learning algorithms that proved very effective in object detection in large datasets from computer vision. However, the majority of works related to wildlife focuses only on small datasets (typically subsets of UAV campaigns), which might be detrimental when presented with the sheer scale of real study areas for large mammal census. Methods may yield thousands of false alarms in such cases. In this paper, we study how to scale CNNs to large wildlife census tasks and present a number of recommendations to train a CNN on a large UAV dataset. We further introduce novel evaluation protocols that are tailored to censuses and model suitability for subsequent human verification of detections. Using our recommendations, we are able to train a CNN reducing the number of false positives by an order of magnitude compared to previous state-of-the-art. Setting the requirements at 90% recall, our CNN allows to reduce the amount of data required for manual verification by three times, thus making it possible for rangers to screen all the data acquired efficiently and to detect almost all animals in the reserve automatically.
ER  -


TY  - Preprint
T1  - A hybrid deep learning approach for medical relation extraction
A1  - Veera Raghavendra Chikka
A1  - Kamalakar Karlapalem
JO  - ArXiv e-prints
Y1  - 26 June, 2018
UR  - https://arxiv.org/abs/1806.11189
N2  - Mining relationships between treatment(s) and medical problem(s) is vital in the biomedical domain. This helps in various applications, such as decision support system, safety surveillance, and new treatment discovery. We propose a deep learning approach that utilizes both word level and sentence-level representations to extract the relationships between treatment and problem. While deep learning techniques demand a large amount of data for training, we make use of a rule-based system particularly for relationship classes with fewer samples. Our final relations are derived by jointly combining the results from deep learning and rule-based models. Our system achieved a promising performance on the relationship classes of I2b2 2010 relation extraction task.
ER  -


TY  - Preprint
T1  - Deep Learning Based Instance Segmentation in 3D Biomedical Images Using Weak Annotation
A1  - Zhuo Zhao
A1  - Lin Yang
A1  - Hao Zheng
A1  - Ian H. Guldner
A1  - Siyuan Zhang
A1  - Danny Z. Chen
JO  - ArXiv e-prints
Y1  - 28 June, 2018
UR  - https://arxiv.org/abs/1806.11137
N2  - Instance segmentation in 3D images is a fundamental task in biomedical image analysis. While deep learning models often work well for 2D instance segmentation, 3D instance segmentation still faces critical challenges, such as insufficient training data due to various annotation difficulties in 3D biomedical images. Common 3D annotation methods (e.g., full voxel annotation) incur high workloads and costs for labeling enough instances for training deep learning 3D instance segmentation models. In this paper, we propose a new weak annotation approach for training a fast deep learning 3D instance segmentation model without using full voxel mask annotation. Our approach needs only 3D bounding boxes for all instances and full voxel annotation for a small fraction of the instances, and uses a novel two-stage 3D instance segmentation model utilizing these two kinds of annotation, respectively. We evaluate our approach on several biomedical image datasets, and the experimental results show that (1) with full annotated boxes and a small amount of masks, our approach can achieve similar performance as the best known methods using full annotation, and (2) with similar annotation time, our approach outperforms the best known methods that use full annotation.
ER  -


TY  - Preprint
T1  - Deep Semi Supervised Generative Learning for Automated PD-L1 Tumor Cell Scoring on NSCLC Tissue Needle Biopsies
A1  - Ansh Kapil
A1  - Armin Meier
A1  - Aleksandra Zuraw
A1  - Keith Steele
A1  - Marlon Rebelatto
A1  - GÃ¼nter Schmidt
A1  - Nicolas Brieu
JO  - ArXiv e-prints
Y1  - 28 June, 2018
UR  - https://arxiv.org/abs/1806.11036
N2  - The level of PD-L1 expression in immunohistochemistry (IHC) assays is a key biomarker for the identification of Non-Small-Cell-Lung-Cancer (NSCLC) patients that may respond to anti PD-1/PD-L1 treatments. The quantification of PD-L1 expression currently includes the visual estimation of a Tumor Cell (TC) score by a pathologist and consists of evaluating the ratio of PD-L1 positive and PD-L1 negative tumor cells. Known challenges like differences in positivity estimation around clinically relevant cut-offs and sub-optimal quality of samples makes visual scoring tedious and subjective, yielding a scoring variability between pathologists. In this work, we propose a novel deep learning solution that enables the first automated and objective scoring of PD-L1 expression in late stage NSCLC needle biopsies. To account for the low amount of tissue available in biopsy images and to restrict the amount of manual annotations necessary for training, we explore the use of semi-supervised approaches against standard fully supervised methods. We consolidate the manual annotations used for training as well the visual TC scores used for quantitative evaluation with multiple pathologists. Concordance measures computed on a set of slides unseen during training provide evidence that our automatic scoring method matches visual scoring on the considered dataset while ensuring repeatability and objectivity.
ER  -


TY  - Preprint
T1  - Deep learning for dehazing: Comparison and analysis
A1  - A Benoit
A1  - Leonel Cuevas
A1  - Jean-Baptiste Thomas
JO  - ArXiv e-prints
Y1  - 28 June, 2018
UR  - https://arxiv.org/abs/1806.10923
N2  - We compare a recent dehazing method based on deep learning, Dehazenet, with traditional state-of-the-art approaches , on benchmark data with reference. Dehazenet estimates the depth map from transmission factor on a single color image, which is used to inverse the Koschmieder model of imaging in the presence of haze. In this sense, the solution is still attached to the Koschmieder model. We demonstrate that the transmission is very well estimated by the network, but also that this method exhibits the same limitation than others due to the use of the same imaging model.
ER  -


TY  - Preprint
T1  - Deep learning in business analytics and operations research: Models, applications and managerial implications
A1  - Mathias Kraus
A1  - Stefan Feuerriegel
A1  - Asil Oztekin
JO  - ArXiv e-prints
Y1  - 28 June, 2018
UR  - https://arxiv.org/abs/1806.10897
N2  - Business analytics refers to methods and practices that create value through data for individuals, firms, and organizations. This field is currently experiencing a radical shift due to the advent of deep learning: deep neural networks promise improvements in prediction performance as compared to models from traditional machine learning. However, our research into the existing body of literature reveals a scarcity of research works utilizing deep learning in our discipline. Accordingly, the objectives of this work are as follows: (1) we motivate why researchers and practitioners from business analytics should utilize deep neural networks and review potential use cases, necessary requirements, and benefits. (2) We investigate the added value to operations research in different case studies with real data from entrepreneurial undertakings. All such cases demonstrate a higher prediction performance in comparison to traditional machine learning and thus direct value gains. (3) We provide guidelines and implications for researchers, managers and practitioners in operations research who want to advance their capabilities for business analytics with regard to deep learning. (4) We finally discuss directions for future research in the field of business analytics.
ER  -


TY  - Preprint
T1  - Deep Learning-Aided Iterative Detector for Massive Overloaded MIMO Channels
A1  - Masayuki Imanishi
A1  - Satoshi Takabe
A1  - Tadashi Wadayama
JO  - ArXiv e-prints
Y1  - 28 June, 2018
UR  - https://arxiv.org/abs/1806.10827
N2  - The paper presents a deep learning-aided iterative detection algorithm for massive overloaded MIMO channels. The proposed algorithm is based on the iterative soft thresholding algorithm for sparse signal recovery. The notable feature of the proposed scheme is that the detector has a reasonably low computational cost and contains trainable parameters which can be optimized with standard deep learning techniques. The number of trainable parameters is constant to the channel size, which promotes fast and stable training processes for the detector. The numerical simulations show that the proposed detector achieves a comparable detection performance to the state-of-the-art IW-SOAV detector for massive overloaded MIMO channels.
ER  -


TY  - Preprint
T1  - Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation
A1  - Niels Justesen
A1  - Ruben Rodriguez Torrado
A1  - Philip Bontrager
A1  - Ahmed Khalifa
A1  - Julian Togelius
A1  - Sebastian Risi
JO  - ArXiv e-prints
Y1  - 7 September, 2018
UR  - https://arxiv.org/abs/1806.10729
N2  - Deep reinforcement learning (RL) has shown impressive results in a variety of domains, learning directly from high-dimensional sensory streams. However, when neural networks are trained in a fixed environment, such as a single level in a video game, they will usually overfit and fail to generalize to new levels. When RL models overfit, even slight modifications to the environment can result in poor agent performance. In this paper, we explore how procedurally generated levels during training increase generality. We show that for some games procedural level generation enables generalization to new levels within the same distribution. Additionally, it is possible to achieve better performance with less data by manipulating the difficulty of the levels in response to the performance of the agent. The generality of the learned behaviors is also evaluated on a set of human-designed levels. Our results show that the ability to generalize to human-designed levels highly depends on the design of the level generators. We apply dimensionality reduction and clustering techniques to visualize the generators&#39; distributions of levels and analyze to what degree they can produce levels similar to those designed by a human.
ER  -


TY  - Preprint
T1  - Gradient Similarity: An Explainable Approach to Detect Adversarial Attacks against Deep Learning
A1  - Jasjeet Dhaliwal
A1  - Saurabh Shintre
JO  - ArXiv e-prints
Y1  - 27 June, 2018
UR  - https://arxiv.org/abs/1806.10707
N2  - Deep neural networks are susceptible to small-but-specific adversarial perturbations capable of deceiving the network. This vulnerability can lead to potentially harmful consequences in security-critical applications. To address this vulnerability, we propose a novel metric called \emph{Gradient Similarity} that allows us to capture the influence of training data on test inputs. We show that \emph{Gradient Similarity} behaves differently for normal and adversarial inputs, and enables us to detect a variety of adversarial attacks with a near perfect ROC-AUC of 95-100\%. Even white-box adversaries equipped with perfect knowledge of the system cannot bypass our detector easily. On the MNIST dataset, white-box attacks are either detected with a high ROC-AUC of 87-96\%, or require very high distortion to bypass our detector.
ER  -


TY  - Preprint
T1  - This looks like that: deep learning for interpretable image recognition
A1  - Chaofan Chen
A1  - Oscar Li
A1  - Alina Barnett
A1  - Jonathan Su
A1  - Cynthia Rudin
JO  - ArXiv e-prints
Y1  - 27 June, 2018
UR  - https://arxiv.org/abs/1806.10574
N2  - When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The algorithm thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, geologists, architects, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training, meaning that there are no labels for parts of images. We demonstrate the method on the CIFAR-10 dataset and 10 classes from the CUB-200-2011 dataset.
ER  -


TY  - Preprint
T1  - Deep Steganalysis: End-to-End Learning with Supervisory Information beyond Class Labels
A1  - Wei Wang
A1  - Jing Dong
A1  - Yinlong Qian
A1  - Tieniu Tan
JO  - ArXiv e-prints
Y1  - 27 June, 2018
UR  - https://arxiv.org/abs/1806.10443
N2  - Recently, deep learning has shown its power in steganalysis. However, the proposed deep models have been often learned from pre-calculated noise residuals with fixed high-pass filters rather than from raw images. In this paper, we propose a new end-to-end learning framework that can learn steganalytic features directly from pixels. In the meantime, the high-pass filters are also automatically learned. Besides class labels, we make use of additional pixel level supervision of cover-stego image pair to jointly and iteratively train the proposed network which consists of a residual calculation network and a steganalysis network. The experimental results prove the effectiveness of the proposed architecture.
ER  -


TY  - Preprint
T1  - A Generalized Data Representation and Training-Performance Analysis for Deep Learning-Based Communications Systems
A1  - Xiao Chen
A1  - Liang Wu
A1  - Zaichen Zhang
JO  - ArXiv e-prints
Y1  - 6 July, 2018
UR  - https://arxiv.org/abs/1806.10333
N2  - Deep learning (DL)-based autoencoder is a potential architecture to implement end-to-end communication systems. In this letter, we first give a brief introduction to the autoencoder-represented communication system. Then, we propose a novel generalized data representation (GDR) aiming to improve the data rate of DL-based communication systems. Finally, simulation results show that the proposed GDR scheme has lower training complexity, comparable block error rate performance and higher channel capacity than the conventional one-hot vector scheme. Furthermore, we investigate the effect of signal-to-noise ratio (SNR) in DL-based communication systems and prove that training at a high SNR could produce a good training performance for autoencoder.
ER  -


TY  - Preprint
T1  - QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation
A1  - Dmitry Kalashnikov
A1  - Alex Irpan
A1  - Peter Pastor
A1  - Julian Ibarz
A1  - Alexander Herzog
A1  - Eric Jang
A1  - Deirdre Quillen
A1  - Ethan Holly
A1  - Mrinal Kalakrishnan
A1  - Vincent Vanhoucke
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 2 July, 2018
UR  - https://arxiv.org/abs/1806.10293
N2  - In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.
ER  -


TY  - Preprint
T1  - Deep $k$-Means: Jointly Clustering with $k$-Means and Learning Representations
A1  - Maziar Moradi Fard
A1  - Thibaut Thonet
A1  - Eric Gaussier
JO  - ArXiv e-prints
Y1  - 26 June, 2018
UR  - https://arxiv.org/abs/1806.10069
N2  - We study in this paper the problem of jointly clustering and learning representations. As several previous studies have shown, learning representations that are both faithful to the data to be clustered and adapted to the clustering algorithm can lead to better clustering performance, all the more so that the two tasks are performed jointly. We propose here such an approach for $k$-Means clustering based on a continuous reparametrization of the objective function that leads to a truly joint solution. The behavior of our approach is illustrated on various datasets showing its efficacy in learning representations for objects while clustering them.
ER  -


TY  - Preprint
T1  - Accuracy-based Curriculum Learning in Deep Reinforcement Learning
A1  - Pierre Fournier
A1  - Olivier Sigaud
A1  - Mohamed Chetouani
A1  - Pierre-Yves Oudeyer
JO  - ArXiv e-prints
Y1  - 21 September, 2018
UR  - https://arxiv.org/abs/1806.09614
N2  - In this paper, we investigate a new form of automated curriculum learning based on adaptive selection of accuracy requirements, called accuracy-based curriculum learning. Using a reinforcement learning agent based on the Deep Deterministic Policy Gradient algorithm and addressing the Reacher environment, we first show that an agent trained with various accuracy requirements sampled randomly learns more efficiently than when asked to be very accurate at all times. Then we show that adaptive selection of accuracy requirements, based on a local measure of competence progress, automatically generates a curriculum where difficulty progressively increases, resulting in a better learning efficiency than sampling randomly.
ER  -


TY  - Preprint
T1  - Pushing the boundaries of parallel Deep Learning -- A practical approach
A1  - Paolo Viviani
A1  - Maurizio Drocco
A1  - Marco Aldinucci
JO  - ArXiv e-prints
Y1  - 25 June, 2018
UR  - https://arxiv.org/abs/1806.09528
N2  - This work aims to assess the state of the art of data parallel deep neural network training, trying to identify potential research tracks to be exploited for performance improvement. Beside, it presents a design for a practical C++ library dedicated at implementing and unifying the current state of the art methodologies for parallel training in a performance-conscious framework, allowing the user to explore novel strategies without departing significantly from its usual work-flow.
ER  -


TY  - Preprint
T1  - SkinNet: A Deep Learning Framework for Skin Lesion Segmentation
A1  - Sulaiman Vesal
A1  - Nishant Ravikumar
A1  - Andreas Maier
JO  - ArXiv e-prints
Y1  - 25 June, 2018
UR  - https://arxiv.org/abs/1806.09522
N2  - There has been a steady increase in the incidence of skin cancer worldwide, with a high rate of mortality. Early detection and segmentation of skin lesions are crucial for timely diagnosis and treatment, necessary to improve the survival rate of patients. However, skin lesion segmentation is a challenging task due to the low contrast of lesions and their high similarity in terms of appearance, to healthy tissue. This underlines the need for an accurate and automatic approach for skin lesion segmentation. To tackle this issue, we propose a convolutional neural network (CNN) called SkinNet. The proposed CNN is a modified version of U-Net. We compared the performance of our approach with other state-of-the-art techniques, using the ISBI 2017 challenge dataset. Our approach outperformed the others in terms of the Dice coefficient, Jaccard index and sensitivity, evaluated on the held-out challenge test data set, across 5-fold cross validation experiments. SkinNet achieved an average value of 85.10, 76.67 and 93.0%, for the DC, JI, and SE, respectively.
ER  -


TY  - Preprint
T1  - A Hierarchical Deep Learning Natural Language Parser for Fashion
A1  - JosÃ© Marcelino
A1  - JoÃ£o Faria
A1  - LuÃ­s BaÃ­a
A1  - Ricardo Gamelas Sousa
JO  - ArXiv e-prints
Y1  - 25 June, 2018
UR  - https://arxiv.org/abs/1806.09511
N2  - This work presents a hierarchical deep learning natural language parser for fashion. Our proposal intends not only to recognize fashion-domain entities but also to expose syntactic and morphologic insights. We leverage the usage of an architecture of specialist models, each one for a different task (from parsing to entity recognition). Such architecture renders a hierarchical model able to capture the nuances of the fashion language. The natural language parser is able to deal with textual ambiguities which are left unresolved by our currently existing solution. Our empirical results establish a robust baseline, which justifies the use of hierarchical architectures of deep learning models while opening new research avenues to explore.
ER  -


TY  - Preprint
T1  - SSIMLayer: Towards Robust Deep Representation Learning via Nonlinear Structural Similarity
A1  - Ahmed Abobakr
A1  - Mohammed Hossny
A1  - Saeid Nahavandi
JO  - ArXiv e-prints
Y1  - 28 July, 2018
UR  - https://arxiv.org/abs/1806.09152
N2  - Deeper convolutional neural networks provide more capacity to approximate complex mapping functions. However, increasing network depth imposes difficulties on training and increases model complexity. This paper presents a new nonlinear computational layer of considerably high capacity to the deep convolutional neural network architectures. This layer performs a set of comprehensive convolution operations that mimics the overall function of the human visual system (HVS) via focusing on learning structural information in its input. The core of its computations is evaluating the components of the structural similarity metric (SSIM) in a setting that allows the kernels to learn to match structural information. The proposed SSIMLayer is inherently nonlinear and hence, it does not require subsequent nonlinear transformations. Experiments conducted on CIFAR-10 benchmark demonstrates that the SSIMLayer provides better convergence than the traditional convolutional layer, bypasses the need for nonlinear transformations and shows more robustness against noise perturbations and adversarial attacks.
ER  -


TY  - Preprint
T1  - Disease Classification in Metagenomics with 2D Embeddings and Deep Learning
A1  - Thanh Hai Nguyen
A1  - Edi Prifti
A1  - Yann Chevaleyre
A1  - Nataliya Sokolovska
A1  - Jean-Daniel Zucker
JO  - ArXiv e-prints
Y1  - 23 June, 2018
UR  - https://arxiv.org/abs/1806.09046
N2  - Deep learning (DL) techniques have shown unprecedented success when applied to images, waveforms, and text. Generally, when the sample size ($N$) is much bigger than the number of features ($d$), DL often outperforms other machine learning (ML) techniques, often through the use of Convolutional Neural Networks (CNNs). However, in many bioinformatics fields (including metagenomics), we encounter the opposite situation where $d$ is significantly greater than $N$. In these situations, applying DL techniques would lead to severe overfitting.
ER  -


TY  - Preprint
T1  - Stroke-based Character Recognition with Deep Reinforcement Learning
A1  - Zhewei Huang
A1  - Wen Heng
A1  - Yuanzheng Tao
A1  - Shuchang Zhou
JO  - ArXiv e-prints
Y1  - 23 June, 2018
UR  - https://arxiv.org/abs/1806.08990
N2  - The stroke sequence of characters is significant for the character recognition task. In this paper, we propose a stroke-based character recognition (SCR) method. We train a stroke inference module under deep reinforcement learning (DRL) framework. This module extracts the sequence of strokes from characters, which can be integrated with character recognizers to improve their robustness to noise. Our experiments show that the module can handle complicated noise and reconstruct the characters. Meanwhile, it can also help achieve great ability in defending adversarial attacks of character recognizers.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning: An Overview
A1  - Seyed Sajad Mousavi
A1  - Michael Schukat
A1  - Enda Howley
JO  - ArXiv e-prints
Y1  - 22 June, 2018
UR  - https://arxiv.org/abs/1806.08894
N2  - In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input. This chapter reviews the recent advances in deep reinforcement learning with a focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework.
ER  -


TY  - Preprint
T1  - The Foundations of Deep Learning with a Path Towards General Intelligence
A1  - Eray Ãzkural
JO  - ArXiv e-prints
Y1  - 22 June, 2018
UR  - https://arxiv.org/abs/1806.08874
N2  - Like any field of empirical science, AI may be approached axiomatically. We formulate requirements for a general-purpose, human-level AI system in terms of postulates. We review the methodology of deep learning, examining the explicit and tacit assumptions in deep learning research. Deep Learning methodology seeks to overcome limitations in traditional machine learning research as it combines facets of model richness, generality, and practical applicability. The methodology so far has produced outstanding results due to a productive synergy of function approximation, under plausible assumptions of irreducibility and the efficiency of back-propagation family of algorithms. We examine these winning traits of deep learning, and also observe the various known failure modes of deep learning. We conclude by giving recommendations on how to extend deep learning methodology to cover the postulates of general-purpose AI including modularity, and cognitive architecture. We also relate deep learning to advances in theoretical neuroscience research.
ER  -


TY  - Preprint
T1  - A deep learning framework for segmentation of retinal layers from OCT images
A1  - Karthik Gopinath
A1  - Samrudhdhi B Rangrej
A1  - Jayanthi Sivaswamy
JO  - ArXiv e-prints
Y1  - 22 June, 2018
UR  - https://arxiv.org/abs/1806.08859
N2  - Segmentation of retinal layers from Optical Coherence Tomography (OCT) volumes is a fundamental problem for any computer aided diagnostic algorithm development. This requires preprocessing steps such as denoising, region of interest extraction, flattening and edge detection all of which involve separate parameter tuning. In this paper, we explore deep learning techniques to automate all these steps and handle the presence/absence of pathologies. A model is proposed consisting of a combination of Convolutional Neural Network (CNN) and Long Short Term Memory (LSTM). The CNN is used to extract layers of interest image and extract the edges, while the LSTM is used to trace the layer boundary. This model is trained on a mixture of normal and AMD cases using minimal data. Validation results on three public datasets show that the pixel-wise mean absolute error obtained with our system is 1.30 plus or minus 0.48 which is lower than the inter-marker error of 1.79 plus or minus 0.76. Our model&#39;s performance is also on par with the existing methods.
ER  -


TY  - Preprint
T1  - Combination of Domain Knowledge and Deep Learning for Sentiment Analysis
A1  - Khuong Vo
A1  - Dang Pham
A1  - Mao Nguyen
A1  - Trung Mai
A1  - Tho Quan
JO  - ArXiv e-prints
Y1  - 26 June, 2018
UR  - https://arxiv.org/abs/1806.08760
N2  - The emerging technique of deep learning has been widely applied in many different areas. However, when adopted in a certain specific domain, this technique should be combined with domain knowledge to improve efficiency and accuracy. In particular, when analyzing the applications of deep learning in sentiment analysis, we found that the current approaches are suffering from the following drawbacks: (i) the existing works have not paid much attention to the importance of different types of sentiment terms, which is an important concept in this area; and (ii) the loss function currently employed does not well reflect the degree of error of sentiment misclassification. To overcome such problem, we propose to combine domain knowledge with deep learning. Our proposal includes using sentiment scores, learnt by regression, to augment training data; and introducing penalty matrix for enhancing the loss function of cross entropy. When experimented, we achieved a significant improvement in classification results.
ER  -


TY  - Preprint
T1  - Towards safe deep learning: accurately quantifying biomarker uncertainty in neural network predictions
A1  - Zach Eaton-Rosen
A1  - Felix Bragman
A1  - Sotirios Bisdas
A1  - Sebastien Ourselin
A1  - M. Jorge Cardoso
JO  - ArXiv e-prints
Y1  - 22 June, 2018
UR  - https://arxiv.org/abs/1806.08640
N2  - Automated medical image segmentation, specifically using deep learning, has shown outstanding performance in semantic segmentation tasks. However, these methods rarely quantify their uncertainty, which may lead to errors in downstream analysis. In this work we propose to use Bayesian neural networks to quantify uncertainty within the domain of semantic segmentation. We also propose a method to convert voxel-wise segmentation uncertainty into volumetric uncertainty, and calibrate the accuracy and reliability of confidence intervals of derived measurements. When applied to a tumour volume estimation application, we demonstrate that by using such modelling of uncertainty, deep learning systems can be made to report volume estimates with well-calibrated error-bars, making them safer for clinical use. We also show that the uncertainty estimates extrapolate to unseen data, and that the confidence intervals are robust in the presence of artificial noise. This could be used to provide a form of quality control and quality assurance, and may permit further adoption of deep learning tools in the clinic.
ER  -


TY  - Preprint
T1  - Deep Drone Racing: Learning Agile Flight in Dynamic Environments
A1  - Elia Kaufmann
A1  - Antonio Loquercio
A1  - Rene Ranftl
A1  - Alexey Dosovitskiy
A1  - Vladlen Koltun
A1  - Davide Scaramuzza
JO  - ArXiv e-prints
Y1  - 9 October, 2018
UR  - https://arxiv.org/abs/1806.08548
N2  - Autonomous agile flight brings up fundamental challenges in robotics, such as coping with unreliable state estimation, reacting optimally to dynamically changing environments, and coupling perception and action in real time under severe resource constraints. In this paper, we consider these challenges in the context of autonomous, vision-based drone racing in dynamic environments. Our approach combines a convolutional neural network (CNN) with a state-of-the-art path-planning and control system. The CNN directly maps raw images into a robust representation in the form of a waypoint and desired speed. This information is then used by the planner to generate a short, minimum-jerk trajectory segment and corresponding motor commands to reach the desired goal. We demonstrate our method in autonomous agile flight scenarios, in which a vision-based quadrotor traverses drone-racing tracks with possibly moving gates. Our method does not require any explicit map of the environment and runs fully onboard. We extensively test the precision and robustness of the approach in simulation and in the physical world. We also evaluate our method against state-of-the-art navigation approaches and professional human drone pilots.
ER  -


TY  - Preprint
T1  - Shape-from-Mask: A Deep Learning Based Human Body Shape Reconstruction from Binary Mask Images
A1  - Zhongping Ji
A1  - Xiao Qi
A1  - Yigang Wang
A1  - Gang Xu
A1  - Peng Du
A1  - Qing Wu
JO  - ArXiv e-prints
Y1  - 22 June, 2018
UR  - https://arxiv.org/abs/1806.08485
N2  - 3D content creation is referred to as one of the most fundamental tasks of computer graphics. And many 3D modeling algorithms from 2D images or curves have been developed over the past several decades. Designers are allowed to align some conceptual images or sketch some suggestive curves, from front, side, and top views, and then use them as references in constructing a 3D model automatically or manually. However, to the best of our knowledge, no studies have investigated on 3D human body reconstruction in a similar manner. In this paper, we propose a deep learning based reconstruction of 3D human body shape from 2D orthographic views. A novel CNN-based regression network, with two branches corresponding to frontal and lateral views respectively, is designed for estimating 3D human body shape from 2D mask images. We train our networks separately to decouple the feature descriptors which encode the body parameters from different views, and fuse them to estimate an accurate human body shape. In addition, to overcome the shortage of training data required for this purpose, we propose some significantly data augmentation schemes for 3D human body shapes, which can be used to promote further research on this topic. Extensive experimen- tal results demonstrate that visually realistic and accurate reconstructions can be achieved effectively using our algorithm. Requiring only binary mask images, our method can help users create their own digital avatars quickly, and also make it easy to create digital human body for 3D game, virtual reality, online fashion shopping.
ER  -


TY  - Preprint
T1  - Can Deep Learning Relax Endomicroscopy Hardware Miniaturization Requirements?
A1  - Saeed Izadi
A1  - Kathleen P. Moriarty
A1  - Ghassan Hamarneh
JO  - ArXiv e-prints
Y1  - 21 June, 2018
UR  - https://arxiv.org/abs/1806.08338
N2  - Confocal laser endomicroscopy (CLE) is a novel imaging modality that provides in vivo histological cross-sections of examined tissue. Recently, attempts have been made to develop miniaturized in vivo imaging devices, specifically confocal laser microscopes, for both clinical and research applications. However, current implementations of miniature CLE components, such as confocal lenses, compromise image resolution, signal-to-noise ratio, or both, which negatively impacts the utility of in vivo imaging. In this work, we demonstrate that software-based techniques can be used to recover lost information due to endomicroscopy hardware miniaturization and reconstruct images of higher resolution. Particularly, a densely connected convolutional neural network is used to reconstruct a high-resolution CLE image from a low-resolution input. In the proposed network, each layer is directly connected to all subsequent layers, which results in an effective combination of low-level and high-level features and efficient information flow throughout the network. To train and evaluate our network, we use a dataset of 181 high-resolution CLE images. Both quantitative and qualitative results indicate superiority of the proposed network compared to traditional interpolation techniques and competing learning-based methods. This work demonstrates that software-based super-resolution is a viable approach to compensate for loss of resolution due to endoscopic hardware miniaturization.
ER  -


TY  - Preprint
T1  - How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement Learning Experiments
A1  - CÃ©dric Colas
A1  - Olivier Sigaud
A1  - Pierre-Yves Oudeyer
JO  - ArXiv e-prints
Y1  - 5 July, 2018
UR  - https://arxiv.org/abs/1806.08295
N2  - Consistently checking the statistical significance of experimental results is one of the mandatory methodological steps to address the so-called &#34;reproducibility crisis&#34; in deep reinforcement learning. In this tutorial paper, we explain how the number of random seeds relates to the probabilities of statistical errors. For both the t-test and the bootstrap confidence interval test, we recall theoretical guidelines to determine the number of random seeds one should use to provide a statistically significant comparison of the performance of two algorithms. Finally, we discuss the influence of deviations from the assumptions usually made by statistical tests. We show that they can lead to inaccurate evaluations of statistical errors and provide guidelines to counter these negative effects. We make our code available to perform the tests.
ER  -


TY  - Preprint
T1  - Layouts from Panoramic Images with Geometry and Deep Learning
A1  - Clara Fernandez-Labrador
A1  - Alejandro Perez-Yus
A1  - Gonzalo Lopez-Nicolas
A1  - Jose J. Guerrero
JO  - ArXiv e-prints
Y1  - 21 June, 2018
UR  - https://arxiv.org/abs/1806.08294
N2  - In this paper, we propose a novel procedure for 3D layout recovery of indoor scenes from single 360 degrees panoramic images. With such images, all scene is seen at once, allowing to recover closed geometries. Our method combines strategically the accuracy provided by geometric reasoning (lines and vanishing points) with the higher level of data abstraction and pattern recognition achieved by deep learning techniques (edge and normal maps). Thus, we extract structural corners from which we generate layout hypotheses of the room assuming Manhattan world. The best layout model is selected, achieving good performance on both simple rooms (box-type) and complex shaped rooms (with more than four walls). Experiments of the proposed approach are conducted within two public datasets, SUN360 and Stanford (2D-3D-S) demonstrating the advantages of estimating layouts by combining geometry and deep learning and the effectiveness of our proposal with respect to the state of the art.
ER  -


TY  - Preprint
T1  - A New Approach for Resource Scheduling with Deep Reinforcement Learning
A1  - Yufei Ye
A1  - Xiaoqin Ren
A1  - Jin Wang
A1  - Lingxiao Xu
A1  - Wenxia Guo
A1  - Wenqiang Huang
A1  - Wenhong Tian
JO  - ArXiv e-prints
Y1  - 21 June, 2018
UR  - https://arxiv.org/abs/1806.08122
N2  - With the rapid development of deep learning, deep reinforcement learning (DRL) began to appear in the field of resource scheduling in recent years. Based on the previous research on DRL in the literature, we introduce online resource scheduling algorithm DeepRM2 and the offline resource scheduling algorithm DeepRM_Off. Compared with the state-of-the-art DRL algorithm DeepRM and heuristic algorithms, our proposed algorithms have faster convergence speed and better scheduling efficiency with regarding to average slowdown time, job completion time and rewards.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Surgical Gesture Segmentation and Classification
A1  - Daochang Liu
A1  - Tingting Jiang
JO  - ArXiv e-prints
Y1  - 21 June, 2018
UR  - https://arxiv.org/abs/1806.08089
N2  - Recognition of surgical gesture is crucial for surgical skill assessment and efficient surgery training. Prior works on this task are based on either variant graphical models such as HMMs and CRFs, or deep learning models such as Recurrent Neural Networks and Temporal Convolutional Networks. Most of the current approaches usually suffer from over-segmentation and therefore low segment-level edit scores. In contrast, we present an essentially different methodology by modeling the task as a sequential decision-making process. An intelligent agent is trained using reinforcement learning with hierarchical features from a deep model. Temporal consistency is integrated into our action design and reward mechanism to reduce over-segmentation errors. Experiments on JIGSAWS dataset demonstrate that the proposed method performs better than state-of-the-art methods in terms of the edit score and on par in frame-wise accuracy. Our code will be released later.
ER  -


TY  - Preprint
T1  - Como funciona o Deep Learning
A1  - Moacir Antonelli Ponti
A1  - Gabriel B. Paranhos da Costa
JO  - ArXiv e-prints
Y1  - 20 June, 2018
UR  - https://arxiv.org/abs/1806.07908
N2  - Deep Learning methods are currently the state-of-the-art in many problems which can be tackled via machine learning, in particular classification problems. However there is still lack of understanding on how those methods work, why they work and what are the limitations involved in using them. In this chapter we will describe in detail the transition from shallow to deep networks, include examples of code on how to implement them, as well as the main issues one faces when training a deep network. Afterwards, we introduce some theoretical background behind the use of deep models, and discuss their limitations.
ER  -


TY  - Preprint
T1  - Edge Intelligence: On-Demand Deep Learning Model Co-Inference with Device-Edge Synergy
A1  - En Li
A1  - Zhi Zhou
A1  - Xu Chen
JO  - ArXiv e-prints
Y1  - 13 September, 2018
UR  - https://arxiv.org/abs/1806.07840
N2  - As the backbone technology of machine learning, deep neural networks (DNNs) have have quickly ascended to the spotlight. Running DNNs on resource-constrained mobile devices is, however, by no means trivial, since it incurs high performance and energy overhead. While offloading DNNs to the cloud for execution suffers unpredictable performance, due to the uncontrolled long wide-area network latency. To address these challenges, in this paper, we propose Edgent, a collaborative and on-demand DNN co-inference framework with device-edge synergy. Edgent pursues two design knobs: (1) DNN partitioning that adaptively partitions DNN computation between device and edge, in order to leverage hybrid computation resources in proximity for real-time DNN inference. (2) DNN right-sizing that accelerates DNN inference through early-exit at a proper intermediate DNN layer to further reduce the computation latency. The prototype implementation and extensive evaluations based on Raspberry Pi demonstrate Edgent&#39;s effectiveness in enabling on-demand low-latency edge intelligence.
ER  -


TY  - Preprint
T1  - Combinatorial Testing for Deep Learning Systems
A1  - Lei Ma
A1  - Fuyuan Zhang
A1  - Minhui Xue
A1  - Bo Li
A1  - Yang Liu
A1  - Jianjun Zhao
A1  - Yadong Wang
JO  - ArXiv e-prints
Y1  - 20 June, 2018
UR  - https://arxiv.org/abs/1806.07723
N2  - Deep learning (DL) has achieved remarkable progress over the past decade and been widely applied to many safety-critical applications. However, the robustness of DL systems recently receives great concerns, such as adversarial examples against computer vision systems, which could potentially result in severe consequences. Adopting testing techniques could help to evaluate the robustness of a DL system and therefore detect vulnerabilities at an early stage. The main challenge of testing such systems is that its runtime state space is too large: if we view each neuron as a runtime state for DL, then a DL system often contains massive states, rendering testing each state almost impossible. For traditional software, combinatorial testing (CT) is an effective testing technique to reduce the testing space while obtaining relatively high defect detection abilities. In this paper, we perform an exploratory study of CT on DL systems. We adapt the concept in CT and propose a set of coverage criteria for DL systems, as well as a CT coverage guided test generation technique. Our evaluation demonstrates that CT provides a promising avenue for testing DL systems. We further pose several open questions and interesting directions for combinatorial testing of DL systems.
ER  -


TY  - Preprint
T1  - Dynamic Risk Assessment for Vehicles of Higher Automation Levels by Deep Learning
A1  - Patrik Feth
A1  - Mohammed Naveed Akram
A1  - RenÃ© Schuster
A1  - Oliver WasenmÃ¼ller
JO  - ArXiv e-prints
Y1  - 20 June, 2018
UR  - https://arxiv.org/abs/1806.07635
N2  - Vehicles of higher automation levels require the creation of situation awareness. One important aspect of this situation awareness is an understanding of the current risk of a driving situation. In this work, we present a novel approach for the dynamic risk assessment of driving situations based on images of a front stereo camera using deep learning. To this end, we trained a deep neural network with recorded monocular images, disparity maps and a risk metric for diverse traffic scenes. Our approach can be used to create the aforementioned situation awareness of vehicles of higher automation levels and can serve as a heterogeneous channel to systems based on radar or lidar sensors that are used traditionally for the calculation of risk metrics.
ER  -


TY  - Preprint
T1  - Improving Online Multiple Object tracking with Deep Metric Learning
A1  - Michael Thoreau
A1  - Navinda Kottege
JO  - ArXiv e-prints
Y1  - 20 June, 2018
UR  - https://arxiv.org/abs/1806.07592
N2  - Tracking by detection is a common approach to solving the Multiple Object Tracking problem. In this paper we show how deep metric learning can be used to improve three aspects of tracking by detection. We train a convolutional neural network to learn an embedding function in a Siamese configuration on a large person re-identification dataset offline. It is then used to improve the online performance of tracking while retaining a high frame rate. We use this learned appearance metric to robustly build estimates of pedestrian&#39;s trajectories in the MOT16 dataset. In breaking with the tracking by detection model, we use our appearance metric to propose detections using the predicted state of a tracklet as a prior in the case where the detector fails. This method achieves competitive results in evaluation, especially among online, real-time approaches. We present an ablative study showing the impact of each of the three uses of our deep appearance metric.
ER  -


TY  - Preprint
T1  - A Simple Fusion of Deep and Shallow Learning for Acoustic Scene Classification
A1  - Eduardo Fonseca
A1  - Rong Gong
A1  - Xavier Serra
JO  - ArXiv e-prints
Y1  - 27 June, 2018
UR  - https://arxiv.org/abs/1806.07506
N2  - In the past, Acoustic Scene Classification systems have been based on hand crafting audio features that are input to a classifier. Nowadays, the common trend is to adopt data driven techniques, e.g., deep learning, where audio representations are learned from data. In this paper, we propose a system that consists of a simple fusion of two methods of the aforementioned types: a deep learning approach where log-scaled mel-spectrograms are input to a convolutional neural network, and a feature engineering approach, where a collection of hand-crafted features is input to a gradient boosting machine. We first show that both methods provide complementary information to some extent. Then, we use a simple late fusion strategy to combine both methods. We report classification accuracy of each method individually and the combined system on the TUT Acoustic Scenes 2017 dataset. The proposed fused system outperforms each of the individual methods and attains a classification accuracy of 72.8% on the evaluation set, improving the baseline system by 11.8%.
ER  -


TY  - Preprint
T1  - On the Learning of Deep Local Features for Robust Face Spoofing Detection
A1  - Gustavo Botelho de Souza
A1  - JoÃ£o Paulo Papa
A1  - Aparecido Nilceu Marana
JO  - ArXiv e-prints
Y1  - 19 June, 2018
UR  - https://arxiv.org/abs/1806.07492
N2  - Biometrics emerged as a robust solution for security systems. However, given the widespread of biometric applications, criminals are developing techniques to circumvent them by simulating physical or behavioral traits of legal users (spoofing attacks). Despite face being a promising characteristic due to its universality, acceptability and presence of cameras almost everywhere, face recognition systems are extremely vulnerable to such frauds since they can be easily fooled with common printed facial photographs. State-of-the-art approaches, based on Convolutional Neural Networks (CNNs), present good results in face spoofing detection. However, these methods do not exploit the importance of learning deep local features from each facial region, even though it is known from face recognition that different face regions have much different visual aspects, that can also be exploited for face spoofing detection. In this work we propose a novel CNN architecture trained in two steps for such task. Initially, each part of the neural network learns features from a given facial region. After, the whole model is fine-tuned on the whole facial images. Results show that such pretraining step allows the CNN to learn different local spoofing cues, improving the performance and convergence speed of the final model, outperforming the state-of-the-art approaches.
ER  -


TY  - Preprint
T1  - Deep Sequence Learning with Auxiliary Information for Traffic Prediction
A1  - Binbing Liao
A1  - Jingqing Zhang
A1  - Chao Wu
A1  - Douglas McIlwraith
A1  - Tong Chen
A1  - Shengwen Yang
A1  - Yike Guo
A1  - Fei Wu
JO  - ArXiv e-prints
Y1  - 13 June, 2018
UR  - https://arxiv.org/abs/1806.07380
N2  - Predicting traffic conditions from online route queries is a challenging task as there are many complicated interactions over the roads and crowds involved. In this paper, we intend to improve traffic prediction by appropriate integration of three kinds of implicit but essential factors encoded in auxiliary information. We do this within an encoder-decoder sequence learning framework that integrates the following data: 1) offline geographical and social attributes. For example, the geographical structure of roads or public social events such as national celebrations; 2) road intersection information. In general, traffic congestion occurs at major junctions; 3) online crowd queries. For example, when many online queries issued for the same destination due to a public performance, the traffic around the destination will potentially become heavier at this location after a while. Qualitative and quantitative experiments on a real-world dataset from Baidu have demonstrated the effectiveness of our framework.
ER  -


TY  - Preprint
T1  - DeepTerramechanics: Terrain Classification and Slip Estimation for Ground Robots via Deep Learning
A1  - Ramon Gonzalez
A1  - Karl Iagnemma
JO  - ArXiv e-prints
Y1  - 12 June, 2018
UR  - https://arxiv.org/abs/1806.07379
N2  - Terramechanics plays a critical role in the areas of ground vehicles and ground mobile robots since understanding and estimating the variables influencing the vehicle-terrain interaction may mean the success or the failure of an entire mission. This research applies state-of-the-art algorithms in deep learning to two key problems: estimating wheel slip and classifying the terrain being traversed by a ground robot. Three data sets collected by ground robotic platforms (MIT single-wheel testbed, MSL Curiosity rover, and tracked robot Fitorobot) are employed in order to compare the performance of traditional machine learning methods (i.e. Support Vector Machine (SVM) and Multi-layer Perceptron (MLP)) against Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs). This work also shows the impact that certain tuning parameters and the network architecture (MLP, DNN and CNN) play on the performance of those methods. This paper also contributes a deep discussion with the lessons learned in the implementation of DNNs and CNNs and how these methods can be extended to solve other problems.
ER  -


TY  - Preprint
T1  - Magnetic Resonance Spectroscopy Quantification using Deep Learning
A1  - Nima Hatami
A1  - MichaÃ«l Sdika
A1  - HÃ©lÃ¨ne Ratiney
JO  - ArXiv e-prints
Y1  - 19 June, 2018
UR  - https://arxiv.org/abs/1806.07237
N2  - Magnetic resonance spectroscopy (MRS) is an important technique in biomedical research and it has the unique capability to give a non-invasive access to the biochemical content (metabolites) of scanned organs. In the literature, the quantification (the extraction of the potential biomarkers from the MRS signals) involves the resolution of an inverse problem based on a parametric model of the metabolite signal. However, poor signal-to-noise ratio (SNR), presence of the macromolecule signal or high correlation between metabolite spectral patterns can cause high uncertainties for most of the metabolites, which is one of the main reasons that prevents use of MRS in clinical routine. In this paper, quantification of metabolites in MR Spectroscopic imaging using deep learning is proposed. A regression framework based on the Convolutional Neural Networks (CNN) is introduced for an accurate estimation of spectral parameters. The proposed model learns the spectral features from a large-scale simulated data set with different variations of human brain spectra and SNRs. Experimental results demonstrate the accuracy of the proposed method, compared to state of the art standard quantification method (QUEST), on concentration of 20 metabolites and the macromolecule.
ER  -


TY  - Preprint
T1  - ASIC Implementation of Time-Domain Digital Backpropagation with Deep-Learned Chromatic Dispersion Filters
A1  - Christoffer Fougstedt
A1  - Christian HÃ¤ger
A1  - Lars Svensson
A1  - Henry D. Pfister
A1  - Per Larsson-Edefors
JO  - ArXiv e-prints
Y1  - 19 June, 2018
UR  - https://arxiv.org/abs/1806.07223
N2  - We consider time-domain digital backpropagation with chromatic dispersion filters jointly optimized and quantized using machine-learning techniques. Compared to the baseline implementations, we show improved BER performance and &gt;40% power dissipation reductions in 28-nm CMOS.
ER  -


TY  - Preprint
T1  - Effect of Hyper-Parameter Optimization on the Deep Learning Model Proposed for Distributed Attack Detection in Internet of Things Environment
A1  - Md Mohaimenuzzaman
A1  - Zahraa Said Abdallah
A1  - Joarder Kamruzzaman
A1  - Bala Srinivasan
JO  - ArXiv e-prints
Y1  - 19 June, 2018
UR  - https://arxiv.org/abs/1806.07057
N2  - This paper studies the effect of various hyper-parameters and their selection for the best performance of the deep learning model proposed in [1] for distributed attack detection in the Internet of Things (IoT). The findings show that there are three hyper-parameters that have more influence on the best performance achieved by the model. As a consequence, this study shows that the model&#39;s accuracy as reported in the paper is not achievable, based on the best selections of parameters, which is also supported by another recent publication [2].
ER  -


TY  - Preprint
T1  - Pressure Predictions of Turbine Blades with Deep Learning
A1  - Cheng&#39;an Bai
A1  - Chao Zhou
JO  - ArXiv e-prints
Y1  - 11 June, 2018
UR  - https://arxiv.org/abs/1806.06940
N2  - Deep learning has been used in many areas, such as feature detections in images and the game of go. This paper presents a study that attempts to use the deep learning method to predict turbomachinery performance. Three different deep neural networks are built and trained to predict the pressure distributions of turbine airfoils. The performance of a library of turbine airfoils were firstly predicted using methods based on Euler equations, which were then used to train and validate the deep learning neural networks. The results show that network with four layers of convolutional neural network and two layers of fully connected neural network provides the best predictions. For the best neural network architecture, the pressure prediction on more than 99% locations are better than 3% and 90% locations are better than 1%.
ER  -


TY  - Preprint
T1  - Deep Learning based Estimation of Weaving Target Maneuvers
A1  - Vitaly Shalumov
A1  - Itzik Klein
JO  - ArXiv e-prints
Y1  - 13 June, 2018
UR  - https://arxiv.org/abs/1806.06913
N2  - In target tracking, the estimation of an unknown weaving target frequency is crucial for improving the miss distance. The estimation process is commonly carried out in a Kalman framework. The objective of this paper is to examine the potential of using neural networks in target tracking applications. To that end, we propose estimating the weaving frequency using deep neural networks, instead of classical Kalman framework based estimation. Particularly, we focus on the case where a set of possible constant target frequencies is known. Several neural network architectures, requiring low computational resources were designed to estimate the unknown frequency out of the known set of frequencies. The proposed approach performance is compared with the multiple model adaptive estimation algorithm. Simulation results show that in the examined scenarios, deep neural network outperforms multiple model adaptive estimation in terms of accuracy and the amount of required measurements to convergence.
ER  -


TY  - Preprint
T1  - Towards an efficient deep learning model for musical onset detection
A1  - Rong Gong
A1  - Xavier Serra
JO  - ArXiv e-prints
Y1  - 19 June, 2018
UR  - https://arxiv.org/abs/1806.06773
N2  - In this paper, we propose an efficient and reproducible deep learning model for musical onset detection (MOD). We first review the state-of-the-art deep learning models for MOD, and identify their shortcomings and challenges: (i) the lack of hyper-parameter tuning details, (ii) the non-availability of code for training models on other datasets, and (iii) ignoring the network capability when comparing different architectures. Taking the above issues into account, we experiment with seven deep learning architectures. The most efficient one achieves equivalent performance to our implementation of the state-of-the-art architecture. However, it has only 28.3% of the total number of trainable parameters compared to the state-of-the-art. Our experiments are conducted using two different datasets: one mainly consists of instrumental music excerpts, and another developed by ourselves includes only solo singing voice excerpts. Further, inter-dataset transfer learning experiments are conducted. The results show that the model pre-trained on one dataset fails to detect onsets on another dataset, which denotes the importance of providing the implementation code to enable re-training the model for a different dataset. Datasets, code and a Jupyter notebook running on Google Colab are publicly available to make this research understandable and easy to reproduce.
ER  -


TY  - Preprint
T1  - Detecting Zero-day Controller Hijacking Attacks on the Power-Grid with Enhanced Deep Learning
A1  - Zecheng He
A1  - Aswin Raghavan
A1  - Sek Chai
A1  - Ruby Lee
JO  - ArXiv e-prints
Y1  - 20 September, 2018
UR  - https://arxiv.org/abs/1806.06496
N2  - Attacks against the control processor of a power-grid system, especially zero-day attacks, can be catastrophic. Earlier detection of the attacks can prevent further damage. However, detecting zero-day attacks can be challenging because they have no known code and have unknown behavior. In order to address the zero-day attack problem, we propose a data-driven defense by training a temporal deep learning model, using only normal data from legitimate processes that run daily in these power-grid systems, to model the normal behavior of the power-grid controller. Then, we can quickly find malicious codes running on the processor, by estimating deviations from the normal behavior with a statistical test. Experimental results on a real power-grid controller show that we can detect anomalous behavior with over 99.9% accuracy and nearly zero false positives.
ER  -


TY  - Preprint
T1  - Automated Image Data Preprocessing with Deep Reinforcement Learning
A1  - Tran Ngoc Minh
A1  - Mathieu Sinn
A1  - Hoang Thanh Lam
A1  - Martin Wistuba
JO  - ArXiv e-prints
Y1  - 15 June, 2018
UR  - https://arxiv.org/abs/1806.05886
N2  - Data preparation, i.e. the process of transforming raw data into a format that can be used for training effective machine learning models, is a tedious and time-consuming task. For image data, preprocessing typically involves a sequence of basic transformations such as cropping, filtering, rotating or flipping images. Currently, data scientists decide manually based on their experience which transformations to apply in which particular order to a given image data set. Besides constituting a bottleneck in real-world data science projects, manual image data preprocessing may yield suboptimal results as data scientists need to rely on intuition or trial-and-error approaches when exploring the space of possible image transformations and thus might not be able to discover the most effective ones. To mitigate the inefficiency and potential ineffectiveness of manual data preprocessing, this paper proposes a deep reinforcement learning framework to automatically discover the optimal data preprocessing steps for training an image classifier. The framework takes as input sets of labeled images and predefined preprocessing transformations. It jointly learns the classifier and the optimal preprocessing transformations for individual images. Experimental results show that the proposed approach not only improves the accuracy of image classifiers, but also makes them substantially more robust to noisy inputs at test time.
ER  -


TY  - Preprint
T1  - Three dimensional Deep Learning approach for remote sensing image classification
A1  - Amina Ben Hamida
A1  - A Benoit
A1  - Patrick Lambert
A1  - Chokri Ben Amar
JO  - ArXiv e-prints
Y1  - 15 June, 2018
UR  - https://arxiv.org/abs/1806.05824
N2  - Recently, a variety of approaches has been enriching the field of Remote Sensing (RS) image processing and analysis. Unfortunately, existing methods remain limited faced to the rich spatio-spectral content of today&#39;s large datasets. It would seem intriguing to resort to Deep Learning (DL) based approaches at this stage with regards to their ability to offer accurate semantic interpretation of the data. However, the specificity introduced by the coexistence of spectral and spatial content in the RS datasets widens the scope of  the challenges presented to adapt DL methods to these contexts. Therefore, the aim of this paper is firstly to explore the performance of DL architectures for the RS hyperspectral dataset classification and secondly to introduce a new three-dimensional DL approach that enables a joint spectral and spatial information process. A set of three-dimensional schemes is proposed and evaluated. Experimental results based on well knownhyperspectral datasets demonstrate that the proposed method is able to achieve a better classification rate than state of the art methods with lower computational costs.
ER  -


TY  - Preprint
T1  - Deep Learning with Convolutional Neural Network for Objective Skill Evaluation in Robot-assisted Surgery
A1  - Ziheng Wang
A1  - Ann Majewicz Fey
JO  - ArXiv e-prints
Y1  - 14 June, 2018
UR  - https://arxiv.org/abs/1806.05796
N2  - With the advent of robot-assisted surgery, the role of data-driven approaches to integrate statistics and machine learning is growing rapidly with prominent interests in objective surgical skill assessment. However, most existing work requires translating robot motion kinematics into intermediate features or gesture segments that are expensive to extract, lack efficiency, and require significant domain-specific knowledge. We propose an analytical deep learning framework for skill assessment in surgical training. A deep convolutional neural network is implemented to map multivariate time series data of the motion kinematics to individual skill levels. We perform experiments on the public minimally invasive surgical robotic dataset, JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS). Our proposed learning model achieved a competitive accuracy of 94.1%, 90.3%, and 86.8%, in the standard training tasks: Suturing, Needle-passing, and Knot-tying, respectively. Without the need of engineered features or carefully-tuned gesture segmentation, our model can successfully decode skill information from raw motion profiles via end-to-end learning. Meanwhile, the proposed model is able to reliably interpret skills within 1-3 second window, without needing an observation of entire training trial. This study highlights the potentials of deep architectures for an proficient online skill assessment in modern surgical training.
ER  -


TY  - Preprint
T1  - Deep Learning Approximation: Zero-Shot Neural Network Speedup
A1  - Michele Pratusevich
JO  - ArXiv e-prints
Y1  - 14 June, 2018
UR  - https://arxiv.org/abs/1806.05779
N2  - Neural networks offer high-accuracy solutions to a range of problems, but are costly to run in production systems because of computational and memory requirements during a forward pass. Given a trained network, we propose a techique called Deep Learning Approximation to build a faster network in a tiny fraction of the time required for training by only manipulating the network structure and coefficients without requiring re-training or access to the training data. Speedup is achieved by by applying a sequential series of independent optimizations that reduce the floating-point operations (FLOPs) required to perform a forward pass. First, lossless optimizations are applied, followed by lossy approximations using singular value decomposition (SVD) and low-rank matrix decomposition. The optimal approximation is chosen by weighing the relative accuracy loss and FLOP reduction according to a single parameter specified by the user. On PASCAL VOC 2007 with the YOLO network, we show an end-to-end 2x speedup in a network forward pass with a 5% drop in mAP that can be re-gained by finetuning.
ER  -


TY  - Preprint
T1  - Interactive Classification for Deep Learning Interpretation
A1  - Angel Cabrera
A1  - Fred Hohman
A1  - Jason Lin
A1  - Duen Horng Chau
JO  - ArXiv e-prints
Y1  - 14 June, 2018
UR  - https://arxiv.org/abs/1806.05660
N2  - We present an interactive system enabling users to manipulate images to explore the robustness and sensitivity of deep learning image classifiers. Using modern web technologies to run in-browser inference, users can remove image features using inpainting algorithms and obtain new classifications in real time, which allows them to ask a variety of &#34;what if&#34; questions by experimentally modifying images and seeing how the model reacts. Our system allows users to compare and contrast what image regions humans and machine learning models use for classification, revealing a wide range of surprising results ranging from spectacular failures (e.g., a &#34;water bottle&#34; image becomes a &#34;concert&#34; when removing a person) to impressive resilience (e.g., a &#34;baseball player&#34; image remains correctly classified even without a glove or base). We demonstrate our system at The 2018 Conference on Computer Vision and Pattern Recognition (CVPR) for the audience to try it live. Our system is open-sourced at https://github.com/poloclub/interactive-classification. A video demo is available at https://youtu.be/llub5GcOF6w.
ER  -


TY  - Preprint
T1  - Deep Multi-Output Forecasting: Learning to Accurately Predict Blood Glucose Trajectories
A1  - Ian Fox
A1  - Lynn Ang
A1  - Mamta Jaiswal
A1  - Rodica Pop-Busui
A1  - Jenna Wiens
JO  - ArXiv e-prints
Y1  - 14 June, 2018
UR  - https://arxiv.org/abs/1806.05357
N2  - In many forecasting applications, it is valuable to predict not only the value of a signal at a certain time point in the future, but also the values leading up to that point. This is especially true in clinical applications, where the future state of the patient can be less important than the patient&#39;s overall trajectory. This requires multi-step forecasting, a forecasting variant where one aims to predict multiple values in the future simultaneously. Standard methods to accomplish this can propagate error from prediction to prediction, reducing quality over the long term. In light of these challenges, we propose multi-output deep architectures for multi-step forecasting in which we explicitly model the distribution of future values of the signal over a prediction horizon. We apply these techniques to the challenging and clinically relevant task of blood glucose forecasting. Through a series of experiments on a real-world dataset consisting of 550K blood glucose measurements, we demonstrate the effectiveness of our proposed approaches in capturing the underlying signal dynamics. Compared to existing shallow and deep methods, we find that our proposed approaches improve performance individually and capture complementary information, leading to a large improvement over the baseline when combined (4.87 vs. 5.31 absolute percentage error (APE)). Overall, the results suggest the efficacy of our proposed approach in predicting blood glucose level and multi-step forecasting more generally.
ER  -


TY  - Preprint
T1  - Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning
A1  - Kunkun Pang
A1  - Mingzhi Dong
A1  - Yang Wu
A1  - Timothy Hospedales
JO  - ArXiv e-prints
Y1  - 12 June, 2018
UR  - https://arxiv.org/abs/1806.04798
N2  - Active learning (AL) aims to enable training high performance classifiers with low annotation cost by predicting which subset of unlabelled instances would be most beneficial to label. The importance of AL has motivated extensive research, proposing a wide variety of manually designed AL algorithms with diverse theoretical and intuitive motivations. In contrast to this body of research, we propose to treat active learning algorithm design as a meta-learning problem and learn the best criterion from data. We model an active learning algorithm as a deep neural network that inputs the base learner state and the unlabelled point set and predicts the best point to annotate next. Training this active query policy network with reinforcement learning, produces the best non-myopic policy for a given dataset. The key challenge in achieving a general solution to AL then becomes that of learner generalisation, particularly across heterogeneous datasets. We propose a multi-task dataset-embedding approach that allows dataset-agnostic active learners to be trained. Our evaluation shows that AL algorithms trained in this way can directly generalise across diverse problems.
ER  -


TY  - Preprint
T1  - A Question-Answering framework for plots using Deep learning
A1  - Revanth Reddy
A1  - Rahul Ramesh
A1  - Ameet Deshpande
A1  - Mitesh M. Khapra
JO  - ArXiv e-prints
Y1  - 12 June, 2018
UR  - https://arxiv.org/abs/1806.04655
N2  - Deep Learning has managed to push boundaries in a wide variety of tasks. One area of interest is to tackle problems in reasoning and understanding, in an aim to emulate human intelligence. In this work, we describe a deep learning model that addresses the reasoning task of question-answering on bar graphs and pie charts. We introduce a novel architecture that learns to identify various plot elements, quantify the represented values and determine a relative ordering of these statistical values. We test our model on the recently released FigureQA dataset, which provides images and accompanying questions, for bar graphs and pie charts, augmented with rich annotations. Our approach outperforms the state-of-the-art Relation Networks baseline and traditional CNN-LSTM models when evaluated on this dataset. Our model also has a considerably faster training time of approximately 2 days on 1 GPU compared to the Relation Networks baseline
ER  -


TY  - Preprint
T1  - Deep Learning to Detect Redundant Method Comments
A1  - Annie Louis
A1  - Santanu Kumar Dash
A1  - Earl T. Barr
A1  - Charles Sutton
JO  - ArXiv e-prints
Y1  - 12 June, 2018
UR  - https://arxiv.org/abs/1806.04616
N2  - Comments in software are critical for maintenance and reuse. But apart from prescriptive advice, there is little practical support or quantitative understanding of what makes a comment useful. In this paper, we introduce the task of identifying comments which are uninformative about the code they are meant to document. To address this problem, we introduce the notion of comment entailment from code, high entailment indicating that a comment&#39;s natural language semantics can be inferred directly from the code. Although not all entailed comments are low quality, comments that are too easily inferred, for example, comments that restate the code, are widely discouraged by authorities on software style. Based on this, we develop a tool called CRAIC which scores method-level comments for redundancy. Highly redundant comments can then be expanded or alternately removed by the developer. CRAIC uses deep language models to exploit large software corpora without requiring expensive manual annotations of entailment. We show that CRAIC can perform the comment entailment task with good agreement with human judgements. Our findings also have implications for documentation tools. For example, we find that common tags in Javadoc are at least two times more predictable from code than non-Javadoc sentences, suggesting that Javadoc tags are less informative than more free-form comments
ER  -


TY  - Preprint
T1  - Deep Learning-based Intelligent Dual Connectivity for Mobility Management in Dense Network
A1  - Chujie Wang
A1  - Zhifeng Zhao
A1  - Qi Sun
A1  - Honggang Zhang
JO  - ArXiv e-prints
Y1  - 30 May, 2018
UR  - https://arxiv.org/abs/1806.04584
N2  - Ultra-dense network deployment has been proposed as a key technique for achieving capacity goals in the fifth-generation (5G) mobile communication system. However, the deployment of smaller cells inevitably leads to more frequent handovers, thus making mobility management more challenging and reducing the capacity gains offered by the dense network deployment. In order to fully reap the gains for mobile users in such a network environment, we propose an intelligent dual connectivity mechanism for mobility management through deep learning-based mobility prediction. We first use LSTM (Long Short Term Memory) algorithm, one of deep learning algorithms, to learn every user equipment&#39;s (UE&#39;s) mobility pattern from its historical trajectories and predict its movement trends in the future. Based on the corresponding prediction results, the network will judge whether a handover is required for the UE. For the handover case, a dual connection will be established for the related UE. Thus, the UE can get the radio signal from two base stations in the handover process. Simulation results verify that the proposed intelligent dual connectivity mechanism can significantly improve the quality of service of mobile users in the handover process while guaranteeing the network energy efficiency.
ER  -


TY  - Preprint
T1  - Multi-Agent Deep Reinforcement Learning with Human Strategies
A1  - Thanh Nguyen
A1  - Ngoc Duy Nguyen
A1  - Saeid Nahavandi
JO  - ArXiv e-prints
Y1  - 12 June, 2018
UR  - https://arxiv.org/abs/1806.04562
N2  - Deep learning has enabled traditional reinforcement learning methods to deal with high-dimensional problems. However, one of the disadvantages of deep reinforcement learning methods is the limited exploration capacity of learning agents. In this paper, we introduce an approach that integrates human strategies to increase the exploration capacity of multiple deep reinforcement learning agents. We also report the development of our own multi-agent environment called Multiple Tank Defence to simulate the proposed approach. The results show the significant performance improvement of multiple agents that have learned cooperatively with human strategies. This implies that there is a critical need for human intellect teamed with machines to solve complex problems. In addition, the success of this simulation indicates that our developed multi-agent environment can be used as a testbed platform to develop and validate other multi-agent control algorithms. Details of the environment implementation can be referred to http://www.deakin.edu.au/~thanhthi/madrl_human.htm
ER  -


TY  - Preprint
T1  - Learning Deep Similarity Metric for 3D MR-TRUS Registration
A1  - Grant Haskins
A1  - Jochen Kruecker
A1  - Uwe Kruger
A1  - Sheng Xu
A1  - Peter A. Pinto
A1  - Brad J. Wood
A1  - Pingkun Yan
JO  - ArXiv e-prints
Y1  - 12 June, 2018
UR  - https://arxiv.org/abs/1806.04548
N2  - Purpose: The fusion of transrectal ultrasound (TRUS) and magnetic resonance (MR) images for guiding targeted prostate biopsy has significantly improved the biopsy yield of aggressive cancers. A key component of MR-TRUS fusion is image registration. However, it is very challenging to obtain a robust automatic MR-TRUS registration due to the large appearance difference between the two imaging modalities. The work presented in this paper aims to tackle this problem by addressing two challenges: (i) the definition of a suitable similarity metric and (ii) the determination of a suitable optimization strategy.
ER  -


TY  - Preprint
T1  - Dank Learning: Generating Memes Using Deep Neural Networks
A1  - Abel L Peirson V
A1  - E Meltem Tolunay
JO  - ArXiv e-prints
Y1  - 7 June, 2018
UR  - https://arxiv.org/abs/1806.04510
N2  - We introduce a novel meme generation system, which given any image can produce a humorous and relevant caption. Furthermore, the system can be conditioned on not only an image but also a user-defined label relating to the meme template, giving a handle to the user on meme content. The system uses a pretrained Inception-v3 network to return an image embedding which is passed to an attention-based deep-layer LSTM model producing the caption - inspired by the widely recognised Show and Tell Model. We implement a modified beam search to encourage diversity in the captions. We evaluate the quality of our model using perplexity and human assessment on both the quality of memes generated and whether they can be differentiated from real ones. Our model produces original memes that cannot on the whole be differentiated from real ones.
ER  -


TY  - Preprint
T1  - End-to-End Learning of Energy-Constrained Deep Neural Networks
A1  - Haichuan Yang
A1  - Yuhao Zhu
A1  - Ji Liu
JO  - ArXiv e-prints
Y1  - 12 June, 2018
UR  - https://arxiv.org/abs/1806.04321
N2  - Deep Neural Networks (DNN) are increasingly deployed in highly energy-constrained environments such as autonomous drones and wearable devices while at the same time must operate in real-time. Therefore, reducing the energy consumption has become a major design consideration in DNN training. This paper proposes the first end-to-end DNN training framework that provides quantitative energy guarantees. The key idea is to formulate the DNN training as an optimization problem in which the energy budget imposes a previously unconsidered optimization constraint. We integrate the quantitative DNN energy estimation into the DNN training process to assist the constraint optimization. We prove that an approximate algorithm can be used to efficiently solve the optimization problem. Compared to the best prior energy-saving techniques, our framework trains DNNs that provide higher accuracies under same or lower energy budgets.
ER  -


TY  - Preprint
T1  - Writing Style Invariant Deep Learning Model for Historical Manuscripts Alignment
A1  - Majeed Kassis
A1  - Jumana Nassour
A1  - Jihad El-Sana
JO  - ArXiv e-prints
Y1  - 7 June, 2018
UR  - https://arxiv.org/abs/1806.03987
N2  - Historical manuscript alignment is a widely known problem in document analysis. Finding the differences between manuscript editions is mostly done manually. In this paper, we present a writer independent deep learning model which is trained on several writing styles, and able to achieve high detection accuracy when tested on writing styles not present in training data. We test our model using cross validation, each time we train the model on five manuscripts, and test it on the other two manuscripts, never seen in the training data. We&#39;ve applied cross validation on seven manuscripts, netting 21 different tests, achieving average accuracy of $\%92.17$. We also present a new alignment algorithm based on dynamic sized sliding window, which is able to successfully handle complex cases.
ER  -


TY  - Preprint
T1  - A Multi-task Deep Learning Architecture for Maritime Surveillance using AIS Data Streams
A1  - Duong Nguyen
A1  - Rodolphe Vadaine
A1  - Guillaume Hajduch
A1  - RenÃ© Garello
A1  - Ronan Fablet
JO  - ArXiv e-prints
Y1  - 7 August, 2018
UR  - https://arxiv.org/abs/1806.03972
N2  - In a world of global trading, maritime safety, security and efficiency are crucial issues. We propose a multi-task deep learning framework for vessel monitoring using Automatic Identification System (AIS) data streams. We combine recurrent neural networks with latent variable modeling and an embedding of AIS messages to a new representation space to jointly address key issues to be dealt with when considering AIS data streams: massive amount of streaming data, noisy data and irregular timesampling. We demonstrate the relevance of the proposed deep learning framework on real AIS datasets for a three-task setting, namely trajectory reconstruction, anomaly detection and vessel type identification.
ER  -


TY  - Preprint
T1  - End to End Brain Fiber Orientation Estimation using Deep Learning
A1  - Nandakishore Puttashamachar
A1  - Ulas Bagci
JO  - ArXiv e-prints
Y1  - 4 June, 2018
UR  - https://arxiv.org/abs/1806.03969
N2  - In this work, we explore the various Brain Neuron tracking techniques, which is one of the most significant applications of Diffusion Tensor Imaging. Tractography provides us with a non-invasive method to analyze underlying tissue micro-structure. Understanding the structure and organization of the tissues facilitates us with a diagnosis method to identify any aberrations and provide acute information on the occurrences of brain ischemia or stroke, the mutation of neurological diseases such as Alzheimer, multiple sclerosis and so on. Time if of essence and accurate localization of the aberrations can help save or change a diseased life. Following up with the limitations introduced by the current Tractography techniques such as computational complexity, reconstruction errors during tensor estimation and standardization, we aim to elucidate these limitations through our research findings. We introduce an end to end Deep Learning framework which can accurately estimate the most probable likelihood orientation at each voxel along a neuronal pathway. We use Probabilistic Tractography as our baseline model to obtain the training data and which also serve as a Tractography Gold Standard for our evaluations. Through experiments we show that our Deep Network can do a significant improvement over current Tractography implementations by reducing the run-time complexity to a significant new level. Our architecture also allows for variable sized input DWI signals eliminating the need to worry about memory issues as seen with the traditional techniques. The advantage of this architecture is that it is perfectly desirable to be processed on a cloud setup and utilize the existing multi GPU frameworks to perform whole brain Tractography in minutes rather than hours. We evaluate our network with Gold Standard and benchmark its performance across several parameters.
ER  -


TY  - Preprint
T1  - Synthetic Perfusion Maps: Imaging Perfusion Deficits in DSC-MRI with Deep Learning
A1  - Andreas Hess
A1  - Raphael Meier
A1  - Johannes Kaesmacher
A1  - Simon Jung
A1  - Fabien Scalzo
A1  - David Liebeskind
A1  - Roland Wiest
A1  - Richard McKinley
JO  - ArXiv e-prints
Y1  - 11 June, 2018
UR  - https://arxiv.org/abs/1806.03848
N2  - In this work, we present a novel convolutional neural net- work based method for perfusion map generation in dynamic suscepti- bility contrast-enhanced perfusion imaging. The proposed architecture is trained end-to-end and solely relies on raw perfusion data for inference. We used a dataset of 151 acute ischemic stroke cases for evaluation. Our method generates perfusion maps that are comparable to the target maps used for clinical routine, while being model-free, fast, and less noisy.
ER  -


TY  - Preprint
T1  - Compression of phase-only holograms with JPEG standard and deep learning
A1  - Shuming Jiao
A1  - Zhi Jin
A1  - Chenliang Chang
A1  - Changyuan Zhou
A1  - Wenbin Zou
A1  - Xia Li
JO  - ArXiv e-prints
Y1  - 11 June, 2018
UR  - https://arxiv.org/abs/1806.03811
N2  - It is a critical issue to reduce the enormous amount of data in the processing, storage and transmission of a hologram in digital format. In photograph compression, the JPEG standard is commonly supported by almost every system and device. It will be favorable if JPEG standard is applicable to hologram compression, with advantages of universal compatibility. However, the reconstructed image from a JPEG compressed hologram suffers from severe quality degradation since some high frequency features in the hologram will be lost during the compression process. In this work, we employ a deep convolutional neural network to reduce the artifacts in a JPEG compressed hologram. Simulation and experimental results reveal that our proposed &#34;JPEG + deep learning&#34; hologram compression scheme can achieve satisfactory reconstruction results for a computer-generated phase-only hologram after compression.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Chinese Zero pronoun Resolution
A1  - Qingyu Yin
A1  - Yu Zhang
A1  - Weinan Zhang
A1  - Ting Liu
A1  - William Yang Wang
JO  - ArXiv e-prints
Y1  - 19 July, 2018
UR  - https://arxiv.org/abs/1806.03711
N2  - Deep neural network models for Chinese zero pronoun resolution learn semantic information for zero pronoun and candidate antecedents, but tend to be short-sighted---they often make local decisions. They typically predict coreference chains between the zero pronoun and one single candidate antecedent one link at a time, while overlooking their long-term influence on future decisions. Ideally, modeling useful information of preceding potential antecedents is critical when later predicting zero pronoun-candidate antecedent pairs. In this study, we show how to integrate local and global decision-making by exploiting deep reinforcement learning models. With the help of the reinforcement learning agent, our model learns the policy of selecting antecedents in a sequential manner, where useful information provided by earlier predicted antecedents could be utilized for making later coreference decisions. Experimental results on OntoNotes 5.0 dataset show that our technique surpasses the state-of-the-art models.
ER  -


TY  - Preprint
T1  - Incorporating Features Learned by an Enhanced Deep Knowledge Tracing Model for STEM/Non-STEM Job Prediction
A1  - Chun-kit Yeung
A1  - Zizheng Lin
A1  - Kai Yang
A1  - Dit-yan Yeung
JO  - ArXiv e-prints
Y1  - 6 June, 2018
UR  - https://arxiv.org/abs/1806.03256
N2  - The 2017 ASSISTments Data Mining competition aims to use data from a longitudinal study for predicting a brand-new outcome of students which had never been studied before by the educational data mining research community. Specifically, it facilitates research in developing predictive models that predict whether the first job of a student out of college belongs to a STEM (the acronym for science, technology, engineering, and mathematics) field. This is based on the student&#39;s learning history on the ASSISTments blended learning platform in the form of extensive clickstream data gathered during the middle school years. To tackle this challenge, we first estimate the expected knowledge state of students with respect to different mathematical skills using a deep knowledge tracing (DKT) model and an enhanced DKT (DKT+) model. We then combine the features corresponding to the DKT/DKT+ expected knowledge state with other features extracted directly from the student profile in the dataset to train several machine learning models for the STEM/non-STEM job prediction. Our experiments show that models trained with the combined features generally perform better than the models trained with the student profile alone. Detailed analysis of the student&#39;s knowledge state reveals that, when compared with non-STEM students, STEM students generally show a higher mastery level and a higher learning gain in mathematics.
ER  -


TY  - Preprint
T1  - Automatic View Planning with Multi-scale Deep Reinforcement Learning Agents
A1  - Amir Alansary
A1  - Loic Le Folgoc
A1  - Ghislain Vaillant
A1  - Ozan Oktay
A1  - Yuanwei Li
A1  - Wenjia Bai
A1  - Jonathan Passerat-Palmbach
A1  - Ricardo Guerrero
A1  - Konstantinos Kamnitsas
A1  - Benjamin Hou
A1  - Steven McDonagh
A1  - Ben Glocker
A1  - Bernhard Kainz
A1  - Daniel Rueckert
JO  - ArXiv e-prints
Y1  - 8 June, 2018
UR  - https://arxiv.org/abs/1806.03228
N2  - We propose a fully automatic method to find standardized view planes in 3D image acquisitions. Standard view images are important in clinical practice as they provide a means to perform biometric measurements from similar anatomical regions. These views are often constrained to the native orientation of a 3D image acquisition. Navigating through target anatomy to find the required view plane is tedious and operator-dependent. For this task, we employ a multi-scale reinforcement learning (RL) agent framework and extensively evaluate several Deep Q-Network (DQN) based strategies. RL enables a natural learning paradigm by interaction with the environment, which can be used to mimic experienced operators. We evaluate our results using the distance between the anatomical landmarks and detected planes, and the angles between their normal vector and target. The proposed algorithm is assessed on the mid-sagittal and anterior-posterior commissure planes of brain MRI, and the 4-chamber long-axis plane commonly used in cardiac MRI, achieving accuracy of 1.53mm, 1.98mm and 4.84mm, respectively.
ER  -


TY  - Preprint
T1  - A Systematic Evaluation of Recent Deep Learning Architectures for Fine-Grained Vehicle Classification
A1  - Krassimir Valev
A1  - Arne Schumann
A1  - Lars Sommer
A1  - JÃ¼rgen Beyerer
JO  - ArXiv e-prints
Y1  - 8 June, 2018
UR  - https://arxiv.org/abs/1806.02987
N2  - Fine-grained vehicle classification is the task of classifying make, model, and year of a vehicle. This is a very challenging task, because vehicles of different types but similar color and viewpoint can often look much more similar than vehicles of same type but differing color and viewpoint. Vehicle make, model, and year in com- bination with vehicle color - are of importance in several applications such as vehicle search, re-identification, tracking, and traffic analysis. In this work we investigate the suitability of several recent landmark convolutional neural network (CNN) architectures, which have shown top results on large scale image classification tasks, for the task of fine-grained classification of vehicles. We compare the performance of the networks VGG16, several ResNets, Inception architectures, the recent DenseNets, and MobileNet. For classification we use the Stanford Cars-196 dataset which features 196 different types of vehicles. We investigate several aspects of CNN training, such as data augmentation and training from scratch vs. fine-tuning. Importantly, we introduce no aspects in the architectures or training process which are specific to vehicle classification. Our final model achieves a state-of-the-art classification accuracy of 94.6% outperforming all related works, even approaches which are specifically tailored for the task, e.g. by including vehicle part detections.
ER  -


TY  - Preprint
T1  - Model-based active learning to detect isometric deformable objects in the wild with deep architectures
A1  - Shrinivasan Sankar
A1  - Adrien Bartoli
JO  - ArXiv e-prints
Y1  - 7 June, 2018
UR  - https://arxiv.org/abs/1806.02850
N2  - In the recent past, algorithms based on Convolutional Neural Networks (CNNs) have achieved significant milestones in object recognition. With large examples of each object class, standard datasets train well for inter-class variability. However, gathering sufficient data to train for a particular instance of an object within a class is impractical. Furthermore, quantitatively assessing the imaging conditions for each image in a given dataset is not feasible. By generating sufficient images with known imaging conditions, we study to what extent CNNs can cope with hard imaging conditions for instance-level recognition in an active learning regime.
ER  -


TY  - Preprint
T1  - Fast Distributed Deep Learning via Worker-adaptive Batch Sizing
A1  - Chen Chen
A1  - Qizhen Weng
A1  - Wei Wang
A1  - Baochun Li
A1  - Bo Li
JO  - ArXiv e-prints
Y1  - 7 June, 2018
UR  - https://arxiv.org/abs/1806.02508
N2  - Deep neural network models are usually trained in cluster environments, where the model parameters are iteratively refined by multiple worker machines in parallel. One key challenge in this regard is the presence of stragglers, which significantly degrades the learning performance. In this paper, we propose to eliminate stragglers by adapting each worker&#39;s training load to its processing capability; that is, slower workers receive a smaller batch of data to process.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for General Video Game AI
A1  - Ruben Rodriguez Torrado
A1  - Philip Bontrager
A1  - Julian Togelius
A1  - Jialin Liu
A1  - Diego Perez-Liebana
JO  - ArXiv e-prints
Y1  - 6 June, 2018
UR  - https://arxiv.org/abs/1806.02448
N2  - The General Video Game AI (GVGAI) competition and its associated software framework provides a way of benchmarking AI algorithms on a large number of games written in a domain-specific description language. While the competition has seen plenty of interest, it has so far focused on online planning, providing a forward model that allows the use of algorithms such as Monte Carlo Tree Search.
ER  -


TY  - Preprint
T1  - Deep Variational Reinforcement Learning for POMDPs
A1  - Maximilian Igl
A1  - Luisa Zintgraf
A1  - Tuan Anh Le
A1  - Frank Wood
A1  - Shimon Whiteson
JO  - ArXiv e-prints
Y1  - 6 June, 2018
UR  - https://arxiv.org/abs/1806.02426
N2  - Many real-world sequential decision making problems are partially observable by nature, and the environment model is typically unknown. Consequently, there is great need for reinforcement learning methods that can tackle such problems given only a stream of incomplete and noisy observations. In this paper, we propose deep variational reinforcement learning (DVRL), which introduces an inductive bias that allows an agent to learn a generative model of the environment and perform inference in that model to effectively aggregate the available information. We develop an n-step approximation to the evidence lower bound (ELBO), allowing the model to be trained jointly with the policy. This ensures that the latent state representation is suitable for the control task. In experiments on Mountain Hike and flickering Atari we show that our method outperforms previous approaches relying on recurrent neural networks to encode the past.
ER  -


TY  - Preprint
T1  - Deep Vessel Segmentation By Learning Graphical Connectivity
A1  - Seung Yeon Shin
A1  - Soochahn Lee
A1  - Il Dong Yun
A1  - Kyoung Mu Lee
JO  - ArXiv e-prints
Y1  - 6 June, 2018
UR  - https://arxiv.org/abs/1806.02279
N2  - We propose a novel deep-learning-based system for vessel segmentation. Existing methods using CNNs have mostly relied on local appearances learned on the regular image grid, without considering the graphical structure of vessel shape. To address this, we incorporate a graph convolutional network into a unified CNN architecture, where the final segmentation is inferred by combining the different types of features. The proposed method can be applied to expand any type of CNN-based vessel segmentation method to enhance the performance. Experiments show that the proposed method outperforms the current state-of-the-art methods on two retinal image datasets as well as a coronary artery X-ray angiography dataset.
ER  -


TY  - Preprint
T1  - Spectral Inference Networks: Unifying Spectral Methods With Deep Learning
A1  - David Pfau
A1  - Stig Petersen
A1  - Ashish Agarwal
A1  - David Barrett
A1  - Kim Stachenfeld
JO  - ArXiv e-prints
Y1  - 6 June, 2018
UR  - https://arxiv.org/abs/1806.02215
N2  - We present Spectral Inference Networks, a framework for learning eigenfunctions of linear operators by stochastic optimization. Spectral Inference Networks generalize Slow Feature Analysis to generic symmetric operators, and are closely related to Variational Monte Carlo methods from computational physics. As such, they can be a powerful tool for unsupervised representation learning from video or pairs of data. We derive a training algorithm for Spectral Inference Networks that addresses the bias in the gradients due to finite batch size and allows for online learning of multiple eigenfunctions. We show results of training Spectral Inference Networks on problems in quantum mechanics and feature learning for videos on synthetic datasets as well as the Arcade Learning Environment. Our results demonstrate that Spectral Inference Networks accurately recover eigenfunctions of linear operators, can discover interpretable representations from video and find meaningful subgoals in reinforcement learning environments.
ER  -


TY  - Preprint
T1  - Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series
A1  - Vincent Fortuin
A1  - Matthias HÃ¼ser
A1  - Francesco Locatello
A1  - Heiko Strathmann
A1  - Gunnar RÃ¤tsch
JO  - ArXiv e-prints
Y1  - 5 October, 2018
UR  - https://arxiv.org/abs/1806.02199
N2  - High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time. To address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space. This model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty. We evaluate our model in terms of clustering performance and interpretability on static (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two macro states, as well as on a challenging real world medical time series application on the eICU data set. Our learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data.
ER  -


TY  - Preprint
T1  - Probabilistic Deep Learning using Random Sum-Product Networks
A1  - Robert Peharz
A1  - Antonio Vergari
A1  - Karl Stelzner
A1  - Alejandro Molina
A1  - Martin Trapp
A1  - Kristian Kersting
A1  - Zoubin Ghahramani
JO  - ArXiv e-prints
Y1  - 22 June, 2018
UR  - https://arxiv.org/abs/1806.01910
N2  - The need for consistent treatment of uncertainty has recently triggered increased interest in probabilistic deep learning methods. However, most current approaches have severe limitations when it comes to inference, since many of these models do not even permit to evaluate exact data likelihoods. Sum-product networks (SPNs), on the other hand, are an excellent architecture in that regard, as they allow to efficiently evaluate likelihoods, as well as arbitrary marginalization and conditioning tasks. Nevertheless, SPNs have not been fully explored as serious deep learning models, likely due to their special structural requirements, which complicate learning. In this paper, we make a drastic simplification and use random SPN structures which are trained in a &#34;classical deep learning manner&#34;, i.e. employing automatic differentiation, SGD, and GPU support. The resulting models, called RAT-SPNs, yield prediction results comparable to deep neural networks, while still being interpretable as generative model and maintaining well-calibrated uncertainties. This property makes them highly robust under missing input features and enables them to naturally detect outliers and peculiar samples.
ER  -


TY  - Preprint
T1  - Performance Evaluation of Deep Learning Networks for Semantic Segmentation of Traffic Stereo-Pair Images
A1  - Vlad Taran
A1  - Nikita Gordienko
A1  - Yuriy Kochura
A1  - Yuri Gordienko
A1  - Alexandr Rokovyi
A1  - Oleg Alienin
A1  - Sergii Stirenko
JO  - ArXiv e-prints
Y1  - 5 June, 2018
UR  - https://arxiv.org/abs/1806.01896
N2  - Semantic image segmentation is one the most demanding task, especially for analysis of traffic conditions for self-driving cars. Here the results of application of several deep learning architectures (PSPNet and ICNet) for semantic image segmentation of traffic stereo-pair images are presented. The images from Cityscapes dataset and custom urban images were analyzed as to the segmentation accuracy and image inference time. For the models pre-trained on Cityscapes dataset, the inference time was equal in the limits of standard deviation, but the segmentation accuracy was different for various cities and stereo channels even. The distributions of accuracy (mean intersection over union - mIoU) values for each city and channel are asymmetric, long-tailed, and have many extreme outliers, especially for PSPNet network in comparison to ICNet network. Some statistical properties of these distributions (skewness, kurtosis) allow us to distinguish these two networks and open the question about relations between architecture of deep learning networks and statistical distribution of the predicted results (mIoU here). The results obtained demonstrated the different sensitivity of these networks to: (1) the local street view peculiarities in different cities that should be taken into account during the targeted fine tuning the models before their practical applications, (2) the right and left data channels in stereo-pairs. For both networks, the difference in the predicted results (mIoU here) for the right and left data channels in stereo-pairs is out of the limits of statistical error in relation to mIoU values. It means that the traffic stereo pairs can be effectively used not only for depth calculations (as it is usually used), but also as an additional data channel that can provide much more information about scene objects than simple duplication of the same street view images.
ER  -


TY  - Preprint
T1  - Relational Deep Reinforcement Learning
A1  - Vinicius Zambaldi
A1  - David Raposo
A1  - Adam Santoro
A1  - Victor Bapst
A1  - Yujia Li
A1  - Igor Babuschkin
A1  - Karl Tuyls
A1  - David Reichert
A1  - Timothy Lillicrap
A1  - Edward Lockhart
A1  - Murray Shanahan
A1  - Victoria Langston
A1  - Razvan Pascanu
A1  - Matthew Botvinick
A1  - Oriol Vinyals
A1  - Peter Battaglia
JO  - ArXiv e-prints
Y1  - 28 June, 2018
UR  - https://arxiv.org/abs/1806.01830
N2  - We introduce an approach for deep reinforcement learning (RL) that improves upon the efficiency, generalization capacity, and interpretability of conventional approaches through structured perception and relational reasoning. It uses self-attention to iteratively reason about the relations between entities in a scene and to guide a model-free policy. Our results show that in a novel navigation and planning task called Box-World, our agent finds interpretable solutions that improve upon baselines in terms of sample complexity, ability to generalize to more complex scenes than experienced during training, and overall performance. In the StarCraft II Learning Environment, our agent achieves state-of-the-art performance on six mini-games -- surpassing human grandmaster performance on four. By considering architectural inductive biases, our work opens new directions for overcoming important, but stubborn, challenges in deep RL.
ER  -


TY  - Preprint
T1  - LSTM Benchmarks for Deep Learning Frameworks
A1  - Stefan Braun
JO  - ArXiv e-prints
Y1  - 5 June, 2018
UR  - https://arxiv.org/abs/1806.01818
N2  - This study provides benchmarks for different implementations of LSTM units between the deep learning frameworks PyTorch, TensorFlow, Lasagne and Keras. The comparison includes cuDNN LSTMs, fused LSTM variants and less optimized, but more flexible LSTM implementations. The benchmarks reflect two typical scenarios for automatic speech recognition, notably continuous speech recognition and isolated digit recognition. These scenarios cover input sequences of fixed and variable length as well as the loss functions CTC and cross entropy. Additionally, a comparison between four different PyTorch versions is included. The code is available online https://github.com/stefbraun/rnn_benchmarks.
ER  -


TY  - Preprint
T1  - Evidential Deep Learning to Quantify Classification Uncertainty
A1  - Murat Sensoy
A1  - Melih Kandemir
A1  - Lance Kaplan
JO  - ArXiv e-prints
Y1  - 6 June, 2018
UR  - https://arxiv.org/abs/1806.01768
N2  - Deterministic neural nets have been shown to learn effective predictors on a wide range of machine learning problems. However, as the standard approach is to train the network to minimize a prediction loss, the resultant model remains ignorant to its prediction confidence. Orthogonally to Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of the same using the theory of subjective logic. By placing a Dirichlet prior on the softmax output, we treat predictions of a neural net as subjective opinions and learn the function that collects the evidence leading to these opinions by a deterministic neural net from data. The resultant predictor for a multi-class classification problem is another Dirichlet distribution whose parameters are set by the continuous output of a neural net. We provide a preliminary analysis on how the peculiarities of our new loss function drive improved uncertainty estimation. We observe that our method achieves unprecedented success on detection of out-of-sample queries and endurance against adversarial perturbations.
ER  -


TY  - Preprint
T1  - Concept-Oriented Deep Learning
A1  - Daniel T Chang
JO  - ArXiv e-prints
Y1  - 5 June, 2018
UR  - https://arxiv.org/abs/1806.01756
N2  - Concepts are the foundation of human deep learning, understanding, and knowledge integration and transfer. We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability. CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data. We discuss the major aspects of CODL including concept graph, concept representations, concept exemplars, and concept representation learning systems supporting incremental and continual learning.
ER  -


TY  - Preprint
T1  - An Explainable Adversarial Robustness Metric for Deep Learning Neural Networks
A1  - Chirag Agarwal
A1  - Bo Dong
A1  - Dan Schonfeld
A1  - Anthony Hoogs
JO  - ArXiv e-prints
Y1  - 6 June, 2018
UR  - https://arxiv.org/abs/1806.01477
N2  - Deep Neural Networks(DNN) have excessively advanced the field of computer vision by achieving state of the art performance in various vision tasks. These results are not limited to the field of vision but can also be seen in speech recognition and machine translation tasks. Recently, DNNs are found to poorly fail when tested with samples that are crafted by making imperceptible changes to the original input images. This causes a gap between the validation and adversarial performance of a DNN. An effective and generalizable robustness metric for evaluating the performance of DNN on these adversarial inputs is still missing from the literature. In this paper, we propose Noise Sensitivity Score (NSS), a metric that quantifies the performance of a DNN on a specific input under different forms of fix-directional attacks. An insightful mathematical explanation is provided for deeply understanding the proposed metric. By leveraging the NSS, we also proposed a skewness based dataset robustness metric for evaluating a DNN&#39;s adversarial performance on a given dataset. Extensive experiments using widely used state of the art architectures along with popular classification datasets, such as MNIST, CIFAR-10, CIFAR-100, and ImageNet, are used to validate the effectiveness and generalization of our proposed metrics. Instead of simply measuring a DNN&#39;s adversarial robustness in the input domain, as previous works, the proposed NSS is built on top of insightful mathematical understanding of the adversarial attack and gives a more explicit explanation of the robustness.
ER  -


TY  - Preprint
T1  - OpenTag: Open Attribute Value Extraction from Product Profiles [Deep Learning, Active Learning, Named Entity Recognition]
A1  - Guineng Zheng
A1  - Subhabrata Mukherjee
A1  - Xin Luna Dong
A1  - Feifei Li
JO  - ArXiv e-prints
Y1  - 6 October, 2018
UR  - https://arxiv.org/abs/1806.01264
N2  - Extraction of missing attribute values is to find values describing an attribute of interest from a free text input. Most past related work on extraction of missing attribute values work with a closed world assumption with the possible set of values known beforehand, or use dictionaries of values and hand-crafted features. How can we discover new attribute values that we have never seen before? Can we do this with limited human annotation or supervision? We study this problem in the context of product catalogs that often have missing values for many attributes of interest.
ER  -


TY  - Preprint
T1  - Relational inductive biases, deep learning, and graph networks
A1  - Peter W. Battaglia
A1  - Jessica B. Hamrick
A1  - Victor Bapst
A1  - Alvaro Sanchez-Gonzalez
A1  - Vinicius Zambaldi
A1  - Mateusz Malinowski
A1  - Andrea Tacchetti
A1  - David Raposo
A1  - Adam Santoro
A1  - Ryan Faulkner
A1  - Caglar Gulcehre
A1  - Francis Song
A1  - Andrew Ballard
A1  - Justin Gilmer
A1  - George Dahl
A1  - Ashish Vaswani
A1  - Kelsey Allen
A1  - Charles Nash
A1  - Victoria Langston
A1  - Chris Dyer
A1  - Nicolas Heess
A1  - Daan Wierstra
A1  - Pushmeet Kohli
A1  - Matt Botvinick
A1  - Oriol Vinyals
JO  - ArXiv e-prints
Y1  - 11 June, 2018
UR  - https://arxiv.org/abs/1806.01261
N2  - Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one&#39;s experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI.
ER  -


TY  - Preprint
T1  - TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning
A1  - Artemij Amiranashvili
A1  - Alexey Dosovitskiy
A1  - Vladlen Koltun
A1  - Thomas Brox
JO  - ArXiv e-prints
Y1  - 4 June, 2018
UR  - https://arxiv.org/abs/1806.01175
N2  - Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.
ER  -


TY  - Preprint
T1  - Performance tuning for deep learning on a many-core processor (master thesis)
A1  - Philippos Papaphilippou
JO  - ArXiv e-prints
Y1  - 4 May, 2018
UR  - https://arxiv.org/abs/1806.01105
N2  - Convolutional neural networks (CNNs) are becoming very successful and popular for a variety of applications. The Loki many-core processor architecture is very promising for achieving specialised hardware performance and efficiency while being a general purpose solution. Loki combines many simple cores with increased control for the programmer. This freedom can be exploited to produce much more efficient code than in conventional multiprocessors but it also creates a very big design space for possible optimisations. In this project, I explore possible optimisations for a CNN application, their portability on different Loki-specific configurations, convolution parameters and inputs. Finally, I investigate the potential for adaptive algorithms for further performance increase.
ER  -


TY  - Preprint
T1  - ALMN: Deep Embedding Learning with Geometrical Virtual Point Generating
A1  - Binghui Chen
A1  - Weihong Deng
JO  - ArXiv e-prints
Y1  - 5 June, 2018
UR  - https://arxiv.org/abs/1806.00974
N2  - Deep embedding learning becomes more attractive for discriminative feature learning, but many methods still require hard-class mining, which is computationally complex and performance-sensitive. To this end, we propose Adaptive Large Margin N-Pair loss (ALMN) to address the aforementioned issues. Instead of exploring hard example-mining strategy, we introduce the concept of large margin constraint. This constraint aims at encouraging local-adaptive large angular decision margin among dissimilar samples in multimodal feature space so as to significantly encourage intraclass compactness and interclass separability. And it is mainly achieved by a simple yet novel geometrical Virtual Point Generating (VPG) method, which converts artificially setting a fixed margin into automatically generating a boundary training sample in feature space and is an open question. We demonstrate the effectiveness of our method on several popular datasets for image retrieval and clustering tasks.
ER  -


TY  - Preprint
T1  - Simultaneous compressive image recovery and deep denoiser learning from undersampled measurements
A1  - Magauiya Zhussip
A1  - Se Young Chun
JO  - ArXiv e-prints
Y1  - 4 June, 2018
UR  - https://arxiv.org/abs/1806.00961
N2  - Compressive image recovery utilizes sparse image priors such as wavelet l1 norm, total-variation (TV) norm, or self-similarity to reconstruct good quality images from highly compressive samples. Recently, there have been some attempts to exploit data-driven image priors from massive amount of clean images in compressive image recovery such as LDAMP algorithm. By utilizing large-scale noiseless images for training deep neural network denoisers, LDAMP outperformed other conventional compressive image reconstruction methods. However, one drawback of LDAMP is that large-scale noiseless images must be acquired for deep learning based denoisers. In this article, we propose a method for simultaneous compressive image recovery and deep denoiser learning from undersampled measurements that enables compressive image recovery methods to use data-driven image priors when only large-scale compressive samples are available without ground truth images. By utilizing the structure of LDAMP and Stein&#39;s Unbiased Risk Estimator (SURE) based deep neural network denoiser, we showed that our proposed methods were able to achieve better performance than other methods such as conventional BM3D-AMP and LDAMP methods trained with the results of BM3D-AMP for training data and/or testing data for all cases with i.i.d. Gaussian random and coded diffraction measurement matrices at various compression ratios. We also investigated accurate noise level estimation methods in LDAMP for coded diffraction measurement matrix to train deep denoiser networks for high performance.
ER  -


TY  - Preprint
T1  - Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced
A1  - Simon S. Du
A1  - Wei Hu
A1  - Jason D. Lee
JO  - ArXiv e-prints
Y1  - 3 June, 2018
UR  - https://arxiv.org/abs/1806.00900
N2  - We study the implicit regularization imposed by gradient descent for learning multi-layer homogeneous functions including feed-forward fully connected and convolutional deep neural networks with linear, ReLU or Leaky ReLU activation. We rigorously prove that gradient flow (i.e. gradient descent with infinitesimal step size) effectively enforces the differences between squared norms across different layers to remain invariant without any explicit regularization. This result implies that if the weights are initially small, gradient flow automatically balances the magnitudes of all layers. Using a discretization argument, we analyze gradient descent with positive step size for the non-convex low-rank asymmetric matrix factorization problem without any regularization. Inspired by our findings for gradient flow, we prove that gradient descent with step sizes $Î·_t = O\left(t^{-\left( \frac12+Î´\right)} \right)$ ($0&lt;Î´\le\frac12$) automatically balances two low-rank factors and converges to a bounded global optimum. Furthermore, for rank-$1$ asymmetric matrix factorization we give a finer analysis showing gradient descent with constant step size converges to the global minimum at a globally linear rate. We believe that the idea of examining the invariance imposed by first order algorithms in learning homogeneous models could serve as a fundamental building block for studying optimization for learning deep models.
ER  -


TY  - Preprint
T1  - Infrastructure Quality Assessment in Africa using Satellite Imagery and Deep Learning
A1  - Barak Oshri
A1  - Annie Hu
A1  - Peter Adelson
A1  - Xiao Chen
A1  - Pascaline Dupas
A1  - Jeremy Weinstein
A1  - Marshall Burke
A1  - David Lobell
A1  - Stefano Ermon
JO  - ArXiv e-prints
Y1  - 3 June, 2018
UR  - https://arxiv.org/abs/1806.00894
N2  - The UN Sustainable Development Goals allude to the importance of infrastructure quality in three of its seventeen goals. However, monitoring infrastructure quality in developing regions remains prohibitively expensive and impedes efforts to measure progress toward these goals. To this end, we investigate the use of widely available remote sensing data for the prediction of infrastructure quality in Africa. We train a convolutional neural network to predict ground truth labels from the Afrobarometer Round 6 survey using Landsat 8 and Sentinel 1 satellite imagery.
ER  -


TY  - Preprint
T1  - k-Space Deep Learning for Parallel MRI: Application to Time-Resolved MR Angiography
A1  - Eunju Cha
A1  - Eung Yeop Kim
A1  - Jong Chul Ye
JO  - ArXiv e-prints
Y1  - 10 June, 2018
UR  - https://arxiv.org/abs/1806.00806
N2  - Time-resolved angiography with interleaved stochastic trajectories (TWIST) has been widely used for dynamic contrast enhanced MRI (DCE-MRI). To achieve highly accelerated acquisitions, TWIST combines the periphery of the k-space data from several adjacent frames to reconstruct one temporal frame. However, this view-sharing scheme limits the true temporal resolution of TWIST. Moreover, the k-space sampling patterns have been specially designed for a specific generalized autocalibrating partial parallel acquisition (GRAPPA) factor so that it is not possible to reduce the number of view-sharing once the k-data is acquired. To address these issues, this paper proposes a novel k-space deep learning approach for parallel MRI. In particular, we have designed our neural network so that accurate k-space interpolations are performed simultaneously for multiple coils by exploiting the redundancies along the coils and images. Reconstruction results using in vivo TWIST data set confirm that the proposed method can immediately generate high-quality reconstruction results with various choices of view- sharing, allowing us to exploit the trade-off between spatial and temporal resolution in time-resolved MR angiography.
ER  -


TY  - Preprint
T1  - Eye in the Sky: Real-time Drone Surveillance System (DSS) for Violent Individuals Identification using ScatterNet Hybrid Deep Learning Network
A1  - Amarjot Singh
A1  - Devendra Patil
A1  - SN Omkar
JO  - ArXiv e-prints
Y1  - 3 June, 2018
UR  - https://arxiv.org/abs/1806.00746
N2  - Drone systems have been deployed by various law enforcement agencies to monitor hostiles, spy on foreign drug cartels, conduct border control operations, etc. This paper introduces a real-time drone surveillance system to identify violent individuals in public areas. The system first uses the Feature Pyramid Network to detect humans from aerial images. The image region with the human is used by the proposed ScatterNet Hybrid Deep Learning (SHDL) network for human pose estimation. The orientations between the limbs of the estimated pose are next used to identify the violent individuals. The proposed deep network can learn meaningful representations quickly using ScatterNet and structural priors with relatively fewer labeled examples. The system detects the violent individuals in real-time by processing the drone images in the cloud. This research also introduces the aerial violent individual dataset used for training the deep network which hopefully may encourage researchers interested in using deep learning for aerial surveillance. The pose estimation and violent individuals identification performance is compared with the state-of-the-art techniques.
ER  -


TY  - Preprint
T1  - Deep Pepper: Expert Iteration based Chess agent in the Reinforcement Learning Setting
A1  - Sai Krishna G. V.
A1  - Kyle Goyette
A1  - Ahmad Chamseddine
A1  - Breandan Considine
JO  - ArXiv e-prints
Y1  - 2 June, 2018
UR  - https://arxiv.org/abs/1806.00683
N2  - An almost-perfect chess playing agent has been a long standing challenge in the field of Artificial Intelligence. Some of the recent advances demonstrate we are approaching that goal. In this project, we provide methods for faster training of self-play style algorithms, mathematical details of the algorithm used, various potential future directions, and discuss most of the relevant work in the area of computer chess. Deep Pepper uses embedded knowledge to accelerate the training of the chess engine over a &#34;tabula rasa&#34; system such as Alpha Zero. We also release our code to promote further research.
ER  -


TY  - Preprint
T1  - BoxNet: Deep Learning Based Biomedical Image Segmentation Using Boxes Only Annotation
A1  - Lin Yang
A1  - Yizhe Zhang
A1  - Zhuo Zhao
A1  - Hao Zheng
A1  - Peixian Liang
A1  - Michael T. C. Ying
A1  - Anil T. Ahuja
A1  - Danny Z. Chen
JO  - ArXiv e-prints
Y1  - 2 June, 2018
UR  - https://arxiv.org/abs/1806.00593
N2  - In recent years, deep learning (DL) methods have become powerful tools for biomedical image segmentation. However, high annotation efforts and costs are commonly needed to acquire sufficient biomedical training data for DL models. To alleviate the burden of manual annotation, in this paper, we propose a new weakly supervised DL approach for biomedical image segmentation using boxes only annotation. First, we develop a method to combine graph search (GS) and DL to generate fine object masks from box annotation, in which DL uses box annotation to compute a rough segmentation for GS and then GS is applied to locate the optimal object boundaries. During the mask generation process, we carefully utilize information from box annotation to filter out potential errors, and then use the generated masks to train an accurate DL segmentation network. Extensive experiments on gland segmentation in histology images, lymph node segmentation in ultrasound images, and fungus segmentation in electron microscopy images show that our approach attains superior performance over the best known state-of-the-art weakly supervised DL method and is able to achieve (1) nearly the same accuracy compared to fully supervised DL methods with far less annotation effort, (2) significantly better results with similar annotation time, and (3) robust performance in various applications.
ER  -


TY  - Preprint
T1  - Deep Curiosity Search: Intra-Life Exploration Improves Performance on Challenging Deep Reinforcement Learning Problems
A1  - Christopher Stanton
A1  - Jeff Clune
JO  - ArXiv e-prints
Y1  - 1 June, 2018
UR  - https://arxiv.org/abs/1806.00553
N2  - Traditional exploration methods in RL require agents to perform random actions to find rewards. But these approaches struggle on sparse-reward domains like Montezuma&#39;s Revenge where the probability that any random action sequence leads to reward is extremely low. Recent algorithms have performed well on such tasks by encouraging agents to visit new states or perform new actions in relation to all prior training episodes (which we call across-training novelty). But such algorithms do not consider whether an agent exhibits intra-life novelty: doing something new within the current episode, regardless of whether those behaviors have been performed in previous episodes. We hypothesize that across-training novelty might discourage agents from revisiting initially non-rewarding states that could become important stepping stones later in training. We introduce Deep Curiosity Search (DeepCS), which encourages intra-life exploration by rewarding agents for visiting as many different states as possible within each episode, and show that DeepCS matches the performance of current state-of-the-art methods on Montezuma&#39;s Revenge. We further show that DeepCS improves exploration on Gravitar (another difficult, sparse-reward game) and performs well on the dense-reward game Amidar. Surprisingly, DeepCS doubles A2C performance on Seaquest, a game we would not have expected to benefit from intra-life exploration because the arena is small and already easily navigated by naive exploration techniques. In one run, DeepCS achieves a maximum training score of 80,000 points on Seaquest, higher than any methods other than Ape-X. The strong performance of DeepCS on these sparse- and dense-reward tasks suggests that encouraging intra-life novelty is an interesting, new approach for improving performance in Deep RL and motivates further research into hybridizing across-training and intra-life exploration methods.
ER  -


TY  - Preprint
T1  - Surgical Activity Recognition in Robot-Assisted Radical Prostatectomy using Deep Learning
A1  - Aneeq Zia
A1  - Andrew Hung
A1  - Irfan Essa
A1  - Anthony Jarc
JO  - ArXiv e-prints
Y1  - 1 June, 2018
UR  - https://arxiv.org/abs/1806.00466
N2  - Adverse surgical outcomes are costly to patients and hospitals. Approaches to benchmark surgical care are often limited to gross measures across the entire procedure despite the performance of particular tasks being largely responsible for undesirable outcomes. In order to produce metrics from tasks as opposed to the whole procedure, methods to recognize automatically individual surgical tasks are needed. In this paper, we propose several approaches to recognize surgical activities in robot-assisted minimally invasive surgery using deep learning. We collected a clinical dataset of 100 robot-assisted radical prostatectomies (RARP) with 12 tasks each and propose `RP-Net&#39;, a modified version of InceptionV3 model, for image based surgical activity recognition. We achieve an average precision of 80.9% and average recall of 76.7% across all tasks using RP-Net which out-performs all other RNN and CNN based models explored in this paper. Our results suggest that automatic surgical activity recognition during RARP is feasible and can be the foundation for advanced analytics.
ER  -


TY  - Preprint
T1  - Deep Imbalanced Learning for Face Recognition and Attribute Prediction
A1  - Chen Huang
A1  - Yining Li
A1  - Chen Change Loy
A1  - Xiaoou Tang
JO  - ArXiv e-prints
Y1  - 1 June, 2018
UR  - https://arxiv.org/abs/1806.00194
N2  - Data for face analysis often exhibit highly-skewed class distribution, i.e., most data belong to a few majority classes, while the minority classes only contain a scarce amount of instances. To mitigate this issue, contemporary deep learning methods typically follow classic strategies such as class re-sampling or cost-sensitive training. In this paper, we conduct extensive and systematic experiments to validate the effectiveness of these classic schemes for representation learning on class-imbalanced data. We further demonstrate that more discriminative deep representation can be learned by enforcing a deep network to maintain inter-cluster margins both within and between classes. This tight constraint effectively reduces the class imbalance inherent in the local data neighborhood, thus carving much more balanced class boundaries locally. We show that it is easy to deploy angular margins between the cluster distributions on a hypersphere manifold. Such learned Cluster-based Large Margin Local Embedding (CLMLE), when combined with a simple k-nearest cluster algorithm, shows significant improvements in accuracy over existing methods on both face recognition and face attribute prediction tasks that exhibit imbalanced class distribution.
ER  -


TY  - Preprint
T1  - k-Space Deep Learning for Reference-free EPI Ghost Correction
A1  - Juyoung Lee
A1  - Yoseob Han
A1  - Jong Chul Ye
JO  - ArXiv e-prints
Y1  - 10 June, 2018
UR  - https://arxiv.org/abs/1806.00153
N2  - Nyquist ghost artifacts in EPI images are originated from phase mismatch between the even and odd echoes. However, conventional correction methods using reference scans often produce erroneous results especially in high-field MRI due to the non-linear and time-varying local magnetic field changes. Recently, it was shown that the problem of ghost correction can be transformed into k-space data interpolation problem that can be solved using the annihilating filter-based low-rank Hankel structured matrix completion approach (ALOHA). Another recent discovery has shown that the deep convolutional neural network is closely related to the data-driven Hankel matrix decomposition. By synergistically combining these findings, here we propose a k-space deep learning approach that immediately corrects the k-space phase mismatch without a reference scan. Reconstruction results using 7T in vivo data showed that the proposed reference-free k-space deep learning approach for EPI ghost correction significantly improves the image quality compared to the existing methods, and the computing time is several orders of magnitude faster.
ER  -


TY  - Preprint
T1  - Deep Learning with unsupervised data labeling for weeds detection on UAV images
A1  - M. Dian. Bah
A1  - Adel Hafiane
A1  - Raphael Canals
JO  - ArXiv e-prints
Y1  - 31 May, 2018
UR  - https://arxiv.org/abs/1805.12395
N2  - In modern agriculture, usually weeds control consists in spraying herbicides all over the agricultural field. This practice involves significant waste and cost of herbicide for farmers and environmental pollution. One way to reduce the cost and environmental impact is to allocate the right doses of herbicide at the right place and at the right time (Precision Agriculture). Nowadays, Unmanned Aerial Vehicle (UAV) is becoming an interesting acquisition system for weeds localization and management due to its ability to obtain the images of the entire agricultural field with a very high spatial resolution and at low cost. Despite the important advances in UAV acquisition systems, automatic weeds detection remains a challenging problem because of its strong similarity with the crops. Recently Deep Learning approach has shown impressive results in different complex classification problem. However, this approach needs a certain amount of training data but, creating large agricultural datasets with pixel-level annotations by expert is an extremely time consuming task. In this paper, we propose a novel fully automatic learning method using Convolutional Neuronal Networks (CNNs) with unsupervised training dataset collection for weeds detection from UAV images. The proposed method consists in three main phases. First we automatically detect the crop lines and using them to identify the interline weeds. In the second phase, interline weeds are used to constitute the training dataset. Finally, we performed CNNs on this dataset to build a model able to detect the crop and weeds in the images. The results obtained are comparable to the traditional supervised training data labeling. The accuracy gaps are 1.5% in the spinach field and 6% in the bean field.
ER  -


TY  - Preprint
T1  - Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update
A1  - Su Young Lee
A1  - Sungik Choi
A1  - Sae-Young Chung
JO  - ArXiv e-prints
Y1  - 31 May, 2018
UR  - https://arxiv.org/abs/1805.12375
N2  - We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by a fast reward propagation. In contrast to the conventional use of the experience replay with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state to its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate efficiently through all transitions of a sampled episode. We evaluate our algorithm on 2D MNIST Maze environment and 49 games of the Atari 2600 environment and show that our method improves sample efficiency with a competitive amount of computational cost.
ER  -


TY  - Preprint
T1  - Collaborative Multi-modal deep learning for the personalized product retrieval in Facebook Marketplace
A1  - Lu Zheng
A1  - Zhao Tan
A1  - Kun Han
A1  - Ren Mao
JO  - ArXiv e-prints
Y1  - 30 May, 2018
UR  - https://arxiv.org/abs/1805.12312
N2  - Facebook Marketplace is quickly gaining momentum among consumers as a favored customer-to-customer (C2C) product trading platform. The recommendation system behind it helps to significantly improve the user experience. Building the recommendation system for Facebook Marketplace is challenging for two reasons: 1) Scalability: the number of products in Facebook Marketplace is huge. Tens of thousands of products need to be scored and recommended within a couple hundred milliseconds for millions of users every day; 2) Cold start: the life span of the C2C products is very short and the user activities on the products are sparse. Thus it is difficult to accumulate enough product level signals for recommendation and we are facing a significant cold start issue. In this paper, we propose to address both the scalability and the cold-start issue by building a collaborative multi-modal deep learning based retrieval system where the compact embeddings for the users and the products are trained with the multi-modal content information. This system shows significant improvement over the benchmark in online and off-line experiments: In the online experiment, it increases the number of messages initiated by the buyer to the seller by +26.95%; in the off-line experiment, it improves the prediction accuracy by +9.58%.
ER  -


TY  - Preprint
T1  - Deep Segment Hash Learning for Music Generation
A1  - Kevin Joslyn
A1  - Naifan Zhuang
A1  - Kien A. Hua
JO  - ArXiv e-prints
Y1  - 30 May, 2018
UR  - https://arxiv.org/abs/1805.12176
N2  - Music generation research has grown in popularity over the past decade, thanks to the deep learning revolution that has redefined the landscape of artificial intelligence. In this paper, we propose a novel approach to music generation inspired by musical segment concatenation methods and hash learning algorithms. Given a segment of music, we use a deep recurrent neural network and ranking-based hash learning to assign a forward hash code to the segment to retrieve candidate segments for continuation with matching backward hash codes. The proposed method is thus called Deep Segment Hash Learning (DSHL). To the best of our knowledge, DSHL is the first end-to-end segment hash learning method for music generation, and the first to use pair-wise training with segments of music. We demonstrate that this method is capable of generating music which is both original and enjoyable, and that DSHL offers a promising new direction for music generation research.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models
A1  - Kurtland Chua
A1  - Roberto Calandra
A1  - Rowan McAllister
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 30 May, 2018
UR  - https://arxiv.org/abs/1805.12114
N2  - Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance, especially those with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g. 25 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).
ER  -


TY  - Preprint
T1  - Counterstrike: Defending Deep Learning Architectures Against Adversarial Samples by Langevin Dynamics with Supervised Denoising Autoencoder
A1  - Vignesh Srinivasan
A1  - Arturo Marban
A1  - Klaus-Robert MÃ¼ller
A1  - Wojciech Samek
A1  - Shinichi Nakajima
JO  - ArXiv e-prints
Y1  - 30 May, 2018
UR  - https://arxiv.org/abs/1805.12017
N2  - Adversarial attacks on deep learning models have been demonstrated to be imperceptible to a human, while decreasing the model performance considerably. Attempts to provide invariance against such attacks have denoised adversarial samples to only send cleaned samples to the classifier. In a similar spirit this paper proposes a novel effective strategy that allows to relax adversarial samples onto the underlying manifold of the (unknown) target class distribution. Specifically, given an off-manifold adversarial example, our Metroplis-adjusted Langevin algorithm (Mala) guided through a supervised denoising autoencoder network (sDAE) allows to drive the adversarial samples towards high density regions of the data generating distribution. So, in a nutshell the adversarial example is transformed back from off-manifold onto the data manifold for which the learning model was originally trained and where it can perform well and robustly. Experiments on various benchmark datasets show that our novel Malade method exhibits a high robustness against blackbox and whitebox attacks and outperforms state-of-the-art defense algorithms.
ER  -


TY  - Preprint
T1  - Automatic Large-Scale Data Acquisition via Crowdsourcing for Crosswalk Classification: A Deep Learning Approach
A1  - Rodrigo F. Berriel
A1  - Franco Schmidt Rossi
A1  - Alberto F. de Souza
A1  - Thiago Oliveira-Santos
JO  - ArXiv e-prints
Y1  - 30 May, 2018
UR  - https://arxiv.org/abs/1805.11970
N2  - Correctly identifying crosswalks is an essential task for the driving activity and mobility autonomy. Many crosswalk classification, detection and localization systems have been proposed in the literature over the years. These systems use different perspectives to tackle the crosswalk classification problem: satellite imagery, cockpit view (from the top of a car or behind the windshield), and pedestrian perspective. Most of the works in the literature are designed and evaluated using small and local datasets, i.e. datasets that present low diversity. Scaling to large datasets imposes a challenge for the annotation procedure. Moreover, there is still need for cross-database experiments in the literature because it is usually hard to collect the data in the same place and conditions of the final application. In this paper, we present a crosswalk classification system based on deep learning. For that, crowdsourcing platforms, such as OpenStreetMap and Google Street View, are exploited to enable automatic training via automatic acquisition and annotation of a large-scale database. Additionally, this work proposes a comparison study of models trained using fully-automatic data acquisition and annotation against models that were partially annotated. Cross-database experiments were also included in the experimentation to show that the proposed methods enable use with real world applications. Our results show that the model trained on the fully-automatic database achieved high overall accuracy (94.12%), and that a statistically significant improvement (to 96.30%) can be achieved by manually annotating a specific part of the database. Finally, the results of the cross-database experiments show that both models are robust to the many variations of image and scenarios, presenting a consistent behavior.
ER  -


TY  - Preprint
T1  - K-Beam Minimax: Efficient Optimization for Deep Adversarial Learning
A1  - Jihun Hamm
A1  - Yung-Kyun Noh
JO  - ArXiv e-prints
Y1  - 6 June, 2018
UR  - https://arxiv.org/abs/1805.11640
N2  - Minimax optimization plays a key role in adversarial training of machine learning algorithms, such as learning generative models, domain adaptation, privacy preservation, and robust learning. In this paper, we demonstrate the failure of alternating gradient descent in minimax optimization problems due to the discontinuity of solutions of the inner maximization. To address this, we propose a new epsilon-subgradient descent algorithm that addresses this problem by simultaneously tracking K candidate solutions. Practically, the algorithm can find solutions that previous saddle-point algorithms cannot find, with only a sublinear increase of complexity in K. We analyze the conditions under which the algorithm converges to the true solution in detail. A significant improvement in stability and convergence speed of the algorithm is observed in simple representative problems, GAN training, and domain-adaptation problems.
ER  -


TY  - Preprint
T1  - Deep Learning under Privileged Information Using Heteroscedastic Dropout
A1  - John Lambert
A1  - Ozan Sener
A1  - Silvio Savarese
JO  - ArXiv e-prints
Y1  - 29 May, 2018
UR  - https://arxiv.org/abs/1805.11614
N2  - Unlike machines, humans learn through rapid, abstract model-building. The role of a teacher is not simply to hammer home right or wrong answers, but rather to provide intuitive comments, comparisons, and explanations to a pupil. This is what the Learning Under Privileged Information (LUPI) paradigm endeavors to model by utilizing extra knowledge only available during training. We propose a new LUPI algorithm specifically designed for Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use a heteroscedastic dropout (i.e. dropout with a varying variance) and make the variance of the dropout a function of privileged information. Intuitively, this corresponds to using the privileged information to control the uncertainty of the model output. We perform experiments using CNNs and RNNs for the tasks of image classification and machine translation. Our method significantly increases the sample efficiency during learning, resulting in higher accuracy with a large margin when the number of training examples is limited. We also theoretically justify the gains in sample efficiency by providing a generalization error bound decreasing with $O(\frac{1}{n})$, where $n$ is the number of training examples, in an oracle case.
ER  -


TY  - Preprint
T1  - NengoDL: Combining deep learning and neuromorphic modelling methods
A1  - Daniel Rasmussen
JO  - ArXiv e-prints
Y1  - 29 May, 2018
UR  - https://arxiv.org/abs/1805.11144
N2  - NengoDL is a software framework designed to combine the strengths of neuromorphic modelling and deep learning. NengoDL allows users to construct biologically detailed neural models, intermix those models with deep learning elements (such as convolutional networks), and then efficiently simulate those models in an easy-to-use, unified framework. In addition, NengoDL allows users to apply deep learning training methods to optimize the parameters of biological neural models. In this paper we present basic usage examples, benchmarking, and details on the key implementation elements of NengoDL. More details can be found at https://www.nengo.ai/nengo-dl .
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning in Ice Hockey for Context-Aware Player Evaluation
A1  - Guiliang Liu
A1  - Oliver Schulte
JO  - ArXiv e-prints
Y1  - 16 July, 2018
UR  - https://arxiv.org/abs/1805.11088
N2  - A variety of machine learning models have been proposed to assess the performance of players in professional sports. However, they have only a limited ability to model how player performance depends on the game context. This paper proposes a new approach to capturing game context: we apply Deep Reinforcement Learning (DRL) to learn an action-value Q function from 3M play-by-play events in the National Hockey League (NHL). The neural network representation integrates both continuous context signals and game history, using a possession-based LSTM. The learned Q-function is used to value players&#39; actions under different game contexts. To assess a player&#39;s overall performance, we introduce a novel Game Impact Metric (GIM) that aggregates the values of the player&#39;s actions. Empirical Evaluation shows GIM is consistent throughout a play season, and correlates highly with standard success measures and future salary.
ER  -


TY  - Preprint
T1  - Hierarchical clustering with deep Q-learning
A1  - Richard Forster
A1  - Agnes Fulop
JO  - ArXiv e-prints
Y1  - 28 May, 2018
UR  - https://arxiv.org/abs/1805.10900
N2  - The reconstruction and analyzation of high energy particle physics data is just as important as the analyzation of the structure in real world networks. In a previous study it was explored how hierarchical clustering algorithms can be combined with kt cluster algorithms to provide a more generic clusterization method. Building on that, this paper explores the possibilities to involve deep learning in the process of cluster computation, by applying reinforcement learning techniques. The result is a model, that by learning on a modest dataset of 10; 000 nodes during 70 epochs can reach 83; 77% precision in predicting the appropriate clusters.
ER  -


TY  - Preprint
T1  - Legal Document Retrieval using Document Vector Embeddings and Deep Learning
A1  - Keet Sugathadasa
A1  - Buddhi Ayesha
A1  - Nisansa de Silva
A1  - Amal Shehan Perera
A1  - Vindula Jayawardana
A1  - Dimuthu Lakmal
A1  - Madhavi Perera
JO  - ArXiv e-prints
Y1  - 27 May, 2018
UR  - https://arxiv.org/abs/1805.10685
N2  - Domain specific information retrieval process has been a prominent and ongoing research in the field of natural language processing. Many researchers have incorporated different techniques to overcome the technical and domain specificity and provide a mature model for various domains of interest. The main bottleneck in these studies is the heavy coupling of domain experts, that makes the entire process to be time consuming and cumbersome. In this study, we have developed three novel models which are compared against a golden standard generated via the on line repositories provided, specifically for the legal domain. The three different models incorporated vector space representations of the legal domain, where document vector generation was done in two different mechanisms and as an ensemble of the above two. This study contains the research being carried out in the process of representing legal case documents into different vector spaces, whilst incorporating semantic word measures and natural language processing techniques. The ensemble model built in this study, shows a significantly higher accuracy level, which indeed proves the need for incorporation of domain specific semantic similarity measures into the information retrieval process. This study also shows, the impact of varying distribution of the word similarity measures, against varying document vector dimensions, which can lead to improvements in the process of legal information retrieval.
ER  -


TY  - Preprint
T1  - Deployment of Customized Deep Learning based Video Analytics On Surveillance Cameras
A1  - Pratik Dubal
A1  - Rohan Mahadev
A1  - Suraj Kothawade
A1  - Kunal Dargan
A1  - Rishabh Iyer
JO  - ArXiv e-prints
Y1  - 27 June, 2018
UR  - https://arxiv.org/abs/1805.10604
N2  - This paper demonstrates the effectiveness of our customized deep learning based video analytics system in various applications focused on security, safety, customer analytics and process compliance. We describe our video analytics system comprising of Search, Summarize, Statistics and real-time alerting, and outline its building blocks. These building blocks include object detection, tracking, face detection and recognition, human and face sub-attribute analytics. In each case, we demonstrate how custom models trained using data from the deployment scenarios provide considerably superior accuracies than off-the-shelf models. Towards this end, we describe our data processing and model training pipeline, which can train and fine-tune models from videos with a quick turnaround time. Finally, since most of these models are deployed on-site, it is important to have resource constrained models which do not require GPUs. We demonstrate how we custom train resource constrained models and deploy them on embedded devices without significant loss in accuracy. To our knowledge, this is the first work which provides a comprehensive evaluation of different deep learning models on various real-world customer deployment scenarios of surveillance video analytics. By sharing our implementation details and the experiences learned from deploying customized deep learning models for various customers, we hope that customized deep learning based video analytics is widely incorporated in commercial products around the world.
ER  -


TY  - Preprint
T1  - Transductive Label Augmentation for Improved Deep Network Learning
A1  - Ismail Elezi
A1  - Alessandro Torcinovich
A1  - Sebastiano Vascon
A1  - Marcello Pelillo
JO  - ArXiv e-prints
Y1  - 26 May, 2018
UR  - https://arxiv.org/abs/1805.10546
N2  - A major impediment to the application of deep learning to real-world problems is the scarcity of labeled data. Small training sets are in fact of no use to deep networks as, due to the large number of trainable parameters, they will very likely be subject to overfitting phenomena. On the other hand, the increment of the training set size through further manual or semi-automatic labellings can be costly, if not possible at times. Thus, the standard techniques to address this issue are transfer learning and data augmentation, which consists of applying some sort of &#34;transformation&#34; to existing labeled instances to let the training set grow in size. Although this approach works well in applications such as image classification, where it is relatively simple to design suitable transformation operators, it is not obvious how to apply it in more structured scenarios. Motivated by the observation that in virtually all application domains it is easy to obtain unlabeled data, in this paper we take a different perspective and propose a \emph{label augmentation} approach. We start from a small, curated labeled dataset and let the labels propagate through a larger set of unlabeled data using graph transduction techniques. This allows us to naturally use (second-order) similarity information which resides in the data, a source of information which is typically neglected by standard augmentation techniques. In particular, we show that by using known game theoretic transductive processes we can create larger and accurate enough labeled datasets which use results in better trained neural networks. Preliminary experiments are reported which demonstrate a consistent improvement over standard image classification datasets.
ER  -


TY  - Preprint
T1  - L1-(2D)2PCANet: A Deep Learning Network for Face Recognition
A1  - YunKun Li
A1  - XiaoJun Wu
A1  - Josef Kittler
JO  - ArXiv e-prints
Y1  - 26 May, 2018
UR  - https://arxiv.org/abs/1805.10476
N2  - In this paper, we propose a novel deep learning network L1-(2D)2PCANet for face recognition, which is based on L1-norm-based two-directional two-dimensional principal component analysis (L1-(2D)2PCA). In our network, the role of L1-(2D)2PCA is to learn the filters of multiple convolution layers. After the convolution layers, we deploy binary hashing and block-wise histogram for pooling. We test our network on some benchmark facial datasets YALE, AR, Extended Yale B, LFW-a and FERET with CNN, PCANet, 2DPCANet and L1-PCANet as comparison. The results show that the recognition performance of L1-(2D)2PCANet in all tests is better than baseline networks, especially when there are outliers in the test data. Owing to the L1-norm, L1-2D2PCANet is robust to outliers and changes of the training images.
ER  -


TY  - Preprint
T1  - Geometric Understanding of Deep Learning
A1  - Na Lei
A1  - Zhongxuan Luo
A1  - Shing-Tung Yau
A1  - David Xianfeng Gu
JO  - ArXiv e-prints
Y1  - 30 May, 2018
UR  - https://arxiv.org/abs/1805.10451
N2  - Deep learning is the mainstream technique for many machine learning tasks, including image recognition, machine translation, speech recognition, and so on. It has outperformed conventional methods in various fields and achieved great successes. Unfortunately, the understanding on how it works remains unclear. It has the central importance to lay down the theoretic foundation for deep learning.
ER  -


TY  - Preprint
T1  - Semi-supervised Deep Kernel Learning: Regression with Unlabeled Data by Minimizing Predictive Variance
A1  - Neal Jean
A1  - Sang Michael Xie
A1  - Stefano Ermon
JO  - ArXiv e-prints
Y1  - 25 May, 2018
UR  - https://arxiv.org/abs/1805.10407
N2  - Large amounts of labeled data are typically required to train deep learning models. For many real-world problems, however, acquiring additional data can be expensive or even impossible. We present semi-supervised deep kernel learning (SSDKL), a semi-supervised regression model based on minimizing predictive variance in the posterior regularization framework. SSDKL combines the hierarchical representation learning of neural networks with the probabilistic modeling capabilities of Gaussian processes. By leveraging unlabeled data, we show improvements on a diverse set of real-world regression tasks over supervised deep kernel learning and semi-supervised methods such as VAT and mean teacher adapted for regression.
ER  -


TY  - Preprint
T1  - Underwater Fish Species Classification using Convolutional Neural Network and Deep Learning
A1  - Dhruv Rathi
A1  - Sushant Jain
A1  - Dr. S. Indu
JO  - ArXiv e-prints
Y1  - 25 May, 2018
UR  - https://arxiv.org/abs/1805.10106
N2  - The target of this paper is to recommend a way for Automated classification of Fish species. A high accuracy fish classification is required for greater understanding of fish behavior in Ichthyology and by marine biologists. Maintaining a ledger of the number of fishes per species and marking the endangered species in large and small water bodies is required by concerned institutions. Majority of available methods focus on classification of fishes outside of water because underwater classification poses challenges such as background noises, distortion of images, the presence of other water bodies in images, image quality and occlusion. This method uses a novel technique based on Convolutional Neural Networks, Deep Learning and Image Processing to achieve an accuracy of 96.29%. This method ensures considerably discrimination accuracy improvements than the previously proposed methods.
ER  -


TY  - Preprint
T1  - A Double-Deep Spatio-Angular Learning Framework for Light Field based Face Recognition
A1  - Alireza Sepas-Moghaddam
A1  - Mohammad A. Haque
A1  - Paulo Lobato Correia
A1  - Kamal Nasrollahi
A1  - Thomas B. Moeslund
A1  - Fernando Pereira
JO  - ArXiv e-prints
Y1  - 9 October, 2018
UR  - https://arxiv.org/abs/1805.10078
N2  - Face recognition has attracted increasing attention due to its wide range of applications, but it is still challenging when facing large variations in the biometric data characteristics. Lenslet light field cameras have recently come into prominence to capture rich spatio-angular information, thus offering new possibilities for advanced biometric recognition systems. This paper proposes a double-deep spatio-angular learning framework for light field based face recognition, which is able to learn both texture and angular dynamics in sequence using convolutional representations; this is a novel recognition framework that has never been proposed before for either face recognition or any other visual recognition task. The proposed double-deep learning framework includes a long short-term memory (LSTM) recurrent network whose inputs are VGG-Face descriptions that are computed using a VGG-Very-Deep-16 convolutional neural network (CNN). The VGG-16 network uses different face viewpoints rendered from a full light field image, which are organised as a pseudo-video sequence. A comprehensive set of experiments has been conducted with the IST-EURECOM light field face database, for varied and challenging recognition tasks. Results show that the proposed framework achieves superior face recognition performance when compared to the state-of-the-art.
ER  -


TY  - Preprint
T1  - Deep Functional Dictionaries: Learning Consistent Semantic Structures on 3D Models from Functions
A1  - Minhyuk Sung
A1  - Hao Su
A1  - Ronald Yu
A1  - Leonidas Guibas
JO  - ArXiv e-prints
Y1  - 24 May, 2018
UR  - https://arxiv.org/abs/1805.09957
N2  - Various 3D semantic attributes such as segmentation masks, geometric features, keypoints, and materials can be encoded as per-point probe functions on 3D geometries. Given a collection of related 3D shapes, we consider how to jointly analyze such probe functions over different shapes, and how to discover common latent structures using a neural network --- even in the absence of any correspondence information. Our network is trained on point cloud representations of shape geometry and associated semantic functions on that point cloud. These functions express a shared semantic understanding of the shapes but are not coordinated in any way. For example, in a segmentation task, the functions can be indicator functions of arbitrary sets of shape parts, with the particular combination involved not known to the network. Our network is able to produce a small dictionary of basis functions for each shape, a dictionary whose span includes the semantic functions provided for that shape. Even though our shapes have independent discretizations and no functional correspondences are provided, the network is able to generate latent bases, in a consistent order, that reflect the shared semantic structure among the shapes. We demonstrate the effectiveness of our technique in various segmentation and keypoint selection applications.
ER  -


TY  - Preprint
T1  - Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning
A1  - Pengda Qin
A1  - Weiran Xu
A1  - William Yang Wang
JO  - ArXiv e-prints
Y1  - 24 May, 2018
UR  - https://arxiv.org/abs/1805.09927
N2  - Distant supervision has become the standard method for relation extraction. However, even though it is an efficient method, it does not come at no cost---The resulted distantly-supervised training samples are often very noisy. To combat the noise, most of the recent state-of-the-art approaches focus on selecting one-best sentence or calculating soft attention weights over the set of the sentences of one specific entity pair. However, these methods are suboptimal, and the false positive problem is still a key stumbling bottleneck for the performance. We argue that those incorrectly-labeled candidate sentences must be treated with a hard decision, rather than being dealt with soft attention weights. To do this, our paper describes a radical solution---We explore a deep reinforcement learning strategy to generate the false-positive indicator, where we automatically recognize false positives for each relation type without any supervised information. Unlike the removal operation in the previous studies, we redistribute them into the negative examples. The experimental results show that the proposed strategy significantly improves the performance of distant supervision comparing to state-of-the-art systems.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning For Sequence to Sequence Models
A1  - Yaser Keneshloo
A1  - Tian Shi
A1  - Naren Ramakrishnan
A1  - Chandan K. Reddy
JO  - ArXiv e-prints
Y1  - 20 July, 2018
UR  - https://arxiv.org/abs/1805.09461
N2  - In recent times, sequence-to-sequence (seq2seq) models have gained a lot of popularity and provide state-of-the-art performance in a wide variety of tasks such as machine translation, headline generation, text summarization, speech to text conversion, and image caption generation. The underlying framework for all these models is usually a deep neural network comprising an encoder and a decoder. Although simple encoder-decoder models produce competitive results, many researchers have proposed additional improvements over these sequence-to-sequence models, e.g., using an attention-based model over the input, pointer-generation models, and self-attention models. However, such seq2seq models suffer from two common problems: 1) exposure bias and 2) inconsistency between train/test measurement. Recently, a completely novel point of view has emerged in addressing these two problems in seq2seq models, leveraging methods from reinforcement learning (RL). In this survey, we consider seq2seq problems from the RL point of view and provide a formulation combining the power of RL methods in decision-making with sequence-to-sequence models that enable remembering long-term memories. We present some of the most recent frameworks that combine concepts from RL and deep neural networks and explain how these two areas could benefit from each other in solving complex seq2seq tasks. Our work aims to provide insights into some of the problems that inherently arise with current approaches and how we can address them with better RL models. We also provide the source code for implementing most of the RL models discussed in this paper to support the complex task of abstractive text summarization.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning of Marked Temporal Point Processes
A1  - Utkarsh Upadhyay
A1  - Abir De
A1  - Manuel Gomez-Rodriguez
JO  - ArXiv e-prints
Y1  - 23 May, 2018
UR  - https://arxiv.org/abs/1805.09360
N2  - In a wide variety of applications, humans interact with a complex environment by means of asynchronous stochastic discrete events in continuous time. Can we design online interventions that will help humans achieve certain goals in such asynchronous setting? In this paper, we address the above problem from the perspective of deep reinforcement learning of marked temporal point processes, where both the actions taken by an agent and the feedback it receives from the environment are asynchronous stochastic discrete events characterized using marked temporal point processes. In doing so, we define the agent&#39;s policy using the intensity and mark distribution of the corresponding process and then derive a flexible policy gradient method, which embeds the agent&#39;s actions and the feedback it receives into real-valued vectors using deep recurrent neural networks. Our method does not make any assumptions on the functional form of the intensity and mark distribution of the feedback and it allows for arbitrarily complex reward functions. We apply our methodology to two different applications in personalized teaching and viral marketing and, using data gathered from Duolingo and Twitter, we show that it may be able to find interventions to help learners and marketers achieve their goals more effectively than alternatives.
ER  -


TY  - Preprint
T1  - Toward a Thinking Microscope: Deep Learning in Optical Microscopy and Image Reconstruction
A1  - Yair Rivenson
A1  - Aydogan Ozcan
JO  - ArXiv e-prints
Y1  - 23 May, 2018
UR  - https://arxiv.org/abs/1805.08970
N2  - We discuss recently emerging applications of the state-of-art deep learning methods on optical microscopy and microscopic image reconstruction, which enable new transformations among different modes and modalities of microscopic imaging, driven entirely by image data. We believe that deep learning will fundamentally change both the hardware and image reconstruction methods used in optical microscopy in a holistic manner.
ER  -


TY  - Preprint
T1  - Step Size Matters in Deep Learning
A1  - Kamil Nar
A1  - S. Shankar Sastry
JO  - ArXiv e-prints
Y1  - 9 October, 2018
UR  - https://arxiv.org/abs/1805.08890
N2  - Training a neural network with the gradient descent algorithm gives rise to a discrete-time nonlinear dynamical system. Consequently, behaviors that are typically observed in these systems emerge during training, such as convergence to an orbit but not to a fixed point or dependence of convergence on the initialization. Step size of the algorithm plays a critical role in these behaviors: it determines the subset of the local optima that the algorithm can converge to, and it specifies the magnitude of the oscillations if the algorithm converges to an orbit. To elucidate the effects of the step size on training of neural networks, we study the gradient descent algorithm as a discrete-time dynamical system, and by analyzing the Lyapunov stability of different solutions, we show the relationship between the step size of the algorithm and the solutions that can be obtained with this algorithm. The results provide an explanation for several phenomena observed in practice, including the deterioration in the training error with increased depth, the hardness of estimating linear mappings with large singular values, and the distinct performance of deep residual networks.
ER  -


TY  - Preprint
T1  - Scalable Centralized Deep Multi-Agent Reinforcement Learning via Policy Gradients
A1  - Arbaaz Khan
A1  - Clark Zhang
A1  - Daniel D. Lee
A1  - Vijay Kumar
A1  - Alejandro Ribeiro
JO  - ArXiv e-prints
Y1  - 21 May, 2018
UR  - https://arxiv.org/abs/1805.08776
N2  - In this paper, we explore using deep reinforcement learning for problems with multiple agents. Most existing methods for deep multi-agent reinforcement learning consider only a small number of agents. When the number of agents increases, the dimensionality of the input and control spaces increase as well, and these methods do not scale well. To address this, we propose casting the multi-agent reinforcement learning problem as a distributed optimization problem. Our algorithm assumes that for multi-agent settings, policies of individual agents in a given population live close to each other in parameter space and can be approximated by a single policy. With this simple assumption, we show our algorithm to be extremely effective for reinforcement learning in multi-agent settings. We demonstrate its effectiveness against existing comparable approaches on co-operative and competitive tasks.
ER  -


TY  - Preprint
T1  - Sparse Binary Compression: Towards Distributed Deep Learning with minimal Communication
A1  - Felix Sattler
A1  - Simon Wiedemann
A1  - Klaus-Robert MÃ¼ller
A1  - Wojciech Samek
JO  - ArXiv e-prints
Y1  - 22 May, 2018
UR  - https://arxiv.org/abs/1805.08768
N2  - Currently, progressively larger deep neural networks are trained on ever growing data corpora. As this trend is only going to increase in the future, distributed training schemes are becoming increasingly relevant. A major issue in distributed training is the limited communication bandwidth between contributing nodes or prohibitive communication cost in general. These challenges become even more pressing, as the number of computation nodes increases. To counteract this development we propose sparse binary compression (SBC), a compression framework that allows for a drastic reduction of communication cost for distributed training. SBC combines existing techniques of communication delay and gradient sparsification with a novel binarization method and optimal weight update encoding to push compression gains to new limits. By doing so, our method also allows us to smoothly trade-off gradient sparsity and temporal sparsity to adapt to the requirements of the learning task. Our experiments show, that SBC can reduce the upstream communication on a variety of convolutional and recurrent neural network architectures by more than four orders of magnitude without significantly harming the convergence speed in terms of forward-backward passes. For instance, we can train ResNet50 on ImageNet in the same number of iterations to the baseline accuracy, using $\times 3531$ less bits or train it to a $1\%$ lower accuracy using $\times 37208$ less bits. In the latter case, the total upstream communication required is cut from 125 terabytes to 3.35 gigabytes for every participating client.
ER  -


TY  - Preprint
T1  - Image Based Fashion Product Recommendation with Deep Learning
A1  - Hessel Tuinhof
A1  - Clemens Pirker
A1  - Markus Haltmeier
JO  - ArXiv e-prints
Y1  - 17 July, 2018
UR  - https://arxiv.org/abs/1805.08694
N2  - We develop a two-stage deep learning framework that recommends fashion images based on other input images of similar style. For that purpose, a neural network classifier is used as a data-driven, visually-aware feature extractor. The latter then serves as input for similarity-based recommendations using a ranking algorithm. Our approach is tested on the publicly available Fashion dataset. Initialization strategies using transfer learning from larger product databases are presented. Combined with more traditional content-based recommendation systems, our framework can help to increase robustness and performance, for example, by better matching a particular customer style.
ER  -


TY  - Preprint
T1  - High throughput quantitative metallography for complex microstructures using deep learning: A case study in ultrahigh carbon steel
A1  - Brian L. DeCost
A1  - Toby Francis
A1  - Elizabeth A. Holm
JO  - ArXiv e-prints
Y1  - 4 May, 2018
UR  - https://arxiv.org/abs/1805.08693
N2  - We apply a deep convolutional neural network segmentation model to enable novel automated microstructure segmentation applications for complex microstructures typically evaluated manually and subjectively. We explore two microstructure segmentation tasks in an openly-available ultrahigh carbon steel microstructure dataset: segmenting cementite particles in the spheroidized matrix, and segmenting larger fields of view featuring grain boundary carbide, spheroidized particle matrix, particle-free grain boundary denuded zone, and WidmanstÃ¤tten cementite. We also demonstrate how to combine these data-driven microstructure segmentation models to obtain empirical cementite particle size and denuded zone width distributions from more complex micrographs containing multiple microconstituents. The full annotated dataset is available on materialsdata.nist.gov (https://materialsdata.nist.gov/handle/11256/964).
ER  -


TY  - Preprint
T1  - Assessing a mobile-based deep learning model for plant disease surveillance
A1  - Amanda Ramcharan
A1  - Peter McCloskey
A1  - Kelsee Baranowski
A1  - Neema Mbilinyi
A1  - Latifa Mrisho
A1  - Mathias Ndalahwa
A1  - James Legg
A1  - David Hughes
JO  - ArXiv e-prints
Y1  - 4 May, 2018
UR  - https://arxiv.org/abs/1805.08692
N2  - Convolutional neural network models (CNNs) have made major advances in computer vision tasks in the last five years. Given the challenge in collecting real world datasets, most studies report performance metrics based on available research datasets. In scenarios where CNNs are to be deployed on images or videos from mobile devices, models are presented with new challenges due to lighting, angle, and camera specifications, which are not accounted for in research datasets. It is essential for assessment to also be conducted on real world datasets if such models are to be reliably integrated with products and services in society. Plant disease datasets can be used to test CNNs in real time and gain insight into real world performance. We train a CNN object detection model to identify foliar symptoms of diseases (or lack thereof) in cassava (Manihot esculenta Crantz). We then deploy the model on a mobile app and test its performance on mobile images and video of 720 diseased leaflets in an agricultural field in Tanzania. Within each disease category we test two levels of severity of symptoms - mild and pronounced, to assess the model performance for early detection of symptoms. In both severities we see a decrease in the F-1 score for real world images and video. The F-1 score dropped by 32% for pronounced symptoms in real world images (the closest data to the training data) due to a drop in model recall. If the potential of smartphone CNNs are to be realized our data suggest it is crucial to consider tuning precision and recall performance in order to achieve the desired performance in real world settings. In addition, the varied performance related to different input data (image or video) is an important consideration for the design of CNNs in real world applications.
ER  -


TY  - Preprint
T1  - Deep Learning Inference on Embedded Devices: Fixed-Point vs Posit
A1  - Seyed H. F. Langroudi
A1  - Tej Pandit
A1  - Dhireesha Kudithipudi
JO  - ArXiv e-prints
Y1  - 22 May, 2018
UR  - https://arxiv.org/abs/1805.08624
N2  - Performing the inference step of deep learning in resource constrained environments, such as embedded devices, is challenging. Success requires optimization at both software and hardware levels. Low precision arithmetic and specifically low precision fixed-point number systems have become the standard for performing deep learning inference. However, representing non-uniform data and distributed parameters (e.g. weights) by using uniformly distributed fixed-point values is still a major drawback when using this number system. Recently, the posit number system was proposed, which represents numbers in a non-uniform manner. Therefore, in this paper we are motivated to explore using the posit number system to represent the weights of Deep Convolutional Neural Networks. However, we do not apply any quantization techniques and hence the network weights do not require re-training. The results of this exploration show that using the posit number system outperformed the fixed point number system in terms of accuracy and memory utilization.
ER  -


TY  - Preprint
T1  - Meta-Learning with Hessian-Free Approach in Deep Neural Nets Training
A1  - Boyu Chen
A1  - Wenlian Lu
A1  - Ernest Fokoue
JO  - ArXiv e-prints
Y1  - 7 September, 2018
UR  - https://arxiv.org/abs/1805.08462
N2  - Meta-learning is a promising method to achieve efficient training method towards deep neural net and has been attracting increases interests in recent years. But most of the current methods are still not capable to train complex neuron net model with long-time training process. In this paper, a novel second-order meta-optimizer, named Meta-learning with Hessian-Free(MLHF) approach, is proposed based on the Hessian-Free approach. Two recurrent neural networks are established to generate the damping and the precondition matrix of this Hessian-Free framework. A series of techniques to meta-train the MLHF towards stable and reinforce the meta-training of this optimizer, including the gradient calculation of $H$. Numerical experiments on deep convolution neural nets, including CUDA-convnet and ResNet18(v2), with datasets of CIFAR10 and ILSVRC2012, indicate that the MLHF shows good and continuous training performance during the whole long-time training process, i.e., both the rapid-decreasing early stage and the steadily-deceasing later stage, and so is a promising meta-learning framework towards elevating the training efficiency in real-world deep neural nets.
ER  -


TY  - Preprint
T1  - RPC Considered Harmful: Fast Distributed Deep Learning on RDMA
A1  - Jilong Xue
A1  - Youshan Miao
A1  - Cheng Chen
A1  - Ming Wu
A1  - Lintao Zhang
A1  - Lidong Zhou
JO  - ArXiv e-prints
Y1  - 22 May, 2018
UR  - https://arxiv.org/abs/1805.08430
N2  - Deep learning emerges as an important new resource-intensive workload and has been successfully applied in computer vision, speech, natural language processing, and so on. Distributed deep learning is becoming a necessity to cope with growing data and model sizes. Its computation is typically characterized by a simple tensor data abstraction to model multi-dimensional matrices, a data-flow graph to model computation, and iterative executions with relatively frequent synchronizations, thereby making it substantially different from Map/Reduce style distributed big data computation.
ER  -


TY  - Preprint
T1  - Adapted Deep Embeddings: A Synthesis of Methods for $k$-Shot Inductive Transfer Learning
A1  - Tyler R. Scott
A1  - Karl Ridgeway
A1  - Michael C. Mozer
JO  - ArXiv e-prints
Y1  - 18 August, 2018
UR  - https://arxiv.org/abs/1805.08402
N2  - The focus in machine learning has branched beyond training classifiers on a single task to investigating how previously acquired knowledge in a source domain can be leveraged to facilitate learning in a related target domain, known as inductive transfer learning. Three active lines of research have independently explored transfer learning using neural networks. In weight transfer, a model trained on the source domain is used as an initialization point for a network to be trained on the target domain. In deep metric learning, the source domain is used to construct an embedding that captures class structure in both the source and target domains. In few-shot learning, the focus is on generalizing well in the target domain based on a limited number of labeled examples. We compare state-of-the-art methods from these three paradigms and also explore hybrid adapted-embedding methods that use limited target-domain data to fine tune embeddings constructed from source-domain data. We conduct a systematic comparison of methods in a variety of domains, varying the number of labeled instances available in the target domain ($k$), as well as the number of target-domain classes. We reach three principal conclusions: (1) Deep embeddings are far superior, compared to weight transfer, as a starting point for inter-domain transfer or model re-use (2) Our hybrid methods robustly outperform every few-shot learning and every deep metric learning method previously proposed, with a mean error reduction of 30% over state-of-the-art. (3) Among loss functions for discovering embeddings, the histogram loss (Ustinova &amp; Lempitsky, 2016) is most robust. We hope our results will motivate a unification of research in weight transfer, deep metric learning, and few-shot learning.
ER  -


TY  - Preprint
T1  - Deep Learning with Cinematic Rendering: Fine-Tuning Deep Neural Networks Using Photorealistic Medical Images
A1  - Faisal Mahmood
A1  - Richard Chen
A1  - Sandra Sudarsky
A1  - Daphne Yu
A1  - Nicholas J. Durr
JO  - ArXiv e-prints
Y1  - 29 September, 2018
UR  - https://arxiv.org/abs/1805.08400
N2  - Deep learning has emerged as a powerful artificial intelligence tool to interpret medical images for a growing variety of applications. However, the paucity of medical imaging data with high-quality annotations that is necessary for training such methods ultimately limits their performance. Medical data is challenging to acquire due to privacy issues, shortage of experts available for annotation, limited representation of rare conditions and cost. This problem has previously been addressed by using synthetically generated data. However, networks trained on synthetic data often fail to generalize to real data. Cinematic rendering simulates the propagation and interaction of light passing through tissue models reconstructed from CT data, enabling the generation of photorealistic images. In this paper, we present one of the first applications of cinematic rendering in deep learning, in which we propose to fine-tune synthetic data-driven networks using cinematically rendered CT data for the task of monocular depth estimation in endoscopy. Our experiments demonstrate that: (a) Convolutional Neural Networks (CNNs) trained on synthetic data and fine-tuned on photorealistic cinematically rendered data adapt better to real medical images and demonstrate more robust performance when compared to networks with no fine-tuning, (b) these fine-tuned networks require less training data to converge to an optimal solution, and (c) fine-tuning with data from a variety of photorealistic rendering conditions of the same scene prevents the network from learning patient-specific information and aids in generalizability of the model. Our empirical evaluation demonstrates that networks fine-tuned with cinematically rendered data predict depth with 56.87% less error for rendered endoscopy images and 27.49% less error for real porcine colon endoscopy images.
ER  -


TY  - Preprint
T1  - Learning to Optimize via Wasserstein Deep Inverse Optimal Control
A1  - Yichen Wang
A1  - Le Song
A1  - Hongyuan Zha
JO  - ArXiv e-prints
Y1  - 22 May, 2018
UR  - https://arxiv.org/abs/1805.08395
N2  - We study the inverse optimal control problem in social sciences: we aim at learning a user&#39;s true cost function from the observed temporal behavior. In contrast to traditional phenomenological works that aim to learn a generative model to fit the behavioral data, we propose a novel variational principle and treat user as a reinforcement learning algorithm, which acts by optimizing his cost function. We first propose a unified KL framework that generalizes existing maximum entropy inverse optimal control methods. We further propose a two-step Wasserstein inverse optimal control framework. In the first step, we compute the optimal measure with a novel mass transport equation. In the second step, we formulate the learning problem as a generative adversarial network. In two real world experiments - recommender systems and social networks, we show that our framework obtains significant performance gains over both existing inverse optimal control methods and point process based generative models.
ER  -


TY  - Preprint
T1  - Opening the black box of deep learning
A1  - Dian Lei
A1  - Xiaoxiao Chen
A1  - Jianfei Zhao
JO  - ArXiv e-prints
Y1  - 21 May, 2018
UR  - https://arxiv.org/abs/1805.08355
N2  - The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physical system, examines deep learning from three different perspectives: microscopic, macroscopic, and physical world views, answers multiple theoretical puzzles in deep learning by using physics principles. For example, from the perspective of quantum mechanics and statistical physics, this dissertation presents the calculation methods for convolution calculation, pooling, normalization, and Restricted Boltzmann Machine, as well as the selection of cost functions, explains why deep learning must be deep, what characteristics are learned in deep learning, why Convolutional Neural Networks do not have to be trained layer by layer, and the limitations of deep learning, etc., and proposes the theoretical direction and basis for the further development of deep learning now and in the future. The brilliance of physics flashes in deep learning, we try to establish the deep learning technology based on the scientific theory of physics.
ER  -


TY  - Preprint
T1  - Small steps and giant leaps: Minimal Newton solvers for Deep Learning
A1  - JoÃ£o F. Henriques
A1  - Sebastien Ehrhardt
A1  - Samuel Albanie
A1  - Andrea Vedaldi
JO  - ArXiv e-prints
Y1  - 21 May, 2018
UR  - https://arxiv.org/abs/1805.08095
N2  - We propose a fast second-order method that can be used as a drop-in replacement for current deep learning solvers. Compared to stochastic gradient descent (SGD), it only requires two additional forward-mode automatic differentiation operations per iteration, which has a computational cost comparable to two standard forward passes and is easy to implement. Our method addresses long-standing issues with current second-order solvers, which invert an approximate Hessian matrix every iteration exactly or by conjugate-gradient methods, a procedure that is both costly and sensitive to noise. Instead, we propose to keep a single estimate of the gradient projected by the inverse Hessian matrix, and update it once per iteration. This estimate has the same size and is similar to the momentum variable that is commonly used in SGD. No estimate of the Hessian is maintained. We first validate our method, called CurveBall, on small problems with known closed-form solutions (noisy Rosenbrock function and degenerate 2-layer linear networks), where current deep learning solvers seem to struggle. We then train several large models on CIFAR and ImageNet, including ResNet and VGG-f networks, where we demonstrate faster convergence with no hyperparameter tuning. Code is available.
ER  -


TY  - Preprint
T1  - Variational based Mixed Noise Removal with CNN Deep Learning Regularization
A1  - Faqiang Wang
A1  - Haiyang Huang
A1  - Jun Liu
JO  - ArXiv e-prints
Y1  - 21 May, 2018
UR  - https://arxiv.org/abs/1805.08094
N2  - In this paper, the traditional model based variational method and learning based algorithms are naturally integrated to address mixed noise removal problem. To be different from single type noise (e.g. Gaussian) removal, it is a challenge problem to accurately discriminate noise types and levels for each pixel. We propose a variational method to iteratively estimate the noise parameters, and then the algorithm can automatically classify the noise according to the different statistical parameters. The proposed variational problem can be separated into regularization, synthesis, parameter estimation and noise classification four steps with the operator splitting scheme. Each step is related to an optimization subproblem. To enforce the regularization, the deep learning method is employed to learn the natural images priori. Compared with some model based regularizations, the CNN regularizer can significantly improve the quality of the restored images. Compared with some learning based methods, the synthesis step can produce better reconstructions by analyzing the recognized noise types and levels. In our method, the convolution neutral network (CNN) can be regarded as an operator which associated to a variational functional. From this viewpoint, the proposed method can be extended to many image reconstruction and inverse problems. Numerical experiments in the paper show that our method can achieve some state-of-the-art results for mixed noise removal.
ER  -


TY  - Preprint
T1  - SmoothOut: Smoothing Out Sharp Minima to Improve Generalization in Deep Learning
A1  - Wei Wen
A1  - Yandan Wang
A1  - Feng Yan
A1  - Cong Xu
A1  - Chunpeng Wu
A1  - Yiran Chen
A1  - Hai Li
JO  - ArXiv e-prints
Y1  - 1 September, 2018
UR  - https://arxiv.org/abs/1805.07898
N2  - In Deep Learning, Stochastic Gradient Descent (SGD) is usually selected as the training method because of its efficiency and scalability; however, recently, a problem in SGD gains research interest: sharp minima in Deep Neural Networks (DNNs) have poor generalization [1][2]; especially, large-batch SGD tends to converge to sharp minima. It becomes an open question whether escaping sharp minima can improve the generalization. To answer this question, we proposed SmoothOut to smooth out sharp minima in DNNs and thereby improve generalization. In a nutshell, SmoothOut perturbs multiple copies of the DNN by noise injection and averages these copies. Injecting noises to SGD is widely for exploration, but SmoothOut differs in lots of ways: (1) de-noising process is applied before parameter updating; (2) uniform noises are injected instead of Gaussian noises; (3) the goal is to obtain an auxiliary function without sharp minima for better generalization, instead of higher exploration. We prove that SmoothOut can eliminate sharp minima. Training multiple DNN copies is inefficient, we further propose a stochastic version of SmoothOut which only introduces the overhead of noise injecting and de-noising per batch. We prove that the Stochastic SmoothOut is an unbiased approximation of the original SmoothOut. In experiments on a variety of DNNs and datasets, SmoothOut consistently improve generalization in both small-batch and large-batch training on the top of state-of-the-art solutions. Our source code is in https://github.com/wenwei202/smoothout
ER  -


TY  - Preprint
T1  - Unsupervised Video Object Segmentation for Deep Reinforcement Learning
A1  - Vik Goel
A1  - Jameson Weng
A1  - Pascal Poupart
JO  - ArXiv e-prints
Y1  - 20 May, 2018
UR  - https://arxiv.org/abs/1805.07780
N2  - We present a new technique for deep reinforcement learning that automatically detects moving objects and uses the relevant information for action selection. The detection of moving objects is done in an unsupervised way by exploiting structure from motion. Instead of directly learning a policy from raw images, the agent first learns to detect and segment moving objects by exploiting flow information in video sequences. The learned representation is then used to focus the policy of the agent on the moving objects. Over time, the agent identifies which objects are critical for decision making and gradually builds a policy based on relevant moving objects. This approach, which we call Motion-Oriented REinforcement Learning (MOREL), is demonstrated on a suite of Atari games where the ability to detect moving objects reduces the amount of interaction needed with the environment to obtain a good policy. Furthermore, the resulting policy is more interpretable than policies that directly map images to actions or values with a black box neural network. We can gain insight into the policy by inspecting the segmentation and motion of each object detected by the agent. This allows practitioners to confirm whether a policy is making decisions based on sensible information.
ER  -


TY  - Preprint
T1  - DLBI: Deep learning guided Bayesian inference for structure reconstruction of super-resolution fluorescence microscopy
A1  - Yu Li
A1  - Fan Xu
A1  - Fa Zhang
A1  - Pingyong Xu
A1  - Mingshu Zhang
A1  - Ming Fan
A1  - Lihua Li
A1  - Xin Gao
A1  - Renmin Han
JO  - ArXiv e-prints
Y1  - 1 September, 2018
UR  - https://arxiv.org/abs/1805.07777
N2  - Super-resolution fluorescence microscopy, with a resolution beyond the diffraction limit of light, has become an indispensable tool to directly visualize biological structures in living cells at a nanometer-scale resolution. Despite advances in high-density super-resolution fluorescent techniques, existing methods still have bottlenecks, including extremely long execution time, artificial thinning and thickening of structures, and lack of ability to capture latent structures. Here we propose a novel deep learning guided Bayesian inference approach, DLBI, for the time-series analysis of high-density fluorescent images. Our method combines the strength of deep learning and statistical inference, where deep learning captures the underlying distribution of the fluorophores that are consistent with the observed time-series fluorescent images by exploring local features and correlation along time-axis, and statistical inference further refines the ultrastructure extracted by deep learning and endues physical meaning to the final image. Comprehensive experimental results on both real and simulated datasets demonstrate that our method provides more accurate and realistic local patch and large-field reconstruction than the state-of-the-art method, the 3B analysis, while our method is more than two orders of magnitude faster. The main program is available at https://github.com/lykaust15/DLBI
ER  -


TY  - Preprint
T1  - Learning Hierarchical Visual Representations in Deep Neural Networks Using Hierarchical Linguistic Labels
A1  - Joshua C. Peterson
A1  - Paul Soulos
A1  - Aida Nematzadeh
A1  - Thomas L. Griffiths
JO  - ArXiv e-prints
Y1  - 19 May, 2018
UR  - https://arxiv.org/abs/1805.07647
N2  - Modern convolutional neural networks (CNNs) are able to achieve human-level object classification accuracy on specific tasks, and currently outperform competing models in explaining complex human visual representations. However, the categorization problem is posed differently for these networks than for humans: the accuracy of these networks is evaluated by their ability to identify single labels assigned to each image. These labels often cut arbitrarily across natural psychological taxonomies (e.g., dogs are separated into breeds, but never jointly categorized as &#34;dogs&#34;), and bias the resulting representations. By contrast, it is common for children to hear both &#34;dog&#34; and &#34;Dalmatian&#34; to describe the same stimulus, helping to group perceptually disparate objects (e.g., breeds) into a common mental class. In this work, we train CNN classifiers with multiple labels for each image that correspond to different levels of abstraction, and use this framework to reproduce classic patterns that appear in human generalization behavior.
ER  -


TY  - Preprint
T1  - Long-term face tracking in the wild using deep learning
A1  - Kunlei Zhang
A1  - Elaheh Rashedi
A1  - Elaheh Barati
A1  - Xue-wen Chen
JO  - ArXiv e-prints
Y1  - 19 May, 2018
UR  - https://arxiv.org/abs/1805.07646
N2  - This paper investigates long-term face tracking of a specific person given his/her face image in a single frame as a query in a video stream. Through taking advantage of pre-trained deep learning models on big data, a novel system is developed for accurate video face tracking in the unconstrained environments depicting various people and objects moving in and out of the frame. In the proposed system, we present a detection-verification-tracking method (dubbed as &#39;DVT&#39;) which accomplishes the long-term face tracking task through the collaboration of face detection, face verification, and (short-term) face tracking. An offline trained detector based on cascaded convolutional neural networks localizes all faces appeared in the frames, and an offline trained face verifier based on deep convolutional neural networks and similarity metric learning decides if any face or which face corresponds to the queried person. An online trained tracker follows the face from frame to frame. When validated on a sitcom episode and a TV show, the DVT method outperforms tracking-learning-detection (TLD) and face-TLD in terms of recall and precision. The proposed system is also tested on many other types of videos and shows very promising results.
ER  -


TY  - Preprint
T1  - CapProNet: Deep Feature Learning via Orthogonal Projections onto Capsule Subspaces
A1  - Liheng Zhang
A1  - Marzieh Edraki
A1  - Guo-Jun Qi
JO  - ArXiv e-prints
Y1  - 19 May, 2018
UR  - https://arxiv.org/abs/1805.07621
N2  - In this paper, we formalize the idea behind capsule nets of using a capsule vector rather than a neuron activation to predict the label of samples. To this end, we propose to learn a group of capsule subspaces onto which an input feature vector is projected. Then the lengths of resultant capsules are used to score the probability of belonging to different classes. We train such a Capsule Projection Network (CapProNet) by learning an orthogonal projection matrix for each capsule subspace, and show that each capsule subspace is updated until it contains input feature vectors corresponding to the associated class. Only a small negligible computing overhead is incurred to train the network in low-dimensional capsule subspaces or through an alternative hyper-power iteration to estimate the normalization matrix. Experiment results on image datasets show the presented model can greatly improve the performance of state-of-the-art ResNet backbones by $10-20\%$ at the same level of computing and memory costs.
ER  -


TY  - Preprint
T1  - Reconciled Polynomial Machine: A Unified Representation of Shallow and Deep Learning Models
A1  - Jiawei Zhang
A1  - Limeng Cui
A1  - Fisher B. Gouza
JO  - ArXiv e-prints
Y1  - 18 May, 2018
UR  - https://arxiv.org/abs/1805.07507
N2  - In this paper, we aim at introducing a new machine learning model, namely reconciled polynomial machine, which can provide a unified representation of existing shallow and deep machine learning models. Reconciled polynomial machine predicts the output by computing the inner product of the feature kernel function and variable reconciling function. Analysis of several concrete models, including Linear Models, FM, MVM, Perceptron, MLP and Deep Neural Networks, will be provided in this paper, which can all be reduced to the reconciled polynomial machine representations. Detailed analysis of the learning error by these models will also be illustrated in this paper based on their reduced representations from the function approximation perspective.
ER  -


TY  - Preprint
T1  - Deep Loopy Neural Network Model for Graph Structured Data Representation Learning
A1  - Jiawei Zhang
A1  - Limeng Cui
A1  - Fisher B. Gouza
JO  - ArXiv e-prints
Y1  - 18 May, 2018
UR  - https://arxiv.org/abs/1805.07504
N2  - Existing deep learning models may encounter great challenges in handling graph structured data. In this paper, we introduce a new deep learning model for graph data specifically, namely the deep loopy neural network. Significantly different from the previous deep models, inside the deep loopy neural network, there exist a large number of loops created by the extensive connections among nodes in the input graph data, which makes model learning an infeasible task. To resolve such a problem, in this paper, we will introduce a new learning algorithm for the deep loopy neural network specifically. Instead of learning the model variables based on the original model, in the proposed learning algorithm, errors will be back-propagated through the edges in a group of extracted spanning trees. Extensive numerical experiments have been done on several real-world graph datasets, and the experimental results demonstrate the effectiveness of both the proposed model and the learning algorithm in handling graph data.
ER  -


TY  - Preprint
T1  - On Deep Ensemble Learning from a Function Approximation Perspective
A1  - Jiawei Zhang
A1  - Limeng Cui
A1  - Fisher B. Gouza
JO  - ArXiv e-prints
Y1  - 18 May, 2018
UR  - https://arxiv.org/abs/1805.07502
N2  - In this paper, we propose to provide a general ensemble learning framework based on deep learning models. Given a group of unit models, the proposed deep ensemble learning framework will effectively combine their learning results via a multilayered ensemble model. In the case when the unit model mathematical mappings are bounded, sigmoidal and discriminatory, we demonstrate that the deep ensemble learning framework can achieve a universal approximation of any functions from the input space to the output space. Meanwhile, to achieve such a performance, the deep ensemble learning framework also impose a strict constraint on the number of involved unit models. According to the theoretic proof provided in this paper, given the input feature space of dimension d, the required unit model number will be 2d, if the ensemble model involves one single layer. Furthermore, as the ensemble component goes deeper, the number of required unit model is proved to be lowered down exponentially.
ER  -


TY  - Preprint
T1  - My camera can see through fences: A deep learning approach for image de-fencing
A1  - Sankaraganesh Jonna
A1  - Krishna Kanth Nakka
A1  - Rajiv R. Sahay
JO  - ArXiv e-prints
Y1  - 18 May, 2018
UR  - https://arxiv.org/abs/1805.07442
N2  - In recent times, the availability of inexpensive image capturing devices such as smartphones/tablets has led to an exponential increase in the number of images/videos captured. However, sometimes the amateur photographer is hindered by fences in the scene which have to be removed after the image has been captured. Conventional approaches to image de-fencing suffer from inaccurate and non-robust fence detection apart from being limited to processing images of only static occluded scenes. In this paper, we propose a semi-automated de-fencing algorithm using a video of the dynamic scene. We use convolutional neural networks for detecting fence pixels. We provide qualitative as well as quantitative comparison results with existing lattice detection algorithms on the existing PSU NRT data set and a proposed challenging fenced image dataset. The inverse problem of fence removal is solved using split Bregman technique assuming total variation of the de-fenced image as the regularization constraint.
ER  -


TY  - Preprint
T1  - General solutions for nonlinear differential equations: a deep reinforcement learning approach
A1  - Shiyin Wei
A1  - Xiaowei Jin
A1  - Hui Li
JO  - ArXiv e-prints
Y1  - 13 May, 2018
UR  - https://arxiv.org/abs/1805.07297
N2  - Physicists use differential equations to describe the physical dynamical world, and the solutions of these equations constitute our understanding of the world. During the hundreds of years, scientists developed several ways to solve these equations, i.e., the analytical solutions and the numerical solutions. However, for some complex equations, there may be no analytical solutions, and the numerical solutions may encounter the curse of the extreme computational cost if the accuracy is the first consideration. Solving equations is a high-level human intelligence work and a crucial step towards general artificial intelligence (AI), where deep reinforcement learning (DRL) may contribute. This work makes the first attempt of applying (DRL) to solve nonlinear differential equations both in discretized and continuous format with the governing equations (physical laws) embedded in the DRL network, including ordinary differential equations (ODEs) and partial differential equations (PDEs). The DRL network consists of an actor that outputs solution approximations policy and a critic that outputs the critic of the actor&#39;s output solution. Deterministic policy network is employed as the actor, and governing equations are embedded in the critic. The effectiveness of the DRL solver in SchrÃ¶dinger equation, Navier-Stocks, Van der Pol equation, Burgers&#39; equation and the equation of motion are discussed.
ER  -


TY  - Preprint
T1  - Learning and Inference Movement with Deep Generative Model
A1  - Mingxuan Jing
A1  - Xiaojian Ma
A1  - Fuchun Sun
A1  - Huaping Liu
JO  - ArXiv e-prints
Y1  - 18 May, 2018
UR  - https://arxiv.org/abs/1805.07252
N2  - Learning and inference movement is a very challenging problem due to its high dimensionality and dependency to varied environments or tasks. In this paper, we propose an effective probabilistic method for learning and inference of basic movements. The motion planning problem is formulated as learning on a directed graphic model and deep generative model is used to perform learning and inference from demonstrations. An important characteristic of this method is that it flexibly incorporates the task descriptors and context information for long-term planning and it can be combined with dynamic systems for robot control. The experimental validations on robotic approaching path planning tasks show the advantages over the base methods with limited training data.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning based Resource Allocation for V2V Communications
A1  - Hao Ye
A1  - Geoffrey Ye Li
A1  - Biing-Hwang Fred Juang
JO  - ArXiv e-prints
Y1  - 16 May, 2018
UR  - https://arxiv.org/abs/1805.07222
N2  - In this paper, we develop a decentralized resource allocation mechanism for vehicle-to-vehicle (V2V) communications based on deep reinforcement learning, which can be applied to both unicast and broadcast scenarios. According to the decentralized resource allocation mechanism, an autonomous agent&#39;, a V2V link or a vehicle, makes its decisions to find the optimal sub-band and power level for transmission without requiring or having to wait for global information. Since the proposed method is decentralized, it incurs only limited transmission overhead. From the simulation results, each agent can effectively learn to satisfy the stringent latency constraints on V2V links while minimizing the interference to vehicle-to-infrastructure (V2I) communications.
ER  -


TY  - Preprint
T1  - Reinforced Imitation: Sample Efficient Deep Reinforcement Learning for Map-less Navigation by Leveraging Prior Demonstrations
A1  - Mark Pfeiffer
A1  - Samarth Shukla
A1  - Matteo Turchetta
A1  - Cesar Cadena
A1  - Andreas Krause
A1  - Roland Siegwart
A1  - Juan Nieto
JO  - ArXiv e-prints
Y1  - 31 August, 2018
UR  - https://arxiv.org/abs/1805.07095
N2  - This work presents a case study of a learning-based approach for target driven map-less navigation. The underlying navigation model is an end-to-end neural network which is trained using a combination of expert demonstrations, imitation learning (IL) and reinforcement learning (RL). While RL and IL suffer from a large sample complexity and the distribution mismatch problem, respectively, we show that leveraging prior expert demonstrations for pre-training can reduce the training time to reach at least the same level of performance compared to plain RL by a factor of 5. We present a thorough evaluation of different combinations of expert demonstrations, different RL algorithms and reward functions, both in simulation and on a real robotic platform. Our results show that the final model outperforms both standalone approaches in the amount of successful navigation tasks. In addition, the RL reward function can be significantly simplified when using pre-training, e.g. by using a sparse reward only. The learned navigation policy is able to generalize to unseen and real-world environments.
ER  -


TY  - Preprint
T1  - Hierarchical Reinforcement Learning with Deep Nested Agents
A1  - Marc Brittain
A1  - Peng Wei
JO  - ArXiv e-prints
Y1  - 17 May, 2018
UR  - https://arxiv.org/abs/1805.07008
N2  - Deep hierarchical reinforcement learning has gained a lot of attention in recent years due to its ability to produce state-of-the-art results in challenging environments where non-hierarchical frameworks fail to learn useful policies. However, as problem domains become more complex, deep hierarchical reinforcement learning can become inefficient, leading to longer convergence times and poor performance. We introduce the Deep Nested Agent framework, which is a variant of deep hierarchical reinforcement learning where information from the main agent is propagated to the low level $nested$ agent by incorporating this information into the nested agent&#39;s state. We demonstrate the effectiveness and performance of the Deep Nested Agent framework by applying it to three scenarios in Minecraft with comparisons to a deep non-hierarchical single agent framework, as well as, a deep hierarchical framework.
ER  -


TY  - Preprint
T1  - Terabyte-scale Deep Multiple Instance Learning for Classification and Localization in Pathology
A1  - Gabriele Campanella
A1  - Vitor Werneck Krauss Silva
A1  - Thomas J. Fuchs
JO  - ArXiv e-prints
Y1  - 27 September, 2018
UR  - https://arxiv.org/abs/1805.06983
N2  - In the field of computational pathology, the use of decision support systems powered by state-of-the-art deep learning solutions has been hampered by the lack of large labeled datasets. Until recently, studies relied on datasets in the order of few hundreds of slides which are not enough to train a model that can work at scale in the clinic. Here, we have gathered a dataset consisting of 12,160 slides, two orders of magnitude larger than previous datasets in pathology and equivalent to 25 times the pixel count of the entire ImageNet dataset. Given the size of our dataset it is possible for us to train a deep learning model under the Multiple Instance Learning (MIL) assumption where only the overall slide diagnosis is necessary for training, avoiding all the expensive pixel-wise annotations that are usually part of supervised learning approaches. We test our framework on a complex task, that of prostate cancer diagnosis on needle biopsies. We performed a thorough evaluation of the performance of our MIL pipeline under several conditions achieving an AUC of 0.98 on a held-out test set of 1,824 slides. These results open the way for training accurate diagnosis prediction models at scale, laying the foundation for decision support system deployment in the clinic.
ER  -


TY  - Preprint
T1  - Dependability in a Multi-tenant Multi-framework Deep Learning as-a-Service Platform
A1  - Scott Boag
A1  - Parijat Dube
A1  - Kaoutar El Maghraoui
A1  - Benjamin Herta
A1  - Waldemar Hummer
A1  - K. R. Jayaram
A1  - Rania Khalaf
A1  - Vinod Muthusamy
A1  - Michael Kalantar
A1  - Archit Verma
JO  - ArXiv e-prints
Y1  - 17 May, 2018
UR  - https://arxiv.org/abs/1805.06801
N2  - Deep learning (DL), a form of machine learning, is becoming increasingly popular in several application domains. As a result, cloud-based Deep Learning as a Service (DLaaS) platforms have become an essential infrastructure in many organizations. These systems accept, schedule, manage and execute DL training jobs at scale.
ER  -


TY  - Preprint
T1  - Deep-learning Based Modeling of Fault Detachment Stability for Power Grid
A1  - Haotian Cui
A1  - Xianggen Liu
A1  - Yanhao Huang
JO  - ArXiv e-prints
Y1  - 17 May, 2018
UR  - https://arxiv.org/abs/1805.06657
N2  - The project intends to model the stability of power system with a deep learning algorithm to the problem, aiming to delay the removal of the fault. The so-called &#34;fail-delay cut-off&#34; refers to the occurrence of N-1 backup protection action on the backbone network of the system, resulting in longer time for the removal of the fault. In practice, through the analysis and calculation of a large number of online data, we have found that the N-1 failure system of the main protection action will not be unstable, which is also a guarantee of the operation mode arrangement. In the case of the N-1 backup protection action, there is an approximately 2.5% probability that the system will be destabilized. Therefore, research is needed to improve the operating arrangement.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Network Slicing
A1  - Zhifeng Zhao
A1  - Rongpeng Li
A1  - Qi Sun
A1  - Chi-Lin I
A1  - Chenyang Yang
A1  - Xianfu Chen
A1  - Minjian Zhao
A1  - Honggang Zhang
JO  - ArXiv e-prints
Y1  - 26 May, 2018
UR  - https://arxiv.org/abs/1805.06591
N2  - Network slicing means an emerging business to operators and allows them to sell the customized slices to various tenants at different prices. In order to provide better-performing and costefficient services, network slicing involves challenging technical issues and urgently looks forward to intelligent innovations to make the resource management consistent with users&#39; activities per slice. In that regard, deep reinforcement learning (DRL), which focuses on how to interact with the environment by trying alternative actions and reinforces the tendency actions producing more rewarding consequences, is emerging as a promising solution. In this paper, after briefly reviewing the fundamental concepts and evolution-driving factors of DRL, we investigate the application of DRL in some typical resource management scenarios of network slicing, which include radio resource slicing and priority-based core network slicing, and demonstrate the performance advantage of DRL over several competing schemes through extensive simulations. Finally, we also discuss the possible challenges to apply DRL in network slicing from a general perspective.
ER  -


TY  - Preprint
T1  - End-to-end Learning of a Convolutional Neural Network via Deep Tensor Decomposition
A1  - Samet Oymak
A1  - Mahdi Soltanolkotabi
JO  - ArXiv e-prints
Y1  - 16 May, 2018
UR  - https://arxiv.org/abs/1805.06523
N2  - In this paper we study the problem of learning the weights of a deep convolutional neural network. We consider a network where convolutions are carried out over non-overlapping patches with a single kernel in each layer. We develop an algorithm for simultaneously learning all the kernels from the training data. Our approach dubbed Deep Tensor Decomposition (DeepTD) is based on a rank-1 tensor decomposition. We theoretically investigate DeepTD under a realizable model for the training data where the inputs are chosen i.i.d. from a Gaussian distribution and the labels are generated according to planted convolutional kernels. We show that DeepTD is data-efficient and provably works as soon as the sample size exceeds the total number of convolutional weights in the network. We carry out a variety of numerical experiments to investigate the effectiveness of DeepTD and verify our theoretical findings.
ER  -


TY  - Preprint
T1  - #phramacovigilance - Exploring Deep Learning Techniques for Identifying Mentions of Medication Intake from Twitter
A1  - Debanjan Mahata
A1  - Jasper Friedrichs
A1  -  Hitkul
A1  - Rajiv Ratn Shah
JO  - ArXiv e-prints
Y1  - 16 May, 2018
UR  - https://arxiv.org/abs/1805.06375
N2  - Mining social media messages for health and drug related information has received significant interest in pharmacovigilance research. Social media sites (e.g., Twitter), have been used for monitoring drug abuse, adverse reactions of drug usage and analyzing expression of sentiments related to drugs. Most of these studies are based on aggregated results from a large population rather than specific sets of individuals. In order to conduct studies at an individual level or specific cohorts, identifying posts mentioning intake of medicine by the user is necessary. Towards this objective, we train different deep neural network classification models on a publicly available annotated dataset and study their performances on identifying mentions of personal intake of medicine in tweets. We also design and train a new architecture of a stacked ensemble of shallow convolutional neural network (CNN) ensembles. We use random search for tuning the hyperparameters of the models and share the details of the values taken by the hyperparameters for the best learnt model in different deep neural network architectures. Our system produces state-of-the-art results, with a micro- averaged F-score of 0.693.
ER  -


TY  - Preprint
T1  - FollowNet: Robot Navigation by Following Natural Language Directions with Deep Reinforcement Learning
A1  - Pararth Shah
A1  - Marek Fiser
A1  - Aleksandra Faust
A1  - J. Chase Kew
A1  - Dilek Hakkani-Tur
JO  - ArXiv e-prints
Y1  - 16 May, 2018
UR  - https://arxiv.org/abs/1805.06150
N2  - Understanding and following directions provided by humans can enable robots to navigate effectively in unknown situations. We present FollowNet, an end-to-end differentiable neural architecture for learning multi-modal navigation policies. FollowNet maps natural language instructions as well as visual and depth inputs to locomotion primitives. FollowNet processes instructions using an attention mechanism conditioned on its visual and depth input to focus on the relevant parts of the command while performing the navigation task. Deep reinforcement learning (RL) a sparse reward learns simultaneously the state representation, the attention function, and control policies. We evaluate our agent on a dataset of complex natural language directions that guide the agent through a rich and realistic dataset of simulated homes. We show that the FollowNet agent learns to execute previously unseen instructions described with a similar vocabulary, and successfully navigates along paths not encountered during training. The agent shows 30% improvement over a baseline model without the attention mechanism, with 52% success rate at novel instructions.
ER  -


TY  - Preprint
T1  - Optimized Computation Offloading Performance in Virtual Edge Computing Systems via Deep Reinforcement Learning
A1  - Xianfu Chen
A1  - Honggang Zhang
A1  - Celimuge Wu
A1  - Shiwen Mao
A1  - Yusheng Ji
A1  - Mehdi Bennis
JO  - ArXiv e-prints
Y1  - 16 May, 2018
UR  - https://arxiv.org/abs/1805.06146
N2  - To improve the quality of computation experience for mobile devices, mobile-edge computing (MEC) is a promising paradigm by providing computing capabilities in close proximity within a sliced radio access network (RAN), which supports both traditional communication and MEC services. Nevertheless, the design of computation offloading policies for a virtual MEC system remains challenging. Specifically, whether to execute a computation task at the mobile device or to offload it for MEC server execution should adapt to the time-varying network dynamics. In this paper, we consider MEC for a representative mobile user in an ultra-dense sliced RAN, where multiple base stations (BSs) are available to be selected for computation offloading. The problem of solving an optimal computation offloading policy is modelled as a Markov decision process, where our objective is to maximize the long-term utility performance whereby an offloading decision is made based on the task queue state, the energy queue state as well as the channel qualities between MU and BSs. To break the curse of high dimensionality in state space, we first propose a double deep Q-network (DQN) based strategic computation offloading algorithm to learn the optimal policy without knowing a priori knowledge of network dynamics. Then motivated by the additive structure of the utility function, a Q-function decomposition technique is combined with the double DQN, which leads to novel learning algorithm for the solving of stochastic computation offloading. Numerical experiments show that our proposed learning algorithms achieve a significant improvement in computation offloading performance compared with the baseline policies.
ER  -


TY  - Preprint
T1  - Do deep reinforcement learning agents model intentions?
A1  - Tambet Matiisen
A1  - Aqeel Labash
A1  - Daniel Majoral
A1  - Jaan Aru
A1  - Raul Vicente
JO  - ArXiv e-prints
Y1  - 21 May, 2018
UR  - https://arxiv.org/abs/1805.06020
N2  - Inferring other agents&#39; mental states such as their knowledge, beliefs and intentions is thought to be essential for effective interactions with other agents. Recently, multiagent systems trained via deep reinforcement learning have been shown to succeed in solving different tasks, but it remains unclear how each agent modeled or represented other agents in their environment. In this work we test whether deep reinforcement learning agents explicitly represent other agents&#39; intentions (their specific aims or goals) during a task in which the agents had to coordinate the covering of different spots in a 2D environment. In particular, we tracked over time the performance of a linear decoder trained to predict the final goal of all agents from the hidden state of each agent&#39;s neural network controller. We observed that the hidden layers of agents represented explicit information about other agents&#39; goals, i.e. the target landmark they ended up covering. We also performed a series of experiments, in which some agents were replaced by others with fixed goals, to test the level of generalization of the trained agents. We noticed that during the training phase the agents developed a differential preference for each goal, which hindered generalization. To alleviate the above problem, we propose simple changes to the MADDPG training algorithm which leads to better generalization against unseen agents. We believe that training protocols promoting more active intention reading mechanisms, e.g. by preventing simple symmetry-breaking solutions, is a promising direction towards achieving a more robust generalization in different cooperative and competitive tasks.
ER  -


TY  - Preprint
T1  - Online Deep Metric Learning
A1  - Wenbin Li
A1  - Jing Huo
A1  - Yinghuan Shi
A1  - Yang Gao
A1  - Lei Wang
A1  - Jiebo Luo
JO  - ArXiv e-prints
Y1  - 14 May, 2018
UR  - https://arxiv.org/abs/1805.05510
N2  - Metric learning learns a metric function from training data to calculate the similarity or distance between samples. From the perspective of feature learning, metric learning essentially learns a new feature space by feature transformation (e.g., Mahalanobis distance metric). However, traditional metric learning algorithms are shallow, which just learn one metric space (feature transformation). Can we further learn a better metric space from the learnt metric space? In other words, can we learn metric progressively and nonlinearly like deep learning by just using the existing metric learning algorithms? To this end, we present a hierarchical metric learning scheme and implement an online deep metric learning framework, namely ODML. Specifically, we take one online metric learning algorithm as a metric layer, followed by a nonlinear layer (i.e., ReLU), and then stack these layers modelled after the deep learning. The proposed ODML enjoys some nice properties, indeed can learn metric progressively and performs superiorly on some datasets. Various experiments with different settings have been conducted to verify these properties of the proposed ODML.
ER  -


TY  - Preprint
T1  - The Concept of the Deep Learning-Based System &#34;Artificial Dispatcher&#34; to Power System Control and Dispatch
A1  - Nikita Tomin
A1  - Victor Kurbatsky
A1  - Michael Negnevitsky
JO  - ArXiv e-prints
Y1  - 7 May, 2018
UR  - https://arxiv.org/abs/1805.05408
N2  - Year by year control of normal and emergency conditions of up-to-date power systems becomes an increasingly complicated problem. With the increasing complexity the existing control system of power system conditions which includes operative actions of the dispatcher and work of special automatic devices proves to be insufficiently effective more and more frequently, which raises risks of dangerous and emergency conditions in power systems. The paper is aimed at compensating for the shortcomings of man (a cognitive barrier, exposure to stresses and so on) and automatic devices by combining their strong points, i.e. the dispatcher&#39;s intelligence and the speed of automatic devices by virtue of development of the intelligent system &#34;Artificial dispatcher&#34; on the basis of deep machine learning technology. For realization of the system &#34;Artificial dispatcher&#34; in addition to deep learning it is planned to attract the game theory approaches to formalize work of the up-to-date power system as a game problem. The &#34;gain&#34; for &#34;Artificial dispatcher&#34; will consist in bringing in a power system in the normal steady-state or post-emergency conditions by means of the required control actions.
ER  -


TY  - Preprint
T1  - Deep Attentional Structured Representation Learning for Visual Recognition
A1  - Krishna Kanth Nakka
A1  - Mathieu Salzmann
JO  - ArXiv e-prints
Y1  - 14 May, 2018
UR  - https://arxiv.org/abs/1805.05389
N2  - Structured representations, such as Bags of Words, VLAD and Fisher Vectors, have proven highly effective to tackle complex visual recognition tasks. As such, they have recently been incorporated into deep architectures. However, while effective, the resulting deep structured representation learning strategies typically aggregate local features from the entire image, ignoring the fact that, in complex recognition tasks, some regions provide much more discriminative information than others.
ER  -


TY  - Preprint
T1  - DeepMutation: Mutation Testing of Deep Learning Systems
A1  - Lei Ma
A1  - Fuyuan Zhang
A1  - Jiyuan Sun
A1  - Minhui Xue
A1  - Bo Li
A1  - Felix Juefei-Xu
A1  - Chao Xie
A1  - Li Li
A1  - Yang Liu
A1  - Jianjun Zhao
A1  - Yadong Wang
JO  - ArXiv e-prints
Y1  - 14 August, 2018
UR  - https://arxiv.org/abs/1805.05206
N2  - Deep learning (DL) defines a new data-driven programming paradigm where the internal system logic is largely shaped by the training data. The standard way of evaluating DL models is to examine their performance on a test dataset. The quality of the test dataset is of great importance to gain confidence of the trained models. Using an inadequate test dataset, DL models that have achieved high test accuracy may still lack generality and robustness. In traditional software testing, mutation testing is a well-established technique for quality evaluation of test suites, which analyzes to what extent a test suite detects the injected faults. However, due to the fundamental difference between traditional software and deep learning-based software, traditional mutation testing techniques cannot be directly applied to DL systems. In this paper, we propose a mutation testing framework specialized for DL systems to measure the quality of test data. To do this, by sharing the same spirit of mutation testing in traditional software, we first define a set of source-level mutation operators to inject faults to the source of DL (i.e., training data and training programs). Then we design a set of model-level mutation operators that directly inject faults into DL models without a training process. Eventually, the quality of test data could be evaluated from the analysis on to what extent the injected faults could be detected. The usefulness of the proposed mutation testing techniques is demonstrated on two public datasets, namely MNIST and CIFAR-10, with three DL models.
ER  -


TY  - Preprint
T1  - A Deep Learning Approach with an Attention Mechanism for Automatic Sleep Stage Classification
A1  - Martin LÃ¤ngkvist
A1  - Amy Loutfi
JO  - ArXiv e-prints
Y1  - 14 May, 2018
UR  - https://arxiv.org/abs/1805.05036
N2  - Automatic sleep staging is a challenging problem and state-of-the-art algorithms have not yet reached satisfactory performance to be used instead of manual scoring by a sleep technician. Much research has been done to find good feature representations that extract the useful information to correctly classify each epoch into the correct sleep stage. While many useful features have been discovered, the amount of features have grown to an extent that a feature reduction step is necessary in order to avoid the curse of dimensionality. One reason for the need of such a large feature set is that many features are good for discriminating only one of the sleep stages and are less informative during other stages. This paper explores how a second feature representation over a large set of pre-defined features can be learned using an auto-encoder with a selective attention for the current sleep stage in the training batch. This selective attention allows the model to learn feature representations that focuses on the more relevant inputs without having to perform any dimensionality reduction of the input data. The performance of the proposed algorithm is evaluated on a large data set of polysomnography (PSG) night recordings of patients with sleep-disordered breathing. The performance of the auto-encoder with selective attention is compared with a regular auto-encoder and previous works using a deep belief network (DBN).
ER  -


TY  - Preprint
T1  - Deep Decision Trees for Discriminative Dictionary Learning with Adversarial Multi-Agent Trajectories
A1  - Tharindu Fernando
A1  - Sridha Sridharan
A1  - Clinton Fookes
A1  - Simon Denman
JO  - ArXiv e-prints
Y1  - 14 May, 2018
UR  - https://arxiv.org/abs/1805.05009
N2  - With the explosion in the availability of spatio-temporal tracking data in modern sports, there is an enormous opportunity to better analyse, learn and predict important events in adversarial group environments. In this paper, we propose a deep decision tree architecture for discriminative dictionary learning from adversarial multi-agent trajectories. We first build up a hierarchy for the tree structure by adding each layer and performing feature weight based clustering in the forward pass. We then fine tune the player role weights using back propagation. The hierarchical architecture ensures the interpretability and the integrity of the group representation. The resulting architecture is a decision tree, with leaf-nodes capturing a dictionary of multi-agent group interactions. Due to the ample volume of data available, we focus on soccer tracking data, although our approach can be used in any adversarial multi-agent domain. We present applications of proposed method for simulating soccer games as well as evaluating and quantifying team strategies.
ER  -


TY  - Preprint
T1  - Deep Learning in Software Engineering
A1  - Xiaochen Li
A1  - He Jiang
A1  - Zhilei Ren
A1  - Ge Li
A1  - Jingxuan Zhang
JO  - ArXiv e-prints
Y1  - 13 May, 2018
UR  - https://arxiv.org/abs/1805.04825
N2  - Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). However, many open issues still remain to be investigated. How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks. To answer these questions, we conduct a bibliography analysis on 98 research papers in SE that use deep learning techniques. We find that 41 SE tasks in all SE phases have been facilitated by deep learning integrated solutions. In which, 84.7% papers only use standard deep learning models and their variants to solve SE problems. The practicability becomes a concern in utilizing deep learning techniques. How to improve the effectiveness, efficiency, understandability, and testability of deep learning based solutions may attract more SE researchers in the future.
ER  -


TY  - Preprint
T1  - Twitter User Geolocation using Deep Multiview Learning
A1  - Tien Huu Do
A1  - Duc Minh Nguyen
A1  - Evaggelia Tsiligianni
A1  - Bruno Cornelis
A1  - Nikos Deligiannis
JO  - ArXiv e-prints
Y1  - 11 May, 2018
UR  - https://arxiv.org/abs/1805.04612
N2  - Predicting the geographical location of users on social networks like Twitter is an active research topic with plenty of methods proposed so far. Most of the existing work follows either a content-based or a network-based approach. The former is based on user-generated content while the latter exploits the structure of the network of users. In this paper, we propose a more generic approach, which incorporates not only both content-based and network-based features, but also other available information into a unified model. Our approach, named Multi-Entry Neural Network (MENET), leverages the latest advances in deep learning and multiview learning. A realization of MENET with textual, network and metadata features results in an effective method for Twitter user geolocation, achieving the state of the art on two well-known datasets.
ER  -


TY  - Preprint
T1  - Laconic Deep Learning Computing
A1  - Sayeh Sharify
A1  - Mostafa Mahmoud
A1  - Alberto Delmas Lascorz
A1  - Milos Nikolic
A1  - Andreas Moshovos
JO  - ArXiv e-prints
Y1  - 10 May, 2018
UR  - https://arxiv.org/abs/1805.04513
N2  - We motivate a method for transparently identifying ineffectual computations in unmodified Deep Learning models and without affecting accuracy. Specifically, we show that if we decompose multiplications down to the bit level the amount of work performed during inference for image classification models can be consistently reduced by two orders of magnitude. In the best case studied of a sparse variant of AlexNet, this approach can ideally reduce computation work by more than 500x. We present Laconic a hardware accelerator that implements this approach to improve execution time, and energy efficiency for inference with Deep Learning Networks. Laconic judiciously gives up some of the work reduction potential to yield a low-cost, simple, and energy efficient design that outperforms other state-of-the-art accelerators. For example, a Laconic configuration that uses a weight memory interface with just 128 wires outperforms a conventional accelerator with a 2K-wire weight memory interface by 2.3x on average while being 2.13x more energy efficient on average. A Laconic configuration that uses a 1K-wire weight memory interface, outperforms the 2K-wire conventional accelerator by 15.4x and is 1.95x more energy efficient. Laconic does not require but rewards advances in model design such as a reduction in precision, the use of alternate numeric representations that reduce the number of bits that are &#34;1&#34;, or an increase in weight or activation sparsity.
ER  -


TY  - Preprint
T1  - Novel Deep Learning Model for Traffic Sign Detection Using Capsule Networks
A1  - Amara Dinesh Kumar
JO  - ArXiv e-prints
Y1  - 11 May, 2018
UR  - https://arxiv.org/abs/1805.04424
N2  - Convolutional neural networks are the most widely used deep learning algorithms for traffic signal classification till date but they fail to capture pose, view, orientation of the images because of the intrinsic inability of max pooling layer.This paper proposes a novel method for Traffic sign detection using deep learning architecture called capsule networks that achieves outstanding performance on the German traffic sign dataset.Capsule network consists of capsules which are a group of neurons representing the instantiating parameters of an object like the pose and orientation by using the dynamic routing and route by agreement algorithms.unlike the previous approaches of manual feature extraction,multiple deep neural networks with many parameters,our method eliminates the manual effort and provides resistance to the spatial variances.CNNs can be fooled easily using various adversary attacks and capsule networks can overcome such attacks from the intruders and can offer more reliability in traffic sign detection for autonomous vehicles.Capsule network have achieved the state-of-the-art accuracy of 97.6% on German Traffic Sign Recognition Benchmark dataset (GTSRB).
ER  -


TY  - Preprint
T1  - Deep Hierarchical Reinforcement Learning Algorithm in Partially Observable Markov Decision Processes
A1  - Le Pham Tuyen
A1  - Ngo Anh Vien
A1  - Abu Layek
A1  - TaeChoong Chung
JO  - ArXiv e-prints
Y1  - 11 May, 2018
UR  - https://arxiv.org/abs/1805.04419
N2  - In recent years, reinforcement learning has achieved many remarkable successes due to the growing adoption of deep learning techniques and the rapid growth in computing power. Nevertheless, it is well-known that flat reinforcement learning algorithms are often not able to learn well and data-efficient in tasks having hierarchical structures, e.g. consisting of multiple subtasks. Hierarchical reinforcement learning is a principled approach that is able to tackle these challenging tasks. On the other hand, many real-world tasks usually have only partial observability in which state measurements are often imperfect and partially observable. The problems of RL in such settings can be formulated as a partially observable Markov decision process (POMDP). In this paper, we study hierarchical RL in POMDP in which the tasks have only partial observability and possess hierarchical properties. We propose a hierarchical deep reinforcement learning approach for learning in hierarchical POMDP. The deep hierarchical RL algorithm is proposed to apply to both MDP and POMDP learning. We evaluate the proposed algorithm on various challenging hierarchical POMDP.
ER  -


TY  - Preprint
T1  - Adaptive Selection of Deep Learning Models on Embedded Systems
A1  - Ben Taylor
A1  - Vicent Sanz Marco
A1  - Willy Wolff
A1  - Yehia Elkhatib
A1  - Zheng Wang
JO  - ArXiv e-prints
Y1  - 11 May, 2018
UR  - https://arxiv.org/abs/1805.04252
N2  - The recent ground-breaking advances in deep learning networks ( DNNs ) make them attractive for embedded systems. However, it can take a long time for DNNs to make an inference on resource-limited embedded devices. Offloading the computation into the cloud is often infeasible due to privacy concerns, high latency, or the lack of connectivity. As such, there is a critical need to find a way to effectively execute the DNN models locally on the devices. This paper presents an adaptive scheme to determine which DNN model to use for a given input, by considering the desired accuracy and inference time. Our approach employs machine learning to develop a predictive model to quickly select a pre-trained DNN to use for a given input and the optimization constraint. We achieve this by first training off-line a predictive model, and then use the learnt model to select a DNN model to use for new, unseen inputs. We apply our approach to the image classification task and evaluate it on a Jetson TX2 embedded deep learning platform using the ImageNet ILSVRC 2012 validation dataset. We consider a range of influential DNN models. Experimental results show that our approach achieves a 7.52% improvement in inference accuracy, and a 1.8x reduction in inference time over the most-capable single DNN model.
ER  -


TY  - Preprint
T1  - Neural Machine Translation for Bilingually Scarce Scenarios: A Deep Multi-task Learning Approach
A1  - Poorya Zaremoodi
A1  - Gholamreza Haffari
JO  - ArXiv e-prints
Y1  - 10 May, 2018
UR  - https://arxiv.org/abs/1805.04237
N2  - Neural machine translation requires large amounts of parallel training text to learn a reasonable-quality translation model. This is particularly inconvenient for language pairs for which enough parallel text is not available. In this paper, we use monolingual linguistic resources in the source side to address this challenging problem based on a multi-task learning approach. More specifically, we scaffold the machine translation task on auxiliary tasks including semantic parsing, syntactic parsing, and named-entity recognition. This effectively injects semantic and/or syntactic knowledge into the translation model, which would otherwise require a large amount of training bitext. We empirically evaluate and show the effectiveness of our multi-task learning approach on three translation tasks: English-to-French, English-to-Farsi, and English-to-Vietnamese.
ER  -


TY  - Preprint
T1  - Unifying Data, Model and Hybrid Parallelism in Deep Learning via Tensor Tiling
A1  - Minjie Wang
A1  - Chien-chin Huang
A1  - Jinyang Li
JO  - ArXiv e-prints
Y1  - 10 May, 2018
UR  - https://arxiv.org/abs/1805.04170
N2  - Deep learning systems have become vital tools across many fields, but the increasing model sizes mean that training must be accelerated to maintain such systems&#39; utility. Current systems like Tensorflow and MXNet focus on one specific parallelization strategy, data parallelism, which requires large training batch sizes in order to scale. We cast the problem of finding the best parallelization strategy as the problem of finding the best tiling to partition tensors with the least overall communication. We propose an algorithm that can find the optimal tiling. Our resulting parallelization solution is a hybrid of data parallelism and model parallelism. We build the SoyBean system that performs automatic parallelization. SoyBean automatically transforms a serial dataflow graph captured by an existing deep learning system frontend into a parallel dataflow graph based on the optimal tiling it has found. Our evaluations show that SoyBean is 1.5x-4x faster than pure data parallelism for AlexNet and VGG. We present this automatic tiling in a new system, SoyBean, that can act as a backend for Tensorflow, MXNet, and others.
ER  -


TY  - Preprint
T1  - Deep Representation Learning for Domain Adaptation of Semantic Image Segmentation
A1  - Assia Benbihi
A1  - Matthieu Geist
A1  - CÃ©dric Pradalier
JO  - ArXiv e-prints
Y1  - 10 May, 2018
UR  - https://arxiv.org/abs/1805.04141
N2  - Deep Convolutional Neural Networks have pushed the state-of-the art for semantic segmentation provided that a large amount of images together with pixel-wise annotations is available. Data collection is expensive and a solution to alleviate it is to use transfer learning. This reduces the amount of annotated data required for the network training but it does not get rid of this heavy processing step. We propose a method of transfer learning without annotations on the target task for datasets with redundant content and distinct pixel distributions. Our method takes advantage of the approximate content alignment of the images between two datasets when the approximation error prevents the reuse of annotation from one dataset to another. Given the annotations for only one dataset, we train a first network in a supervised manner. This network autonomously learns to generate deep data representations relevant to the semantic segmentation. Then the images in the new dataset, we train a new network to generate a deep data representation that matches the one from the first network on the previous dataset. The training consists in a regression between feature maps and does not require any annotations on the new dataset. We show that this method reaches performances similar to a classic transfer learning on the PASCAL VOC dataset with synthetic transformations.
ER  -


TY  - Preprint
T1  - Unsupervised Deep Representations for Learning Audience Facial Behaviors
A1  - Suman Saha
A1  - Rajitha Navarathna
A1  - Leonhard Helminger
A1  - Romann Weber
JO  - ArXiv e-prints
Y1  - 10 May, 2018
UR  - https://arxiv.org/abs/1805.04136
N2  - In this paper, we present an unsupervised learning approach for analyzing facial behavior based on a deep generative model combined with a convolutional neural network (CNN). We jointly train a variational auto-encoder (VAE) and a generative adversarial network (GAN) to learn a powerful latent representation from footage of audiences viewing feature-length movies. We show that the learned latent representation successfully encodes meaningful signatures of behaviors related to audience engagement (smiling &amp; laughing) and disengagement (yawning). Our results provide a proof of concept for a more general methodology for annotating hard-to-label multimedia data featuring sparse examples of signals of interest.
ER  -


TY  - Preprint
T1  - A DAG Model of Synchronous Stochastic Gradient Descent in Distributed Deep Learning
A1  - Shaohuai Shi
A1  - Qiang Wang
A1  - Xiaowen Chu
A1  - Bo Li
JO  - ArXiv e-prints
Y1  - 25 September, 2018
UR  - https://arxiv.org/abs/1805.03812
N2  - With huge amounts of training data, deep learning has made great breakthroughs in many artificial intelligence (AI) applications. However, such large-scale data sets present computational challenges, requiring training to be distributed on a cluster equipped with accelerators like GPUs. With the fast increase of GPU computing power, the data communications among GPUs have become a potential bottleneck on the overall training performance. In this paper, we first propose a general directed acyclic graph (DAG) model to describe the distributed synchronous stochastic gradient descent (S-SGD) algorithm, which has been widely used in distributed deep learning frameworks. To understand the practical impact of data communications on training performance, we conduct extensive empirical studies on four state-of-the-art distributed deep learning frameworks (i.e., Caffe-MPI, CNTK, MXNet and TensorFlow) over multi-GPU and multi-node environments with different data communication techniques, including PCIe, NVLink, 10GbE, and InfiniBand. Through both analytical and experimental studies, we identify the potential bottlenecks and overheads that could be further optimized. At last, we make the data set of our experimental traces publicly available, which could be used to support simulation-based studies.
ER  -


TY  - Preprint
T1  - Deep Learning of Geometric Constellation Shaping including Fiber Nonlinearities
A1  - Rasmus T. Jones
A1  - Tobias A. Eriksson
A1  - Metodi P. Yankov
A1  - Darko Zibar
JO  - ArXiv e-prints
Y1  - 9 May, 2018
UR  - https://arxiv.org/abs/1805.03785
N2  - A new geometric shaping method is proposed, leveraging unsupervised machine learning to optimize the constellation design. The learned constellation mitigates nonlinear effects with gains up to 0.13 bit/4D when trained with a simplified fiber channel model.
ER  -


TY  - Preprint
T1  - k-Space Deep Learning for Accelerated MRI
A1  - Yoseob Han
A1  - Jong Chul Ye
JO  - ArXiv e-prints
Y1  - 9 May, 2018
UR  - https://arxiv.org/abs/1805.03779
N2  - The annihilating filter-based low-rank Hanel matrix approach (ALOHA) is one of the state-of-the-art compressed sensing approaches that directly interpolates the missing k-space data using low-rank Hankel matrix completion. Inspired by the recent mathematical discovery that links deep neural networks to Hankel matrix decomposition using data-driven framelet basis, here we propose a fully data-driven deep learning algorithm for k-space interpolation. Our network can be also easily applied to non-Cartesian k-space trajectories by simply adding an additional re-gridding layer. Extensive numerical experiments show that the proposed deep learning method significantly outperforms the existing image-domain deep learning approaches.
ER  -


TY  - Preprint
T1  - DeepWalking: Enabling Smartphone-based Walking Speed Estimation Using Deep Learning
A1  - Aawesh Shrestha
A1  - Myounggyu Won
JO  - ArXiv e-prints
Y1  - 9 May, 2018
UR  - https://arxiv.org/abs/1805.03368
N2  - Walking speed estimation is an essential component of mobile apps in various fields such as fitness, transportation, navigation, and health-care. Most existing solutions are focused on specialized medical applications that utilize body-worn motion sensors. These approaches do not serve effectively the general use case of numerous apps where the user holding a smartphone tries to find his or her walking speed solely based on smartphone sensors. However, existing smartphone-based approaches fail to provide acceptable precision for walking speed estimation. This leads to a question: is it possible to achieve comparable speed estimation accuracy using a smartphone over wearable sensor based obtrusive solutions?
ER  -


TY  - Preprint
T1  - Reward Estimation for Variance Reduction in Deep Reinforcement Learning
A1  - Joshua Romoff
A1  - Alexandre PichÃ©
A1  - Peter Henderson
A1  - Vincent Francois-Lavet
A1  - Joelle Pineau
JO  - ArXiv e-prints
Y1  - 8 May, 2018
UR  - https://arxiv.org/abs/1805.03359
N2  - In reinforcement learning (RL), stochastic environments can make learning a policy difficult due to high degrees of variance. As such, variance reduction methods have been investigated in other works, such as advantage estimation and control-variates estimation. Here, we propose to learn a separate reward estimator to train the value function, to help reduce variance caused by a noisy reward signal. This results in theoretical reductions in variance in the tabular case, as well as empirical improvements in both the function approximation and tabular settings in environments where rewards are stochastic. To do so, we use a modified version of Advantage Actor Critic (A2C) on variations of Atari games.
ER  -


TY  - Preprint
T1  - Holarchic Structures for Decentralized Deep Learning - A Performance Analysis
A1  - Evangelos Pournaras
A1  - Srivatsan Yadhunathan
A1  - Ada Diaconescu
JO  - ArXiv e-prints
Y1  - 17 September, 2018
UR  - https://arxiv.org/abs/1805.02686
N2  - Structure plays a key role in learning performance. In centralized computational systems, hyperparameter optimization and regularization techniques such as dropout are computational means to enhance learning performance by adjusting the deep hierarchical structure. However, in decentralized deep learning by the Internet of Things, the structure is an actual network of autonomous interconnected devices such as smart phones that interact via complex network protocols. Self-adaptation of the learning structure is a challenge. Uncertainties such as network latency, node and link failures or even bottlenecks by limited processing capacity and energy availability can signif- icantly downgrade learning performance. Network self-organization and self-management is complex, while it requires additional computational and network resources that hinder the feasibility of decentralized deep learning. In contrast, this paper introduces a self-adaptive learning approach based on holarchic learning structures for exploring, mitigating and boosting learning performance in distributed environments with uncertainties. A large-scale performance analysis with 864000 experiments fed with synthetic and real-world data from smart grid and smart city pilot projects confirm the cost-effectiveness of holarchic structures for decentralized deep learning.
ER  -


TY  - Preprint
T1  - QARC: Video Quality Aware Rate Control for Real-Time Video Streaming based on Deep Reinforcement Learning
A1  - Tianchi Huang
A1  - Rui-Xiao Zhang
A1  - Chao Zhou
A1  - Lifeng Sun
JO  - ArXiv e-prints
Y1  - 15 May, 2018
UR  - https://arxiv.org/abs/1805.02482
N2  - Real-time video streaming is now one of the main applications in all network environments. Due to the fluctuation of throughput under various network conditions, how to choose a proper bitrate adaptively has become an upcoming and interestingly issue. To tackle this problem, most adaptive bitrate control methods have been proposed to provide high video bitrates instead of video qualities. Nevertheless, we notice that there exists a trade-off between sending bitrate and video quality, which motivates us to focus on how to get a balance between them. In this paper, we propose QARC (video Quality Awareness Rate Control), a rate control algorithm that aims to have a higher perceptual video quality with possibly lower sending rate and transmission latency. Starting from scratch, QARC uses deep reinforcement learning(DRL) algorithm to train a neural network to select future bitrates based on previously observed network status and past video frames. To overcome the &#34;state explosion problem&#34;, we design a neural network to predict future perceptual video quality as a vector for taking the place of the raw picture in the DRL&#39;s inputs. We evaluate QARC over a trace-driven emulation, outperforming existing approach with improvements in average video quality of 18\% - 25\% and decreases in average latency with 23% -45%. Meanwhile, Comparing QARC with offline optimal high bitrate method on various network conditions also yields a solid result.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Page-wise Recommendations
A1  - Xiangyu Zhao
A1  - Long Xia
A1  - Liang Zhang
A1  - Zhuoye Ding
A1  - Dawei Yin
A1  - Jiliang Tang
JO  - ArXiv e-prints
Y1  - 9 August, 2018
UR  - https://arxiv.org/abs/1805.02343
N2  - Recommender systems can mitigate the information overload problem by suggesting users&#39; personalized items. In real-world recommendations such as e-commerce, a typical interaction between the system and its users is -- users are recommended a page of items and provide feedback; and then the system recommends a new page of items. To effectively capture such interaction for recommendations, we need to solve two key problems -- (1) how to update recommending strategy according to user&#39;s \textit{real-time feedback}, and 2) how to generate a page of items with proper display, which pose tremendous challenges to traditional recommender systems. In this paper, we study the problem of page-wise recommendations aiming to address aforementioned two challenges simultaneously. In particular, we propose a principled approach to jointly generate a set of complementary items and the corresponding strategy to display them in a 2-D page; and propose a novel page-wise recommendation framework based on deep reinforcement learning, DeepPage, which can optimize a page of items with proper display based on real-time feedback from users. The experimental results based on a real-world e-commerce dataset demonstrate the effectiveness of the proposed framework.
ER  -


TY  - Preprint
T1  - Examining the Use of Neural Networks for Feature Extraction: A Comparative Analysis using Deep Learning, Support Vector Machines, and K-Nearest Neighbor Classifiers
A1  - Stephen Notley
A1  - Malik Magdon-Ismail
JO  - ArXiv e-prints
Y1  - 12 June, 2018
UR  - https://arxiv.org/abs/1805.02294
N2  - Neural networks in many varieties are touted as very powerful machine learning tools because of their ability to distill large amounts of information from different forms of data, extracting complex features and enabling powerful classification abilities. In this study, we use neural networks to extract features from both images and numeric data and use these extracted features as inputs for other machine learning models, namely support vector machines (SVMs) and k-nearest neighbor classifiers (KNNs), in order to see if neural-network-extracted features enhance the capabilities of these models. We tested 7 different neural network architectures in this manner, 4 for images and 3 for numeric data, training each for varying lengths of time and then comparing the results of the neural network independently to those of an SVM and KNN on the data, and finally comparing these results to models of SVM and KNN trained using features extracted via the neural network architecture. This process was repeated on 3 different image datasets and 2 different numeric datasets. The results show that, in many cases, the features extracted using the neural network significantly improve the capabilities of SVMs and KNNs compared to running these algorithms on the raw features, and in some cases also surpass the performance of the neural network alone. This in turn suggests that it may be a reasonable practice to use neural networks as a means to extract features for classification by other machine learning models for some datasets.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Playing 2.5D Fighting Games
A1  - Yu-Jhe Li
A1  - Hsin-Yu Chang
A1  - Yu-Jing Lin
A1  - Po-Wei Wu
A1  - Yu-Chiang Frank Wang
JO  - ArXiv e-prints
Y1  - 5 May, 2018
UR  - https://arxiv.org/abs/1805.02070
N2  - Deep reinforcement learning has shown its success in game playing. However, 2.5D fighting games would be a challenging task to handle due to ambiguity in visual appearances like height or depth of the characters. Moreover, actions in such games typically involve particular sequential action orders, which also makes the network design very difficult. Based on the network of Asynchronous Advantage Actor-Critic (A3C), we create an OpenAI-gym-like gaming environment with the game of Little Fighter 2 (LF2), and present a novel A3C+ network for learning RL agents. The introduced model includes a Recurrent Info network, which utilizes game-related info features with recurrent layers to observe combo skills for fighting. In the experiments, we consider LF2 in different settings, which successfully demonstrates the use of our proposed model for learning 2.5D fighting games.
ER  -


TY  - Preprint
T1  - Motion Planning Among Dynamic, Decision-Making Agents with Deep Reinforcement Learning
A1  - Michael Everett
A1  - Yu Fan Chen
A1  - Jonathan P. How
JO  - ArXiv e-prints
Y1  - 4 May, 2018
UR  - https://arxiv.org/abs/1805.01956
N2  - Robots that navigate among pedestrians use collision avoidance algorithms to enable safe and efficient operation. Recent works present deep reinforcement learning as a framework to model the complex interactions and cooperation. However, they are implemented using key assumptions about other agents&#39; behavior that deviate from reality as the number of agents in the environment increases. This work extends our previous approach to develop an algorithm that learns collision avoidance among a variety of types of dynamic agents without assuming they follow any particular behavior rules. This work also introduces a strategy using LSTM that enables the algorithm to use observations of an arbitrary number of other agents, instead of previous methods that have a fixed observation size. The proposed algorithm outperforms our previous approach in simulation as the number of agents increases, and the algorithm is demonstrated on a fully autonomous robotic vehicle traveling at human walking speed, without the use of a 3D Lidar.
ER  -


TY  - Preprint
T1  - RMDL: Random Multimodel Deep Learning for Classification
A1  - Kamran Kowsari
A1  - Mojtaba Heidarysafa
A1  - Donald E. Brown
A1  - Kiana Jafari Meimandi
A1  - Laura E. Barnes
JO  - ArXiv e-prints
Y1  - 31 May, 2018
UR  - https://arxiv.org/abs/1805.01890
N2  - The continually increasing number of complex datasets each year necessitates ever improving machine learning methods for robust and accurate categorization of these data. This paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble, deep learning approach for classification. Deep learning models have achieved state-of-the-art results across many domains. RMDL solves the problem of finding the best deep learning structure and architecture while simultaneously improving robustness and accuracy through ensembles of deep learning architectures. RDML can accept as input a variety data to include text, video, images, and symbolic. This paper describes RMDL and shows test results for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB, and 20newsgroup. These test results show that RDML produces consistently better performance than standard methods over a broad range of data types and classification problems.
ER  -


TY  - Preprint
T1  - Ultra Low Power Deep-Learning-powered Autonomous Nano Drones
A1  - Daniele Palossi
A1  - Antonio Loquercio
A1  - Francesco Conti
A1  - Eric Flamand
A1  - Davide Scaramuzza
A1  - Luca Benini
JO  - ArXiv e-prints
Y1  - 4 May, 2018
UR  - https://arxiv.org/abs/1805.01831
N2  - Flying in dynamic, urban, highly-populated environments represents an open problem in robotics. State-of-the-art (SoA) autonomous Unmanned Aerial Vehicles (UAVs) employ advanced computer vision techniques based on computationally expensive algorithms, such as Simultaneous Localization and Mapping (SLAM) or Convolutional Neural Networks (CNNs) to navigate in such environments. In the Internet-of-Things (IoT) era, nano-size UAVs capable of autonomous navigation would be extremely desirable as self-aware mobile IoT nodes. However, autonomous flight is considered unaffordable in the context of nano-scale UAVs, where the ultra-constrained power envelopes of tiny rotor-crafts limit the on-board computational capabilities to low-power microcontrollers. In this work, we present the first vertically integrated system for fully autonomous deep neural network-based navigation on nano-size UAVs. Our system is based on GAP8, a novel parallel ultra-low-power computing platform, and deployed on a 27 g commercial, open-source CrazyFlie 2.0 nano-quadrotor. We discuss a methodology and software mapping tools that enable the SoA CNN presented in [1] to be fully executed on-board within a strict 12 fps real-time constraint with no compromise in terms of flight results, while all processing is done with only 94 mW on average - 1% of the power envelope of the deployed nano-aircraft.
ER  -


TY  - Preprint
T1  - Intracranial Error Detection via Deep Learning
A1  - Martin VÃ¶lker
A1  - JiÅÃ­ Hammer
A1  - Robin T. Schirrmeister
A1  - Joos Behncke
A1  - Lukas D. J. Fiederer
A1  - Andreas Schulze-Bonhage
A1  - Petr MarusiÄ
A1  - Wolfram Burgard
A1  - Tonio Ball
JO  - ArXiv e-prints
Y1  - 22 June, 2018
UR  - https://arxiv.org/abs/1805.01667
N2  - Deep learning techniques have revolutionized the field of machine learning and were recently successfully applied to various classification problems in noninvasive electroencephalography (EEG). However, these methods were so far only rarely evaluated for use in intracranial EEG. We employed convolutional neural networks (CNNs) to classify and characterize the error-related brain response as measured in 24 intracranial EEG recordings. Decoding accuracies of CNNs were significantly higher than those of a regularized linear discriminant analysis. Using time-resolved deep decoding, it was possible to classify errors in various regions in the human brain, and further to decode errors over 200 ms before the actual erroneous button press, e.g., in the precentral gyrus. Moreover, deeper networks performed better than shallower networks in distinguishing correct from error trials in all-channel decoding. In single recordings, up to 100 % decoding accuracy was achieved. Visualization of the networks&#39; learned features indicated that multivariate decoding on an ensemble of channels yields related, albeit non-redundant information compared to single-channel decoding. In summary, here we show the usefulness of deep learning for both intracranial error decoding and mapping of the spatio-temporal structure of the human error processing network.
ER  -


TY  - Preprint
T1  - A Deep Learning Model with Hierarchical LSTMs and Supervised Attention for Anti-Phishing
A1  - Minh Nguyen
A1  - Toan Nguyen
A1  - Thien Huu Nguyen
JO  - ArXiv e-prints
Y1  - 3 May, 2018
UR  - https://arxiv.org/abs/1805.01554
N2  - Anti-phishing aims to detect phishing content/documents in a pool of textual data. This is an important problem in cybersecurity that can help to guard users from fraudulent information. Natural language processing (NLP) offers a natural solution for this problem as it is capable of analyzing the textual content to perform intelligent recognition. In this work, we investigate state-of-the-art techniques for text categorization in NLP to address the problem of anti-phishing for emails (i.e, predicting if an email is phishing or not). These techniques are based on deep learning models that have attracted much attention from the community recently. In particular, we present a framework with hierarchical long short-term memory networks (H-LSTMs) and attention mechanisms to model the emails simultaneously at the word and the sentence level. Our expectation is to produce an effective model for anti-phishing and demonstrate the effectiveness of deep learning for problems in cybersecurity.
ER  -


TY  - Preprint
T1  - Multi-label Learning Based Deep Transfer Neural Network for Facial Attribute Classification
A1  - Ni Zhuang
A1  - Yan Yan
A1  - Si Chen
A1  - Hanzi Wang
A1  - Chunhua Shen
JO  - ArXiv e-prints
Y1  - 3 May, 2018
UR  - https://arxiv.org/abs/1805.01282
N2  - Deep Neural Network (DNN) has recently achieved outstanding performance in a variety of computer vision tasks, including facial attribute classification. The great success of classifying facial attributes with DNN often relies on a massive amount of labelled data. However, in real-world applications, labelled data are only provided for some commonly used attributes (such as age, gender); whereas, unlabelled data are available for other attributes (such as attraction, hairline). To address the above problem, we propose a novel deep transfer neural network method based on multi-label learning for facial attribute classification, termed FMTNet, which consists of three sub-networks: the Face detection Network (FNet), the Multi-label learning Network (MNet) and the Transfer learning Network (TNet). Firstly, based on the Faster Region-based Convolutional Neural Network (Faster R-CNN), FNet is fine-tuned for face detection. Then, MNet is fine-tuned by FNet to predict multiple attributes with labelled data, where an effective loss weight scheme is developed to explicitly exploit the correlation between facial attributes based on attribute grouping. Finally, based on MNet, TNet is trained by taking advantage of unsupervised domain adaptation for unlabelled facial attribute classification. The three sub-networks are tightly coupled to perform effective facial attribute classification. A distinguishing characteristic of the proposed FMTNet method is that the three sub-networks (FNet, MNet and TNet) are constructed in a similar network structure. Extensive experimental results on challenging face datasets demonstrate the effectiveness of our proposed method compared with several state-of-the-art methods.
ER  -


TY  - Preprint
T1  - Binarizer at SemEval-2018 Task 3: Parsing dependency and deep learning for irony detection
A1  - Nishant Nikhil
A1  - Muktabh Mayank Srivastava
JO  - ArXiv e-prints
Y1  - 3 May, 2018
UR  - https://arxiv.org/abs/1805.01112
N2  - In this paper, we describe the system submitted for the SemEval 2018 Task 3 (Irony detection in English tweets) Subtask A by the team Binarizer. Irony detection is a key task for many natural language processing works. Our method treats ironical tweets to consist of smaller parts containing different emotions. We break down tweets into separate phrases using a dependency parser. We then embed those phrases using an LSTM-based neural network model which is pre-trained to predict emoticons for tweets. Finally, we train a fully-connected network to achieve classification.
ER  -


TY  - Preprint
T1  - Large-Scale Unsupervised Deep Representation Learning for Brain Structure
A1  - Ayush Jaiswal
A1  - Dong Guo
A1  - Cauligi S. Raghavendra
A1  - Paul Thompson
JO  - ArXiv e-prints
Y1  - 2 May, 2018
UR  - https://arxiv.org/abs/1805.01049
N2  - Machine Learning (ML) is increasingly being used for computer aided diagnosis of brain related disorders based on structural magnetic resonance imaging (MRI) data. Most of such work employs biologically and medically meaningful hand-crafted features calculated from different regions of the brain. The construction of such highly specialized features requires a considerable amount of time, manual oversight and careful quality control to ensure the absence of errors in the computational process. Recent advances in Deep Representation Learning have shown great promise in extracting highly non-linear and information-rich features from data. In this paper, we present a novel large-scale deep unsupervised approach to learn generic feature representations of structural brain MRI scans, which requires no specialized domain knowledge or manual intervention. Our method produces low-dimensional representations of brain structure, which can be used to reconstruct brain images with very low error and exhibit performance comparable to FreeSurfer features on various classification tasks.
ER  -


TY  - Preprint
T1  - Robust Deep Reinforcement Learning for Security and Safety in Autonomous Vehicle Systems
A1  - Aidin Ferdowsi
A1  - Ursula Challita
A1  - Walid Saad
A1  - Narayan B. Mandayam
JO  - ArXiv e-prints
Y1  - 8 May, 2018
UR  - https://arxiv.org/abs/1805.00983
N2  - To operate effectively in tomorrow&#39;s smart cities, autonomous vehicles (AVs) must rely on intra-vehicle sensors such as camera and radar as well as inter-vehicle communication. Such dependence on sensors and communication links exposes AVs to cyber-physical (CP) attacks by adversaries that seek to take control of the AVs by manipulating their data. Thus, to ensure safe and optimal AV dynamics control, the data processing functions at AVs must be robust to such CP attacks. To this end, in this paper, the state estimation process for monitoring AV dynamics, in presence of CP attacks, is analyzed and a novel adversarial deep reinforcement learning (RL) algorithm is proposed to maximize the robustness of AV dynamics control to CP attacks. The attacker&#39;s action and the AV&#39;s reaction to CP attacks are studied in a game-theoretic framework. In the formulated game, the attacker seeks to inject faulty data to AV sensor readings so as to manipulate the inter-vehicle optimal safe spacing and potentially increase the risk of AV accidents or reduce the vehicle flow on the roads. Meanwhile, the AV, acting as a defender, seeks to minimize the deviations of spacing so as to ensure robustness to the attacker&#39;s actions. Since the AV has no information about the attacker&#39;s action and due to the infinite possibilities for data value manipulations, the outcome of the players&#39; past interactions are fed to long-short term memory (LSTM) blocks. Each player&#39;s LSTM block learns the expected spacing deviation resulting from its own action and feeds it to its RL algorithm. Then, the the attacker&#39;s RL algorithm chooses the action which maximizes the spacing deviation, while the AV&#39;s RL algorithm tries to find the optimal action that minimizes such deviation.
ER  -


TY  - Preprint
T1  - Internet of Things Meets Brain-Computer Interface: A Unified Deep Learning Framework for Enabling Human-Thing Cognitive Interactivity
A1  - Xiang Zhang
A1  - Lina Yao
A1  - Shuai Zhang
A1  - Salil S. Kanhere
A1  - Quan Z. Sheng
A1  - Yunhao Liu
JO  - ArXiv e-prints
Y1  - 5 May, 2018
UR  - https://arxiv.org/abs/1805.00789
N2  - A Brain-Computer Interface (BCI) acquires brain signals, analyzes and translates them into commands that are relayed to actuation devices for carrying out desired actions. With the widespread connectivity of everyday devices realized by the advent of the Internet of Things (IoT), BCI can empower individuals to directly control objects such as smart home appliances or assistive robots, directly via their thoughts. However, realization of this vision is faced with a number of challenges, most importantly being the issue of accurately interpreting the intent of the individual from the raw brain signals that are often of low fidelity and subject to noise. Moreover, pre-processing brain signals and the subsequent feature engineering are both time-consuming and highly reliant on human domain expertise. To address the aforementioned issues, in this paper, we propose a unified deep learning based framework that enables effective human-thing cognitive interactivity in order to bridge individuals and IoT objects. We design a reinforcement learning based Selective Attention Mechanism (SAM) to discover the distinctive features from the input brain signals. In addition, we propose a modified Long Short-Term Memory (LSTM) to distinguish the inter-dimensional information forwarded from the SAM. To evaluate the efficiency of the proposed framework, we conduct extensive real-world experiments and demonstrate that our model outperforms a number of competitive state-of-the-art baselines. Two practical real-time human-thing cognitive interaction applications are presented to validate the feasibility of our approach.
ER  -


TY  - Preprint
T1  - Deep Perm-Set Net: Learn to predict sets with unknown permutation and cardinality using deep neural networks
A1  - S. Hamid Rezatofighi
A1  - Roman Kaskman
A1  - Farbod T. Motlagh
A1  - Qinfeng Shi
A1  - Daniel Cremers
A1  - Laura Leal-TaixÃ©
A1  - Ian Reid
JO  - ArXiv e-prints
Y1  - 2 October, 2018
UR  - https://arxiv.org/abs/1805.00613
N2  - Many real-world problems, e.g. object detection, have outputs that are naturally expressed as sets of entities. This creates a challenge for traditional deep neural networks which naturally deal with structured outputs such as vectors, matrices or tensors. We present a novel approach for learning to predict sets with unknown permutation and cardinality using deep neural networks. Specifically, in our formulation we incorporate the permutation as unobservable variable and estimate its distribution during the learning process using alternating optimization. We demonstrate the validity of this new formulation on two relevant vision problems: object detection, for which our formulation outperforms state-of-the-art detectors such as Faster R-CNN and YOLO, and a complex CAPTCHA test, where we observe that, surprisingly, our set based network acquired the ability of mimicking arithmetics without any rules being coded.
ER  -


TY  - Preprint
T1  - Deep learning approach to Fourier ptychographic microscopy
A1  - Thanh Nguyen
A1  - Yujia Xue
A1  - Yunzhe Li
A1  - Lei Tian
A1  - George Nehmetallah
JO  - ArXiv e-prints
Y1  - 30 July, 2018
UR  - https://arxiv.org/abs/1805.00334
N2  - Convolutional neural networks (CNNs) have gained tremendous success in solving complex inverse problems. The aim of this work is to develop a novel CNN framework to reconstruct video sequence of dynamic live cells captured using a computational microscopy technique, Fourier ptychographic microscopy (FPM). The unique feature of the FPM is its capability to reconstruct images with both wide field-of-view (FOV) and high resolution, i.e. a large space-bandwidth-product (SBP), by taking a series of low resolution intensity images. For live cell imaging, a single FPM frame contains thousands of cell samples with different morphological features. Our idea is to fully exploit the statistical information provided by this large spatial ensemble so as to make predictions in a sequential measurement, without using any additional temporal dataset. Specifically, we show that it is possible to reconstruct high-SBP dynamic cell videos by a CNN trained only on the first FPM dataset captured at the beginning of a time-series experiment. Our CNN approach reconstructs a 12800X10800 pixels phase image using only ~25 seconds, a 50X speedup compared to the model-based FPM algorithm. In addition, the CNN further reduces the required number of images in each time frame by ~6X. Overall, this significantly improves the imaging throughput by reducing both the acquisition and computational times. The proposed CNN is based on the conditional generative adversarial network (cGAN) framework. Additionally, we also exploit transfer learning so that our pre-trained CNN can be further optimized to image other cell types. Our technique demonstrates a promising deep learning approach to continuously monitor large live-cell populations over an extended time and gather useful spatial and temporal information with sub-cellular resolution.
ER  -


TY  - Preprint
T1  - Predicting resonant properties of plasmonic structures by deep learning
A1  - Iman Sajedian
A1  - Jeonghyun Kim
A1  - Junsuk Rho
JO  - ArXiv e-prints
Y1  - 19 April, 2018
UR  - https://arxiv.org/abs/1805.00312
N2  - Deep learning can be used to extract meaningful results from images. In this paper, we used convolutional neural networks combined with recurrent neural networks on images of plasmonic structures and extract absorption data form them. To provide the required data for the model we did 100,000 simulations with similar setups and random structures. By designing a deep network we could find a model that could predict the absorption of any structure with similar setup. We used convolutional neural networks to get the spatial information from the images and we used recurrent neural networks to help the model find the relationship between the spatial information obtained from convolutional neural network model. With this design we could reach a very low loss in predicting the absorption compared to the results obtained from numerical simulation in a very short time.
ER  -


TY  - Preprint
T1  - Falsification of Cyber-Physical Systems Using Deep Reinforcement Learning
A1  - Takumi Akazaki
A1  - Shuang Liu
A1  - Yoriyuki Yamagata
A1  - Yihai Duan
A1  - Jianye Hao
JO  - ArXiv e-prints
Y1  - 1 May, 2018
UR  - https://arxiv.org/abs/1805.00200
N2  - With the rapid development of software and distributed computing, Cyber-Physical Systems (CPS) are widely adopted in many application areas, e.g., smart grid, autonomous automobile. It is difficult to detect defects in CPS models due to the complexities involved in the software and physical systems. To find defects in CPS models efficiently, robustness guided falsification of CPS is introduced. Existing methods use several optimization techniques to generate counterexamples, which falsify the given properties of a CPS. However those methods may require a large number of simulation runs to find the counterexample and is far from practical. In this work, we explore state-of-the-art Deep Reinforcement Learning (DRL) techniques to reduce the number of simulation runs required to find such counterexamples. We report our method and the preliminary evaluation results.
ER  -


TY  - Preprint
T1  - Counterfactual Learning-to-Rank for Additive Metrics and Deep Models
A1  - Aman Agarwal
A1  - Ivan Zaitsev
A1  - Thorsten Joachims
JO  - ArXiv e-prints
Y1  - 21 June, 2018
UR  - https://arxiv.org/abs/1805.00065
N2  - Implicit feedback (e.g., clicks, dwell times) is an attractive source of training data for Learning-to-Rank, but it inevitably suffers from biases such as position bias. It was recently shown how counterfactual inference techniques can provide a rigorous approach for handling these biases, but existing methods are restricted to the special case of optimizing average rank for linear ranking functions. In this work, we generalize the counterfactual learning-to-rank approach to a broad class of additive rank metrics -- like Discounted Cumulative Gain (DCG) and Precision@k -- as well as non-linear deep network models. Focusing on DCG, this conceptual generalization gives rise to two new learning methods that both directly optimize an unbiased estimate of DCG despite the bias in the implicit feedback data. The first, SVM PropDCG, generalizes the Propensity Ranking SVM (SVM PropRank), and we show how the resulting optimization problem can be addressed via the Convex Concave Procedure (CCP). The second, Deep PropDCG, further generalizes the counterfactual learning-to-rank approach to deep networks as non-linear ranking functions. In addition to the theoretical support, we empirically find that SVM PropDCG significantly outperforms SVM PropRank in terms of DCG, and that it is robust to varying severity of presentation bias, noise, and propensity-model misspecification. Moreover, the ability to train non-linear ranking functions via Deep PropDCG further improves DCG.
ER  -


TY  - Preprint
T1  - Deep learning improved by biological activation functions
A1  - Gardave S Bhumbra
JO  - ArXiv e-prints
Y1  - 18 May, 2018
UR  - https://arxiv.org/abs/1804.11237
N2  - `Biologically inspired&#39; activation functions, such as the logistic sigmoid, have been instrumental in the historical advancement of machine learning. However in the field of deep learning, they have been largely displaced by rectified linear units (ReLU) or similar functions, such as its exponential linear unit (ELU) variant, to mitigate the effects of vanishing gradients associated with error back-propagation. The logistic sigmoid however does not represent the true input-output relation in neuronal cells under physiological conditions. Here, bionodal root unit (BRU) activation functions are introduced, exhibiting input-output non-linearities that are substantially more biologically plausible since their functional form is based on known biophysical properties of neuronal cells.
ER  -


TY  - Preprint
T1  - Learning Explicit Deep Representations from Deep Kernel Networks
A1  - Mingyuan Jiu
A1  - Hichem Sahbi
JO  - ArXiv e-prints
Y1  - 30 April, 2018
UR  - https://arxiv.org/abs/1804.11159
N2  - Deep kernel learning aims at designing nonlinear combinations of multiple standard elementary kernels by training deep networks. This scheme has proven to be effective, but intractable when handling large-scale datasets especially when the depth of the trained networks increases; indeed, the complexity of evaluating these networks scales quadratically w.r.t. the size of training data and linearly w.r.t. the depth of the trained networks. In this paper, we address the issue of efficient computation in Deep Kernel Networks (DKNs) by designing effective maps in the underlying Reproducing Kernel Hilbert Spaces. Given a pretrained DKN, our method builds its associated Deep Map Network (DMN) whose inner product approximates the original network while being far more efficient. The design principle of our method is greedy and achieved layer-wise, by finding maps that approximate DKNs at different (input, intermediate and output) layers. This design also considers an extra fine-tuning step based on unsupervised learning, that further enhances the generalization ability of the trained DMNs. When plugged into SVMs, these DMNs turn out to be as accurate as the underlying DKNs while being at least an order of magnitude faster on large-scale datasets, as shown through extensive experiments on the challenging ImageCLEF and COREL5k benchmarks.
ER  -


TY  - Preprint
T1  - Deep Co-attention based Comparators For Relative Representation Learning in Person Re-identification
A1  - Lin Wu
A1  - Yang Wang
A1  - Junbin Gao
A1  - Dacheng Tao
JO  - ArXiv e-prints
Y1  - 29 April, 2018
UR  - https://arxiv.org/abs/1804.11027
N2  - Person re-identification (re-ID) requires rapid, flexible yet discriminant representations to quickly generalize to unseen observations on-the-fly and recognize the same identity across disjoint camera views. Recent effective methods are developed in a pair-wise similarity learning system to detect a fixed set of features from distinct regions which are mapped to their vector embeddings for the distance measuring. However, the most relevant and crucial parts of each image are detected independently without referring to the dependency conditioned on one and another. Also, these region based methods rely on spatial manipulation to position the local features in comparable similarity measuring. To combat these limitations, in this paper we introduce the Deep Co-attention based Comparators (DCCs) that fuse the co-dependent representations of the paired images so as to focus on the relevant parts of both images and produce their \textit{relative representations}. Given a pair of pedestrian images to be compared, the proposed model mimics the foveation of human eyes to detect distinct regions concurrent on both images, namely co-dependent features, and alternatively attend to relevant regions to fuse them into the similarity learning. Our comparator is capable of producing dynamic representations relative to a particular sample every time, and thus well-suited to the case of re-identifying pedestrians on-the-fly. We perform extensive experiments to provide the insights and demonstrate the effectiveness of the proposed DCCs in person re-ID. Moreover, our approach has achieved the state-of-the-art performance on three benchmark data sets: DukeMTMC-reID \cite{DukeMTMC}, CUHK03 \cite{FPNN}, and Market-1501 \cite{Market1501}.
ER  -


TY  - Preprint
T1  - Scalable Angular Discriminative Deep Metric Learning for Face Recognition
A1  - Bowen Wu
A1  - Huaming Wu
A1  - Monica M. Y. Zhang
JO  - ArXiv e-prints
Y1  - 30 April, 2018
UR  - https://arxiv.org/abs/1804.10899
N2  - With the development of deep learning, Deep Metric Learning (DML) has achieved great improvements in face recognition. Specifically, the widely used softmax loss in the training process often bring large intra-class variations, and feature normalization is only exploited in the testing process to compute the pair similarities. To bridge the gap, we impose the intra-class cosine similarity between the features and weight vectors in softmax loss larger than a margin in the training step, and extend it from four aspects. First, we explore the effect of a hard sample mining strategy. To alleviate the human labor of adjusting the margin hyper-parameter, a self-adaptive margin updating strategy is proposed. Then, a normalized version is given to take full advantage of the cosine similarity constraint. Furthermore, we enhance the former constraint to force the intra-class cosine similarity larger than the mean inter-class cosine similarity with a margin in the exponential feature projection space. Extensive experiments on Labeled Face in the Wild (LFW), Youtube Faces (YTF) and IARPA Janus Benchmark A (IJB-A) datasets demonstrate that the proposed methods outperform the mainstream DML methods and approach the state-of-the-art performance.
ER  -


TY  - Preprint
T1  - Local Learning with Deep and Handcrafted Features for Facial Expression Recognition
A1  - Mariana-Iuliana Georgescu
A1  - Radu Tudor Ionescu
A1  - Marius Popescu
JO  - ArXiv e-prints
Y1  - 25 September, 2018
UR  - https://arxiv.org/abs/1804.10892
N2  - We present an approach that combines automatic features learned by convolutional neural networks (CNN) and handcrafted features computed by the bag-of-visual-words (BOVW) model in order to achieve state-of-the-art results in facial expression recognition. To obtain automatic features, we experiment with multiple CNN architectures, pre-trained models and training procedures, e.g. Dense-Sparse-Dense. After fusing the two types of features, we employ a local learning framework to predict the class label for each test image. The local learning framework is based on three steps. First, a k-nearest neighbors model is applied for selecting the nearest training samples for an input test image. Second, a one-versus-all Support Vector Machines (SVM) classifier is trained on the selected training samples. Finally, the SVM classifier is used for predicting the class label only for the test image it was trained for. Although we used local learning in combination with handcrafted features in our previous work, to the best of our knowledge, local learning has never been employed in combination with deep features. The experiments on the 2013 Facial Expression Recognition (FER) Challenge data set and the FER+ data set demonstrate that our approach achieves state-of-the-art results. With a top accuracy of 75.42% on the FER 2013 data set and 87.76% on the FER+ data set, we surpass all competition by more than 2% on both data sets.
ER  -


TY  - Preprint
T1  - Imbalanced Deep Learning by Minority Class Incremental Rectification
A1  - Qi Dong
A1  - Shaogang Gong
A1  - Xiatian Zhu
JO  - ArXiv e-prints
Y1  - 28 April, 2018
UR  - https://arxiv.org/abs/1804.10851
N2  - Model learning from class imbalanced training data is a long-standing and significant challenge for machine learning. In particular, existing deep learning methods consider mostly either class balanced data or moderately imbalanced data in model training, and ignore the challenge of learning from significantly imbalanced training data. To address this problem, we formulate a class imbalanced deep learning model based on batch-wise incremental minority (sparsely sampled) class rectification by hard sample mining in majority (frequently sampled) classes during model training. This model is designed to minimise the dominant effect of majority classes by discovering sparsely sampled boundaries of minority classes in an iterative batch-wise learning process. To that end, we introduce a Class Rectification Loss (CRL) function that can be deployed readily in deep network architectures. Extensive experimental evaluations are conducted on three imbalanced person attribute benchmark datasets (CelebA, X-Domain, DeepFashion) and one balanced object category benchmark dataset (CIFAR-100). These experimental results demonstrate the performance advantages and model scalability of the proposed batch-wise incremental minority class rectification model over the existing state-of-the-art models for addressing the problem of imbalanced data learning.
ER  -


TY  - Preprint
T1  - Learning Cross-Modal Deep Embeddings for Multi-Object Image Retrieval using Text and Sketch
A1  - Sounak Dey
A1  - Anjan Dutta
A1  - Suman K. Ghosh
A1  - Ernest Valveny
A1  - Josep LladÃ³s
A1  - Umapada Pal
JO  - ArXiv e-prints
Y1  - 28 April, 2018
UR  - https://arxiv.org/abs/1804.10819
N2  - In this work we introduce a cross modal image retrieval system that allows both text and sketch as input modalities for the query. A cross-modal deep network architecture is formulated to jointly model the sketch and text input modalities as well as the the image output modality, learning a common embedding between text and images and between sketches and images. In addition, an attention model is used to selectively focus the attention on the different objects of the image, allowing for retrieval with multiple objects in the query. Experiments show that the proposed method performs the best in both single and multiple object image retrieval in standard datasets.
ER  -


TY  - Preprint
T1  - Deep Learning based Inter-Modality Image Registration Supervised by Intra-Modality Similarity
A1  - Xiaohuan Cao
A1  - Jianhua Yang
A1  - Li Wang
A1  - Zhong Xue
A1  - Qian Wang
A1  - Dinggang Shen
JO  - ArXiv e-prints
Y1  - 27 April, 2018
UR  - https://arxiv.org/abs/1804.10735
N2  - Non-rigid inter-modality registration can facilitate accurate information fusion from different modalities, but it is challenging due to the very different image appearances across modalities. In this paper, we propose to train a non-rigid inter-modality image registration network, which can directly predict the transformation field from the input multimodal images, such as CT and MR images. In particular, the training of our inter-modality registration network is supervised by intra-modality similarity metric based on the available paired data, which is derived from a pre-aligned CT and MR dataset. Specifically, in the training stage, to register the input CT and MR images, their similarity is evaluated on the warped MR image and the MR image that is paired with the input CT. So that, the intra-modality similarity metric can be directly applied to measure whether the input CT and MR images are well registered. Moreover, we use the idea of dual-modality fashion, in which we measure the similarity on both CT modality and MR modality. In this way, the complementary anatomies in both modalities can be jointly considered to more accurately train the inter-modality registration network. In the testing stage, the trained inter-modality registration network can be directly applied to register the new multimodal images without any paired data. Experimental results have shown that, the proposed method can achieve promising accuracy and efficiency for the challenging non-rigid inter-modality registration task and also outperforms the state-of-the-art approaches.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning to Acquire Navigation Skills for Wheel-Legged Robots in Complex Environments
A1  - Xi Chen
A1  - Ali Ghadirzadeh
A1  - John Folkesson
A1  - Patric Jensfelt
JO  - ArXiv e-prints
Y1  - 27 April, 2018
UR  - https://arxiv.org/abs/1804.10500
N2  - Mobile robot navigation in complex and dynamic environments is a challenging but important problem. Reinforcement learning approaches fail to solve these tasks efficiently due to reward sparsities, temporal complexities and high-dimensionality of sensorimotor spaces which are inherent in such problems. We present a novel approach to train action policies to acquire navigation skills for wheel-legged robots using deep reinforcement learning. The policy maps height-map image observations to motor commands to navigate to a target position while avoiding obstacles. We propose to acquire the multifaceted navigation skill by learning and exploiting a number of manageable navigation behaviors. We also introduce a domain randomization technique to improve the versatility of the training samples. We demonstrate experimentally a significant improvement in terms of data-efficiency, success rate, robustness against irrelevant sensory data, and also the quality of the maneuver skills.
ER  -


TY  - Preprint
T1  - Automatic classification of trees using a UAV onboard camera and deep learning
A1  - Masanori Onishi
A1  - Takeshi Ise
JO  - ArXiv e-prints
Y1  - 27 April, 2018
UR  - https://arxiv.org/abs/1804.10390
N2  - Automatic classification of trees using remotely sensed data has been a dream of many scientists and land use managers. Recently, Unmanned aerial vehicles (UAV) has been expected to be an easy-to-use, cost-effective tool for remote sensing of forests, and deep learning has attracted attention for its ability concerning machine vision. In this study, using a commercially available UAV and a publicly available package for deep learning, we constructed a machine vision system for the automatic classification of trees. In our method, we segmented a UAV photography image of forest into individual tree crowns and carried out object-based deep learning. As a result, the system was able to classify 7 tree types at 89.0% accuracy. This performance is notable because we only used basic RGB images from a standard UAV. In contrast, most of previous studies used expensive hardware such as multispectral imagers to improve the performance. This result means that our method has the potential to classify individual trees in a cost-effective manner. This can be a usable tool for many forest researchers and managements.
ER  -


TY  - Preprint
T1  - dhSegment: A generic deep-learning approach for document segmentation
A1  - Sofia Ares Oliveira
A1  - Benoit Seguin
A1  - Frederic Kaplan
JO  - ArXiv e-prints
Y1  - 27 April, 2018
UR  - https://arxiv.org/abs/1804.10371
N2  - In recent years there have been multiple successful attempts tackling document processing problems separately by designing task specific hand-tuned strategies. We argue that the diversity of historical document processing tasks prohibits to solve them one at a time and shows a need for designing generic approaches in order to handle the variability of historical series. In this paper, we address multiple tasks simultaneously such as page extraction, baseline extraction, layout analysis or multiple typologies of illustrations and photograph extraction. We propose an open-source implementation of a CNN-based pixel-wise predictor coupled with task dependent post-processing blocks. We show that a single CNN-architecture can be used across tasks with competitive results. Moreover most of the task-specific post-precessing steps can be decomposed in a small number of simple and standard reusable operations, adding to the flexibility of our approach.
ER  -


TY  - Preprint
T1  - Deep Learning Coordinated Beamforming for Highly-Mobile Millimeter Wave Systems
A1  - Ahmed Alkhateeb
A1  - Sam Alex
A1  - Paul Varkey
A1  - Ying Li
A1  - Qi Qu
A1  - Djordje Tujkovic
JO  - ArXiv e-prints
Y1  - 27 April, 2018
UR  - https://arxiv.org/abs/1804.10334
N2  - Supporting high mobility in millimeter wave (mmWave) systems enables a wide range of important applications such as vehicular communications and wireless virtual/augmented reality. Realizing this in practice, though, requires overcoming several challenges. First, the use of narrow beams and the sensitivity of mmWave signals to blockage greatly impact the coverage and reliability of highly-mobile links. Second, highly-mobile users in dense mmWave deployments need to frequently hand-off between base stations (BSs), which is associated with critical control and latency overhead. Further, identifying the optimal beamforming vectors in large antenna array mmWave systems requires considerable training overhead, which significantly affects the efficiency of these mobile systems. In this paper, a novel integrated machine learning and coordinated beamforming solution is developed to overcome these challenges and enable highly-mobile mmWave applications. In the proposed solution, a number of distributed yet coordinating BSs simultaneously serve a mobile user. This user ideally needs to transmit only one uplink training pilot sequence that will be jointly received at the coordinating BSs using omni or quasi-omni beam patterns. These received signals draw a defining signature not only for the user location, but also for its interaction with the surrounding environment. The developed solution then leverages a deep learning model that learns how to use these signatures to predict the beamforming vectors at the BSs. This renders a comprehensive solution that supports highly-mobile mmWave applications with reliable coverage, low latency, and negligible training overhead. Simulation results show that the proposed deep-learning coordinated beamforming strategy approaches the achievable rate of the genie-aided solution that knows the optimal beamforming vectors with no training overhead.
ER  -


TY  - Preprint
T1  - CD-CNN: A Partially Supervised Cross-Domain Deep Learning Model or Urban Resident Recognition
A1  - Jingyuan Wang
A1  - Xu He
A1  - Ze Wang
A1  - Junjie Wu
A1  - Nicholas Jing Yuan
A1  - Xing Xie
A1  - Zhang Xiong
JO  - ArXiv e-prints
Y1  - 26 April, 2018
UR  - https://arxiv.org/abs/1804.09901
N2  - Driven by the wave of urbanization in recent decades, the research topic about migrant behavior analysis draws great attention from both academia and the government. Nevertheless, subject to the cost of data collection and the lack of modeling methods, most of existing studies use only questionnaire surveys with sparse samples and non-individual level statistical data to achieve coarse-grained studies of migrant behaviors. In this paper, a partially supervised cross-domain deep learning model named CD-CNN is proposed for migrant/native recognition using mobile phone signaling data as behavioral features and questionnaire survey data as incomplete labels. Specifically, CD-CNN features in decomposing the mobile data into location domain and communication domain, and adopts a joint learning framework that combines two convolutional neural networks with a feature balancing scheme. Moreover, CD-CNN employs a three-step algorithm for training, in which the co-training step is of great value to partially supervised cross-domain learning. Comparative experiments on the city Wuxi demonstrate the high predictive power of CD-CNN. Two interesting applications further highlight the ability of CD-CNN for in-depth migrant behavioral analysis.
ER  -


TY  - Preprint
T1  - Prospects for Theranostics in Neurosurgical Imaging: Empowering Confocal Laser Endomicroscopy Diagnostics via Deep Learning
A1  - Mohammadhassan Izadyyazdanabadi
A1  - Evgenii Belykh
A1  - Michael Mooney
A1  - Jennifer Eschbacher
A1  - Peter Nakaji
A1  - Yezhou Yang
A1  - Mark C. Preul
JO  - ArXiv e-prints
Y1  - 18 August, 2018
UR  - https://arxiv.org/abs/1804.09873
N2  - Confocal laser endomicroscopy (CLE) is an advanced optical fluorescence imaging technology that has the potential to increase intraoperative precision, extend resection, and tailor surgery for malignant invasive brain tumors because of its subcellular dimension resolution. Despite its promising diagnostic potential, interpreting the gray tone fluorescence images can be difficult for untrained users. In this review, we provide a detailed description of bioinformatical analysis methodology of CLE images that begins to assist the neurosurgeon and pathologist to rapidly connect on-the-fly intraoperative imaging, pathology, and surgical observation into a conclusionary system within the concept of theranostics. We present an overview and discuss deep learning models for automatic detection of the diagnostic CLE images and discuss various training regimes and ensemble modeling effect on the power of deep learning predictive models. Two major approaches reviewed in this paper include the models that can automatically classify CLE images into diagnostic/nondiagnostic, glioma/nonglioma, tumor/injury/normal categories and models that can localize histological features on the CLE images using weakly supervised methods. We also briefly review advances in the deep learning approaches used for CLE image analysis in other organs. Significant advances in speed and precision of automated diagnostic frame selection would augment the diagnostic potential of CLE, improve operative workflow and integration into brain tumor surgery. Such technology and bioinformatics analytics lend themselves to improved precision, personalization, and theranostics in brain tumor treatment.
ER  -


TY  - Preprint
T1  - Off the Beaten Track: Using Deep Learning to Interpolate Between Music Genres
A1  - Tijn Borghuis
A1  - Alessandro Tibo
A1  - Simone Conforti
A1  - Luca Canciello
A1  - Lorenzo Brusci
A1  - Paolo Frasconi
JO  - ArXiv e-prints
Y1  - 2 May, 2018
UR  - https://arxiv.org/abs/1804.09808
N2  - We describe a system based on deep learning that generates drum patterns in the electronic dance music domain. Experimental results reveal that generated patterns can be employed to produce musically sound and creative transitions between different genres, and that the process of generation is of interest to practitioners in the field.
ER  -


TY  - Preprint
T1  - 3D Consistent &amp; Robust Segmentation of Cardiac Images by Deep Learning with Spatial Propagation
A1  - Qiao Zheng
A1  - HervÃ© Delingette
A1  - Nicolas Duchateau
A1  - Nicholas Ayache
JO  - ArXiv e-prints
Y1  - 25 April, 2018
UR  - https://arxiv.org/abs/1804.09400
N2  - We propose a method based on deep learning to perform cardiac segmentation on short axis MRI image stacks iteratively from the top slice (around the base) to the bottom slice (around the apex). At each iteration, a novel variant of U-net is applied to propagate the segmentation of a slice to the adjacent slice below it. In other words, the prediction of a segmentation of a slice is dependent upon the already existing segmentation of an adjacent slice. 3D-consistency is hence explicitly enforced. The method is trained on a large database of 3078 cases from UK Biobank. It is then tested on 756 different cases from UK Biobank and three other state-of-the-art cohorts (ACDC with 100 cases, Sunnybrook with 30 cases, RVSC with 16 cases). Results comparable or even better than the state-of-the-art in terms of distance measures are achieved. They also emphasize the assets of our method, namely enhanced spatial consistency (currently neither considered nor achieved by the state-of-the-art), and the generalization ability to unseen cases even from other databases.
ER  -


TY  - Preprint
T1  - Adaptation and Re-Identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-Identification
A1  - Yu-Jhe Li
A1  - Fu-En Yang
A1  - Yen-Cheng Liu
A1  - Yu-Ying Yeh
A1  - Xiaofei Du
A1  - Yu-Chiang Frank Wang
JO  - ArXiv e-prints
Y1  - 25 April, 2018
UR  - https://arxiv.org/abs/1804.09347
N2  - Person re-identification (Re-ID) aims at recognizing the same person from images taken across different cameras. To address this task, one typically requires a large amount labeled data for training an effective Re-ID model, which might not be practical for real-world applications. To alleviate this limitation, we choose to exploit a sufficient amount of pre-existing labeled data from a different (auxiliary) dataset. By jointly considering such an auxiliary dataset and the dataset of interest (but without label information), our proposed adaptation and re-identification network (ARN) performs unsupervised domain adaptation, which leverages information across datasets and derives domain-invariant features for Re-ID purposes. In our experiments, we verify that our network performs favorably against state-of-the-art unsupervised Re-ID approaches, and even outperforms a number of baseline Re-ID methods which require fully supervised data for training.
ER  -


TY  - Preprint
T1  - Automated Mouse Organ Segmentation: A Deep Learning Based Solution
A1  - Naveen Ashish
A1  - Mi-Youn Brusniak
JO  - ArXiv e-prints
Y1  - 16 July, 2018
UR  - https://arxiv.org/abs/1804.09205
N2  - The analysis of animal cross section images, such as cross sections of laboratory mice, is critical in assessing the effect of experimental drugs such as the biodistribution of candidate compounds in preclinical drug development stage. Tissue distribution of radiolabeled candidate therapeutic compounds can be quantified using techniques like Quantitative Whole-Body Autoradiography (QWBA).QWBA relies, among other aspects, on the accurate segmentation or identification of key organs of interest in the animal cross section image such as the brain, spine, heart, liver and others. We present a deep learning based organ segmentation solution to this problem, using which we can achieve automated organ segmentation with high precision (dice coefficient in the 0.83-0.95 range depending on organ) for the key organs of interest.
ER  -


TY  - Preprint
T1  - Realistic Evaluation of Deep Semi-Supervised Learning Algorithms
A1  - Avital Oliver
A1  - Augustus Odena
A1  - Colin Raffel
A1  - Ekin D. Cubuk
A1  - Ian J. Goodfellow
JO  - ArXiv e-prints
Y1  - 23 May, 2018
UR  - https://arxiv.org/abs/1804.09170
N2  - Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that these algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, that SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and that performance can degrade substantially when the unlabeled dataset contains out-of-class examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.
ER  -


TY  - Preprint
T1  - Deep Neural Network Based Subspace Learning of Robotic Manipulator Workspace Mapping
A1  - Peiyuan Liao
JO  - ArXiv e-prints
Y1  - 24 April, 2018
UR  - https://arxiv.org/abs/1804.08951
N2  - The manipulator workspace mapping is an important problem in robotics and has attracted significant attention in the community. However, most of the pre-existing algorithms have expensive time complexity due to the reliance on sophisticated kinematic equations. To solve this problem, this paper introduces subspace learning (SL), a variant of subspace embedding, where a set of robot and scope parameters is mapped to the corresponding workspace by a deep neural network (DNN). Trained on a large dataset of around $\mathbf{6\times 10^4}$ samples obtained from a MATLAB$^\circledR$ implementation of a classical method and sampling of designed uniform distributions, the experiments demonstrate that the embedding significantly reduces run-time from $\mathbf{5.23 \times 10^3}$ s of traditional discretization method to $\mathbf{0.224}$ s, with high accuracies (average F-measure is $\mathbf{0.9665}$ with batch gradient descent and resilient backpropagation).
ER  -


TY  - Preprint
T1  - Towards Dependable Deep Convolutional Neural Networks (CNNs) with Out-distribution Learning
A1  - Mahdieh Abbasi
A1  - Arezoo Rajabi
A1  - Christian GagnÃ©
A1  - Rakesh B. Bobba
JO  - ArXiv e-prints
Y1  - 16 May, 2018
UR  - https://arxiv.org/abs/1804.08794
N2  - Detection and rejection of adversarial examples in security sensitive and safety-critical systems using deep CNNs is essential. In this paper, we propose an approach to augment CNNs with out-distribution learning in order to reduce misclassification rate by rejecting adversarial examples. We empirically show that our augmented CNNs can either reject or classify correctly most adversarial examples generated using well-known methods ( &gt;95% for MNIST and &gt;75% for CIFAR-10 on average). Furthermore, we achieve this without requiring to train using any specific type of adversarial examples and without sacrificing the accuracy of models on clean samples significantly (&lt; 4%).
ER  -


TY  - Preprint
T1  - Query-Efficient GAN Based Black-Box Attack Against Sequence Based Machine and Deep Learning Classifiers
A1  - Ishai Rosenberg
A1  - Asaf Shabtai
A1  - Yuval Elovici
A1  - Lior Rokach
JO  - ArXiv e-prints
Y1  - 22 September, 2018
UR  - https://arxiv.org/abs/1804.08778
N2  - In this paper, we present a generic black-box attack, demonstrated against API call based machine learning malware classifiers. We generate adversarial examples combining sequences (API call sequences) and other features (e.g., printable strings) that will be misclassified by the classifier without affecting the malware functionality. Our attack minimizes the number of target classifier queries and only requires access to the predicted label of the attacked model (without the confidence level). We evaluate the attack&#39;s effectiveness against many classifiers such as RNN variants, DNN, SVM, GBDT, etc. We show that the attack requires fewer queries and less knowledge about the attacked model&#39;s architecture than other existing black-box attacks, making it optimal to attack cloud based models at a minimal cost. Finally, we discuss the robustness of this attack to existing defense mechanisms.
ER  -


TY  - Preprint
T1  - All-Optical Machine Learning Using Diffractive Deep Neural Networks
A1  - Xing Lin
A1  - Yair Rivenson
A1  - Nezih T. Yardimci
A1  - Muhammed Veli
A1  - Mona Jarrahi
A1  - Aydogan Ozcan
JO  - ArXiv e-prints
Y1  - 26 July, 2018
UR  - https://arxiv.org/abs/1804.08711
N2  - We introduce an all-optical Diffractive Deep Neural Network (D2NN) architecture that can learn to implement various functions after deep learning-based design of passive diffractive layers that work collectively. We experimentally demonstrated the success of this framework by creating 3D-printed D2NNs that learned to implement handwritten digit classification and the function of an imaging lens at terahertz spectrum. With the existing plethora of 3D-printing and other lithographic fabrication methods as well as spatial-light-modulators, this all-optical deep learning framework can perform, at the speed of light, various complex functions that computer-based neural networks can implement, and will find applications in all-optical image analysis, feature detection and object classification, also enabling new camera designs and optical components that can learn to perform unique tasks using D2NNs.
ER  -


TY  - Preprint
T1  - State Distribution-aware Sampling for Deep Q-learning
A1  - Weichao Li
A1  - Fuxian Huang
A1  - Xi Li
A1  - Gang Pan
A1  - Fei Wu
JO  - ArXiv e-prints
Y1  - 23 April, 2018
UR  - https://arxiv.org/abs/1804.08619
N2  - A critical and challenging problem in reinforcement learning is how to learn the state-action value function from the experience replay buffer and simultaneously keep sample efficiency and faster convergence to a high quality solution. In prior works, transitions are uniformly sampled at random from the replay buffer or sampled based on their priority measured by temporal-difference (TD) error. However, these approaches do not fully take into consideration the intrinsic characteristics of transition distribution in the state space and could result in redundant and unnecessary TD updates, slowing down the convergence of the learning procedure. To overcome this problem, we propose a novel state distribution-aware sampling method to balance the replay times for transitions with skew distribution, which takes into account both the occurrence frequencies of transitions and the uncertainty of state-action values. Consequently, our approach could reduce the unnecessary TD updates and increase the TD updates for state-action value with more uncertainty, making the experience replay more effective and efficient. Extensive experiments are conducted on both classic control tasks and Atari 2600 games based on OpenAI gym platform and the experimental results demonstrate the effectiveness of our approach in comparison with the standard DQN approach.
ER  -


TY  - Preprint
T1  - BrainSlug: Transparent Acceleration of Deep Learning Through Depth-First Parallelism
A1  - Nicolas Weber
A1  - Florian Schmidt
A1  - Mathias Niepert
A1  - Felipe Huici
JO  - ArXiv e-prints
Y1  - 23 April, 2018
UR  - https://arxiv.org/abs/1804.08378
N2  - Neural network frameworks such as PyTorch and TensorFlow are the workhorses of numerous machine learning applications ranging from object recognition to machine translation. While these frameworks are versatile and straightforward to use, the training of and inference in deep neural networks is resource (energy, compute, and memory) intensive. In contrast to recent works focusing on algorithmic enhancements, we introduce BrainSlug, a framework that transparently accelerates neural network workloads by changing the default layer-by-layer processing to a depth-first approach, reducing the amount of data required by the computations and thus improving the performance of the available hardware caches. BrainSlug achieves performance improvements of up to 41.1% on CPUs and 35.7% on GPUs. These optimizations come at zero cost to the user as they do not require hardware changes and only need tiny adjustments to the software.
ER  -


TY  - Preprint
T1  - VLocNet++: Deep Multitask Learning for Semantic Visual Localization and Odometry
A1  - Noha Radwan
A1  - Abhinav Valada
A1  - Wolfram Burgard
JO  - ArXiv e-prints
Y1  - 26 September, 2018
UR  - https://arxiv.org/abs/1804.08366
N2  - Semantic understanding and localization are fundamental enablers of robot autonomy that have for the most part been tackled as disjoint problems. While deep learning has enabled recent breakthroughs across a wide spectrum of scene understanding tasks, its applicability to state estimation tasks has been limited due to the direct formulation that renders it incapable of encoding scene-specific constrains. In this work, we propose the VLocNet++ architecture that employs a multitask learning approach to exploit the inter-task relationship between learning semantics, regressing 6-DoF global pose and odometry, for the mutual benefit of each of these tasks. Our network overcomes the aforementioned limitation by simultaneously embedding geometric and semantic knowledge of the world into the pose regression network. We propose a novel adaptive weighted fusion layer to aggregate motion-specific temporal information and to fuse semantic features into the localization stream based on region activations. Furthermore, we propose a self-supervised warping technique that uses the relative motion to warp intermediate network representations in the segmentation stream for learning consistent semantics. Finally, we introduce a first-of-a-kind urban outdoor localization dataset with pixel-level semantic labels and multiple loops for training deep networks. Extensive experiments on the challenging Microsoft 7-Scenes benchmark and our DeepLoc dataset demonstrate that our approach exceeds the state-of-the-art outperforming local feature-based methods while simultaneously performing multiple tasks and exhibiting substantial robustness in challenging scenarios.
ER  -


TY  - Preprint
T1  - Deep Learning in Spiking Neural Networks
A1  - Amirhossein Tavanaei
A1  - Masoud Ghodrati
A1  - Saeed Reza Kheradpisheh
A1  - Timothee Masquelier
A1  - Anthony S. Maida
JO  - ArXiv e-prints
Y1  - 1 September, 2018
UR  - https://arxiv.org/abs/1804.08150
N2  - In recent years, deep learning has been a revolution in the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained in a supervised manner using backpropagation. Huge amounts of labeled examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and arguably the only viable option if one wants to understand how the brain computes. SNNs are also more hardware friendly and energy-efficient than ANNs, and are thus appealing for technology, especially for portable devices. However, training deep SNNs remains a challenge. Spiking neurons&#39; transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy, but also computational cost and hardware friendliness. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while the SNNs typically require much fewer operations.
ER  -


TY  - Preprint
T1  - A Deep Learning Approach for Forecasting Air Pollution in South Korea Using LSTM
A1  - Tien-Cuong Bui
A1  - Van-Duc Le
A1  - Sang-Kyun Cha
JO  - ArXiv e-prints
Y1  - 10 May, 2018
UR  - https://arxiv.org/abs/1804.07891
N2  - Tackling air pollution is an imperative problem in South Korea, especially in urban areas, over the last few years. More specially, South Korea has joined the ranks of the world&#39;s most polluted countries alongside with other Asian capitals, such as Beijing or Delhi. Much research is being conducted in environmental science to evaluate the dangerous impact of particulate matters on public health. Besides that, deterministic models of air pollutant behavior are also generated; however, this is both complex and often inaccurate. On the contrary, deep recurrent neural network reveals potent potential on forecasting out-comes of time-series data and has become more prevalent. This paper uses Recurrent Neural Network (RNN) with Long Short-Term Memory units as a framework for leveraging knowledge from time-series data of air pollution and meteorological information in Daegu, Seoul, Beijing, and Shenyang. Additionally, we use encoder-decoder model, which is similar to machine comprehension problems, as a crucial part of our prediction machine. Finally, we investigate the prediction accuracy of various configurations. Our experiments prevent the efficiency of integrating multiple layers of RNN on prediction model when forecasting far timesteps ahead. This research is a significant motivation for not only continuing researching on urban air quality but also help the government leverage that insight to enact beneficial policies
ER  -


TY  - Preprint
T1  - GritNet: Student Performance Prediction with Deep Learning
A1  - Byung-Hak Kim
A1  - Ethan Vizitei
A1  - Varun Ganapathi
JO  - ArXiv e-prints
Y1  - 19 April, 2018
UR  - https://arxiv.org/abs/1804.07405
N2  - Student performance prediction - where a machine forecasts the future performance of students as they interact with online coursework - is a challenging problem. Reliable early-stage predictions of a student&#39;s future performance could be critical to facilitate timely educational interventions during a course. However, very few prior studies have explored this problem from a deep learning perspective. In this paper, we recast the student performance prediction problem as a sequential event prediction problem and propose a new deep learning based algorithm, termed GritNet, which builds upon the bidirectional long short term memory (BLSTM). Our results, from real Udacity students&#39; graduation predictions, show that the GritNet not only consistently outperforms the standard logistic-regression based method, but that improvements are substantially pronounced in the first few weeks when accurate predictions are most challenging.
ER  -


TY  - Preprint
T1  - Minimizing Area and Energy of Deep Learning Hardware Design Using Collective Low Precision and Structured Compression
A1  - Shihui Yin
A1  - Gaurav Srivastava
A1  - Shreyas K. Venkataramanaiah
A1  - Chaitali Chakrabarti
A1  - Visar Berisha
A1  - Jae-sun Seo
JO  - ArXiv e-prints
Y1  - 19 April, 2018
UR  - https://arxiv.org/abs/1804.07370
N2  - Deep learning algorithms have shown tremendous success in many recognition tasks; however, these algorithms typically include a deep neural network (DNN) structure and a large number of parameters, which makes it challenging to implement them on power/area-constrained embedded platforms. To reduce the network size, several studies investigated compression by introducing element-wise or row-/column-/block-wise sparsity via pruning and regularization. In addition, many recent works have focused on reducing precision of activations and weights with some reducing down to a single bit. However, combining various sparsity structures with binarized or very-low-precision (2-3 bit) neural networks have not been comprehensively explored. In this work, we present design techniques for minimum-area/-energy DNN hardware with minimal degradation in accuracy. During training, both binarization/low-precision and structured sparsity are applied as constraints to find the smallest memory footprint for a given deep learning algorithm. The DNN model for CIFAR-10 dataset with weight memory reduction of 50X exhibits accuracy comparable to that of the floating-point counterpart. Area, performance and energy results of DNN hardware in 40nm CMOS are reported for the MNIST dataset. The optimized DNN that combines 8X structured compression and 3-bit weight precision showed 98.4% accuracy at 20nJ per classification.
ER  -


TY  - Preprint
T1  - Deep Layered Learning in MIR
A1  - Anders Elowsson
JO  - ArXiv e-prints
Y1  - 29 April, 2018
UR  - https://arxiv.org/abs/1804.07297
N2  - Deep learning has boosted the performance of many music information retrieval (MIR) systems in recent years. Yet, the complex hierarchical arrangement of music makes end-to-end learning hard for some MIR tasks - a very deep and structurally flexible processing chain is necessary to extract high-level features from a spectrogram representation. Mid-level representations such as tones, pitched onsets, chords, and beats are fundamental building blocks of music. This paper discusses how these can be used as intermediate representations in MIR to facilitate deep processing that generalizes well: each music concept is predicted individually in learning modules that are connected through latent representations in a directed acyclic graph. It is suggested that this strategy for inference, defined as deep layered learning (DLL), can help generalization by (1) - enforcing the validity of intermediate representations during processing, and by (2) - letting the inferred representations establish disentangled structures that support high-level invariant processing. A background to DLL and modular music processing is provided, and relevant concepts such as pruning, skip-connections, and layered performance supervision are reviewed.
ER  -


TY  - Preprint
T1  - Cell Selection with Deep Reinforcement Learning in Sparse Mobile Crowdsensing
A1  - Leye Wang
A1  - Wenbin Liu
A1  - Daqing Zhang
A1  - Yasha Wang
A1  - En Wang
A1  - Yongjian Yang
JO  - ArXiv e-prints
Y1  - 24 May, 2018
UR  - https://arxiv.org/abs/1804.07047
N2  - Sparse Mobile CrowdSensing (MCS) is a novel MCS paradigm where data inference is incorporated into the MCS process for reducing sensing costs while its quality is guaranteed. Since the sensed data from different cells (sub-areas) of the target sensing area will probably lead to diverse levels of inference data quality, cell selection (i.e., choose which cells of the target area to collect sensed data from participants) is a critical issue that will impact the total amount of data that requires to be collected (i.e., data collection costs) for ensuring a certain level of quality. To address this issue, this paper proposes a Deep Reinforcement learning based Cell selection mechanism for Sparse MCS, called DR-Cell. First, we properly model the key concepts in reinforcement learning including state, action, and reward, and then propose to use a deep recurrent Q-network for learning the Q-function that can help decide which cell is a better choice under a certain state during cell selection. Furthermore, we leverage the transfer learning techniques to reduce the amount of data required for training the Q-function if there are multiple correlated MCS tasks that need to be conducted in the same target area. Experiments on various real-life sensing datasets verify the effectiveness of DR-Cell over the state-of-the-art cell selection mechanisms in Sparse MCS by reducing up to 15% of sensed cells with the same data inference quality guarantee.
ER  -


TY  - Preprint
T1  - Semantic Adversarial Deep Learning
A1  - Tommaso Dreossi
A1  - Somesh Jha
A1  - Sanjit A. Seshia
JO  - ArXiv e-prints
Y1  - 18 May, 2018
UR  - https://arxiv.org/abs/1804.07045
N2  - Fueled by massive amounts of data, models produced by machine-learning (ML) algorithms, especially deep neural networks, are being used in diverse domains where trustworthiness is a concern, including automotive systems, finance, health care, natural language processing, and malware detection. Of particular concern is the use of ML algorithms in cyber-physical systems (CPS), such as self-driving cars and aviation, where an adversary can cause serious consequences. However, existing approaches to generating adversarial examples and devising robust ML algorithms mostly ignore the semantics and context of the overall system containing the ML component. For example, in an autonomous vehicle using deep learning for perception, not every adversarial example for the neural network might lead to a harmful consequence. Moreover, one may want to prioritize the search for adversarial examples towards those that significantly modify the desired semantics of the overall system. Along the same lines, existing algorithms for constructing robust ML algorithms ignore the specification of the overall system. In this paper, we argue that the semantics and specification of the overall system has a crucial role to play in this line of research. We present preliminary research results that support this claim.
ER  -


TY  - Preprint
T1  - Learning to Extract Coherent Summary via Deep Reinforcement Learning
A1  - Yuxiang Wu
A1  - Baotian Hu
JO  - ArXiv e-prints
Y1  - 19 April, 2018
UR  - https://arxiv.org/abs/1804.07036
N2  - Coherence plays a critical role in producing a high-quality summary from a document. In recent years, neural extractive summarization is becoming increasingly attractive. However, most of them ignore the coherence of summaries when extracting sentences. As an effort towards extracting coherent summaries, we propose a neural coherence model to capture the cross-sentence semantic and syntactic coherence patterns. The proposed neural coherence model obviates the need for feature engineering and can be trained in an end-to-end fashion using unlabeled data. Empirical results show that the proposed neural coherence model can efficiently capture the cross-sentence coherence patterns. Using the combined output of the neural coherence model and ROUGE package as the reward, we design a reinforcement learning method to train a proposed neural extractive summarizer which is named Reinforced Neural Extractive Summarization (RNES) model. The RNES model learns to optimize coherence and informative importance of the summary simultaneously. Experimental results show that the proposed RNES outperforms existing baselines and achieves state-of-the-art performance in term of ROUGE on CNN/Daily Mail dataset. The qualitative evaluation indicates that summaries produced by RNES are more coherent and readable.
ER  -


TY  - Preprint
T1  - Infrared and Visible Image Fusion using a Deep Learning Framework
A1  - Hui Li
A1  - Xiao-Jun Wu
A1  - Josef Kittler
JO  - ArXiv e-prints
Y1  - 19 May, 2018
UR  - https://arxiv.org/abs/1804.06992
N2  - In recent years, deep learning has become a very active research tool which is used in many image processing fields. In this paper, we propose an effective image fusion method using a deep learning framework to generate a single image which contains all the features from infrared and visible images. First, the source images are decomposed into base parts and detail content. Then the base parts are fused by weighted-averaging. For the detail content, we use a deep learning network to extract multi-layer features. Using these features, we use l_1-norm and weighted-average strategy to generate several candidates of the fused detail content. Once we get these candidates, the max selection strategy is used to get final fused detail content. Finally, the fused image will be reconstructed by combining the fused base part and detail content. The experimental results demonstrate that our proposed method achieves state-of-the-art performance in both objective assessment and visual quality. The Code of our fusion method is available at https://github.com/exceptionLi/imagefusion_deeplearning
ER  -


TY  - Preprint
T1  - A Study on Overfitting in Deep Reinforcement Learning
A1  - Chiyuan Zhang
A1  - Oriol Vinyals
A1  - Remi Munos
A1  - Samy Bengio
JO  - ArXiv e-prints
Y1  - 20 April, 2018
UR  - https://arxiv.org/abs/1804.06893
N2  - Recent years have witnessed significant progresses in deep Reinforcement Learning (RL). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging RL problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep RL techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard RL agents and find that they could overfit in various ways. Moreover, overfitting could happen &#34;robustly&#34;: commonly used techniques in RL that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in RL. We conclude with a general discussion on overfitting in RL and a study of the generalization behaviors from the perspective of inductive bias.
ER  -


TY  - Preprint
T1  - DPRed: Making Typical Activation Values Matter In Deep Learning Computing
A1  - Alberto Delmas
A1  - Sayeh Sharify
A1  - Patrick Judd
A1  - Kevin Siu
A1  - Milos Nikolic
A1  - Andreas Moshovos
JO  - ArXiv e-prints
Y1  - 15 May, 2018
UR  - https://arxiv.org/abs/1804.06732
N2  - We show that selecting a fixed precision for all values in Convolutional Neural Networks, even if that precision is different per layer, amounts to worst case design. We show that much lower precisions can be used if we could target the common case instead by tailoring the precision at a much finer granularity than that of a layer. While this observation may not be surprising, to date no design takes advantage of it in practice. We propose Dynamic Prediction Reduction (DPRed), where hardware on-the-fly detects the precision activations need at a much finer granularity than a whole layer. Further we encode activations and weights using the respective per group dynamically and statically detected precisions to reduce off- and on-chip storage and communication. We demonstrate a practical implementation of DPRed with DPRed Stripes (DPRS), a data-parallel hardware accelerator that adjusts precision on-the-fly to accommodate the values of the activations it processes concurrently. DPRS accelerates convolutional layers and executes unmodified convolutional neural networks. Ignoring offchip communication, DPRS is 2.61x faster and 1.84x more energy efficient than a fixed-precision accelerator for a set of convolutional neural networks. We further extend DPRS to exploit activation and weight precisions for fully-connected layers. The enhanced design improves average performance and energy efficiency respectively by 2.59x and 1.19x over the fixed-precision accelerator for a broader set of neural networks. Finally, we consider a lower cost variant that supports only even precision widths which offers better energy efficiency. Taking into account off-chip communication, DPRed compression reduces off-chip traffic to nearly 35% on average compared to no compression making it possible to sustain higher performance for a given off-chip memory interface while also boosting energy efficiency.
ER  -


TY  - Preprint
T1  - NTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning
A1  - Christos Baziotis
A1  - Nikos Athanasiou
A1  - Alexandra Chronopoulou
A1  - Athanasia Kolovou
A1  - Georgios Paraskevopoulos
A1  - Nikolaos Ellinas
A1  - Shrikanth Narayanan
A1  - Alexandros Potamianos
JO  - ArXiv e-prints
Y1  - 18 April, 2018
UR  - https://arxiv.org/abs/1804.06658
N2  - In this paper we present deep-learning models that submitted to the SemEval-2018 Task~1 competition: &#34;Affect in Tweets&#34;. We participated in all subtasks for English tweets. We propose a Bi-LSTM architecture equipped with a multi-layer self attention mechanism. The attention mechanism improves the model performance and allows us to identify salient words in tweets, as well as gain insight into the models making them more interpretable. Our model utilizes a set of word2vec word embeddings trained on a large collection of 550 million Twitter messages, augmented by a set of word affective features. Due to the limited amount of task-specific training data, we opted for a transfer learning approach by pretraining the Bi-LSTMs on the dataset of Semeval 2017, Task 4A. The proposed approach ranked 1st in Subtask E &#34;Multi-Label Emotion Classification&#34;, 2nd in Subtask A &#34;Emotion Intensity Regression&#34; and achieved competitive results in other subtasks.
ER  -


TY  - Preprint
T1  - The Limits and Potentials of Deep Learning for Robotics
A1  - Niko SÃ¼nderhauf
A1  - Oliver Brock
A1  - Walter Scheirer
A1  - Raia Hadsell
A1  - Dieter Fox
A1  - JÃ¼rgen Leitner
A1  - Ben Upcroft
A1  - Pieter Abbeel
A1  - Wolfram Burgard
A1  - Michael Milford
A1  - Peter Corke
JO  - ArXiv e-prints
Y1  - 18 April, 2018
UR  - https://arxiv.org/abs/1804.06557
N2  - The application of deep learning in robotics leads to very specific problems and research questions that are typically not addressed by the computer vision and machine learning communities. In this paper we discuss a number of robotics-specific learning, reasoning, and embodiment challenges for deep learning. We explain the need for better evaluation metrics, highlight the importance and unique challenges for deep robotic learning in simulation, and explore the spectrum between purely data-driven and model-driven approaches. We hope this paper provides a motivating overview of important research directions to overcome the current limitations, and help fulfill the promising potentials of deep learning in robotics.
ER  -


TY  - Preprint
T1  - Learning how to be robust: Deep polynomial regression
A1  - Juan-Manuel Perez-Rua
A1  - Tomas Crivelli
A1  - Patrick Bouthemy
A1  - Patrick Perez
JO  - ArXiv e-prints
Y1  - 23 May, 2018
UR  - https://arxiv.org/abs/1804.06504
N2  - Polynomial regression is a recurrent problem with a large number of applications. In computer vision it often appears in motion analysis. Whatever the application, standard methods for regression of polynomial models tend to deliver biased results when the input data is heavily contaminated by outliers. Moreover, the problem is even harder when outliers have strong structure. Departing from problem-tailored heuristics for robust estimation of parametric models, we explore deep convolutional neural networks. Our work aims to find a generic approach for training deep regression models without the explicit need of supervised annotation. We bypass the need for a tailored loss function on the regression parameters by attaching to our model a differentiable hard-wired decoder corresponding to the polynomial operation at hand. We demonstrate the value of our findings by comparing with standard robust regression methods. Furthermore, we demonstrate how to use such models for a real computer vision problem, i.e., video stabilization. The qualitative and quantitative experiments show that neural networks are able to learn robustness for general polynomial regression, with results that well overpass scores of traditional robust estimation methods.
ER  -


TY  - Preprint
T1  - On Improving Deep Reinforcement Learning for POMDPs
A1  - Pengfei Zhu
A1  - Xin Li
A1  - Pascal Poupart
A1  - Guanghui Miao
JO  - ArXiv e-prints
Y1  - 8 May, 2018
UR  - https://arxiv.org/abs/1804.06309
N2  - Deep Reinforcement Learning (RL) recently emerged as one of the most competitive approaches for learning in sequential decision making problems with fully observable environments, e.g., computer Go. However, very little work has been done in deep RL to handle partially observable environments. We propose a new architecture called Action-specific Deep Recurrent Q-Network (ADRQN) to enhance learning performance in partially observable domains. Actions are encoded by a fully connected layer and coupled with a convolutional observation to form an action-observation pair. The time series of action-observation pairs are then integrated by an LSTM layer that learns latent states based on which a fully connected layer computes Q-values as in conventional Deep Q-Networks (DQNs). We demonstrate the effectiveness of our new architecture in several partially observable domains, including flickering Atari games.
ER  -


TY  - Preprint
T1  - PredRNN++: Towards A Resolution of the Deep-in-Time Dilemma in Spatiotemporal Predictive Learning
A1  - Yunbo Wang
A1  - Zhifeng Gao
A1  - Mingsheng Long
A1  - Jianmin Wang
A1  - Philip S. Yu
JO  - ArXiv e-prints
Y1  - 17 April, 2018
UR  - https://arxiv.org/abs/1804.06300
N2  - We present PredRNN++, an improved recurrent network for video predictive learning. In pursuit of a greater spatiotemporal modeling capability, our approach increases the transition depth between adjacent states by leveraging a novel recurrent unit, which is named Causal LSTM for re-organizing the spatial and temporal memories in a cascaded mechanism. However, there is still a dilemma in video predictive learning: increasingly deep-in-time models have been designed for capturing complex variations, while introducing more difficulties in the gradient back-propagation. To alleviate this undesirable effect, we propose a Gradient Highway architecture, which provides alternative shorter routes for gradient flows from outputs back to long-range inputs. This architecture works seamlessly with causal LSTMs, enabling PredRNN++ to capture short-term and long-term dependencies adaptively. We assess our model on both synthetic and real video datasets, showing its ability to ease the vanishing gradient problem and yield state-of-the-art prediction results even in a difficult objects occlusion scenario.
ER  -


TY  - Preprint
T1  - Network Signatures from Image Representation of Adjacency Matrices: Deep/Transfer Learning for Subgraph Classification
A1  - Kshiteesh Hegde
A1  - Malik Magdon-Ismail
A1  - Ram Ramanathan
A1  - Bishal Thapa
JO  - ArXiv e-prints
Y1  - 17 April, 2018
UR  - https://arxiv.org/abs/1804.06275
N2  - We propose a novel subgraph image representation for classification of network fragments with the targets being their parent networks. The graph image representation is based on 2D image embeddings of adjacency matrices. We use this image representation in two modes. First, as the input to a machine learning algorithm. Second, as the input to a pure transfer learner. Our conclusions from several datasets are that (a) deep learning using our structured image features performs the best compared to benchmark graph kernel and classical features based methods; and, (b) pure transfer learning works effectively with minimum interference from the user and is robust against small data.
ER  -


TY  - Preprint
T1  - Automated vehicle&#39;s behavior decision making using deep reinforcement learning and high-fidelity simulation environment
A1  - Yingjun Ye
A1  - Xiaohui Zhang
A1  - Jian Sun
JO  - ArXiv e-prints
Y1  - 17 April, 2018
UR  - https://arxiv.org/abs/1804.06264
N2  - Automated vehicles are deemed to be the key element for the intelligent transportation system in the future. Many studies have been made to improve the Automated vehicles&#39; ability of environment recognition and vehicle control, while the attention paid to decision making is not enough though the decision algorithms so far are very preliminary. Therefore, a framework of the decision-making training and learning is put forward in this paper. It consists of two parts: the deep reinforcement learning training program and the high-fidelity virtual simulation environment. Then the basic microscopic behavior, car-following, is trained within this framework. In addition, theoretical analysis and experiments were conducted on setting reward function for accelerating training using deep reinforcement learning. The results show that on the premise of driving comfort, the efficiency of the trained Automated vehicle increases 7.9% compared to the classical traffic model, intelligent driver model. Later on, on a more complex three-lane section, we trained the integrated model combines both car-following and lane-changing behavior, the average speed further grows 2.4%. It indicates that our framework is effective for Automated vehicle&#39;s decision-making learning.
ER  -


TY  - Preprint
T1  - Deep Learning on Operational Facility Data Related to Large-Scale Distributed Area Scientific Workflows
A1  - Alok Singh
A1  - Eric Stephan
A1  - Malachi Schram
A1  - Ilkay Altintas
JO  - ArXiv e-prints
Y1  - 20 April, 2018
UR  - https://arxiv.org/abs/1804.06062
N2  - Distributed computing platforms provide a robust mechanism to perform large-scale computations by splitting the task and data among multiple locations, possibly located thousands of miles apart geographically. Although such distribution of resources can lead to benefits, it also comes with its associated problems such as rampant duplication of file transfers increasing congestion, long job completion times, unexpected site crashing, suboptimal data transfer rates, unpredictable reliability in a time range, and suboptimal usage of storage elements. In addition, each sub-system becomes a potential failure node that can trigger system wide disruptions. In this vision paper, we outline our approach to leveraging Deep Learning algorithms to discover solutions to unique problems that arise in a system with computational infrastructure that is spread over a wide area. The presented vision, motivated by a real scientific use case from Belle II experiments, is to develop multilayer neural networks to tackle forecasting, anomaly detection and optimization challenges in a complex and distributed data movement environment. Through this vision based on Deep Learning principles, we aim to achieve reduced congestion events, faster file transfer rates, and enhanced site reliability.
ER  -


TY  - Preprint
T1  - Learning a Deep Listwise Context Model for Ranking Refinement
A1  - Qingyao Ai
A1  - Keping Bi
A1  - Jiafeng Guo
A1  - W. Bruce Croft
JO  - ArXiv e-prints
Y1  - 23 April, 2018
UR  - https://arxiv.org/abs/1804.05936
N2  - Learning to rank has been intensively studied and widely applied in information retrieval. Typically, a global ranking function is learned from a set of labeled data, which can achieve good performance on average but may be suboptimal for individual queries by ignoring the fact that relevant documents for different queries may have different distributions in the feature space. Inspired by the idea of pseudo relevance feedback where top ranked documents, which we refer as the \textit{local ranking context}, can provide important information about the query&#39;s characteristics, we propose to use the inherent feature distributions of the top results to learn a Deep Listwise Context Model that helps us fine tune the initial ranked list. Specifically, we employ a recurrent neural network to sequentially encode the top results using their feature vectors, learn a local context model and use it to re-rank the top results. There are three merits with our model: (1) Our model can capture the local ranking context based on the complex interactions between top results using a deep neural network; (2) Our model can be built upon existing learning-to-rank methods by directly using their extracted feature vectors; (3) Our model is trained with an attention-based loss function, which is more effective and efficient than many existing listwise methods. Experimental results show that the proposed model can significantly improve the state-of-the-art learning to rank methods on benchmark retrieval corpora.
ER  -


TY  - Preprint
T1  - BigDL: A Distributed Deep Learning Framework for Big Data
A1  - Jason Dai
A1  - Yiheng Wang
A1  - Xin Qiu
A1  - Ding Ding
A1  - Yao Zhang
A1  - Yanzhang Wang
A1  - Xianyan Jia
A1  - Cherry Zhang
A1  - Yan Wan
A1  - Zhichao Li
A1  - Jiao Wang
A1  - Shengsheng Huang
A1  - Zhongyuan Wu
A1  - Yang Wang
A1  - Yuhao Yang
A1  - Bowen She
A1  - Dongjie Shi
A1  - Qi Lu
A1  - Kai Huang
A1  - Guoqiong Song
JO  - ArXiv e-prints
Y1  - 24 June, 2018
UR  - https://arxiv.org/abs/1804.05839
N2  - In this paper, we present BigDL, a distributed deep learning framework for Big Data platforms and workflows. It is implemented on top of Apache Spark, and allows users to write their deep learning applications as standard Spark programs (running directly on large-scale big data clusters in a distributed fashion). It provides an expressive, &#34;data-analytics integrated&#34; deep learning programming model, so that users can easily build the end-to-end analytics + AI pipelines under a unified programming paradigm; by implementing an AllReduce like operation using existing primitives in Spark (e.g., shuffle, broadcast, and in-memory data persistence), it also provides a highly efficient &#34;parameter server&#34; style architecture, so as to achieve highly scalable, data-parallel distributed training. Since its initial open source release, BigDL users have built many analytics and deep learning applications (e.g., object detection, sequence-to-sequence generation, visual similarity, neural recommendations, fraud detection, etc.) on Spark.
ER  -


TY  - Preprint
T1  - Multi-Modal Emotion recognition on IEMOCAP Dataset using Deep Learning
A1  - Samarth Tripathi
A1  - Homayoon Beigi
JO  - ArXiv e-prints
Y1  - 16 April, 2018
UR  - https://arxiv.org/abs/1804.05788
N2  - Emotion recognition has become an important field of research in Human Computer Interactions as we improve upon the techniques for modelling the various aspects of behaviour. With the advancement of technology our understanding of emotions are advancing, there is a growing need for automatic emotion recognition systems. One of the directions the research is heading is the use of Neural Networks which are adept at estimating complex functions that depend on a large number and diverse source of input data. In this paper we attempt to exploit this effectiveness of Neural networks to enable us to perform multimodal Emotion recognition on IEMOCAP dataset using data from Speech, Text, and Motion capture data from face expressions, rotation and hand movements. Prior research has concentrated on Emotion detection from Speech on the IEMOCAP dataset, but our approach is the first that uses the multiple modes of data offered by IEMOCAP for a more robust and accurate emotion detection.
ER  -


TY  - Preprint
T1  - Deep Learning on Key Performance Indicators for Predictive Maintenance in SAP HANA
A1  - Jaekoo Lee
A1  - Byunghan Lee
A1  - Jongyoon Song
A1  - Jaesik Yoon
A1  - Yongsik Lee
A1  - Donghun Lee
A1  - Sungroh Yoon
JO  - ArXiv e-prints
Y1  - 15 April, 2018
UR  - https://arxiv.org/abs/1804.05497
N2  - With a new era of cloud and big data, Database Management Systems (DBMSs) have become more crucial in numerous enterprise business applications in all the industries. Accordingly, the importance of their proactive and preventive maintenance has also increased. However, detecting problems by predefined rules or stochastic modeling has limitations, particularly when analyzing the data on high-dimensional Key Performance Indicators (KPIs) from a DBMS. In recent years, Deep Learning (DL) has opened new opportunities for this complex analysis. In this paper, we present two complementary DL approaches to detect anomalies in SAP HANA. A temporal learning approach is used to detect abnormal patterns based on unlabeled historical data, whereas a spatial learning approach is used to classify known anomalies based on labeled data. We implement a system in SAP HANA integrated with Google TensorFlow. The experimental results with real-world data confirm the effectiveness of the system and models.
ER  -


TY  - Preprint
T1  - Adversarial Attacks Against Medical Deep Learning Systems
A1  - Samuel G. Finlayson
A1  - Hyung Won Chung
A1  - Isaac S. Kohane
A1  - Andrew L. Beam
JO  - ArXiv e-prints
Y1  - 20 May, 2018
UR  - https://arxiv.org/abs/1804.05296
N2  - The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we argue that the field of medicine may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud, we extend adversarial attacks to three popular medical imaging tasks, and we provide concrete examples of how and why such attacks could be realistically carried out. For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful. We urge caution in deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.
ER  -


TY  - Preprint
T1  - A Deep Learning Approach to Fast, Format-Agnostic Detection of Malicious Web Content
A1  - Joshua Saxe
A1  - Richard Harang
A1  - Cody Wild
A1  - Hillary Sanders
JO  - ArXiv e-prints
Y1  - 13 April, 2018
UR  - https://arxiv.org/abs/1804.05020
N2  - Malicious web content is a serious problem on the Internet today. In this paper we propose a deep learning approach to detecting malevolent web pages. While past work on web content detection has relied on syntactic parsing or on emulation of HTML and Javascript to extract features, our approach operates directly on a language-agnostic stream of tokens extracted directly from static HTML files with a simple regular expression. This makes it fast enough to operate in high-frequency data contexts like firewalls and web proxies, and allows it to avoid the attack surface exposure of complex parsing and emulation code. Unlike well-known approaches such as bag-of-words models, which ignore spatial information, our neural network examines content at hierarchical spatial scales, allowing our model to capture locality and yielding superior accuracy compared to bag-of-words baselines. Our proposed architecture achieves a 97.5% detection rate at a 0.1% false positive rate, and classifies small-batched web pages at a rate of over 100 per second on commodity hardware. The speed and accuracy of our approach makes it appropriate for deployment to endpoints, firewalls, and web proxies.
ER  -


TY  - Preprint
T1  - DeepFM: An End-to-End Wide &amp; Deep Learning Framework for CTR Prediction
A1  - Huifeng Guo
A1  - Ruiming Tang
A1  - Yunming Ye
A1  - Zhenguo Li
A1  - Xiuqiang He
A1  - Zhenhua Dong
JO  - ArXiv e-prints
Y1  - 16 May, 2018
UR  - https://arxiv.org/abs/1804.04950
N2  - Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods have a strong bias towards low- or high-order interactions, or rely on expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed framework, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide &amp; Deep model from Google, DeepFM has a shared raw feature input to both its &#34;wide&#34; and &#34;deep&#34; components, with no need of feature engineering besides raw features. DeepFM, as a general learning framework, can incorporate various network architectures in its deep component. In this paper, we study two instances of DeepFM where its &#34;deep&#34; component is DNN and PNN respectively, for which we denote as DeepFM-D and DeepFM-P. Comprehensive experiments are conducted to demonstrate the effectiveness of DeepFM-D and DeepFM-P over the existing models for CTR prediction, on both benchmark data and commercial data. We conduct online A/B test in Huawei App Market, which reveals that DeepFM-D leads to more than 10% improvement of click-through rate in the production environment, compared to a well-engineered LR model. We also covered related practice in deploying our framework in Huawei App Market.
ER  -


TY  - Preprint
T1  - Scalable and Interpretable One-class SVMs with Deep Learning and Random Fourier features
A1  - Minh-Nghia Nguyen
A1  - Ngo Anh Vien
JO  - ArXiv e-prints
Y1  - 13 April, 2018
UR  - https://arxiv.org/abs/1804.04888
N2  - One-class Support Vector Machine (OC-SVM) for a long time has been one of the most effective anomaly detection methods and widely adopted in both research as well as industrial applications. The biggest issue for OC-SVM is, however, the capability to operate with large and high-dimensional datasets due to inefficient features and optimization complexity. Those problems might be mitigated via dimensionality reduction techniques such as manifold learning or auto-encoder. However, previous work often treats representation learning and anomaly prediction separately. In this paper, we propose autoencoder based one-class SVM (AE-1SVM) that brings OC-SVM, with the aid of random Fourier features to approximate the radial basis kernel, into deep learning context by combining it with a representation learning architecture and jointly exploit stochastic gradient descend to obtain end-to-end training. Interestingly, this also opens up the possible use of gradient-based attribution methods to explain the decision making for anomaly detection, which has ever been challenging as a result of the implicit mappings between the input space and the kernel space. To the best of our knowledge, this is the first work to study the interpretability of deep learning in anomaly detection. We evaluate our method on a wide range of unsupervised anomaly detection tasks in which our end-to-end training architecture achieves a performance significantly better than the previous work using separate training.
ER  -


TY  - Preprint
T1  - Î¼-cuDNN: Accelerating Deep Learning Frameworks with Micro-Batching
A1  - Yosuke Oyama
A1  - Tal Ben-Nun
A1  - Torsten Hoefler
A1  - Satoshi Matsuoka
JO  - ArXiv e-prints
Y1  - 13 April, 2018
UR  - https://arxiv.org/abs/1804.04806
N2  - NVIDIA cuDNN is a low-level library that provides GPU kernels frequently used in deep learning. Specifically, cuDNN implements several equivalent convolution algorithms, whose performance and memory footprint may vary considerably, depending on the layer dimensions. When an algorithm is automatically selected by cuDNN, the decision is performed on a per-layer basis, and thus it often resorts to slower algorithms that fit the workspace size constraints. We present Î¼-cuDNN, a transparent wrapper library for cuDNN, which divides layers&#39; mini-batch computation into several micro-batches. Based on Dynamic Programming and Integer Linear Programming, Î¼-cuDNN enables faster algorithms by decreasing the workspace requirements. At the same time, Î¼-cuDNN keeps the computational semantics unchanged, so that it decouples statistical efficiency from the hardware efficiency safely. We demonstrate the effectiveness of Î¼-cuDNN over two frameworks, Caffe and TensorFlow, achieving speedups of 1.63x for AlexNet and 1.21x for ResNet-18 on P100-SXM2 GPU. These results indicate that using micro-batches can seamlessly increase the performance of deep learning, while maintaining the same memory footprint.
ER  -


TY  - Preprint
T1  - Learning Deep Sketch Abstraction
A1  - Umar Riaz Muhammad
A1  - Yongxin Yang
A1  - Yi-Zhe Song
A1  - Tao Xiang
A1  - Timothy M. Hospedales
JO  - ArXiv e-prints
Y1  - 13 April, 2018
UR  - https://arxiv.org/abs/1804.04804
N2  - Human free-hand sketches have been studied in various contexts including sketch recognition, synthesis and fine-grained sketch-based image retrieval (FG-SBIR). A fundamental challenge for sketch analysis is to deal with drastically different human drawing styles, particularly in terms of abstraction level. In this work, we propose the first stroke-level sketch abstraction model based on the insight of sketch abstraction as a process of trading off between the recognizability of a sketch and the number of strokes used to draw it. Concretely, we train a model for abstract sketch generation through reinforcement learning of a stroke removal policy that learns to predict which strokes can be safely removed without affecting recognizability. We show that our abstraction model can be used for various sketch analysis tasks including: (1) modeling stroke saliency and understanding the decision of sketch recognition models, (2) synthesizing sketches of variable abstraction for a given category, or reference object instance in a photo, and (3) training a FG-SBIR model with photos only, bypassing the expensive photo-sketch pair collection step.
ER  -


TY  - Preprint
T1  - Outline Objects using Deep Reinforcement Learning
A1  - Zhenxin Wang
A1  - Sayan Sarcar
A1  - Jingxin Liu
A1  - Yilin Zheng
A1  - Xiangshi Ren
JO  - ArXiv e-prints
Y1  - 20 April, 2018
UR  - https://arxiv.org/abs/1804.04603
N2  - Image segmentation needs both local boundary position information and global object context information. The performance of the recent state-of-the-art method, fully convolutional networks, reaches a bottleneck due to the neural network limit after balancing between the two types of information simultaneously in an end-to-end training style. To overcome this problem, we divide the semantic image segmentation into temporal subtasks. First, we find a possible pixel position of some object boundary; then trace the boundary at steps within a limited length until the whole object is outlined. We present the first deep reinforcement learning approach to semantic image segmentation, called DeepOutline, which outperforms other algorithms in Coco detection leaderboard in the middle and large size person category in Coco val2017 dataset. Meanwhile, it provides an insight into a divide and conquer way by reinforcement learning on computer vision problems.
ER  -


TY  - Preprint
T1  - Feature-Based Aggregation and Deep Reinforcement Learning: A Survey and Some New Implementations
A1  - Dimitri P. Bertsekas
JO  - ArXiv e-prints
Y1  - 21 August, 2018
UR  - https://arxiv.org/abs/1804.04577
N2  - In this paper we discuss policy iteration methods for approximate solution of a finite-state discounted Markov decision problem, with a focus on feature-based aggregation methods and their connection with deep reinforcement learning schemes. We introduce features of the states of the original problem, and we formulate a smaller &#34;aggregate&#34; Markov decision problem, whose states relate to the features. We discuss properties and possible implementations of this type of aggregation, including a new approach to approximate policy iteration. In this approach the policy improvement operation combines feature-based aggregation with feature construction using deep neural networks or other calculations. We argue that the cost function of a policy may be approximated much more accurately by the nonlinear function of the features provided by aggregation, than by the linear function of the features provided by neural network-based reinforcement learning, thereby potentially leading to more effective policy improvement.
ER  -


TY  - Preprint
T1  - Forecasting Future Humphrey Visual Fields Using Deep Learning
A1  - Joanne C. Wen
A1  - Cecilia S. Lee
A1  - Pearse A. Keane
A1  - Sa Xiao
A1  - Yue Wu
A1  - Ariel Rokem
A1  - Philip P. Chen
A1  - Aaron Y. Lee
JO  - ArXiv e-prints
Y1  - 2 April, 2018
UR  - https://arxiv.org/abs/1804.04543
N2  - Purpose: To determine if deep learning networks could be trained to forecast a future 24-2 Humphrey Visual Field (HVF).
ER  -


TY  - Preprint
T1  - Distort-and-Recover: Color Enhancement using Deep Reinforcement Learning
A1  - Jongchan Park
A1  - Joon-Young Lee
A1  - Donggeun Yoo
A1  - In So Kweon
JO  - ArXiv e-prints
Y1  - 15 April, 2018
UR  - https://arxiv.org/abs/1804.04450
N2  - Learning-based color enhancement approaches typically learn to map from input images to retouched images. Most of existing methods require expensive pairs of input-retouched images or produce results in a non-interpretable way. In this paper, we present a deep reinforcement learning (DRL) based method for color enhancement to explicitly model the step-wise nature of human retouching process. We cast a color enhancement process as a Markov Decision Process where actions are defined as global color adjustment operations. Then we train our agent to learn the optimal global enhancement sequence of the actions. In addition, we present a &#39;distort-and-recover&#39; training scheme which only requires high-quality reference images for training instead of input and retouched image pairs. Given high-quality reference images, we distort the images&#39; color distribution and form distorted-reference image pairs for training. Through extensive experiments, we show that our method produces decent enhancement results and our DRL approach is more suitable for the &#39;distort-and-recover&#39; training scheme than previous supervised approaches. Supplementary material and code are available at https://sites.google.com/view/distort-and-recover/
ER  -


TY  - Preprint
T1  - End-to-end Deep Learning of Optical Fiber Communications
A1  - Boris Karanov
A1  - Mathieu Chagnon
A1  - FÃ©lix Thouin
A1  - Tobias A. Eriksson
A1  - Henning BÃ¼low
A1  - DomaniÃ§ Lavery
A1  - Polina Bayvel
A1  - Laurent Schmalen
JO  - ArXiv e-prints
Y1  - 3 August, 2018
UR  - https://arxiv.org/abs/1804.04097
N2  - In this paper, we implement an optical fiber communication system as an end-to-end deep neural network, including the complete chain of transmitter, channel model, and receiver. This approach enables the optimization of the transceiver in a single end-to-end process. We illustrate the benefits of this method by applying it to intensity modulation/direct detection (IM/DD) systems and show that we can achieve bit error rates below the 6.7\% hard-decision forward error correction (HD-FEC) threshold. We model all componentry of the transmitter and receiver, as well as the fiber channel, and apply deep learning to find transmitter and receiver configurations minimizing the symbol error rate. We propose and verify in simulations a training method that yields robust and flexible transceivers that allow---without reconfiguration---reliable transmission over a large range of link dispersions. The results from end-to-end deep learning are successfully verified for the first time in an experiment. In particular, we achieve information rates of 42\,Gb/s below the HD-FEC threshold at distances beyond 40\,km. We find that our results outperform conventional IM/DD solutions based on 2 and 4 level pulse amplitude modulation (PAM2/PAM4) with feedforward equalization (FFE) at the receiver. Our study is the first step towards end-to-end deep learning-based optimization of optical fiber communication systems.
ER  -


TY  - Preprint
T1  - EmoRL: Continuous Acoustic Emotion Classification using Deep Reinforcement Learning
A1  - Egor Lakomkin
A1  - Mohammad Ali Zamani
A1  - Cornelius Weber
A1  - Sven Magg
A1  - Stefan Wermter
JO  - ArXiv e-prints
Y1  - 3 April, 2018
UR  - https://arxiv.org/abs/1804.04053
N2  - Acoustically expressed emotions can make communication with a robot more efficient. Detecting emotions like anger could provide a clue for the robot indicating unsafe/undesired situations. Recently, several deep neural network-based models have been proposed which establish new state-of-the-art results in affective state evaluation. These models typically start processing at the end of each utterance, which not only requires a mechanism to detect the end of an utterance but also makes it difficult to use them in a real-time communication scenario, e.g. human-robot interaction. We propose the EmoRL model that triggers an emotion classification as soon as it gains enough confidence while listening to a person speaking. As a result, we minimize the need for segmenting the audio signal for classification and achieve lower latency as the audio signal is processed incrementally. The method is competitive with the accuracy of a strong baseline model, while allowing much earlier prediction.
ER  -


TY  - Preprint
T1  - Flexible and Scalable Deep Learning with MMLSpark
A1  - Mark Hamilton
A1  - Sudarshan Raghunathan
A1  - Akshaya Annavajhala
A1  - Danil Kirsanov
A1  - Eduardo de Leon
A1  - Eli Barzilay
A1  - Ilya Matiach
A1  - Joe Davison
A1  - Maureen Busch
A1  - Miruna Oprescu
A1  - Ratan Sur
A1  - Roope Astala
A1  - Tong Wen
A1  - ChangYoung Park
JO  - ArXiv e-prints
Y1  - 11 April, 2018
UR  - https://arxiv.org/abs/1804.04031
N2  - In this work we detail a novel open source library, called MMLSpark, that combines the flexible deep learning library Cognitive Toolkit, with the distributed computing framework Apache Spark. To achieve this, we have contributed Java Language bindings to the Cognitive Toolkit, and added several new components to the Spark ecosystem. In addition, we also integrate the popular image processing library OpenCV with Spark, and present a tool for the automated generation of PySpark wrappers from any SparkML estimator and use this tool to expose all work to the PySpark ecosystem. Finally, we provide a large library of tools for working and developing within the Spark ecosystem. We apply this work to the automated classification of Snow Leopards from camera trap images, and provide an end to end solution for the non-profit conservation organization, the Snow Leopard Trust.
ER  -


TY  - Preprint
T1  - VR IQA NET: Deep Virtual Reality Image Quality Assessment using Adversarial Learning
A1  - Heoun-taek Lim
A1  - Hak Gu Kim
A1  - Yong Man Ro
JO  - ArXiv e-prints
Y1  - 11 April, 2018
UR  - https://arxiv.org/abs/1804.03943
N2  - In this paper, we propose a novel virtual reality image quality assessment (VR IQA) with adversarial learning for omnidirectional images. To take into account the characteristics of the omnidirectional image, we devise deep networks including novel quality score predictor and human perception guider. The proposed quality score predictor automatically predicts the quality score of distorted image using the latent spatial and position feature. The proposed human perception guider criticizes the predicted quality score of the predictor with the human perceptual score using adversarial learning. For evaluation, we conducted extensive subjective experiments with omnidirectional image dataset. Experimental results show that the proposed VR IQA metric outperforms the 2-D IQA and the state-of-the-arts VR IQA.
ER  -


TY  - Preprint
T1  - Deep Learning For Computer Vision Tasks: A review
A1  - Rajat Kumar Sinha
A1  - Ruchi Pandey
A1  - Rohan Pattnaik
JO  - ArXiv e-prints
Y1  - 11 April, 2018
UR  - https://arxiv.org/abs/1804.03928
N2  - Deep learning has recently become one of the most popular sub-fields of machine learning owing to its distributed data representation with multiple levels of abstraction. A diverse range of deep learning algorithms are being employed to solve conventional artificial intelligence problems. This paper gives an overview of some of the most widely used deep learning algorithms applied in the field of computer vision. It first inspects the various approaches of deep learning algorithms, followed by a description of their applications in image classification, object identification, image extraction and semantic segmentation in the presence of noise. The paper concludes with the discussion of the future scope and challenges for construction and training of deep neural networks.
ER  -


TY  - Preprint
T1  - Unsupervised Segmentation of 3D Medical Images Based on Clustering and Deep Representation Learning
A1  - Takayasu Moriya
A1  - Holger R. Roth
A1  - Shota Nakamura
A1  - Hirohisa Oda
A1  - Kai Nagara
A1  - Masahiro Oda
A1  - Kensaku Mori
JO  - ArXiv e-prints
Y1  - 11 April, 2018
UR  - https://arxiv.org/abs/1804.03830
N2  - This paper presents a novel unsupervised segmentation method for 3D medical images. Convolutional neural networks (CNNs) have brought significant advances in image segmentation. However, most of the recent methods rely on supervised learning, which requires large amounts of manually annotated data. Thus, it is challenging for these methods to cope with the growing amount of medical images. This paper proposes a unified approach to unsupervised deep representation learning and clustering for segmentation. Our proposed method consists of two phases. In the first phase, we learn deep feature representations of training patches from a target image using joint unsupervised learning (JULE) that alternately clusters representations generated by a CNN and updates the CNN parameters using cluster labels as supervisory signals. We extend JULE to 3D medical images by utilizing 3D convolutions throughout the CNN architecture. In the second phase, we apply k-means to the deep representations from the trained CNN and then project cluster labels to the target image in order to obtain the fully segmented image. We evaluated our methods on three images of lung cancer specimens scanned with micro-computed tomography (micro-CT). The automatic segmentation of pathological regions in micro-CT could further contribute to the pathological examination process. Hence, we aim to automatically divide each image into the regions of invasive carcinoma, noninvasive carcinoma, and normal tissue. Our experiments show the potential abilities of unsupervised deep representation learning for medical image segmentation.
ER  -


TY  - Preprint
T1  - Deep Learning for Digital Text Analytics: Sentiment Analysis
A1  - Reshma U
A1  - Barathi Ganesh H B
A1  - Mandar Kale
A1  - Prachi Mankame
A1  - Gouri Kulkarni
JO  - ArXiv e-prints
Y1  - 10 April, 2018
UR  - https://arxiv.org/abs/1804.03673
N2  - In today&#39;s scenario, imagining a world without negativity is something very unrealistic, as bad NEWS spreads more virally than good ones. Though it seems impractical in real life, this could be implemented by building a system using Machine Learning and Natural Language Processing techniques in identifying the news datum with negative shade and filter them by taking only the news with positive shade (good news) to the end user. In this work, around two lakhs datum have been trained and tested using a combination of rule-based and data driven approaches. VADER along with a filtration method has been used as an annotating tool followed by statistical Machine Learning approach that have used Document Term Matrix (representation) and Support Vector Machine (classification). Deep Learning algorithms then came into picture to make this system reliable (Doc2Vec) which finally ended up with Convolutional Neural Network(CNN) that yielded better results than the other experimented modules. It showed up a training accuracy of 96%, while a test accuracy of (internal and external news datum) above 85% was obtained.
ER  -


TY  - Preprint
T1  - Crafting a Toolchain for Image Restoration by Deep Reinforcement Learning
A1  - Ke Yu
A1  - Chao Dong
A1  - Liang Lin
A1  - Chen Change Loy
JO  - ArXiv e-prints
Y1  - 9 April, 2018
UR  - https://arxiv.org/abs/1804.03312
N2  - We investigate a novel approach for image restoration by reinforcement learning. Unlike existing studies that mostly train a single large network for a specialized task, we prepare a toolbox consisting of small-scale convolutional networks of different complexities and specialized in different tasks. Our method, RL-Restore, then learns a policy to select appropriate tools from the toolbox to progressively restore the quality of a corrupted image. We formulate a step-wise reward function proportional to how well the image is restored at each step to learn the action policy. We also devise a joint learning scheme to train the agent and tools for better performance in handling uncertainty. In comparison to conventional human-designed networks, RL-Restore is capable of restoring images corrupted with complex and unknown distortions in a more parameter-efficient manner using the dynamically formed toolchain.
ER  -


TY  - Preprint
T1  - Planning Multi-Fingered Grasps as Probabilistic Inference in a Learned Deep Network
A1  - Qingkai Lu
A1  - Kautilya Chenna
A1  - Balakumar Sundaralingam
A1  - Tucker Hermans
JO  - ArXiv e-prints
Y1  - 9 April, 2018
UR  - https://arxiv.org/abs/1804.03289
N2  - We propose a novel approach to multi-fingered grasp planning leveraging learned deep neural network models. We train a convolutional neural network to predict grasp success as a function of both visual information of an object and grasp configuration. We can then formulate grasp planning as inferring the grasp configuration which maximizes the probability of grasp success. We efficiently perform this inference using a gradient-ascent optimization inside the neural network using the backpropagation algorithm. Our work is the first to directly plan high quality multifingered grasps in configuration space using a deep neural network without the need of an external planner. We validate our inference method performing both multifinger and two-finger grasps on real robots. Our experimental results show that our planning method outperforms existing planning methods for neural networks; while offering several other benefits including being data-efficient in learning and fast enough to be deployed in real robotic applications.
ER  -


TY  - Preprint
T1  - Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing
A1  - Jian Zhao
A1  - Jianshu Li
A1  - Yu Cheng
A1  - Li Zhou
A1  - Terence Sim
A1  - Shuicheng Yan
A1  - Jiashi Feng
JO  - ArXiv e-prints
Y1  - 6 July, 2018
UR  - https://arxiv.org/abs/1804.03287
N2  - Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification and autonomous driving, etc. To this end, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as the multi-human parsing task. In this paper, we present a new large-scale database &#34;Multi-Human Parsing (MHP)&#34; for algorithm development and evaluation, and advances the state-of-the-art in understanding humans in crowded scenes. MHP contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels, involving 2-26 persons per image and captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing. NAN consists of three Generative Adversarial Network (GAN)-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, and serves as a strong baseline to drive the future research for multi-human parsing.
ER  -


TY  - Preprint
T1  - Echo-Liquid State Deep Learning for $360^\circ$ Content Transmission and Caching in Wireless VR Networks with Cellular-Connected UAVs
A1  - Mingzhe Chen
A1  - Walid Saad
A1  - Changchuan Yin
JO  - ArXiv e-prints
Y1  - 9 April, 2018
UR  - https://arxiv.org/abs/1804.03284
N2  - In this paper, the problem of content caching and transmission is studied for a wireless virtual reality (VR) network in which unmanned aerial vehicles (UAVs) capture videos on live games or sceneries and transmit them to small base stations (SBSs) that service the VR users. However, due to its limited capacity, the wireless network may not be able to meet the delay requirements of such 360 content transmissions. To meet the VR delay requirements, the UAVs can extract specific visible content (e.g., user field of view) from the original 360 data and send this visible content to the users so as to reduce the traffic load over backhaul and radio access links. To further alleviate the UAV-SBS backhaul traffic, the SBSs can also cache the popular contents that users request. This joint content caching and transmission problem is formulated as an optimization problem whose goal is to maximize the users&#39; reliability, defined as the probability that the content transmission delay of each user satisfies the instantaneous VR delay target. To address this problem, a distributed deep learning algorithm that brings together new neural network ideas from liquid state machine (LSM) and echo state networks (ESNs) is proposed. The proposed algorithm enables each SBS to predict the users&#39; reliability so as to find the optimal contents to cache and content transmission format for each UAV. Analytical results are derived to expose the various network factors that impact content caching and content transmission format selection. Simulation results show that the proposed algorithm yields 25.4% gain of reliability compared to Q-learning. The results also show that the proposed algorithm can achieve 14.7% gain of reliability due to the reduction of traffic load over backhaul compared to the proposed algorithm with random caching.
ER  -


TY  - Preprint
T1  - Deep Learning Classification of Polygenic Obesity using Genome Wide Association Study SNPs
A1  - Casimiro Adays Curbelo MontaÃ±ez
A1  - Paul Fergus
A1  - Almudena Curbelo MontaÃ±ez
A1  - Carl Chalmers
JO  - ArXiv e-prints
Y1  - 24 August, 2018
UR  - https://arxiv.org/abs/1804.03198
N2  - In this paper, association results from genome-wide association studies (GWAS) are combined with a deep learning framework to test the predictive capacity of statistically significant single nucleotide polymorphism (SNPs) associated with obesity phenotype. Our approach demonstrates the potential of deep learning as a powerful framework for GWAS analysis that can capture information about SNPs and the important interactions between them. Basic statistical methods and techniques for the analysis of genetic SNP data from population-based genome-wide studies have been considered. Statistical association testing between individual SNPs and obesity was conducted under an additive model using logistic regression. Four subsets of loci after quality-control (QC) and association analysis were selected: P-values lower than 1x10-5 (5 SNPs), 1x10-4 (32 SNPs), 1x10-3 (248 SNPs) and 1x10-2 (2465 SNPs). A deep learning classifier is initialised using these sets of SNPs and fine-tuned to classify obese and non-obese observations. Using a deep learning classifier model and genetic variants with P-value &lt; 1x10-2 (2465 SNPs) it was possible to obtain results (SE=0.9604, SP=0.9712, Gini=0.9817, LogLoss=0.1150, AUC=0.9908 and MSE=0.0300). As the P-value increased, an evident deterioration in performance was observed. Results demonstrate that single SNP analysis fails to capture the cumulative effect of less significant variants and their overall contribution to the outcome in disease prediction, which is captured using a deep learning framework.
ER  -


TY  - Preprint
T1  - Markerless tracking of user-defined features with deep learning
A1  - Alexander Mathis
A1  - Pranav Mamidanna
A1  - Taiga Abe
A1  - Kevin M. Cury
A1  - Venkatesh N. Murthy
A1  - Mackenzie W. Mathis
A1  - Matthias Bethge
JO  - ArXiv e-prints
Y1  - 9 April, 2018
UR  - https://arxiv.org/abs/1804.03142
N2  - Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, yet markers are intrusive (especially for smaller animals), and the number and location of the markers must be determined a priori. Here, we present a highly efficient method for markerless tracking based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in a broad collection of experimental settings: mice odor trail-tracking, egg-laying behavior in drosophila, and mouse hand articulation in a skilled forelimb task. For example, during the skilled reaching behavior, individual joints can be automatically tracked (and a confidence score is reported). Remarkably, even when a small number of frames are labeled ($\approx 200$), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy.
ER  -


TY  - Preprint
T1  - Polyphonic Pitch Tracking with Deep Layered Learning
A1  - Anders Elowsson
JO  - ArXiv e-prints
Y1  - 25 April, 2018
UR  - https://arxiv.org/abs/1804.02918
N2  - This paper presents a polyphonic pitch tracking system able to extract both framewise and note-based estimates from audio. The system uses six artificial neural networks in a deep layered learning setup. First, cascading networks are applied to a spectrogram for framewise fundamental frequency (f0) estimation. A sparse receptive field is learned by the first network and then used for weight-sharing throughout the system. The f0 activations are connected across time to extract pitch ridges. These ridges define a framework, within which subsequent networks perform tone-shift-invariant onset and offset detection. The networks convolve the pitch ridges across time, using as input, e.g., variations of latent representations from the f0 estimation networks, defined as the &#34;neural flux.&#34; Finally, incorrect tentative notes are removed one by one in an iterative procedure that allows a network to classify notes within an accurate context. The system was evaluated on four public test sets: MAPS, Bach10, TRIOS, and the MIREX Woodwind quintet, and performed state-of-the-art results for all four datasets. It performs well across all subtasks: f0, pitched onset, and pitched offset tracking.
ER  -


TY  - Preprint
T1  - Deep Learning of the Nonlinear SchrÃ¶dinger Equation in Fiber-Optic Communications
A1  - Christian HÃ¤ger
A1  - Henry D. Pfister
JO  - ArXiv e-prints
Y1  - 8 April, 2018
UR  - https://arxiv.org/abs/1804.02799
N2  - An important problem in fiber-optic communications is to invert the nonlinear SchrÃ¶dinger equation in real time to reverse the deterministic effects of the channel. Interestingly, the popular split-step Fourier method (SSFM) leads to a computation graph that is reminiscent of a deep neural network. This observation allows one to leverage tools from machine learning to reduce complexity. In particular, the main disadvantage of the SSFM is that its complexity using M steps is at least M times larger than a linear equalizer. This is because the linear SSFM operator is a dense matrix. In previous work, truncation methods such as frequency sampling, wavelets, or least-squares have been used to obtain &#34;cheaper&#34; operators that can be implemented using filters. However, a large number of filter taps are typically required to limit truncation errors. For example, Ip and Kahn showed that for a 10 Gbaud signal and 2000 km optical link, a truncated SSFM with 25 steps would require 70-tap filters in each step and 100 times more operations than linear equalization. We find that, by jointly optimizing all filters with deep learning, the complexity can be reduced significantly for similar accuracy. Using optimized 5-tap and 3-tap filters in an alternating fashion, one requires only around 2-6 times the complexity of linear equalization, depending on the implementation.
ER  -


TY  - Preprint
T1  - DeepASL: Kinetic Model Incorporated Loss for Denoising Arterial Spin Labeled MRI via Deep Residual Learning
A1  - Cagdas Ulas
A1  - Giles Tetteh
A1  - Stephan Kaczmarz
A1  - Christine Preibisch
A1  - Bjoern H. Menze
JO  - ArXiv e-prints
Y1  - 12 June, 2018
UR  - https://arxiv.org/abs/1804.02755
N2  - Arterial spin labeling (ASL) allows to quantify the cerebral blood flow (CBF) by magnetic labeling of the arterial blood water. ASL is increasingly used in clinical studies due to its noninvasiveness, repeatability and benefits in quantification. However, ASL suffers from an inherently low-signal-to-noise ratio (SNR) requiring repeated measurements of control/spin-labeled (C/L) pairs to achieve a reasonable image quality, which in return increases motion sensitivity. This leads to clinically prolonged scanning times increasing the risk of motion artifacts. Thus, there is an immense need of advanced imaging and processing techniques in ASL. In this paper, we propose a novel deep learning based approach to improve the perfusion-weighted image quality obtained from a subset of all available pairwise C/L subtractions. Specifically, we train a deep fully convolutional network (FCN) to learn a mapping from noisy perfusion-weighted image and its subtraction (residual) from the clean image. Additionally, we incorporate the CBF estimation model in the loss function during training, which enables the network to produce high quality images while simultaneously enforcing the CBF estimates to be as close as reference CBF values. Extensive experiments on synthetic and clinical ASL datasets demonstrate the effectiveness of our method in terms of improved ASL image quality, accurate CBF parameter estimation and considerably small computation time during testing.
ER  -


TY  - Preprint
T1  - DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills
A1  - Xue Bin Peng
A1  - Pieter Abbeel
A1  - Sergey Levine
A1  - Michiel van de Panne
JO  - ArXiv e-prints
Y1  - 26 July, 2018
UR  - https://arxiv.org/abs/1804.02717
N2  - A longstanding goal in character animation is to combine data-driven specification of behavior with a system that can execute a similar behavior in a physical simulation, thus enabling realistic responses to perturbations and environmental variation. We show that well-known reinforcement learning (RL) methods can be adapted to learn robust control policies capable of imitating a broad range of example motion clips, while also learning complex recoveries, adapting to changes in morphology, and accomplishing user-specified goals. Our method handles keyframed motions, highly-dynamic actions such as motion-captured flips and spins, and retargeted motions. By combining a motion-imitation objective with a task objective, we can train characters that react intelligently in interactive settings, e.g., by walking in a desired direction or throwing a ball at a user-specified target. This approach thus combines the convenience and motion quality of using motion clips to define the desired style and appearance, with the flexibility and generality afforded by RL methods and physics-based animation. We further explore a number of methods for integrating multiple clips into the learning process to develop multi-skilled agents capable of performing a rich repertoire of diverse skills. We demonstrate results using multiple characters (human, Atlas robot, bipedal dinosaur, dragon) and a large variety of skills, including locomotion, acrobatics, and martial arts.
ER  -


TY  - Preprint
T1  - Visual Analytics for Explainable Deep Learning
A1  - Jaegul Choo
A1  - Shixia Liu
JO  - ArXiv e-prints
Y1  - 7 April, 2018
UR  - https://arxiv.org/abs/1804.02527
N2  - Recently, deep learning has been advancing the state of the art in artificial intelligence to a new level, and humans rely on artificial intelligence techniques more than ever. However, even with such unprecedented advancements, the lack of explanation regarding the decisions made by deep learning models and absence of control over their internal processes act as major drawbacks in critical decision-making processes, such as precision medicine and law enforcement. In response, efforts are being made to make deep learning interpretable and controllable by humans. In this paper, we review visual analytics, information visualization, and machine learning perspectives relevant to this aim, and discuss potential challenges and future research directions.
ER  -


TY  - Preprint
T1  - Semantically Enhanced Software Traceability Using Deep Learning Techniques
A1  - Jin Guo
A1  - Jinghui Cheng
A1  - Jane Cleland-Huang
JO  - ArXiv e-prints
Y1  - 6 April, 2018
UR  - https://arxiv.org/abs/1804.02438
N2  - In most safety-critical domains the need for traceability is prescribed by certifying bodies. Trace links are generally created among requirements, design, source code, test cases and other artifacts, however, creating such links manually is time consuming and error prone. Automated solutions use information retrieval and machine learning techniques to generate trace links, however, current techniques fail to understand semantics of the software artifacts or to integrate domain knowledge into the tracing process and therefore tend to deliver imprecise and inaccurate results. In this paper, we present a solution that uses deep learning to incorporate requirements artifact semantics and domain knowledge into the tracing solution. We propose a tracing network architecture that utilizes Word Embedding and Recurrent Neural Network (RNN) models to generate trace links. Word embedding learns word vectors that represent knowledge of the domain corpus and RNN uses these word vectors to learn the sentence semantics of requirements artifacts. We trained 360 different configurations of the tracing network using existing trace links in the Positive Train Control domain and identified the Bidirectional Gated Recurrent Unit (BI-GRU) as the best model for the tracing task. BI-GRU significantly out-performed state-of-the-art tracing methods including the Vector Space Model and Latent Semantic Indexing.
ER  -


TY  - Preprint
T1  - Noise-resistant Deep Learning for Object Classification in 3D Point Clouds Using a Point Pair Descriptor
A1  - Dmytro Bobkov
A1  - Sili Chen
A1  - Ruiqing Jian
A1  - Muhammad Iqbal
A1  - Eckehard Steinbach
JO  - ArXiv e-prints
Y1  - 5 April, 2018
UR  - https://arxiv.org/abs/1804.02077
N2  - Object retrieval and classification in point cloud data is challenged by noise, irregular sampling density and occlusion. To address this issue, we propose a point pair descriptor that is robust to noise and occlusion and achieves high retrieval accuracy. We further show how the proposed descriptor can be used in a 4D convolutional neural network for the task of object classification. We propose a novel 4D convolutional layer that is able to learn class-specific clusters in the descriptor histograms. Finally, we provide experimental validation on 3 benchmark datasets, which confirms the superiority of the proposed approach.
ER  -


TY  - Preprint
T1  - Towards radiologist-level cancer risk assessment in CT lung screening using deep learning
A1  - Stojan Trajanovski
A1  - Dimitrios Mavroeidis
A1  - Christine Leon Swisher
A1  - Binyam Gebrekidan Gebre
A1  - Bas Veeling
A1  - Rafael Wiemker
A1  - Tobias Klinder
A1  - Amir Tahmasebi
A1  - Shawn M. Regis
A1  - Christoph Wald
A1  - Brady J. McKee
A1  - Heber MacMahon
A1  - Homer Pien
JO  - ArXiv e-prints
Y1  - 5 April, 2018
UR  - https://arxiv.org/abs/1804.01901
N2  - Lung cancer is the leading cause of cancer mortality in the US, responsible for more deaths than breast, prostate, colon and pancreas cancer combined. Recently, it has been demonstrated that screening those at high-risk for lung cancer low-dose computed tomography (CT) of the chest can significantly reduce this death rate. The process of evaluating a chest CT scan involves the identification of nodules that are contained within a scan as well as the evaluation of the likelihood that a nodule is malignant based on its imaging characteristics. This has motivated researchers to develop image analysis research tools, such as nodule detectors and nodule classifiers that can assist radiologists to make accurate assessments of the patient cancer risk.
ER  -


TY  - Preprint
T1  - A Human Mixed Strategy Approach to Deep Reinforcement Learning
A1  - Ngoc Duy Nguyen
A1  - Saeid Nahavandi
A1  - Thanh Nguyen
JO  - ArXiv e-prints
Y1  - 5 April, 2018
UR  - https://arxiv.org/abs/1804.01874
N2  - In 2015, Google&#39;s DeepMind announced an advancement in creating an autonomous agent based on deep reinforcement learning (DRL) that could beat a professional player in a series of 49 Atari games. However, the current manifestation of DRL is still immature, and has significant drawbacks. One of DRL&#39;s imperfections is its lack of &#34;exploration&#34; during the training process, especially when working with high-dimensional problems. In this paper, we propose a mixed strategy approach that mimics behaviors of human when interacting with environment, and create a &#34;thinking&#34; agent that allows for more efficient exploration in the DRL training process. The simulation results based on the Breakout game show that our scheme achieves a higher probability of obtaining a maximum score than does the baseline DRL algorithm, i.e., the asynchronous advantage actor-critic method. The proposed scheme therefore can be applied effectively to solving a complicated task in a real-world application.
ER  -


TY  - Preprint
T1  - GoSGD: Distributed Optimization for Deep Learning with Gossip Exchange
A1  - Michael Blot
A1  - David Picard
A1  - Matthieu Cord
JO  - ArXiv e-prints
Y1  - 4 April, 2018
UR  - https://arxiv.org/abs/1804.01852
N2  - We address the issue of speeding up the training of convolutional neural networks by studying a distributed method adapted to stochastic gradient descent. Our parallel optimization setup uses several threads, each applying individual gradient descents on a local variable. We propose a new way of sharing information between different threads based on gossip algorithms that show good consensus convergence properties. Our method called GoSGD has the advantage to be fully asynchronous and decentralized.
ER  -


TY  - Preprint
T1  - Processing of Electronic Health Records using Deep Learning: A review
A1  - Venet Osmani
A1  - Li Li
A1  - Matteo Danieletto
A1  - Benjamin Glicksberg
A1  - Joel Dudley
A1  - Oscar Mayora
JO  - ArXiv e-prints
Y1  - 5 April, 2018
UR  - https://arxiv.org/abs/1804.01758
N2  - Availability of large amount of clinical data is opening up new research avenues in a number of fields. An exciting field in this respect is healthcare, where secondary use of healthcare data is beginning to revolutionize healthcare. Except for availability of Big Data, both medical data from healthcare institutions (such as EMR data) and data generated from health and wellbeing devices (such as personal trackers), a significant contribution to this trend is also being made by recent advances on machine learning, specifically deep learning algorithms.
ER  -


TY  - Preprint
T1  - Learning Strict Identity Mappings in Deep Residual Networks
A1  - Xin Yu
A1  - Zhiding Yu
A1  - Srikumar Ramalingam
JO  - ArXiv e-prints
Y1  - 16 May, 2018
UR  - https://arxiv.org/abs/1804.01661
N2  - A family of super deep networks, referred to as residual networks or ResNet, achieved record-beating performance in various visual tasks such as image recognition, object detection, and semantic segmentation. The ability to train very deep networks naturally pushed the researchers to use enormous resources to achieve the best performance. Consequently, in many applications super deep residual networks were employed for just a marginal improvement in performance. In this paper, we propose epsilon-ResNet that allows us to automatically discard redundant layers, which produces responses that are smaller than a threshold epsilon, with a marginal or no loss in performance. The epsilon-ResNet architecture can be achieved using a few additional rectified linear units in the original ResNet. Our method does not use any additional variables nor numerous trials like other hyper-parameter optimization techniques. The layer selection is achieved using a single training process and the evaluation is performed on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. In some instances, we achieve about 80% reduction in the number of parameters.
ER  -


TY  - Preprint
T1  - Review of Deep Learning
A1  - Rong Zhang
A1  - Weiping Li
A1  - Tong Mo
JO  - ArXiv e-prints
Y1  - 28 August, 2018
UR  - https://arxiv.org/abs/1804.01653
N2  - In recent years, China, the United States and other countries, Google and other high-tech companies have increased investment in artificial intelligence. Deep learning is one of the current artificial intelligence research&#39;s key areas. This paper analyzes and summarizes the latest progress and future research directions of deep learning. Firstly, three basic models of deep learning are outlined, including multilayer perceptrons, convolutional neural networks, and recurrent neural networks. On this basis, we further analyze the emerging new models of convolution neural networks and recurrent neural networks. This paper then summarizes deep learning&#39;s applications in many areas of artificial intelligence, including speech processing, computer vision, natural language processing and so on. Finally, this paper discusses the existing problems of deep learning and gives the corresponding possible solutions.
ER  -


TY  - Preprint
T1  - Fine-grained Video Attractiveness Prediction Using Multimodal Deep Learning on a Large Real-world Dataset
A1  - Xinpeng Chen
A1  - Jingyuan Chen
A1  - Lin Ma
A1  - Jian Yao
A1  - Wei Liu
A1  - Jiebo Luo
A1  - Tong Zhang
JO  - ArXiv e-prints
Y1  - 6 April, 2018
UR  - https://arxiv.org/abs/1804.01373
N2  - Nowadays, billions of videos are online ready to be viewed and shared. Among an enormous volume of videos, some popular ones are widely viewed by online users while the majority attract little attention. Furthermore, within each video, different segments may attract significantly different numbers of views. This phenomenon leads to a challenging yet important problem, namely fine-grained video attractiveness prediction. However, one major obstacle for such a challenging problem is that no suitable benchmark dataset currently exists. To this end, we construct the first fine-grained video attractiveness dataset, which is collected from one of the most popular video websites in the world. In total, the constructed FVAD consists of 1,019 drama episodes with 780.6 hours covering different categories and a wide variety of video contents. Apart from the large amount of videos, hundreds of millions of user behaviors during watching videos are also included, such as &#34;view counts&#34;, &#34;fast-forward&#34;, &#34;fast-rewind&#34;, and so on, where &#34;view counts&#34; reflects the video attractiveness while other engagements capture the interactions between the viewers and videos. First, we demonstrate that video attractiveness and different engagements present different relationships. Second, FVAD provides us an opportunity to study the fine-grained video attractiveness prediction problem. We design different sequential models to perform video attractiveness prediction by relying solely on video contents. The sequential models exploit the multimodal relationships between visual and audio components of the video contents at different levels. Experimental results demonstrate the effectiveness of our proposed sequential models with different visual and audio representations, the necessity of incorporating the two modalities, and the complementary behaviors of the sequential prediction models at different levels.
ER  -


TY  - Preprint
T1  - Event-based Vision meets Deep Learning on Steering Prediction for Self-driving Cars
A1  - Ana I. Maqueda
A1  - Antonio Loquercio
A1  - Guillermo Gallego
A1  - Narciso Garcia
A1  - Davide Scaramuzza
JO  - ArXiv e-prints
Y1  - 4 April, 2018
UR  - https://arxiv.org/abs/1804.01310
N2  - Event cameras are bio-inspired vision sensors that naturally capture the dynamics of a scene, filtering out redundant information. This paper presents a deep neural network approach that unlocks the potential of event cameras on a challenging motion-estimation task: prediction of a vehicle&#39;s steering angle. To make the best out of this sensor-algorithm combination, we adapt state-of-the-art convolutional architectures to the output of event sensors and extensively evaluate the performance of our approach on a publicly available large scale event-camera dataset (~1000 km). We present qualitative and quantitative explanations of why event cameras allow robust steering prediction even in cases where traditional cameras fail, e.g. challenging illumination conditions and fast motion. Finally, we demonstrate the advantages of leveraging transfer learning from traditional to event-based vision, and show that our approach outperforms state-of-the-art algorithms based on standard cameras.
ER  -


TY  - Preprint
T1  - Towards Deep Learning based Hand Keypoints Detection for Rapid Sequential Movements from RGB Images
A1  - Srujana Gattupalli
A1  - Ashwin Ramesh Babu
A1  - James Robert Brady
A1  - Fillia Makedon
A1  - Vassilis Athitsos
JO  - ArXiv e-prints
Y1  - 3 April, 2018
UR  - https://arxiv.org/abs/1804.01174
N2  - Hand keypoints detection and pose estimation has numerous applications in computer vision, but it is still an unsolved problem in many aspects. An application of hand keypoints detection is in performing cognitive assessments of a subject by observing the performance of that subject in physical tasks involving rapid finger motion. As a part of this work, we introduce a novel hand key-points benchmark dataset that consists of hand gestures recorded specifically for cognitive behavior monitoring. We explore the state of the art methods in hand keypoint detection and we provide quantitative evaluations for the performance of these methods on our dataset. In future, these results and our dataset can serve as a useful benchmark for hand keypoint recognition for rapid finger movements.
ER  -


TY  - Preprint
T1  - CIKM AnalytiCup 2017 Lazada Product Title Quality Challenge An Ensemble of Deep and Shallow Learning to predict the Quality of Product Titles
A1  - Karamjit Singh
A1  - Vishal Sunder
JO  - ArXiv e-prints
Y1  - 1 April, 2018
UR  - https://arxiv.org/abs/1804.01000
N2  - We present an approach where two different models (Deep and Shallow) are trained separately on the data and a weighted average of the outputs is taken as the final result. For the Deep approach, we use different combinations of models like Convolution Neural Network, pretrained word2vec embeddings and LSTMs to get representations which are then used to train a Deep Neural Network. For Clarity prediction, we also use an Attentive Pooling approach for the pooling operation so as to be aware of the Title-Category pair. For the shallow approach, we use boosting technique LightGBM on features generated using title and categories. We find that an ensemble of these approaches does a better job than using them alone suggesting that the results of the deep and shallow approach are highly complementary
ER  -


TY  - Preprint
T1  - DeepSigns: A Generic Watermarking Framework for IP Protection of Deep Learning Models
A1  - Bita Darvish Rouhani
A1  - Huili Chen
A1  - Farinaz Koushanfar
JO  - ArXiv e-prints
Y1  - 31 May, 2018
UR  - https://arxiv.org/abs/1804.00750
N2  - Deep Learning (DL) models have caused a paradigm shift in our ability to comprehend raw data in various important fields, ranging from intelligence warfare and healthcare to autonomous transportation and automated manufacturing. A practical concern, in the rush to adopt DL models as a service, is protecting the models against Intellectual Property (IP) infringement. The DL models are commonly built by allocating significant computational resources that process vast amounts of proprietary training data. The resulting models are therefore considered to be the IP of the model builder and need to be protected to preserve the owner&#39;s competitive advantage.
ER  -


TY  - Preprint
T1  - Performance Optimization in Mobile-Edge Computing via Deep Reinforcement Learning
A1  - Xianfu Chen
A1  - Honggang Zhang
A1  - Celimuge Wu
A1  - Shiwen Mao
A1  - Yusheng Ji
A1  - Mehdi Bennis
JO  - ArXiv e-prints
Y1  - 25 March, 2018
UR  - https://arxiv.org/abs/1804.00514
N2  - To improve the quality of computation experience for mobile devices, mobile-edge computing (MEC) is emerging as a promising paradigm by providing computing capabilities within radio access networks in close proximity. Nevertheless, the design of computation offloading policies for a MEC system remains challenging. Specifically, whether to execute an arriving computation task at local mobile device or to offload a task for cloud execution should adapt to the environmental dynamics in a smarter manner. In this paper, we consider MEC for a representative mobile user in an ultra dense network, where one of multiple base stations (BSs) can be selected for computation offloading. The problem of solving an optimal computation offloading policy is modelled as a Markov decision process, where our objective is to minimize the long-term cost and an offloading decision is made based on the channel qualities between the mobile user and the BSs, the energy queue state as well as the task queue state. To break the curse of high dimensionality in state space, we propose a deep $Q$-network-based strategic computation offloading algorithm to learn the optimal policy without having a priori knowledge of the dynamic statistics. Numerical experiments provided in this paper show that our proposed algorithm achieves a significant improvement in average cost compared with baseline policies.
ER  -


TY  - Preprint
T1  - Land use mapping in the Three Gorges Reservoir Area based on semantic segmentation deep learning method
A1  - Xin Zhang
A1  - Bingfang Wu
A1  - Liang Zhu
A1  - Fuyou Tian
A1  - Miao Zhang
A1  -  Yuanzeng
JO  - ArXiv e-prints
Y1  - 18 March, 2018
UR  - https://arxiv.org/abs/1804.00498
N2  - The Three Gorges Dam, a massive cross-century project spans the Yangtze River by the town of Sandouping, located in Yichang, Hubei province, China, was built to provide great power, improve the River shipping, control floods in the upper reaches of the Yangtze River, and increase the dry season flow in the middle and lower reaches of the Yangtze River. Benefits are enormous and comprehensive. However, the social and environmental impacts are also immense and far-reaching to its surrounding areas. Mapping land use /land cover changed (LUCC) is critical for tracking the impacts. Remote sensing has been proved to be an effective way to map and monitor land use change in real time and in large areas such as the Three Gorges Reservoir Area(TGRA) by using pixel based or oriented based classifier in different resolution. In this paper, we first test the state of the art semantic segmentation deep learning classifiers for LUCC mapping with 7 categories in the TGRA area with rapideye 5m resolution data. The topographic information was also added for better accuracy in mountain area. By compared with the pixel-based classifier, the semantic segmentation deep learning method has better accuracy and robustness at 5m resolution level.
ER  -


TY  - Preprint
T1  - Curiosity-driven Exploration for Mapless Navigation with Deep Reinforcement Learning
A1  - Oleksii Zhelo
A1  - Jingwei Zhang
A1  - Lei Tai
A1  - Ming Liu
A1  - Wolfram Burgard
JO  - ArXiv e-prints
Y1  - 14 May, 2018
UR  - https://arxiv.org/abs/1804.00456
N2  - This paper investigates exploration strategies of Deep Reinforcement Learning (DRL) methods to learn navigation policies for mobile robots. In particular, we augment the normal external reward for training DRL algorithms with intrinsic reward signals measured by curiosity. We test our approach in a mapless navigation setting, where the autonomous agent is required to navigate without the occupancy map of the environment, to targets whose relative locations can be easily acquired through low-cost solutions (e.g., visible light localization, Wi-Fi signal localization). We validate that the intrinsic motivation is crucial for improving DRL performance in tasks with challenging exploration requirements. Our experimental results show that our proposed method is able to more effectively learn navigation policies, and has better generalization capabilities in previously unseen environments. A video of our experimental results can be found at https://goo.gl/pWbpcF.
ER  -


TY  - Preprint
T1  - Deep Residual Learning for Accelerated MRI using Magnitude and Phase Networks
A1  - Dongwook Lee
A1  - Jaejun Yoo
A1  - Sungho Tak
A1  - Jong Chul Ye
JO  - ArXiv e-prints
Y1  - 2 April, 2018
UR  - https://arxiv.org/abs/1804.00432
N2  - Accelerated magnetic resonance (MR) scan acquisition with compressed sensing (CS) and parallel imaging is a powerful method to reduce MR imaging scan time. However, many reconstruction algorithms have high computational costs. To address this, we investigate deep residual learning networks to remove aliasing artifacts from artifact corrupted images. The proposed deep residual learning networks are composed of magnitude and phase networks that are separately trained. If both phase and magnitude information are available, the proposed algorithm can work as an iterative k-space interpolation algorithm using framelet representation. When only magnitude data is available, the proposed approach works as an image domain post-processing algorithm. Even with strong coherent aliasing artifacts, the proposed network successfully learned and removed the aliasing artifacts, whereas current parallel and CS reconstruction methods were unable to remove these artifacts. Comparisons using single and multiple coil show that the proposed residual network provides good reconstruction results with orders of magnitude faster computational time than existing compressed sensing methods. The proposed deep learning framework may have a great potential for accelerated MR reconstruction by generating accurate results immediately.
ER  -


TY  - Preprint
T1  - A Vehicle Detection Approach using Deep Learning Methodologies
A1  - Abdullah Asim Yilmaz
A1  - Mehmet Serdar Guzel
A1  - Iman Askerbeyli
A1  - Erkan Bostanci
JO  - ArXiv e-prints
Y1  - 2 April, 2018
UR  - https://arxiv.org/abs/1804.00429
N2  - The purpose of this study is to successfully train our vehicle detector using R-CNN, Faster R-CNN deep learning methods on a sample vehicle data sets and to optimize the success rate of the trained detector by providing efficient results for vehicle detection by testing the trained vehicle detector on the test data. The working method consists of six main stages. These are respectively; loading the data set, the design of the convolutional neural network, configuration of training options, training of the Faster R-CNN object detector and evaluation of trained detector. In addition, in the scope of the study, Faster R-CNN, R-CNN deep learning methods were mentioned and experimental analysis comparisons were made with the results obtained from vehicle detection.
ER  -


TY  - Preprint
T1  - Attention-based Ensemble for Deep Metric Learning
A1  - Wonsik Kim
A1  - Bhavya Goyal
A1  - Kunal Chawla
A1  - Jungmin Lee
A1  - Keunjoo Kwon
JO  - ArXiv e-prints
Y1  - 31 August, 2018
UR  - https://arxiv.org/abs/1804.00382
N2  - Deep metric learning aims to learn an embedding function, modeled as deep neural network. This embedding function usually puts semantically similar images close while dissimilar images far from each other in the learned embedding space. Recently, ensemble has been applied to deep metric learning to yield state-of-the-art results. As one important aspect of ensemble, the learners should be diverse in their feature embeddings. To this end, we propose an attention-based ensemble, which uses multiple attention masks, so that each learner can attend to different parts of the object. We also propose a divergence loss, which encourages diversity among the learners. The proposed method is applied to the standard benchmarks of deep metric learning and experimental results show that it outperforms the state-of-the-art methods by a significant margin on image retrieval tasks.
ER  -


TY  - Preprint
T1  - Robust Fruit Counting: Combining Deep Learning, Tracking, and Structure from Motion
A1  - Xu Liu
A1  - Steven W. Chen
A1  - Shreyas Aditya
A1  - Nivedha Sivakumar
A1  - Sandeep Dcunha
A1  - Chao Qu
A1  - Camillo J. Taylor
A1  - Jnaneshwar Das
A1  - Vijay Kumar
JO  - ArXiv e-prints
Y1  - 2 August, 2018
UR  - https://arxiv.org/abs/1804.00307
N2  - We present a novel fruit counting pipeline that combines deep segmentation, frame to frame tracking, and 3D localization to accurately count visible fruits across a sequence of images. Our pipeline works on image streams from a monocular camera, both in natural light, as well as with controlled illumination at night. We first train a Fully Convolutional Network (FCN) and segment video frame images into fruit and non-fruit pixels. We then track fruits across frames using the Hungarian Algorithm where the objective cost is determined from a Kalman Filter corrected Kanade-Lucas-Tomasi (KLT) Tracker. In order to correct the estimated count from tracking process, we combine tracking results with a Structure from Motion (SfM) algorithm to calculate relative 3D locations and size estimates to reject outliers and double counted fruit tracks. We evaluate our algorithm by comparing with ground-truth human-annotated visual counts. Our results demonstrate that our pipeline is able to accurately and reliably count fruits across image sequences, and the correction step can significantly improve the counting accuracy and robustness. Although discussed in the context of fruit counting, our work can extend to detection, tracking, and counting of a variety of other stationary features of interest such as leaf-spots, wilt, and blossom.
ER  -


TY  - Preprint
T1  - Learning to Run challenge: Synthesizing physiologically accurate motion using deep reinforcement learning
A1  - Åukasz KidziÅski
A1  - Sharada P. Mohanty
A1  - Carmichael Ong
A1  - Jennifer L. Hicks
A1  - Sean F. Carroll
A1  - Sergey Levine
A1  - Marcel SalathÃ©
A1  - Scott L. Delp
JO  - ArXiv e-prints
Y1  - 31 March, 2018
UR  - https://arxiv.org/abs/1804.00198
N2  - Synthesizing physiologically-accurate human movement in a variety of conditions can help practitioners plan surgeries, design experiments, or prototype assistive devices in simulated environments, reducing time and costs and improving treatment outcomes. Because of the large and complex solution spaces of biomechanical models, current methods are constrained to specific movements and models, requiring careful design of a controller and hindering many possible applications. We sought to discover if modern optimization methods efficiently explore these complex spaces. To do this, we posed the problem as a competition in which participants were tasked with developing a controller to enable a physiologically-based human model to navigate a complex obstacle course as quickly as possible, without using any experimental data. They were provided with a human musculoskeletal model and a physics-based simulation environment. In this paper, we discuss the design of the competition, technical difficulties, results, and analysis of the top controllers. The challenge proved that deep reinforcement learning techniques, despite their high computational cost, can be successfully employed as an optimization method for synthesizing physiologically feasible motion in high-dimensional biomechanical systems.
ER  -


TY  - Preprint
T1  - SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters
A1  - Yifan Xu
A1  - Tianqi Fan
A1  - Mingye Xu
A1  - Long Zeng
A1  - Yu Qiao
JO  - ArXiv e-prints
Y1  - 12 September, 2018
UR  - https://arxiv.org/abs/1803.11527
N2  - Deep neural networks have enjoyed remarkable success for various vision tasks, however it remains challenging to apply CNNs to domains lacking a regular underlying structures such as 3D point clouds. Towards this we propose a novel convolutional architecture, termed SpiderCNN, to efficiently extract geometric features from point clouds. SpiderCNN is comprised of units called SpiderConv, which extend convolutional operations from regular grids to irregular point sets that can be embedded in R^n, by parametrizing a family of convolutional filters. We design the filter as a product of a simple step function that captures local geodesic information and a Taylor polynomial that ensures the expressiveness. SpiderCNN inherits the multi-scale hierarchical architecture from classical CNNs, which allows it to extract semantic deep features. Experiments on ModelNet40 demonstrate that SpiderCNN achieves state-of-the-art accuracy 92.4% on standard benchmarks, and shows competitive performance on segmentation task.
ER  -


TY  - Preprint
T1  - QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning
A1  - Tabish Rashid
A1  - Mikayel Samvelyan
A1  - Christian Schroeder de Witt
A1  - Gregory Farquhar
A1  - Jakob Foerster
A1  - Shimon Whiteson
JO  - ArXiv e-prints
Y1  - 6 June, 2018
UR  - https://arxiv.org/abs/1803.11485
N2  - In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.
ER  -


TY  - Preprint
T1  - Scalable Deep Learning Logo Detection
A1  - Hang Su
A1  - Shaogang Gong
A1  - Xiatian Zhu
JO  - ArXiv e-prints
Y1  - 2 April, 2018
UR  - https://arxiv.org/abs/1803.11417
N2  - Existing logo detection methods usually consider a small number of logo classes and limited images per class with a strong assumption of requiring tedious object bounding box annotations, therefore not scalable to real-world dynamic applications. In this work, we tackle these challenges by exploring the webly data learning principle without the need for exhaustive manual labelling. Specifically, we propose a novel incremental learning approach, called Scalable Logo Self-co-Learning (SL^2), capable of automatically self-discovering informative training images from noisy web data for progressively improving model capability in a cross-model co-learning manner. Moreover, we introduce a very large (2,190,757 images of 194 logo classes) logo dataset &#34;WebLogo-2M&#34; by an automatic web data collection and processing method. Extensive comparative evaluations demonstrate the superiority of the proposed SL^2 method over the state-of-the-art strongly and weakly supervised detection models and contemporary webly data learning approaches.
ER  -


TY  - Preprint
T1  - Cache-Enabled Dynamic Rate Allocation via Deep Self-Transfer Reinforcement Learning
A1  - Zhengming Zhang
A1  - Yaru Zheng
A1  - Meng Hua
A1  - Yongming Huang
A1  - Luxi Yang
JO  - ArXiv e-prints
Y1  - 30 March, 2018
UR  - https://arxiv.org/abs/1803.11334
N2  - Caching and rate allocation are two promising approaches to support video streaming over wireless network. However, existing rate allocation designs do not fully exploit the advantages of the two approaches. This paper investigates the problem of cache-enabled QoE-driven video rate allocation problem. We establish a mathematical model for this problem, and point out that it is difficult to solve the problem with traditional dynamic programming. Then we propose a deep reinforcement learning approaches to solve it. First, we model the problem as a Markov decision problem. Then we present a deep Q-learning algorithm with a special knowledge transfer process to find out effective allocation policy. Finally, numerical results are given to demonstrate that the proposed solution can effectively maintain high-quality user experience of mobile user moving among small cells. We also investigate the impact of configuration of critical parameters on the performance of our algorithm.
ER  -


TY  - Preprint
T1  - Learning View-Specific Deep Networks for Person Re-Identification
A1  - Zhanxiang Feng
A1  - Jianhuang Lai
A1  - Xiaohua Xie
JO  - ArXiv e-prints
Y1  - 30 March, 2018
UR  - https://arxiv.org/abs/1803.11333
N2  - In recent years, a growing body of research has focused on the problem of person re-identification (re-id). The re-id techniques attempt to match the images of pedestrians from disjoint non-overlapping camera views. A major challenge of re-id is the serious intra-class variations caused by changing viewpoints. To overcome this challenge, we propose a deep neural network-based framework which utilizes the view information in the feature extraction stage. The proposed framework learns a view-specific network for each camera view with a cross-view Euclidean constraint (CV-EC) and a cross-view center loss (CV-CL). We utilize CV-EC to decrease the margin of the features between diverse views and extend the center loss metric to a view-specific version to better adapt the re-id problem. Moreover, we propose an iterative algorithm to optimize the parameters of the view-specific networks from coarse to fine. The experiments demonstrate that our approach significantly improves the performance of the existing deep networks and outperforms the state-of-the-art methods on the VIPeR, CUHK01, CUHK03, SYSU-mReId, and Market-1501 benchmarks.
ER  -


TY  - Preprint
T1  - Deep Cascade Multi-task Learning for Slot Filling in Chinese E-commerce Shopping Guide Assistant
A1  - Yu Gong
A1  - Xusheng Luo
A1  - Kenny Q. Zhu
A1  - Xi Chen
A1  - Wenwu Ou
JO  - ArXiv e-prints
Y1  - 4 April, 2018
UR  - https://arxiv.org/abs/1803.11326
N2  - Slot filling is a critical task in natural language understanding (NLU) for dialog systems. State-of-the-art solutions regard it as a sequence label- ing task and adopt BiLSTM-CRF models. While BiLSTM-CRF models works relatively well on standard datasets it faces challenges in Chinese E-commerce slot filling due to more informative slot labels and richer expressions. In this paper, we propose a deep multi-task learning model with cascade and residual connections. Experimental results show that our framework not only achieves competitive performance with state-of-the-arts on a standard dataset, but also significantly outperforms strong baselines by a substantial gain of 14.6% on a Chinese E-commerce dataset.
ER  -


TY  - Preprint
T1  - Deep learning-based virtual histology staining using auto-fluorescence of label-free tissue
A1  - Yair Rivenson
A1  - Hongda Wang
A1  - Zhensong Wei
A1  - Yibo Zhang
A1  - Harun Gunaydin
A1  - Aydogan Ozcan
JO  - ArXiv e-prints
Y1  - 29 March, 2018
UR  - https://arxiv.org/abs/1803.11293
N2  - Histological analysis of tissue samples is one of the most widely used methods for disease diagnosis. After taking a sample from a patient, it goes through a lengthy and laborious preparation, which stains the tissue to visualize different histological features under a microscope. Here, we demonstrate a label-free approach to create a virtually-stained microscopic image using a single wide-field auto-fluorescence image of an unlabeled tissue sample, bypassing the standard histochemical staining process, saving time and cost. This method is based on deep learning, and uses a convolutional neural network trained using a generative adversarial network model to transform an auto-fluorescence image of an unlabeled tissue section into an image that is equivalent to the bright-field image of the stained-version of the same sample. We validated this method by successfully creating virtually-stained microscopic images of human tissue samples, including sections of salivary gland, thyroid, kidney, liver and lung tissue, also covering three different stains. This label-free virtual-staining method eliminates cumbersome and costly histochemical staining procedures, and would significantly simplify tissue preparation in pathology and histology fields.
ER  -


TY  - Preprint
T1  - Security Consideration For Deep Learning-Based Image Forensics
A1  - Wei Zhao
A1  - Pengpeng Yang
A1  - Rongrong Ni
A1  - Yao Zhao
A1  - Haorui Wu
JO  - ArXiv e-prints
Y1  - 3 April, 2018
UR  - https://arxiv.org/abs/1803.11157
N2  - Recently, image forensics community has paied attention to the research on the design of effective algorithms based on deep learning technology and facts proved that combining the domain knowledge of image forensics and deep learning would achieve more robust and better performance than the traditional schemes. Instead of improving it, in this paper, the safety of deep learning based methods in the field of image forensics is taken into account. To the best of our knowledge, this is a first work focusing on this topic. Specifically, we experimentally find that the method using deep learning would fail when adding the slight noise into the images (adversarial images). Furthermore, two kinds of strategys are proposed to enforce security of deep learning-based method. Firstly, an extra penalty term to the loss function is added, which is referred to the 2-norm of the gradient of the loss with respect to the input images, and then an novel training method are adopt to train the model by fusing the normal and adversarial images. Experimental results show that the proposed algorithm can achieve good performance even in the case of adversarial images and provide a safety consideration for deep learning-based image forensics
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Traffic Light Control in Vehicular Networks
A1  - Xiaoyuan Liang
A1  - Xunsheng Du
A1  - Guiling Wang
A1  - Zhu Han
JO  - ArXiv e-prints
Y1  - 29 March, 2018
UR  - https://arxiv.org/abs/1803.11115
N2  - Existing inefficient traffic light control causes numerous problems, such as long delay and waste of energy. To improve efficiency, taking real-time traffic information as an input and dynamically adjusting the traffic light duration accordingly is a must. In terms of how to dynamically adjust traffic signals&#39; duration, existing works either split the traffic signal into equal duration or extract limited traffic information from the real data. In this paper, we study how to decide the traffic signals&#39; duration based on the collected data from different sensors and vehicular networks. We propose a deep reinforcement learning model to control the traffic light. In the model, we quantify the complex traffic scenario as states by collecting data and dividing the whole intersection into small grids. The timing changes of a traffic light are the actions, which are modeled as a high-dimension Markov decision process. The reward is the cumulative waiting time difference between two cycles. To solve the model, a convolutional neural network is employed to map the states to rewards. The proposed model is composed of several components to improve the performance, such as dueling network, target network, double Q-learning network, and prioritized experience replay. We evaluate our model via simulation in the Simulation of Urban MObility (SUMO) in a vehicular network, and the simulation results show the efficiency of our model in controlling traffic lights.
ER  -


TY  - Preprint
T1  - Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision
A1  - Yaojie Liu
A1  - Amin Jourabloo
A1  - Xiaoming Liu
JO  - ArXiv e-prints
Y1  - 29 March, 2018
UR  - https://arxiv.org/abs/1803.11097
N2  - Face anti-spoofing is the crucial step to prevent face recognition systems from a security breach. Previous deep learning approaches formulate face anti-spoofing as a binary classification problem. Many of them struggle to grasp adequate spoofing cues and generalize poorly. In this paper, we argue the importance of auxiliary supervision to guide the learning toward discriminative and generalizable cues. A CNN-RNN model is learned to estimate the face depth with pixel-wise supervision, and to estimate rPPG signals with sequence-wise supervision. Then we fuse the estimated depth and rPPG to distinguish live vs. spoof faces. In addition, we introduce a new face anti-spoofing database that covers a large range of illumination, subject, and pose variations. Experimental results show that our model achieves the state-of-the-art performance on both intra-database and cross-database testing.
ER  -


TY  - Preprint
T1  - 3D Consistent Biventricular Myocardial Segmentation Using Deep Learning for Mesh Generation
A1  - Qiao Zheng
A1  - HervÃ© Delingette
A1  - Nicolas Duchateau
A1  - Nicholas Ayache
JO  - ArXiv e-prints
Y1  - 29 March, 2018
UR  - https://arxiv.org/abs/1803.11080
N2  - We present a novel automated method to segment the myocardium of both left and right ventricles in MRI volumes. The segmentation is consistent in 3D across the slices such that it can be directly used for mesh generation. Two specific neural networks with multi-scale coarse-to-fine prediction structure are proposed to cope with the small training dataset and trained using an original loss function. The former segments a slice in the middle of the volume. Then the latter iteratively propagates the slice segmentations towards the base and the apex, in a spatially consistent way. We perform 5-fold cross-validation on the 15 cases from STACOM to validate the method. For training, we use real cases and their synthetic variants generated by combining motion simulation and image synthesis. Accurate and consistent testing results are obtained.
ER  -


TY  - Preprint
T1  - A Survey on Deep Learning Methods for Robot Vision
A1  - Javier Ruiz-del-Solar
A1  - Patricio Loncomilla
A1  - Naiomi Soto
JO  - ArXiv e-prints
Y1  - 28 March, 2018
UR  - https://arxiv.org/abs/1803.10862
N2  - Deep learning has allowed a paradigm shift in pattern recognition, from using hand-crafted features together with statistical classifiers to using general-purpose learning procedures for learning data-driven representations, features, and classifiers together. The application of this new paradigm has been particularly successful in computer vision, in which the development of deep learning methods for vision applications has become a hot research topic. Given that deep learning has already attracted the attention of the robot vision community, the main purpose of this survey is to address the use of deep learning in robot vision. To achieve this, a comprehensive overview of deep learning and its usage in computer vision is given, that includes a description of the most frequently used neural models and their main application areas. Then, the standard methodology and tools used for designing deep-learning based vision systems are presented. Afterwards, a review of the principal work using deep learning in robot vision is presented, as well as current and future trends related to the use of deep learning in robotics. This survey is intended to be a guide for the developers of robot vision systems.
ER  -


TY  - Preprint
T1  - Deep Learning Object Detection Methods for Ecological Camera Trap Data
A1  - Stefan Schneider
A1  - Graham W. Taylor
A1  - Stefan C. Kremer
JO  - ArXiv e-prints
Y1  - 28 March, 2018
UR  - https://arxiv.org/abs/1803.10842
N2  - Deep learning methods for computer vision tasks show promise for automating the data analysis of camera trap images. Ecological camera traps are a common approach for monitoring an ecosystem&#39;s animal population, as they provide continual insight into an environment without being intrusive. However, the analysis of camera trap images is expensive, labour intensive, and time consuming. Recent advances in the field of deep learning for object detection show promise towards automating the analysis of camera trap images. Here, we demonstrate their capabilities by training and comparing two deep learning object detection classifiers, Faster R-CNN and YOLO v2.0, to identify, quantify, and localize animal species within camera trap images using the Reconyx Camera Trap and the self-labeled Gold Standard Snapshot Serengeti data sets. When trained on large labeled datasets, object recognition methods have shown success. We demonstrate their use, in the context of realistically sized ecological data sets, by testing if object detection methods are applicable for ecological research scenarios when utilizing transfer learning. Faster R-CNN outperformed YOLO v2.0 with average accuracies of 93.0\% and 76.7\% on the two data sets, respectively. Our findings show promising steps towards the automation of the labourious task of labeling camera trap images, which can be used to improve our understanding of the population dynamics of ecosystems across the planet.
ER  -


TY  - Preprint
T1  - Learning Deep Representations with Probabilistic Knowledge Transfer
A1  - Nikolaos Passalis
A1  - Anastasios Tefas
JO  - ArXiv e-prints
Y1  - 14 August, 2018
UR  - https://arxiv.org/abs/1803.10837
N2  - Knowledge Transfer (KT) techniques tackle the problem of transferring the knowledge from a large and complex neural network into a smaller and faster one. However, existing KT methods are tailored towards classification tasks and they cannot be used efficiently for other representation learning tasks. In this paper a novel knowledge transfer technique, that is capable of training a student model that maintains the same amount of mutual information between the learned representation and a set of (possible unknown) labels as the teacher model, is proposed. Apart from outperforming existing KT techniques, the proposed method allows for overcoming several limitations of existing methods providing new insight into KT as well as novel KT applications, ranging from knowledge transfer from handcrafted feature extractors to {cross-modal} KT from the textual modality into the representation extracted from the visual modality of the data.
ER  -


TY  - Preprint
T1  - Learning to Become an Expert: Deep Networks Applied To Super-Resolution Microscopy
A1  - Louis-Ãmile Robitaille
A1  - Audrey Durand
A1  - Marc-AndrÃ© Gardner
A1  - Christian GagnÃ©
A1  - Paul De Koninck
A1  - Flavie Lavoie-Cardinal
JO  - ArXiv e-prints
Y1  - 28 March, 2018
UR  - https://arxiv.org/abs/1803.10806
N2  - With super-resolution optical microscopy, it is now possible to observe molecular interactions in living cells. The obtained images have a very high spatial precision but their overall quality can vary a lot depending on the structure of interest and the imaging parameters. Moreover, evaluating this quality is often difficult for non-expert users. In this work, we tackle the problem of learning the quality function of super- resolution images from scores provided by experts. More specifically, we are proposing a system based on a deep neural network that can provide a quantitative quality measure of a STED image of neuronal structures given as input. We conduct a user study in order to evaluate the quality of the predictions of the neural network against those of a human expert. Results show the potential while highlighting some of the limits of the proposed approach.
ER  -


TY  - Preprint
T1  - Unreasonable Effectivness of Deep Learning
A1  - Finn Macleod
JO  - ArXiv e-prints
Y1  - 28 March, 2018
UR  - https://arxiv.org/abs/1803.10768
N2  - We show how well known rules of back propagation arise from a weighted combination of finite automata. By redefining a finite automata as a predictor we combine the set of all $k$-state finite automata using a weighted majority algorithm. This aggregated prediction algorithm can be simplified using symmetry, and we prove the equivalence of an algorithm that does this. We demonstrate that this algorithm is equivalent to a form of a back propagation acting in a completely connected $k$-node neural network. Thus the use of the weighted majority algorithm allows a bound on the general performance of deep learning approaches to prediction via known results from online statistics. The presented framework opens more detailed questions about network topology; it is a bridge to the well studied techniques of semigroup theory and applying these techniques to answer what specific network topologies are capable of predicting. This informs both the design of artificial networks and the exploration of neuroscience models.
ER  -


TY  - Preprint
T1  - What deep learning can tell us about higher cognitive functions like mindreading?
A1  - Jaan Aru
A1  - Raul Vicente
JO  - ArXiv e-prints
Y1  - 28 March, 2018
UR  - https://arxiv.org/abs/1803.10470
N2  - Can deep learning (DL) guide our understanding of computations happening in biological brain? We will first briefly consider how DL has contributed to the research on visual object recognition. In the main part we will assess whether DL could also help us to clarify the computations underlying higher cognitive functions such as Theory of Mind. In addition, we will compare the objectives and learning signals of brains and machines, leading us to conclude that simply scaling up the current DL algorithms will not lead to human level mindreading skills. We then provide some insights about how to fairly compare human and DL performance. In the end we find that DL can contribute to our understanding of biological computations by providing an example of an end-to-end algorithm that solves the same problems the biological agents face.
ER  -


TY  - Preprint
T1  - Automated Speed and Lane Change Decision Making using Deep Reinforcement Learning
A1  - Carl-Johan Hoel
A1  - Krister Wolff
A1  - Leo Laine
JO  - ArXiv e-prints
Y1  - 14 March, 2018
UR  - https://arxiv.org/abs/1803.10056
N2  - This paper introduces a method, based on deep reinforcement learning, for automatically generating a general purpose decision making function. A Deep Q-Network agent was trained in a simulated environment to handle speed and lane change decisions for a truck-trailer combination. In a highway driving case, it is shown that the method produced an agent that matched or surpassed the performance of a commonly used reference model. To demonstrate the generality of the method, the exact same algorithm was also tested by training it for an overtaking case on a road with oncoming traffic. Furthermore, a novel way of applying a convolutional neural network to high level input that represents interchangeable objects is also introduced.
ER  -


TY  - Preprint
T1  - Learning Depth from Single Images with Deep Neural Network Embedding Focal Length
A1  - Lei He
A1  - Guanghui Wang
A1  - Zhanyi Hu
JO  - ArXiv e-prints
Y1  - 27 March, 2018
UR  - https://arxiv.org/abs/1803.10039
N2  - Learning depth from a single image, as an important issue in scene understanding, has attracted a lot of attention in the past decade. The accuracy of the depth estimation has been improved from conditional Markov random fields, non-parametric methods, to deep convolutional neural networks most recently. However, there exist inherent ambiguities in recovering 3D from a single 2D image. In this paper, we first prove the ambiguity between the focal length and monocular depth learning, and verify the result using experiments, showing that the focal length has a great influence on accurate depth recovery. In order to learn monocular depth by embedding the focal length, we propose a method to generate synthetic varying-focal-length dataset from fixed-focal-length datasets, and a simple and effective method is implemented to fill the holes in the newly generated images. For the sake of accurate depth recovery, we propose a novel deep neural network to infer depth through effectively fusing the middle-level information on the fixed-focal-length dataset, which outperforms the state-of-the-art methods built on pre-trained VGG. Furthermore, the newly generated varying-focal-length dataset is taken as input to the proposed network in both learning and inference phases. Extensive experiments on the fixed- and varying-focal-length datasets demonstrate that the learned monocular depth with embedded focal length is significantly improved compared to that without embedding the focal length information.
ER  -


TY  - Preprint
T1  - Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning
A1  - Andy Zeng
A1  - Shuran Song
A1  - Stefan Welker
A1  - Johnny Lee
A1  - Alberto Rodriguez
A1  - Thomas Funkhouser
JO  - ArXiv e-prints
Y1  - 30 September, 2018
UR  - https://arxiv.org/abs/1803.09956
N2  - Skilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. In this work, we demonstrate that it is possible to discover and learn these synergies from scratch through model-free deep reinforcement learning. Our method involves training two fully convolutional networks that map from visual observations to actions: one infers the utility of pushes for a dense pixel-wise sampling of end effector orientations and locations, while the other does the same for grasping. Both networks are trained jointly in a Q-learning framework and are entirely self-supervised by trial and error, where rewards are provided from successful grasps. In this way, our policy learns pushing motions that enable future grasps, while learning grasps that can leverage past pushes. During picking experiments in both simulation and real-world scenarios, we find that our system quickly learns complex behaviors amid challenging cases of clutter, and achieves better grasping success rates and picking efficiencies than baseline alternatives after only a few hours of training. We further demonstrate that our method is capable of generalizing to novel objects. Qualitative results (videos), code, pre-trained models, and simulation environments are available at http://vpg.cs.princeton.edu
ER  -


TY  - Preprint
T1  - Deep learning as a tool for neural data analysis: speech classification and cross-frequency coupling in human sensorimotor cortex
A1  - Jesse A. Livezey
A1  - Kristofer E. Bouchard
A1  - Edward F. Chang
JO  - ArXiv e-prints
Y1  - 26 March, 2018
UR  - https://arxiv.org/abs/1803.09807
N2  - A fundamental challenge in neuroscience is to understand what structure in the world is represented in spatially distributed patterns of neural activity from multiple single-trial measurements. This is often accomplished by learning a simple, linear transformations between neural features and features of the sensory stimuli or motor task. While successful in some early sensory processing areas, linear mappings are unlikely to be ideal tools for elucidating nonlinear, hierarchical representations of higher-order brain areas during complex tasks, such as the production of speech by humans. Here, we apply deep networks to predict produced speech syllables from cortical surface electric potentials recorded from human sensorimotor cortex. We found that deep networks had higher decoding prediction accuracy compared to baseline models, and also exhibited greater improvements in accuracy with increasing dataset size. We further demonstrate that deep network&#39;s confusions revealed hierarchical latent structure in the neural data, which recapitulated the underlying articulatory nature of speech motor control. Finally, we used deep networks to compare task-relevant information in different neural frequency bands, and found that the high-gamma band contains the vast majority of information relevant for the speech prediction task, with little-to-no additional contribution from lower-frequencies. Together, these results demonstrate the utility of deep networks as a data analysis tool for neuroscience.
ER  -


TY  - Preprint
T1  - Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification
A1  - Jingya Wang
A1  - Xiatian Zhu
A1  - Shaogang Gong
A1  - Wei Li
JO  - ArXiv e-prints
Y1  - 26 March, 2018
UR  - https://arxiv.org/abs/1803.09786
N2  - Most existing person re-identification (re-id) methods require supervised model learning from a separate large set of pairwise labelled training data for every single camera pair. This significantly limits their scalability and usability in real-world large scale deployments with the need for performing re-id across many camera views. To address this scalability problem, we develop a novel deep learning method for transferring the labelled information of an existing dataset to a new unseen (unlabelled) target domain for person re-id without any supervised learning in the target domain. Specifically, we introduce an Transferable Joint Attribute-Identity Deep Learning (TJ-AIDL) for simultaneously learning an attribute-semantic and identitydiscriminative feature representation space transferrable to any new (unseen) target domain for re-id tasks without the need for collecting new labelled training data from the target domain (i.e. unsupervised learning in the target domain). Extensive comparative evaluations validate the superiority of this new TJ-AIDL model for unsupervised person re-id over a wide range of state-of-the-art methods on four challenging benchmarks including VIPeR, PRID, Market-1501, and DukeMTMC-ReID.
ER  -


TY  - Preprint
T1  - Flow From Motion: A Deep Learning Approach
A1  - Cem Eteke
A1  - Hayati Havlucu
A1  - Nisa Ä°rem KÄ±rbaÃ§
A1  - Mehmet Cengiz OnbaÅlÄ±
A1  - Aykut CoÅkun
A1  - Terry Eskenazi
A1  - OÄuzhan Ãzcan
A1  - BarÄ±Å AkgÃ¼n
JO  - ArXiv e-prints
Y1  - 26 March, 2018
UR  - https://arxiv.org/abs/1803.09689
N2  - Wearable devices have the potential to enhance sports performance, yet they are not fulfilling this promise. Our previous studies with 6 professional tennis coaches and 20 players indicate that this could be due the lack of psychological or mental state feedback, which the coaches claim to provide. Towards this end, we propose to detect the flow state, mental state of optimal performance, using wearables data to be later used in training. We performed a study with a professional tennis coach and two players. The coach provided labels about the players&#39; flow state while each player had a wearable device on their racket holding wrist. We trained multiple models using the wearables data and the coach labels. Our deep neural network models achieved around 98% testing accuracy for a variety of conditions. This suggests that the flow state or what coaches recognize as flow, can be detected using wearables data in tennis which is a novel result. The implication for the HCI community is that having access to such information would allow for design of novel hardware and interaction paradigms that would be helpful in professional athlete training.
ER  -


TY  - Preprint
T1  - Efficient Image Dataset Classification Difficulty Estimation for Predicting Deep-Learning Accuracy
A1  - Florian Scheidegger
A1  - Roxana Istrate
A1  - Giovanni Mariani
A1  - Luca Benini
A1  - Costas Bekas
A1  - Cristiano Malossi
JO  - ArXiv e-prints
Y1  - 26 March, 2018
UR  - https://arxiv.org/abs/1803.09588
N2  - In the deep-learning community new algorithms are published at an incredible pace. Therefore, solving an image classification problem for new datasets becomes a challenging task, as it requires to re-evaluate published algorithms and their different configurations in order to find a close to optimal classifier. To facilitate this process, before biasing our decision towards a class of neural networks or running an expensive search over the network space, we propose to estimate the classification difficulty of the dataset. Our method computes a single number that characterizes the dataset difficulty 27x faster than training state-of-the-art networks. The proposed method can be used in combination with network topology and hyper-parameter search optimizers to efficiently drive the search towards promising neural-network configurations.
ER  -


TY  - Preprint
T1  - A Provably Correct Algorithm for Deep Learning that Actually Works
A1  - Eran Malach
A1  - Shai Shalev-Shwartz
JO  - ArXiv e-prints
Y1  - 24 June, 2018
UR  - https://arxiv.org/abs/1803.09522
N2  - We describe a layer-by-layer algorithm for training deep convolutional networks, where each step involves gradient updates for a two layer network followed by a simple clustering algorithm. Our algorithm stems from a deep generative model that generates mages level by level, where lower resolution images correspond to latent semantic classes. We analyze the convergence rate of our algorithm assuming that the data is indeed generated according to this model (as well as additional assumptions). While we do not pretend to claim that the assumptions are realistic for natural images, we do believe that they capture some true properties of real data. Furthermore, we show that our algorithm actually works in practice (on the CIFAR dataset), achieving results in the same ballpark as that of vanilla convolutional neural networks that are being trained by stochastic gradient descent. Finally, our proof techniques may be of independent interest.
ER  -


TY  - Preprint
T1  - Distinguishing Computer-generated Graphics from Natural Images Based on Sensor Pattern Noise and Deep Learning
A1  - Ye Yao
A1  - Weitong Hu
A1  - Wei Zhang
A1  - Ting Wu
A1  - Yun-Qing Shi
JO  - ArXiv e-prints
Y1  - 25 April, 2018
UR  - https://arxiv.org/abs/1803.09403
N2  - Computer-generated graphics (CGs) are images generated by computer software. The~rapid development of computer graphics technologies has made it easier to generate photorealistic computer graphics, and these graphics are quite difficult to distinguish from natural images (NIs) with the naked eye. In this paper, we propose a method based on sensor pattern noise (SPN) and deep learning to distinguish CGs from NIs. Before being fed into our convolutional neural network (CNN)-based model, these images---CGs and NIs---are clipped into image patches. Furthermore, three high-pass filters (HPFs) are used to remove low-frequency signals, which represent the image content. These filters are also used to reveal the residual signal as well as SPN introduced by the digital camera device. Different from the traditional methods of distinguishing CGs from NIs, the proposed method utilizes a five-layer CNN to classify the input image patches. Based on the classification results of the image patches, we deploy a majority vote scheme to obtain the classification results for the full-size images. The~experiments have demonstrated that (1) the proposed method with three HPFs can achieve better results than that with only one HPF or no HPF and that (2) the proposed method with three HPFs achieves 100\% accuracy, although the NIs undergo a JPEG compression with a quality factor of 75.
ER  -


TY  - Preprint
T1  - A Systematic Comparison of Deep Learning Architectures in an Autonomous Vehicle
A1  - Michael Teti
A1  - Elan Barenholtz
A1  - Shawn Martin
A1  - William Hahn
JO  - ArXiv e-prints
Y1  - 25 March, 2018
UR  - https://arxiv.org/abs/1803.09386
N2  - Self-driving technology is advancing rapidly, largely due to recent developments in deep learning algorithms. To date, however, there has been no systematic comparison of how different deep learning architectures perform at such tasks, or an attempt to determine a correlation between classification performance and performance in an actual vehicle. Here, we introduce the first controlled comparison of seven contemporary deep-learning architectures in an end-to-end autonomous driving task. We use a simple and affordable platform consisting of of an off-the-shelf, remotely operated vehicle, a GPU equipped computer and an indoor foam-rubber racetrack. We compare a fully-connected network, a 2-layer CNN, AlexNet, VGG-16, Inception-V3, ResNet-26, and LSTM and report the number of laps they are able to successfully complete without crashing while traversing an indoor racetrack under identical testing conditions. Based on these tests, AlexNet completed the most laps without crashing out of all networks, and ResNet-26 is the most &#39;efficient&#39; architecture examined, with respect to the number of laps completed relative to the number of parameters. We also observe whether spatial, color, or temporal features - or some combination - are more important for such tasks. Finally, we show that validation loss/accuracy is not sufficiently indicative of the model&#39;s performance even when employed in a real vehicle with a simple task, emphasizing the need for greater accessibility to research platforms within the self-driving community.
ER  -


TY  - Preprint
T1  - Goldbach&#39;s Function Approximation Using Deep Learning
A1  - Avigail Stekel
A1  - Merav Chkroun
A1  - Amos Azaria
JO  - ArXiv e-prints
Y1  - 25 March, 2018
UR  - https://arxiv.org/abs/1803.09237
N2  - Goldbach conjecture is one of the most famous open mathematical problems. It states that every even number, bigger than two, can be presented as a sum of 2 prime numbers. % In this work we present a deep learning based model that predicts the number of Goldbach partitions for a given even number. Surprisingly, our model outperforms all state-of-the-art analytically derived estimations for the number of couples, while not requiring prime factorization of the given number. We believe that building a model that can accurately predict the number of couples brings us one step closer to solving one of the world most famous open problems. To the best of our knowledge, this is the first attempt to consider machine learning based data-driven methods to approximate open mathematical problems in the field of number theory, and hope that this work will encourage such attempts.
ER  -


TY  - Preprint
T1  - Learning to Reweight Examples for Robust Deep Learning
A1  - Mengye Ren
A1  - Wenyuan Zeng
A1  - Bin Yang
A1  - Raquel Urtasun
JO  - ArXiv e-prints
Y1  - 8 June, 2018
UR  - https://arxiv.org/abs/1803.09050
N2  - Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.
ER  -


TY  - Preprint
T1  - Feature Transfer Learning for Deep Face Recognition with Long-Tail Data
A1  - Xi Yin
A1  - Xiang Yu
A1  - Kihyuk Sohn
A1  - Xiaoming Liu
A1  - Manmohan Chandraker
JO  - ArXiv e-prints
Y1  - 23 March, 2018
UR  - https://arxiv.org/abs/1803.09014
N2  - Real-world face recognition datasets exhibit long-tail characteristics, which results in biased classifiers in conventionally-trained deep neural networks, or insufficient data when long-tail classes are ignored. In this paper, we propose to handle long-tail classes in the training of a face recognition engine by augmenting their feature space under a center-based feature transfer framework. A Gaussian prior is assumed across all the head (regular) classes and the variance from regular classes are transferred to the long-tail class representation. This encourages the long-tail distribution to be closer to the regular distribution, while enriching and balancing the limited training data. Further, an alternating training regimen is proposed to simultaneously achieve less biased decision boundaries and a more discriminative feature representation. We conduct empirical studies that mimic long-tail datasets by limiting the number of samples and the proportion of long-tail classes on the MS-Celeb-1M dataset. We compare our method with baselines not designed to handle long-tail classes and also with state-of-the-art methods on face recognition benchmarks. State-of-the-art results on LFW, IJB-A and MS-Celeb-1M datasets demonstrate the effectiveness of our feature transfer approach and training strategy. Finally, our feature transfer allows smooth visual interpolation, which demonstrates disentanglement to preserve identity of a class while augmenting its feature space with non-identity variations.
ER  -


TY  - Preprint
T1  - Deep Learning Phase Segregation
A1  - Amir Barati Farimani
A1  - Joseph Gomes
A1  - Rishi Sharma
A1  - Franklin L. Lee
A1  - Vijay S. Pande
JO  - ArXiv e-prints
Y1  - 23 March, 2018
UR  - https://arxiv.org/abs/1803.08993
N2  - Phase segregation, the process by which the components of a binary mixture spontaneously separate, is a key process in the evolution and design of many chemical, mechanical, and biological systems. In this work, we present a data-driven approach for the learning, modeling, and prediction of phase segregation. A direct mapping between an initially dispersed, immiscible binary fluid and the equilibrium concentration field is learned by conditional generative convolutional neural networks. Concentration field predictions by the deep learning model conserve phase fraction, correctly predict phase transition, and reproduce area, perimeter, and total free energy distributions up to 98% accuracy.
ER  -


TY  - Preprint
T1  - Effective deep learning training for single-image super-resolution in endomicroscopy exploiting video-registration-based reconstruction
A1  - Daniele RavÃ¬
A1  - Agnieszka Barbara Szczotka
A1  - Dzhoshkun Ismail Shakir
A1  - Stephen P Pereira
A1  - Tom Vercauteren
JO  - ArXiv e-prints
Y1  - 23 March, 2018
UR  - https://arxiv.org/abs/1803.08840
N2  - Purpose: Probe-based Confocal Laser Endomicroscopy (pCLE) is a recent imaging modality that allows performing in vivo optical biopsies. The design of pCLE hardware, and its reliance on an optical fibre bundle, fundamentally limits the image quality with a few tens of thousands fibres, each acting as the equivalent of a single-pixel detector, assembled into a single fibre bundle. Video-registration techniques can be used to estimate high-resolution (HR) images by exploiting the temporal information contained in a sequence of low-resolution (LR) images. However, the alignment of LR frames, required for the fusion, is computationally demanding and prone to artefacts. Methods: In this work, we propose a novel synthetic data generation approach to train exemplar-based Deep Neural Networks (DNNs). HR pCLE images with enhanced quality are recovered by the models trained on pairs of estimated HR images (generated by the video-registration algorithm) and realistic synthetic LR images. Performance of three different state-of-the-art DNNs techniques were analysed on a Smart Atlas database of 8806 images from 238 pCLE video sequences. The results were validated through an extensive Image Quality Assessment (IQA) that takes into account different quality scores, including a Mean Opinion Score (MOS). Results: Results indicate that the proposed solution produces an effective improvement in the quality of the obtained reconstructed image. Conclusion: The proposed training strategy and associated DNNs allows us to perform convincing super-resolution of pCLE images.
ER  -


TY  - Preprint
T1  - Learning Deep Context-Network Architectures for Image Annotation
A1  - Mingyuan Jiu
A1  - Hichem Sahbi
JO  - ArXiv e-prints
Y1  - 23 March, 2018
UR  - https://arxiv.org/abs/1803.08794
N2  - Context plays an important role in visual pattern recognition as it provides complementary clues for different learning tasks including image classification and annotation. In the particular scenario of kernel learning, the general recipe of context-based kernel design consists in learning positive semi-definite similarity functions that return high values not only when data share similar content but also similar context. However, in spite of having a positive impact on performance, the use of context in these kernel design methods has not been fully explored; indeed, context has been handcrafted instead of being learned. In this paper, we introduce a novel context-aware kernel design framework based on deep learning. Our method discriminatively learns spatial geometric context as the weights of a deep network (DN). The architecture of this network is fully determined by the solution of an objective function that mixes content, context and regularization, while the parameters of this network determine the most relevant (discriminant) parts of the learned context. We apply this context and kernel learning framework to image classification using the challenging ImageCLEF Photo Annotation benchmark; the latter shows that our deep context learning provides highly effective kernels for image classification as corroborated through extensive experiments.
ER  -


TY  - Preprint
T1  - Deep learning and its application to medical image segmentation
A1  - Holger R. Roth
A1  - Chen Shen
A1  - Hirohisa Oda
A1  - Masahiro Oda
A1  - Yuichiro Hayashi
A1  - Kazunari Misawa
A1  - Kensaku Mori
JO  - ArXiv e-prints
Y1  - 23 March, 2018
UR  - https://arxiv.org/abs/1803.08691
N2  - One of the most common tasks in medical imaging is semantic segmentation. Achieving this segmentation automatically has been an active area of research, but the task has been proven very challenging due to the large variation of anatomy across different patients. However, recent advances in deep learning have made it possible to significantly improve the performance of image recognition and semantic segmentation methods in the field of computer vision. Due to the data driven approaches of hierarchical feature learning in deep learning frameworks, these advances can be translated to medical images without much difficulty. Several variations of deep convolutional neural networks have been successfully applied to medical images. Especially fully convolutional architectures have been proven efficient for segmentation of 3D medical images. In this article, we describe how to build a 3D fully convolutional network (FCN) that can process 3D images in order to produce automatic semantic segmentations. The model is trained and evaluated on a clinical computed tomography (CT) dataset and shows state-of-the-art performance in multi-organ segmentation.
ER  -


TY  - Preprint
T1  - Learning State Representations for Query Optimization with Deep Reinforcement Learning
A1  - Jennifer Ortiz
A1  - Magdalena Balazinska
A1  - Johannes Gehrke
A1  - S. Sathiya Keerthi
JO  - ArXiv e-prints
Y1  - 22 March, 2018
UR  - https://arxiv.org/abs/1803.08604
N2  - Deep reinforcement learning is quickly changing the field of artificial intelligence. These models are able to capture a high level understanding of their environment, enabling them to learn difficult dynamic tasks in a variety of domains. In the database field, query optimization remains a difficult problem. Our goal in this work is to explore the capabilities of deep reinforcement learning in the context of query optimization. At each state, we build queries incrementally and encode properties of subqueries through a learned representation. The challenge here lies in the formation of the state transition function, which defines how the current subquery state combines with the next query operation (action) to yield the next state. As a first step in this direction, we focus the state representation problem and the formation of the state transition function. We describe our approach and show preliminary results. We further discuss how we can use the state representation to improve query optimization using reinforcement learning.
ER  -


TY  - Preprint
T1  - End-to-End Learning for the Deep Multivariate Probit Model
A1  - Di Chen
A1  - Yexiang Xue
A1  - Carla P. Gomes
JO  - ArXiv e-prints
Y1  - 13 July, 2018
UR  - https://arxiv.org/abs/1803.08591
N2  - The multivariate probit model (MVP) is a popular classic model for studying binary responses of multiple entities. Nevertheless, the computational challenge of learning the MVP model, given that its likelihood involves integrating over a multidimensional constrained space of latent variables, significantly limits its application in practice. We propose a flexible deep generalization of the classic MVP, the Deep Multivariate Probit Model (DMVP), which is an end-to-end learning scheme that uses an efficient parallel sampling process of the multivariate probit model to exploit GPU-boosted deep neural networks. We present both theoretical and empirical analysis of the convergence behavior of DMVP&#39;s sampling process with respect to the resolution of the correlation structure. We provide convergence guarantees for DMVP and our empirical analysis demonstrates the advantages of DMVP&#39;s sampling compared with standard MCMC-based methods. We also show that when applied to multi-entity modelling problems, which are natural DMVP applications, DMVP trains faster than classical MVP, by at least an order of magnitude, captures rich correlations among entities, and further improves the joint likelihood of entities compared with several competitive models.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning with Model Learning and Monte Carlo Tree Search in Minecraft
A1  - Stephan Alaniz
JO  - ArXiv e-prints
Y1  - 22 March, 2018
UR  - https://arxiv.org/abs/1803.08456
N2  - Deep reinforcement learning has been successfully applied to several visual-input tasks using model-free methods. In this paper, we propose a model-based approach that combines learning a DNN-based transition model with Monte Carlo tree search to solve a block-placing task in Minecraft. Our learned transition model predicts the next frame and the rewards one step ahead given the last four frames of the agent&#39;s first-person-view image and the current action. Then a Monte Carlo tree search algorithm uses this model to plan the best sequence of actions for the agent to perform. On the proposed task in Minecraft, our model-based approach reaches the performance comparable to the Deep Q-Network&#39;s, but learns faster and, thus, is more training sample efficient.
ER  -


TY  - Preprint
T1  - Demystifying Deep Learning: A Geometric Approach to Iterative Projections
A1  - Ashkan Panahi
A1  - Hamid Krim
A1  - Liyi Dai
JO  - ArXiv e-prints
Y1  - 22 March, 2018
UR  - https://arxiv.org/abs/1803.08416
N2  - Parametric approaches to Learning, such as deep learning (DL), are highly popular in nonlinear regression, in spite of their extremely difficult training with their increasing complexity (e.g. number of layers in DL). In this paper, we present an alternative semi-parametric framework which foregoes the ordinarily required feedback, by introducing the novel idea of geometric regularization. We show that certain deep learning techniques such as residual network (ResNet) architecture are closely related to our approach. Hence, our technique can be used to analyze these types of deep learning. Moreover, we present preliminary results which confirm that our approach can be easily trained to obtain complex structures.
ER  -


TY  - Preprint
T1  - Deep Learning using Rectified Linear Units (ReLU)
A1  - Abien Fred Agarap
JO  - ArXiv e-prints
Y1  - 22 March, 2018
UR  - https://arxiv.org/abs/1803.08375
N2  - We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer $h_{n - 1}$ in a neural network, then multiply it by weight parameters $Î¸$ to get the raw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$, i.e. $f(o) = \max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide class predictions $\hat{y}$ through argmax function, i.e. argmax $f(x)$.
ER  -


TY  - Preprint
T1  - Learning Eligibility in Cancer Clinical Trials using Deep Neural Networks
A1  - Aurelia Bustos
A1  - Antonio Pertusa
JO  - ArXiv e-prints
Y1  - 25 July, 2018
UR  - https://arxiv.org/abs/1803.08312
N2  - Interventional cancer clinical trials are generally too restrictive, and some patients are often excluded on the basis of comorbidity, past or concomitant treatments, or the fact that they are over a certain age. The efficacy and safety of new treatments for patients with these characteristics are, therefore, not defined. In this work, we built a model to automatically predict whether short clinical statements were considered inclusion or exclusion criteria. We used protocols from cancer clinical trials that were available in public registries from the last 18 years to train word-embeddings, and we constructed a~dataset of 6M short free-texts labeled as eligible or not eligible. A text classifier was trained using deep neural networks, with pre-trained word-embeddings as inputs, to predict whether or not short free-text statements describing clinical information were considered eligible. We additionally analyzed the semantic reasoning of the word-embedding representations obtained and were able to identify equivalent treatments for a type of tumor analogous with the drugs used to treat other tumors. We show that representation learning using {deep} neural networks can be successfully leveraged to extract the medical knowledge from clinical trial protocols for potentially assisting practitioners when prescribing treatments.
ER  -


TY  - Preprint
T1  - Extended depth-of-field in holographic image reconstruction using deep learning based auto-focusing and phase-recovery
A1  - Yichen Wu
A1  - Yair Rivenson
A1  - Yibo Zhang
A1  - Zhensong Wei
A1  - Harun Gunaydin
A1  - Xing Lin
A1  - Aydogan Ozcan
JO  - ArXiv e-prints
Y1  - 21 March, 2018
UR  - https://arxiv.org/abs/1803.08138
N2  - Holography encodes the three dimensional (3D) information of a sample in the form of an intensity-only recording. However, to decode the original sample image from its hologram(s), auto-focusing and phase-recovery are needed, which are in general cumbersome and time-consuming to digitally perform. Here we demonstrate a convolutional neural network (CNN) based approach that simultaneously performs auto-focusing and phase-recovery to significantly extend the depth-of-field (DOF) in holographic image reconstruction. For this, a CNN is trained by using pairs of randomly de-focused back-propagated holograms and their corresponding in-focus phase-recovered images. After this training phase, the CNN takes a single back-propagated hologram of a 3D sample as input to rapidly achieve phase-recovery and reconstruct an in focus image of the sample over a significantly extended DOF. This deep learning based DOF extension method is non-iterative, and significantly improves the algorithm time-complexity of holographic image reconstruction from O(nm) to O(1), where n refers to the number of individual object points or particles within the sample volume, and m represents the focusing search space within which each object point or particle needs to be individually focused. These results highlight some of the unique opportunities created by data-enabled statistical image reconstruction methods powered by machine learning, and we believe that the presented approach can be broadly applicable to computationally extend the DOF of other imaging modalities.
ER  -


TY  - Preprint
T1  - Information Theoretic Interpretation of Deep learning
A1  - Tianchen Zhao
JO  - ArXiv e-prints
Y1  - 21 March, 2018
UR  - https://arxiv.org/abs/1803.07980
N2  - We interpret part of the experimental results of Shwartz-Ziv and Tishby [2017]. Inspired by these results, we established a conjecture of the dynamics of the machinary of deep neural network. This conjecture can be used to explain the counterpart result by Saxe et al. [2018].
ER  -


TY  - Preprint
T1  - Learning and Recognizing Human Action from Skeleton Movement with Deep Residual Neural Networks
A1  - Huy-Hieu Pham
A1  - Louahdi Khoudour
A1  - Alain Crouzil
A1  - Pablo Zegers
A1  - Sergio A. Velastin
JO  - ArXiv e-prints
Y1  - 21 March, 2018
UR  - https://arxiv.org/abs/1803.07780
N2  - Automatic human action recognition is indispensable for almost artificial intelligent systems such as video surveillance, human-computer interfaces, video retrieval, etc. Despite a lot of progress, recognizing actions in an unknown video is still a challenging task in computer vision. Recently, deep learning algorithms have proved its great potential in many vision-related recognition tasks. In this paper, we propose the use of Deep Residual Neural Networks (ResNets) to learn and recognize human action from skeleton data provided by Kinect sensor. Firstly, the body joint coordinates are transformed into 3D-arrays and saved in RGB images space. Five different deep learning models based on ResNet have been designed to extract image features and classify them into classes. Experiments are conducted on two public video datasets for human action recognition containing various challenges. The results show that our method achieves the state-of-the-art performance comparing with existing approaches.
ER  -


TY  - Preprint
T1  - A Survey of Deep Learning Techniques for Mobile Robot Applications
A1  - Jahanzaib Shabbir
A1  - Tarique Anwer
JO  - ArXiv e-prints
Y1  - 20 March, 2018
UR  - https://arxiv.org/abs/1803.07608
N2  - Advancements in deep learning over the years have attracted research into how deep artificial neural networks can be used in robotic systems. This research survey will present a summarization of the current research with a specific focus on the gains and obstacles for deep learning to be applied to mobile robotics.
ER  -


TY  - Preprint
T1  - DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems
A1  - Lei Ma
A1  - Felix Juefei-Xu
A1  - Fuyuan Zhang
A1  - Jiyuan Sun
A1  - Minhui Xue
A1  - Bo Li
A1  - Chunyang Chen
A1  - Ting Su
A1  - Li Li
A1  - Yang Liu
A1  - Jianjun Zhao
A1  - Yadong Wang
JO  - ArXiv e-prints
Y1  - 14 August, 2018
UR  - https://arxiv.org/abs/1803.07519
N2  - Deep learning (DL) defines a new data-driven programming paradigm that constructs the internal system logic of a crafted neuron network through a set of training data. We have seen wide adoption of DL in many safety-critical scenarios. However, a plethora of studies have shown that the state-of-the-art DL systems suffer from various vulnerabilities which can lead to severe consequences when applied to real-world applications. Currently, the testing adequacy of a DL system is usually measured by the accuracy of test data. Considering the limitation of accessible high quality test data, good accuracy performance on test data can hardly provide confidence to the testing adequacy and generality of DL systems. Unlike traditional software systems that have clear and controllable logic and functionality, the lack of interpretability in a DL system makes system analysis and defect detection difficult, which could potentially hinder its real-world deployment. In this paper, we propose DeepGauge, a set of multi-granularity testing criteria for DL systems, which aims at rendering a multi-faceted portrayal of the testbed. The in-depth evaluation of our proposed testing criteria is demonstrated on two well-known datasets, five DL systems, and with four state-of-the-art adversarial attack techniques against DL. The potential usefulness of DeepGauge sheds light on the construction of more generic and robust DL systems.
ER  -


TY  - Preprint
T1  - Explanation Methods in Deep Learning: Users, Values, Concerns and Challenges
A1  - Gabrielle Ras
A1  - Marcel van Gerven
A1  - Pim Haselager
JO  - ArXiv e-prints
Y1  - 29 March, 2018
UR  - https://arxiv.org/abs/1803.07517
N2  - Issues regarding explainable AI involve four components: users, laws &amp; regulations, explanations and algorithms. Together these components provide a context in which explanation methods can be evaluated regarding their adequacy. The goal of this chapter is to bridge the gap between expert users and lay users. Different kinds of users are identified and their concerns revealed, relevant statements from the General Data Protection Regulation are analyzed in the context of Deep Neural Networks (DNNs), a taxonomy for the classification of existing explanation methods is introduced, and finally, the various classes of explanation methods are analyzed to verify if user concerns are justified. Overall, it is clear that (visual) explanations can be given about various aspects of the influence of the input on the output. However, it is noted that explanation methods or interfaces for lay users are missing and we speculate which criteria these methods / interfaces should satisfy. Finally it is noted that two important concerns are difficult to address with explanation methods: the concern about bias in datasets that leads to biased DNNs, as well as the suspicion about unfair outcomes.
ER  -


TY  - Preprint
T1  - Natural Gradient Deep Q-learning
A1  - Ethan Knight
A1  - Osher Lerner
JO  - ArXiv e-prints
Y1  - 20 March, 2018
UR  - https://arxiv.org/abs/1803.07482
N2  - This paper presents findings for training a Q-learning reinforcement learning agent using natural gradient techniques. We compare the original deep Q-network (DQN) algorithm to its natural gradient counterpart (NGDQN), measuring NGDQN and DQN performance on classic controls environments without target networks. We find that NGDQN performs favorably relative to DQN, converging to significantly better policies faster and more frequently. These results indicate that natural gradient could be used for value function optimization in reinforcement learning to accelerate and stabilize training.
ER  -


TY  - Preprint
T1  - Optimizing Sponsored Search Ranking Strategy by Deep Reinforcement Learning
A1  - Li He
A1  - Liang Wang
A1  - Kaipeng Liu
A1  - Bo Wu
A1  - Weinan Zhang
JO  - ArXiv e-prints
Y1  - 26 March, 2018
UR  - https://arxiv.org/abs/1803.07347
N2  - Sponsored search is an indispensable business model and a major revenue contributor of almost all the search engines. From the advertisers&#39; side, participating in ranking the search results by paying for the sponsored search advertisement to attract more awareness and purchase facilitates their commercial goal. From the users&#39; side, presenting personalized advertisement reflecting their propensity would make their online search experience more satisfactory. Sponsored search platforms rank the advertisements by a ranking function to determine the list of advertisements to show and the charging price for the advertisers. Hence, it is crucial to find a good ranking function which can simultaneously satisfy the platform, the users and the advertisers. Moreover, advertisements showing positions under different queries from different users may associate with advertisement candidates of different bid price distributions and click probability distributions, which requires the ranking functions to be optimized adaptively to the traffic characteristics. In this work, we proposed a generic framework to optimize the ranking functions by deep reinforcement learning methods. The framework is composed of two parts: an offline learning part which initializes the ranking functions by learning from a simulated advertising environment, allowing adequate exploration of the ranking function parameter space without hurting the performance of the commercial platform. An online learning part which further optimizes the ranking functions by adapting to the online data distribution. Experimental results on a large-scale sponsored search platform confirm the effectiveness of the proposed method.
ER  -


TY  - Preprint
T1  - Flex-Convolution (Deep Learning Beyond Grid-Worlds)
A1  - Fabian Groh
A1  - Patrick Wieschollek
A1  - Hendrik P. A. Lensch
JO  - ArXiv e-prints
Y1  - 8 July, 2018
UR  - https://arxiv.org/abs/1803.07289
N2  - Traditional convolution layers are specifically designed to exploit the natural data representation of images -- a fixed and regular grid. However, unstructured data like 3D point clouds containing irregular neighborhoods constantly breaks the grid-based data assumption. Therefore applying best-practices and design choices from 2D-image learning methods towards processing point clouds are not readily possible. In this work, we introduce a natural generalization flex-convolution of the conventional convolution layer along with an efficient GPU implementation. We demonstrate competitive performance on rather small benchmark sets using fewer parameters and lower memory consumption and obtain significant improvements on a million-scale real-world dataset. Ours is the first which allows to efficiently process 7 million points concurrently.
ER  -


TY  - Preprint
T1  - Learning the Hierarchical Parts of Objects by Deep Non-Smooth Nonnegative Matrix Factorization
A1  - Jinshi Yu
A1  - Guoxu Zhou
A1  - Andrzej Cichocki
A1  - Shengli Xie
JO  - ArXiv e-prints
Y1  - 19 March, 2018
UR  - https://arxiv.org/abs/1803.07226
N2  - Nonsmooth Nonnegative Matrix Factorization (nsNMF) is capable of producing more localized, less overlapped feature representations than other variants of NMF while keeping satisfactory fit to data. However, nsNMF as well as other existing NMF methods is incompetent to learn hierarchical features of complex data due to its shallow structure. To fill this gap, we propose a deep nsNMF method coined by the fact that it possesses a deeper architecture compared with standard nsNMF. The deep nsNMF not only gives parts-based features due to the nonnegativity constraints, but also creates higher-level, more abstract features by combing lower-level ones. The in-depth description of how deep architecture can help to efficiently discover abstract features in dnsNMF is presented. And we also show that the deep nsNMF has close relationship with the deep autoencoder, suggesting that the proposed model inherits the major advantages from both deep learning and NMF. Extensive experiments demonstrate the standout performance of the proposed method in clustering analysis.
ER  -


TY  - Preprint
T1  - Live Target Detection with Deep Learning Neural Network and Unmanned Aerial Vehicle on Android Mobile Device
A1  - Ali Canberk Anar
A1  - Erkan Bostanci
A1  - Mehmet Serdar Guzel
JO  - ArXiv e-prints
Y1  - 22 March, 2018
UR  - https://arxiv.org/abs/1803.07015
N2  - This paper describes the stages faced during the development of an Android program which obtains and decodes live images from DJI Phantom 3 Professional Drone and implements certain features of the TensorFlow Android Camera Demo application. Test runs were made and outputs of the application were noted. A lake was classified as seashore, breakwater and pier with the proximities of 24.44%, 21.16% and 12.96% respectfully. The joystick of the UAV controller and laptop keyboard was classified with the proximities of 19.10% and 13.96% respectfully. The laptop monitor was classified as screen, monitor and television with the proximities of 18.77%, 14.76% and 14.00% respectfully. The computer used during the development of this study was classified as notebook and laptop with the proximities of 20.04% and 11.68% respectfully. A tractor parked at a parking lot was classified with the proximity of 12.88%. A group of cars in the same parking lot were classified as sports car, racer and convertible with the proximities of 31.75%, 18.64% and 13.45% respectfully at an inference time of 851ms.
ER  -


TY  - Preprint
T1  - Composable Deep Reinforcement Learning for Robotic Manipulation
A1  - Tuomas Haarnoja
A1  - Vitchyr Pong
A1  - Aurick Zhou
A1  - Murtaza Dalal
A1  - Pieter Abbeel
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 18 March, 2018
UR  - https://arxiv.org/abs/1803.06773
N2  - Model-free deep reinforcement learning has been shown to exhibit good performance in domains ranging from video games to simulated robotic manipulation and locomotion. However, model-free methods are known to perform poorly when the interaction time with the environment is limited, as is the case for most real-world robotic tasks. In this paper, we study how maximum entropy policies trained using soft Q-learning can be applied to real-world robotic manipulation. The application of this method to real-world manipulation is facilitated by two important features of soft Q-learning. First, soft Q-learning can learn multimodal exploration strategies by learning policies represented by expressive energy-based models. Second, we show that policies learned with soft Q-learning can be composed to create new policies, and that the optimality of the resulting policy can be bounded in terms of the divergence between the composed policies. This compositionality provides an especially valuable tool for real-world manipulation, where constructing new policies by composing existing skills can provide a large gain in efficiency over training from scratch. Our experimental evaluation demonstrates that soft Q-learning is substantially more sample efficient than prior model-free deep reinforcement learning methods, and that compositionality can be performed for both simulated and real-world tasks.
ER  -


TY  - Preprint
T1  - Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains
A1  - Jiahao Pang
A1  - Wenxiu Sun
A1  - Chengxi Yang
A1  - Jimmy Ren
A1  - Ruichao Xiao
A1  - Jin Zeng
A1  - Liang Lin
JO  - ArXiv e-prints
Y1  - 18 March, 2018
UR  - https://arxiv.org/abs/1803.06641
N2  - Despite the recent success of stereo matching with convolutional neural networks (CNNs), it remains arduous to generalize a pre-trained deep stereo model to a novel domain. A major difficulty is to collect accurate ground-truth disparities for stereo pairs in the target domain. In this work, we propose a self-adaptation approach for CNN training, utilizing both synthetic training data (with ground-truth disparities) and stereo pairs in the new domain (without ground-truths). Our method is driven by two empirical observations. By feeding real stereo pairs of different domains to stereo models pre-trained with synthetic data, we see that: i) a pre-trained model does not generalize well to the new domain, producing artifacts at boundaries and ill-posed regions; however, ii) feeding an up-sampled stereo pair leads to a disparity map with extra details. To avoid i) while exploiting ii), we formulate an iterative optimization problem with graph Laplacian regularization. At each iteration, the CNN adapts itself better to the new domain: we let the CNN learn its own higher-resolution output; at the meanwhile, a graph Laplacian regularization is imposed to discriminatively keep the desired edges while smoothing out the artifacts. We demonstrate the effectiveness of our method in two domains: daily scenes collected by smartphone cameras, and street views captured in a driving car.
ER  -


TY  - Preprint
T1  - Efficient and accurate inversion of multiple scattering with deep learning
A1  - Yu Sun
A1  - Zhihao Xia
A1  - Ulugbek S. Kamilov
JO  - ArXiv e-prints
Y1  - 5 April, 2018
UR  - https://arxiv.org/abs/1803.06594
N2  - Image reconstruction under multiple light scattering is crucial in a number of applications such as diffraction tomography. The reconstruction problem is often formulated as a nonconvex optimization, where a nonlinear measurement model is used to account for multiple scattering and regularization is used to enforce prior constraints on the object. In this paper, we propose a powerful alternative to this optimization-based view of image reconstruction by designing and training a deep convolutional neural network that can invert multiple scattered measurements to produce a high-quality image of the refractive index. Our results on both simulated and experimental datasets show that the proposed approach is substantially faster and achieves higher imaging quality compared to the state-of-the-art methods based on optimization.
ER  -


TY  - Preprint
T1  - Constrained Deep Learning using Conditional Gradient and Applications in Computer Vision
A1  - Sathya N. Ravi
A1  - Tuan Dinh
A1  - Vishnu Sai Rao Lokhande
A1  - Vikas Singh
JO  - ArXiv e-prints
Y1  - 16 March, 2018
UR  - https://arxiv.org/abs/1803.06453
N2  - A number of results have recently demonstrated the benefits of incorporating various constraints when training deep architectures in vision and machine learning. The advantages range from guarantees for statistical generalization to better accuracy to compression. But support for general constraints within widely used libraries remains scarce and their broader deployment within many applications that can benefit from them remains under-explored. Part of the reason is that Stochastic gradient descent (SGD), the workhorse for training deep neural networks, does not natively deal with constraints with global scope very well. In this paper, we revisit a classical first order scheme from numerical optimization, Conditional Gradients (CG), that has, thus far had limited applicability in training deep models. We show via rigorous analysis how various constraints can be naturally handled by modifications of this algorithm. We provide convergence guarantees and show a suite of immediate benefits that are possible -- from training ResNets with fewer layers but better accuracy simply by substituting in our version of CG to faster training of GANs with 50% fewer epochs in image inpainting applications to provably better generalization guarantees using efficiently implementable forms of recently proposed regularizers.
ER  -


TY  - Preprint
T1  - Deep learning for affective computing: text-based emotion recognition in decision support
A1  - Bernhard Kratzwald
A1  - Suzana Ilic
A1  - Mathias Kraus
A1  - Stefan Feuerriegel
A1  - Helmut Prendinger
JO  - ArXiv e-prints
Y1  - 10 September, 2018
UR  - https://arxiv.org/abs/1803.06397
N2  - Emotions widely affect human decision-making. This fact is taken into account by affective computing with the goal of tailoring decision support to the emotional states of individuals. However, the accurate recognition of emotions within narrative documents presents a challenging undertaking due to the complexity and ambiguity of language. Performance improvements can be achieved through deep learning; yet, as demonstrated in this paper, the specific nature of this task requires the customization of recurrent neural networks with regard to bidirectional processing, dropout layers as a means of regularization, and weighted loss functions. In addition, we propose sent2affect, a tailored form of transfer learning for affective computing: here the network is pre-trained for a different task (i.e. sentiment analysis), while the output layer is subsequently tuned to the task of emotion recognition. The resulting performance is evaluated in a holistic setting across 6 benchmark datasets, where we find that both recurrent neural networks and transfer learning consistently outperform traditional machine learning. Altogether, the findings have considerable implications for the use of affective computing.
ER  -


TY  - Preprint
T1  - Learning deep structured active contours end-to-end
A1  - Diego Marcos
A1  - Devis Tuia
A1  - Benjamin Kellenberger
A1  - Lisa Zhang
A1  - Min Bai
A1  - Renjie Liao
A1  - Raquel Urtasun
JO  - ArXiv e-prints
Y1  - 16 March, 2018
UR  - https://arxiv.org/abs/1803.06329
N2  - The world is covered with millions of buildings, and precisely knowing each instance&#39;s position and extents is vital to a multitude of applications. Recently, automated building footprint segmentation models have shown superior detection accuracy thanks to the usage of Convolutional Neural Networks (CNN). However, even the latest evolutions struggle to precisely delineating borders, which often leads to geometric distortions and inadvertent fusion of adjacent building instances. We propose to overcome this issue by exploiting the distinct geometric properties of buildings. To this end, we present Deep Structured Active Contours (DSAC), a novel framework that integrates priors and constraints into the segmentation process, such as continuous boundaries, smooth edges, and sharp corners. To do so, DSAC employs Active Contour Models (ACM), a family of constraint- and prior-based polygonal models. We learn ACM parameterizations per instance using a CNN, and show how to incorporate all components in a structured output model, making DSAC trainable end-to-end. We evaluate DSAC on three challenging building instance segmentation datasets, where it compares favorably against state-of-the-art. Code will be made available.
ER  -


TY  - Preprint
T1  - Learning Sparse Deep Feedforward Networks via Tree Skeleton Expansion
A1  - Zhourong Chen
A1  - Xiaopeng Li
A1  - Nevin L. Zhang
JO  - ArXiv e-prints
Y1  - 16 March, 2018
UR  - https://arxiv.org/abs/1803.06120
N2  - Despite the popularity of deep learning, structure learning for deep models remains a relatively under-explored area. In contrast, structure learning has been studied extensively for probabilistic graphical models (PGMs). In particular, an efficient algorithm has been developed for learning a class of tree-structured PGMs called hierarchical latent tree models (HLTMs), where there is a layer of observed variables at the bottom and multiple layers of latent variables on top. In this paper, we propose a simple method for learning the structures of feedforward neural networks (FNNs) based on HLTMs. The idea is to expand the connections in the tree skeletons from HLTMs and to use the resulting structures for FNNs. An important characteristic of FNN structures learned this way is that they are sparse. We present extensive empirical results to show that, compared with standard FNNs tuned-manually, sparse FNNs learned by our method achieve better or comparable classification performance with much fewer parameters. They are also more interpretable.
ER  -


TY  - Preprint
T1  - Deep Multiple Instance Learning for Zero-shot Image Tagging
A1  - Shafin Rahman
A1  - Salman Khan
JO  - ArXiv e-prints
Y1  - 15 March, 2018
UR  - https://arxiv.org/abs/1803.06051
N2  - In-line with the success of deep learning on traditional recognition problem, several end-to-end deep models for zero-shot recognition have been proposed in the literature. These models are successful to predict a single unseen label given an input image, but does not scale to cases where multiple unseen objects are present. In this paper, we model this problem within the framework of Multiple Instance Learning (MIL). To the best of our knowledge, we propose the first end-to-end trainable deep MIL framework for the multi-label zero-shot tagging problem. Due to its novel design, the proposed framework has several interesting features: (1) Unlike previous deep MIL models, it does not use any off-line procedure (e.g., Selective Search or EdgeBoxes) for bag generation. (2) During test time, it can process any number of unseen labels given their semantic embedding vectors. (3) Using only seen labels per image as weak annotation, it can produce a bounding box for each predicted labels. We experiment with the NUS-WIDE dataset and achieve superior performance across conventional, zero-shot and generalized zero-shot tagging tasks.
ER  -


TY  - Preprint
T1  - GossipGraD: Scalable Deep Learning using Gossip Communication based Asynchronous Gradient Descent
A1  - Jeff Daily
A1  - Abhinav Vishnu
A1  - Charles Siegel
A1  - Thomas Warfel
A1  - Vinay Amatya
JO  - ArXiv e-prints
Y1  - 15 March, 2018
UR  - https://arxiv.org/abs/1803.05880
N2  - In this paper, we present GossipGraD - a gossip communication protocol based Stochastic Gradient Descent (SGD) algorithm for scaling Deep Learning (DL) algorithms on large-scale systems. The salient features of GossipGraD are: 1) reduction in overall communication complexity from Î(log(p)) for p compute nodes in well-studied SGD to O(1), 2) model diffusion such that compute nodes exchange their updates (gradients) indirectly after every log(p) steps, 3) rotation of communication partners for facilitating direct diffusion of gradients, 4) asynchronous distributed shuffle of samples during the feedforward phase in SGD to prevent over-fitting, 5) asynchronous communication of gradients for further reducing the communication cost of SGD and GossipGraD. We implement GossipGraD for GPU and CPU clusters and use NVIDIA GPUs (Pascal P100) connected with InfiniBand, and Intel Knights Landing (KNL) connected with Aries network. We evaluate GossipGraD using well-studied dataset ImageNet-1K (~250GB), and widely studied neural network topologies such as GoogLeNet and ResNet50 (current winner of ImageNet Large Scale Visualization Research Challenge (ILSVRC)). Our performance evaluation using both KNL and Pascal GPUs indicates that GossipGraD can achieve perfect efficiency for these datasets and their associated neural network topologies. Specifically, for ResNet50, GossipGraD is able to achieve ~100% compute efficiency using 128 NVIDIA Pascal P100 GPUs - while matching the top-1 classification accuracy published in literature.
ER  -


TY  - Preprint
T1  - Development and Validation of Deep Learning Algorithms for Detection of Critical Findings in Head CT Scans
A1  - Sasank Chilamkurthy
A1  - Rohit Ghosh
A1  - Swetha Tanamala
A1  - Mustafa Biviji
A1  - Norbert G. Campeau
A1  - Vasantha Kumar Venugopal
A1  - Vidur Mahajan
A1  - Pooja Rao
A1  - Prashant Warier
JO  - ArXiv e-prints
Y1  - 12 April, 2018
UR  - https://arxiv.org/abs/1803.05854
N2  - Importance: Non-contrast head CT scan is the current standard for initial imaging of patients with head trauma or stroke symptoms.
ER  -


TY  - Preprint
T1  - Accurate Facial Parts Localization and Deep Learning for 3D Facial Expression Recognition
A1  - Asim Jan
A1  - Huaxiong Ding
A1  - Hongying Meng
A1  - Liming Chen
A1  - Huibin Li
JO  - ArXiv e-prints
Y1  - 4 March, 2018
UR  - https://arxiv.org/abs/1803.05846
N2  - Meaningful facial parts can convey key cues for both facial action unit detection and expression prediction. Textured 3D face scan can provide both detailed 3D geometric shape and 2D texture appearance cues of the face which are beneficial for Facial Expression Recognition (FER). However, accurate facial parts extraction as well as their fusion are challenging tasks. In this paper, a novel system for 3D FER is designed based on accurate facial parts extraction and deep feature fusion of facial parts. In particular, each textured 3D face scan is firstly represented as a 2D texture map and a depth map with one-to-one dense correspondence. Then, the facial parts of both texture map and depth map are extracted using a novel 4-stage process consists of facial landmark localization, facial rotation correction, facial resizing, facial parts bounding box extraction and post-processing procedures. Finally, deep fusion Convolutional Neural Networks (CNNs) features of all facial parts are learned from both texture maps and depth maps, respectively and nonlinear SVMs are used for expression prediction. Experiments are conducted on the BU-3DFE database, demonstrating the effectiveness of combing different facial parts, texture and depth cues and reporting the state-of-the-art results in comparison with all existing methods under the same setting.
ER  -


TY  - Preprint
T1  - Rearrangement with Nonprehensile Manipulation Using Deep Reinforcement Learning
A1  - Weihao Yuan
A1  - Johannes A. Stork
A1  - Danica Kragic
A1  - Michael Y. Wang
A1  - Kaiyu Hang
JO  - ArXiv e-prints
Y1  - 15 March, 2018
UR  - https://arxiv.org/abs/1803.05752
N2  - Rearranging objects on a tabletop surface by means of nonprehensile manipulation is a task which requires skillful interaction with the physical world. Usually, this is achieved by precisely modeling physical properties of the objects, robot, and the environment for explicit planning. In contrast, as explicitly modeling the physical environment is not always feasible and involves various uncertainties, we learn a nonprehensile rearrangement strategy with deep reinforcement learning based on only visual feedback. For this, we model the task with rewards and train a deep Q-network. Our potential field-based heuristic exploration strategy reduces the amount of collisions which lead to suboptimal outcomes and we actively balance the training set to avoid bias towards poor examples. Our training process leads to quicker learning and better performance on the task as compared to uniform exploration and standard experience replay. We demonstrate empirical evidence from simulation that our method leads to a success rate of 85%, show that our system can cope with sudden changes of the environment, and compare our performance with human level performance.
ER  -


TY  - Preprint
T1  - Feedback Control For Cassie With Deep Reinforcement Learning
A1  - Zhaoming Xie
A1  - Glen Berseth
A1  - Patrick Clary
A1  - Jonathan Hurst
A1  - Michiel van de Panne
JO  - ArXiv e-prints
Y1  - 27 July, 2018
UR  - https://arxiv.org/abs/1803.05580
N2  - Bipedal locomotion skills are challenging to develop. Control strategies often use local linearization of the dynamics in conjunction with reduced-order abstractions to yield tractable solutions. In these model-based control strategies, the controller is often not fully aware of many details, including torque limits, joint limits, and other non-linearities that are necessarily excluded from the control computations for simplicity. Deep reinforcement learning (DRL) offers a promising model-free approach for controlling bipedal locomotion which can more fully exploit the dynamics. However, current results in the machine learning literature are often based on ad-hoc simulation models that are not based on corresponding hardware. Thus it remains unclear how well DRL will succeed on realizable bipedal robots. In this paper, we demonstrate the effectiveness of DRL using a realistic model of Cassie, a bipedal robot. By formulating a feedback control problem as finding the optimal policy for a Markov Decision Process, we are able to learn robust walking controllers that imitate a reference motion with DRL. Controllers for different walking speeds are learned by imitating simple time-scaled versions of the original reference motion. Controller robustness is demonstrated through several challenging tests, including sensory delay, walking blindly on irregular terrain and unexpected pushes at the pelvis. We also show we can interpolate between individual policies and that robustness can be improved with an interpolated policy.
ER  -


TY  - Preprint
T1  - Computer-aided diagnosis of lung carcinoma using deep learning - a pilot study
A1  - Zhang Li
A1  - Zheyu Hu
A1  - Jiaolong Xu
A1  - Tao Tan
A1  - Hui Chen
A1  - Zhi Duan
A1  - Ping Liu
A1  - Jun Tang
A1  - Guoping Cai
A1  - Quchang Ouyang
A1  - Yuling Tang
A1  - Geert Litjens
A1  - Qiang Li
JO  - ArXiv e-prints
Y1  - 14 March, 2018
UR  - https://arxiv.org/abs/1803.05471
N2  - Aim: Early detection and correct diagnosis of lung cancer are the most important steps in improving patient outcome. This study aims to assess which deep learning models perform best in lung cancer diagnosis. Methods: Non-small cell lung carcinoma and small cell lung carcinoma biopsy specimens were consecutively obtained and stained. The specimen slides were diagnosed by two experienced pathologists (over 20 years). Several deep learning models were trained to discriminate cancer and non-cancer biopsies. Result: Deep learning models give reasonable AUC from 0.8810 to 0.9119. Conclusion: The deep learning analysis could help to speed up the detection process for the whole-slide image (WSI) and keep the comparable detection rate with human observer.
ER  -


TY  - Preprint
T1  - Real-time Cardiovascular MR with Spatio-temporal Artifact Suppression using Deep Learning - Proof of Concept in Congenital Heart Disease
A1  - Andreas Hauptmann
A1  - Simon Arridge
A1  - Felix Lucka
A1  - Vivek Muthurangu
A1  - Jennifer A. Steeden
JO  - ArXiv e-prints
Y1  - 14 June, 2018
UR  - https://arxiv.org/abs/1803.05192
N2  - PURPOSE: Real-time assessment of ventricular volumes requires high acceleration factors. Residual convolutional neural networks (CNN) have shown potential for removing artifacts caused by data undersampling. In this study we investigated the effect of different radial sampling patterns on the accuracy of a CNN. We also acquired actual real-time undersampled radial data in patients with congenital heart disease (CHD), and compare CNN reconstruction to Compressed Sensing (CS).
ER  -


TY  - Preprint
T1  - Predicting Human Performance in Vertical Menu Selection Using Deep Learning
A1  - Yang Li
A1  - Samy Bengio
A1  - Gilles Bailly
JO  - ArXiv e-prints
Y1  - 13 March, 2018
UR  - https://arxiv.org/abs/1803.05073
N2  - Predicting human performance in interaction tasks allows designers or developers to understand the expected performance of a target interface without actually testing it with real users. In this work, we present a deep neural net to model and predict human performance in performing a sequence of UI tasks. In particular, we focus on a dominant class of tasks, i.e., target selection from a vertical list or menu. We experimented with our deep neural net using a public dataset collected from a desktop laboratory environment and a dataset collected from hundreds of touchscreen smartphone users via crowdsourcing. Our model significantly outperformed previous methods on these datasets. Importantly, our method, as a deep model, can easily incorporate additional UI attributes such as visual appearance and content semantics without changing model architectures. By understanding about how a deep learning model learns from human behaviors, our approach can be seen as a vehicle to discover new patterns about human behaviors to advance analytical modeling.
ER  -


TY  - Preprint
T1  - A Survey on Deep Learning Toolkits and Libraries for Intelligent User Interfaces
A1  - Jan Zacharias
A1  - Michael Barz
A1  - Daniel Sonntag
JO  - ArXiv e-prints
Y1  - 14 March, 2018
UR  - https://arxiv.org/abs/1803.04818
N2  - This paper provides an overview of prominent deep learning toolkits and, in particular, reports on recent publications that contributed open source software for implementing tasks that are common in intelligent user interfaces (IUI). We provide a scientific reference for researchers and software engineers who plan to utilise deep learning techniques within their IUI research and development projects.
ER  -


TY  - Preprint
T1  - Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning
A1  - Nicolas Papernot
A1  - Patrick McDaniel
JO  - ArXiv e-prints
Y1  - 13 March, 2018
UR  - https://arxiv.org/abs/1803.04765
N2  - Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model&#39;s training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures.
ER  -


TY  - Preprint
T1  - Replication study: Development and validation of deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs
A1  - Mike Voets
A1  - Kajsa MÃ¸llersen
A1  - Lars Ailo Bongo
JO  - ArXiv e-prints
Y1  - 29 August, 2018
UR  - https://arxiv.org/abs/1803.04337
N2  - Replication studies are essential for validation of new methods, and are crucial to maintain the high standards of scientific publications, and to use the results in practice. We have attempted to replicate the main method in &#39;Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs&#39; published in JAMA 2016; 316(22). We re-implemented the method since the source code is not available, and we used publicly available data sets. The original study used non-public fundus images from EyePACS and three hospitals in India for training. We used a different EyePACS data set from Kaggle. The original study used the benchmark data set Messidor-2 to evaluate the algorithm&#39;s performance. We used the same data set. In the original study, ophthalmologists re-graded all images for diabetic retinopathy, macular edema, and image gradability. There was one diabetic retinopathy grade per image for our data sets, and we assessed image gradability ourselves. Hyper-parameter settings were not described in the original study. But some of these were later published. We were not able to replicate the original study. Our algorithm&#39;s area under the receiver operating curve (AUC) of 0.94 on the Kaggle EyePACS test set and 0.80 on Messidor-2 did not come close to the reported AUC of 0.99 in the original study. This may be caused by the use of a single grade per image, different data, or different not described hyper-parameter settings. This study shows the challenges of replicating deep learning, and the need for more replication studies to validate deep learning methods, especially for medical image analysis.
ER  -


TY  - Preprint
T1  - Deep Learning in Mobile and Wireless Networking: A Survey
A1  - Chaoyun Zhang
A1  - Paul Patras
A1  - Hamed Haddadi
JO  - ArXiv e-prints
Y1  - 17 September, 2018
UR  - https://arxiv.org/abs/1803.04311
N2  - The rapid uptake of mobile devices and the rising popularity of mobile applications and services pose unprecedented demands on mobile and wireless networking infrastructure. Upcoming 5G systems are evolving to support exploding mobile traffic volumes, agile management of network resource to maximize user experience, and extraction of fine-grained real-time analytics. Fulfilling these tasks is challenging, as mobile environments are increasingly complex, heterogeneous, and evolving. One potential solution is to resort to advanced machine learning techniques to help managing the rise in data volumes and algorithm-driven applications. The recent success of deep learning underpins new and powerful tools that tackle problems in this space.
ER  -


TY  - Preprint
T1  - Super-resolution of Sentinel-2 images: Learning a globally applicable deep neural network
A1  - Charis Lanaras
A1  - JosÃ© Bioucas-Dias
A1  - Silvano Galliani
A1  - Emmanuel Baltsavias
A1  - Konrad Schindler
JO  - ArXiv e-prints
Y1  - 1 October, 2018
UR  - https://arxiv.org/abs/1803.04271
N2  - The Sentinel-2 satellite mission delivers multi-spectral imagery with 13 spectral bands, acquired at three different spatial resolutions. The aim of this research is to super-resolve the lower-resolution (20 m and 60 m Ground Sampling Distance - GSD) bands to 10 m GSD, so as to obtain a complete data cube at the maximal sensor resolution. We employ a state-of-the-art convolutional neural network (CNN) to perform end-to-end upsampling, which is trained with data at lower resolution, i.e., from 40-&gt;20 m, respectively 360-&gt;60 m GSD. In this way, one has access to a virtually infinite amount of training data, by downsampling real Sentinel-2 images. We use data sampled globally over a wide range of geographical locations, to obtain a network that generalises across different climate zones and land-cover types, and can super-resolve arbitrary Sentinel-2 images without the need of retraining. In quantitative evaluations (at lower scale, where ground truth is available), our network, which we call DSen2, outperforms the best competing approach by almost 50% in RMSE, while better preserving the spectral characteristics. It also delivers visually convincing results at the full 10 m GSD. The code is available at https://github.com/lanha/DSen2
ER  -


TY  - Preprint
T1  - Leveraging Crowdsourcing Data For Deep Active Learning - An Application: Learning Intents in Alexa
A1  - Jie Yang
A1  - Thomas Drake
A1  - Andreas Damianou
A1  - Yoelle Maarek
JO  - ArXiv e-prints
Y1  - 12 March, 2018
UR  - https://arxiv.org/abs/1803.04223
N2  - This paper presents a generic Bayesian framework that enables any deep learning model to actively learn from targeted crowds. Our framework inherits from recent advances in Bayesian deep learning, and extends existing work by considering the targeted crowdsourcing approach, where multiple annotators with unknown expertise contribute an uncontrolled amount (often limited) of annotations. Our framework leverages the low-rank structure in annotations to learn individual annotator expertise, which then helps to infer the true labels from noisy and sparse annotations. It provides a unified Bayesian model to simultaneously infer the true labels and train the deep learning model in order to reach an optimal learning efficacy. Finally, our framework exploits the uncertainty of the deep learning model during prediction as well as the annotators&#39; estimated expertise to minimize the number of required annotations and annotators for optimally training the deep learning model.
ER  -


TY  - Preprint
T1  - Adversarial Malware Binaries: Evading Deep Learning for Malware Detection in Executables
A1  - Bojan Kolosnjaji
A1  - Ambra Demontis
A1  - Battista Biggio
A1  - Davide Maiorca
A1  - Giorgio Giacinto
A1  - Claudia Eckert
A1  - Fabio Roli
JO  - ArXiv e-prints
Y1  - 12 March, 2018
UR  - https://arxiv.org/abs/1803.04173
N2  - Machine-learning methods have already been exploited as useful tools for detecting malicious executable files. They leverage data retrieved from malware samples, such as header fields, instruction sequences, or even raw bytes, to learn models that discriminate between benign and malicious software. However, it has also been shown that machine learning and deep neural networks can be fooled by evasion attacks (also referred to as adversarial examples), i.e., small changes to the input data that cause misclassification at test time. In this work, we investigate the vulnerability of malware detection methods that use deep networks to learn from raw bytes. We propose a gradient-based attack that is capable of evading a recently-proposed deep network suited to this purpose by only changing few specific bytes at the end of each malware sample, while preserving its intrusive functionality. Promising results show that our adversarial malware binaries evade the targeted network with high probability, even though less than 1% of their bytes are modified.
ER  -


TY  - Preprint
T1  - A Deep Learning Based Behavioral Approach to Indoor Autonomous Navigation
A1  - Gabriel Sepulveda
A1  - Juan Carlos Niebles
A1  - Alvaro Soto
JO  - ArXiv e-prints
Y1  - 12 March, 2018
UR  - https://arxiv.org/abs/1803.04119
N2  - We present a semantically rich graph representation for indoor robotic navigation. Our graph representation encodes: semantic locations such as offices or corridors as nodes, and navigational behaviors such as enter office or cross a corridor as edges. In particular, our navigational behaviors operate directly from visual inputs to produce motor controls and are implemented with deep learning architectures. This enables the robot to avoid explicit computation of its precise location or the geometry of the environment, and enables navigation at a higher level of semantic abstraction. We evaluate the effectiveness of our representation by simulating navigation tasks in a large number of virtual environments. Our results show that using a simple sets of perceptual and navigational behaviors, the proposed approach can successfully guide the way of the robot as it completes navigational missions such as going to a specific office. Furthermore, our implementation shows to be effective to control the selection and switching of behaviors.
ER  -


TY  - Preprint
T1  - Pseudo-task Augmentation: From Deep Multitask Learning to Intratask Sharing---and Back
A1  - Elliot Meyerson
A1  - Risto Miikkulainen
JO  - ArXiv e-prints
Y1  - 11 June, 2018
UR  - https://arxiv.org/abs/1803.04062
N2  - Deep multitask learning boosts performance by sharing learned structure across related tasks. This paper adapts ideas from deep multitask learning to the setting where only a single task is available. The method is formalized as pseudo-task augmentation, in which models are trained with multiple decoders for each task. Pseudo-tasks simulate the effect of training towards closely-related tasks drawn from the same universe. In a suite of experiments, pseudo-task augmentation is shown to improve performance on single-task learning problems. When combined with multitask learning, further improvements are achieved, including state-of-the-art performance on the CelebA dataset, showing that pseudo-task augmentation and multitask learning have complementary value. All in all, pseudo-task augmentation is a broadly applicable and efficient way to boost performance in deep learning systems.
ER  -


TY  - Preprint
T1  - Deep Dictionary Learning: A PARametric NETwork Approach
A1  - Shahin Mahdizadehaghdam
A1  - Ashkan Panahi
A1  - Hamid Krim
A1  - Liyi Dai
JO  - ArXiv e-prints
Y1  - 11 March, 2018
UR  - https://arxiv.org/abs/1803.04022
N2  - Deep dictionary learning seeks multiple dictionaries at different image scales to capture complementary coherent characteristics. We propose a method for learning a hierarchy of synthesis dictionaries with an image classification goal. The dictionaries and classification parameters are trained by a classification objective, and the sparse features are extracted by reducing a reconstruction loss in each layer. The reconstruction objectives in some sense regularize the classification problem and inject source signal information in the extracted features. The performance of the proposed hierarchical method increases by adding more layers, which consequently makes this model easier to tune and adapt. The proposed algorithm furthermore, shows remarkably lower fooling rate in presence of adversarial perturbation. The validation of the proposed approach is based on its classification performance using four benchmark datasets and is compared to a CNN of similar size.
ER  -


TY  - Preprint
T1  - Deep reinforcement learning for time series: playing idealized trading games
A1  - Xiang Gao
JO  - ArXiv e-prints
Y1  - 11 March, 2018
UR  - https://arxiv.org/abs/1803.03916
N2  - Deep Q-learning is investigated as an end-to-end solution to estimate the optimal strategies for acting on time series input. Experiments are conducted on two idealized trading games. 1) Univariate: the only input is a wave-like price time series, and 2) Bivariate: the input includes a random stepwise price time series and a noisy signal time series, which is positively correlated with future price changes. The Univariate game tests whether the agent can capture the underlying dynamics, and the Bivariate game tests whether the agent can utilize the hidden relation among the inputs. Stacked Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM) units, Convolutional Neural Network (CNN), and multi-layer perceptron (MLP) are used to model Q values. For both games, all agents successfully find a profitable strategy. The GRU-based agents show best overall performance in the Univariate game, while the MLP-based agents outperform others in the Bivariate game.
ER  -


TY  - Preprint
T1  - Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction
A1  - Huangying Zhan
A1  - Ravi Garg
A1  - Chamara Saroj Weerasekera
A1  - Kejie Li
A1  - Harsh Agarwal
A1  - Ian Reid
JO  - ArXiv e-prints
Y1  - 4 April, 2018
UR  - https://arxiv.org/abs/1803.03893
N2  - Despite learning based methods showing promising results in single view depth estimation and visual odometry, most existing approaches treat the tasks in a supervised manner. Recent approaches to single view depth estimation explore the possibility of learning without full supervision via minimizing photometric error. In this paper, we explore the use of stereo sequences for learning depth and visual odometry. The use of stereo sequences enables the use of both spatial (between left-right pairs) and temporal (forward backward) photometric warp error, and constrains the scene depth and camera motion to be in a common, real-world scale. At test time our framework is able to estimate single view depth and two-view odometry from a monocular sequence. We also show how we can improve on a standard photometric warp loss by considering a warp of deep features. We show through extensive experiments that: (i) jointly training for single view depth and visual odometry improves depth prediction because of the additional constraint imposed on depths and achieves competitive results for visual odometry; (ii) deep feature-based warping loss improves upon simple photometric warp loss for both single view depth estimation and visual odometry. Our method outperforms existing learning based methods on the KITTI driving dataset in both tasks. The source code is available at https://github.com/Huangying-Zhan/Depth-VO-Feat
ER  -


TY  - Preprint
T1  - A Deep Learning Approach for Pose Estimation from Volumetric OCT Data
A1  - Nils Gessert
A1  - Matthias SchlÃ¼ter
A1  - Alexander Schlaefer
JO  - ArXiv e-prints
Y1  - 10 March, 2018
UR  - https://arxiv.org/abs/1803.03852
N2  - Tracking the pose of instruments is a central problem in image-guided surgery. For microscopic scenarios, optical coherence tomography (OCT) is increasingly used as an imaging modality. OCT is suitable for accurate pose estimation due to its micrometer range resolution and volumetric field of view. However, OCT image processing is challenging due to speckle noise and reflection artifacts in addition to the images&#39; 3D nature. We address pose estimation from OCT volume data with a new deep learning-based tracking framework. For this purpose, we design a new 3D convolutional neural network (CNN) architecture to directly predict the 6D pose of a small marker geometry from OCT volumes. We use a hexapod robot to automatically acquire labeled data points which we use to train 3D CNN architectures for multi-output regression. We use this setup to provide an in-depth analysis on deep learning-based pose estimation from volumes. Specifically, we demonstrate that exploiting volume information for pose estimation yields higher accuracy than relying on 2D representations with depth information. Supporting this observation, we provide quantitative and qualitative results that 3D CNNs effectively exploit the depth structure of marker objects. Regarding the deep learning aspect, we present efficient design principles for 3D CNNs, making use of insights from the 2D deep learning community. In particular, we present Inception3D as a new architecture which performs best for our application. We show that our deep learning approach reaches errors at our ground-truth label&#39;s resolution. We achieve a mean average error of $\SI{14.89 \pm 9.3}{\micro\metre}$ and $\SI{0.096 \pm 0.072}{\degree}$ for position and orientation learning, respectively.
ER  -


TY  - Preprint
T1  - Kickstarting Deep Reinforcement Learning
A1  - Simon Schmitt
A1  - Jonathan J. Hudson
A1  - Augustin Zidek
A1  - Simon Osindero
A1  - Carl Doersch
A1  - Wojciech M. Czarnecki
A1  - Joel Z. Leibo
A1  - Heinrich Kuttler
A1  - Andrew Zisserman
A1  - Karen Simonyan
A1  - S. M. Ali Eslami
JO  - ArXiv e-prints
Y1  - 10 March, 2018
UR  - https://arxiv.org/abs/1803.03835
N2  - We present a method for using previously-trained &#39;teacher&#39; agents to kickstart the training of a new &#39;student&#39; agent. To this end, we leverage ideas from policy distillation and population based training. Our method places no constraints on the architecture of the teacher or student agents, and it regulates itself to allow the students to surpass their teachers in performance. We show that, on a challenging and computationally-intensive multi-task benchmark (DMLab-30), kickstarted training improves the data efficiency of new agents, making it significantly easier to iterate on their design. We also show that the same kickstarting pipeline can allow a single student agent to leverage multiple &#39;expert&#39; teachers which specialize on individual tasks. In this setting kickstarting yields surprisingly large gains, with the kickstarted agent matching the performance of an agent trained from scratch in almost 10x fewer steps, and surpassing its final performance by 42 percent. Kickstarting is conceptually simple and can easily be incorporated into reinforcement learning experiments.
ER  -


TY  - Preprint
T1  - IcoRating: A Deep-Learning System for Scam ICO Identification
A1  - Shuqing Bian
A1  - Zhenpeng Deng
A1  - Fei Li
A1  - Will Monroe
A1  - Peng Shi
A1  - Zijun Sun
A1  - Wei Wu
A1  - Sikuang Wang
A1  - William Yang Wang
A1  - Arianna Yuan
A1  - Tianwei Zhang
A1  - Jiwei Li
JO  - ArXiv e-prints
Y1  - 8 March, 2018
UR  - https://arxiv.org/abs/1803.03670
N2  - Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP, NEO) have been rapidly gaining ground in use, value, and understanding among the public, bringing astonishing profits to investors. Unlike other money and banking systems, most digital tokens do not require central authorities. Being decentralized poses significant challenges for credit rating. Most ICOs are currently not subject to government regulations, which makes a reliable credit rating system for ICO projects necessary and urgent.
ER  -


TY  - Preprint
T1  - Deep Auxiliary Learning for Visual Localization and Odometry
A1  - Abhinav Valada
A1  - Noha Radwan
A1  - Wolfram Burgard
JO  - ArXiv e-prints
Y1  - 9 March, 2018
UR  - https://arxiv.org/abs/1803.03642
N2  - Localization is an indispensable component of a robot&#39;s autonomy stack that enables it to determine where it is in the environment, essentially making it a precursor for any action execution or planning. Although convolutional neural networks have shown promising results for visual localization, they are still grossly outperformed by state-of-the-art local feature-based techniques. In this work, we propose VLocNet, a new convolutional neural network architecture for 6-DoF global pose regression and odometry estimation from consecutive monocular images. Our multitask model incorporates hard parameter sharing, thus being compact and enabling real-time inference, in addition to being end-to-end trainable. We propose a novel loss function that utilizes auxiliary learning to leverage relative pose information during training, thereby constraining the search space to obtain consistent pose estimates. We evaluate our proposed VLocNet on indoor as well as outdoor datasets and show that even our single task model exceeds the performance of state-of-the-art deep architectures for global localization, while achieving competitive performance for visual odometry estimation. Furthermore, we present extensive experimental evaluations utilizing our proposed Geometric Consistency Loss that show the effectiveness of multitask learning and demonstrate that our model is the first deep learning technique to be on par with, and in some cases outperforms state-of-the-art SIFT-based approaches.
ER  -


TY  - Preprint
T1  - Construction of neural networks for realization of localized deep learning
A1  - Charles K. Chui
A1  - Shao-Bo Lin
A1  - Ding-Xuan Zhou
JO  - ArXiv e-prints
Y1  - 9 March, 2018
UR  - https://arxiv.org/abs/1803.03503
N2  - The subject of deep learning has recently attracted users of machine learning from various disciplines, including: medical diagnosis and bioinformatics, financial market analysis and online advertisement, speech and handwriting recognition, computer vision and natural language processing, time series forecasting, and search engines. However, theoretical development of deep learning is still at its infancy. The objective of this paper is to introduce a deep neural network (also called deep-net) approach to localized manifold learning, with each hidden layer endowed with a specific learning task. For the purpose of illustrations, we only focus on deep-nets with three hidden layers, with the first layer for dimensionality reduction, the second layer for bias reduction, and the third layer for variance reduction. A feedback component also designed to eliminate outliers. The main theoretical result in this paper is the order $\mathcal O\left(m^{-2s/(2s+d)}\right)$ of approximation of the regression function with regularity $s$, in terms of the number $m$ of sample points, where the (unknown) manifold dimension $d$ replaces the dimension $D$ of the sampling (Euclidean) space for shallow nets.
ER  -


TY  - Preprint
T1  - Deep Visuo-Tactile Learning: Estimation of Tactile Properties from Images
A1  - Kuniyuki Takahashi
A1  - Jethro Tan
JO  - ArXiv e-prints
Y1  - 3 October, 2018
UR  - https://arxiv.org/abs/1803.03435
N2  - Estimation of tactile properties from vision, such as slipperiness or roughness, is important to effectively interact with the environment. These tactile properties help us decide which actions we should choose and how to perform them. E.g., we can drive slower if we see that we have bad traction or grasp tighter if an item looks slippery. We believe that this ability also helps robots to enhance their understanding of the environment, and thus enables them to tailor their actions to the situation at hand. We therefore propose a model to estimate the degree of tactile properties from visual perception alone (e.g., the level of slipperiness or roughness). Our method extends a encoder-decoder network, in which the latent variables are visual and tactile features. In contrast to previous works, our method does not require manual labeling, but only RGB images and the corresponding tactile sensor data. All our data is collected with a webcam and uSkin tactile sensor mounted on the end-effector of a Sawyer robot, which strokes the surfaces of 25 different materials. We show that our model generalizes to materials not included in the training data by evaluating the feature space, indicating that it has learned to associate important tactile properties with images.
ER  -


TY  - Preprint
T1  - Learning Deep Generative Models of Graphs
A1  - Yujia Li
A1  - Oriol Vinyals
A1  - Chris Dyer
A1  - Razvan Pascanu
A1  - Peter Battaglia
JO  - ArXiv e-prints
Y1  - 8 March, 2018
UR  - https://arxiv.org/abs/1803.03324
N2  - Graphs are fundamental data structures which concisely capture the relational structure in many important real-world domains, such as knowledge graphs, physical and social interactions, language, and chemistry. Here we introduce a powerful new approach for learning generative models over graphs, which can capture both their structure and attributes. Our approach uses graph neural networks to express probabilistic dependencies among a graph&#39;s nodes and edges, and can, in principle, learn distributions over any arbitrary graph. In a series of experiments our results show that once trained, our models can generate good quality samples of both synthetic graphs as well as real molecular graphs, both unconditionally and conditioned on data. Compared to baselines that do not use graph-structured representations, our models often perform far better. We also explore key challenges of learning generative models of graphs, such as how to handle symmetries and ordering of elements during the graph generation process, and offer possible solutions. Our work is the first and most general approach for learning generative models over arbitrary graphs, and opens new directions for moving away from restrictions of vector- and sequence-like knowledge representations, toward more expressive and flexible relational data structures.
ER  -


TY  - Preprint
T1  - TicTac: Accelerating Distributed Deep Learning with Communication Scheduling
A1  - Sayed Hadi Hashemi
A1  - Sangeetha Abdu Jyothi
A1  - Roy H. Campbell
JO  - ArXiv e-prints
Y1  - 3 October, 2018
UR  - https://arxiv.org/abs/1803.03288
N2  - State-of-the-art deep learning systems rely on iterative distributed training to tackle the increasing complexity of models and input data. The iteration time in these communication-heavy systems depends on the computation time, communication time and the extent of overlap of computation and communication.
ER  -


TY  - Preprint
T1  - GONet: A Semi-Supervised Deep Learning Approach For Traversability Estimation
A1  - Noriaki Hirose
A1  - Amir Sadeghian
A1  - Marynel VÃ¡zquez
A1  - Patrick Goebel
A1  - Silvio Savarese
JO  - ArXiv e-prints
Y1  - 8 March, 2018
UR  - https://arxiv.org/abs/1803.03254
N2  - We present semi-supervised deep learning approaches for traversability estimation from fisheye images. Our method, GONet, and the proposed extensions leverage Generative Adversarial Networks (GANs) to effectively predict whether the area seen in the input image(s) is safe for a robot to traverse. These methods are trained with many positive images of traversable places, but just a small set of negative images depicting blocked and unsafe areas. This makes the proposed methods practical. Positive examples can be collected easily by simply operating a robot through traversable spaces, while obtaining negative examples is time consuming, costly, and potentially dangerous. Through extensive experiments and several demonstrations, we show that the proposed traversability estimation approaches are robust and can generalize to unseen scenarios. Further, we demonstrate that our methods are memory efficient and fast, allowing for real-time operation on a mobile robot with single or stereo fisheye cameras. As part of our contributions, we open-source two new datasets for traversability estimation. These datasets are composed of approximately 24h of videos from more than 25 indoor environments. Our methods outperform baseline approaches for traversability estimation on these new datasets.
ER  -


TY  - Preprint
T1  - Generating Artificial Data for Private Deep Learning
A1  - Aleksei Triastcyn
A1  - Boi Faltings
JO  - ArXiv e-prints
Y1  - 7 June, 2018
UR  - https://arxiv.org/abs/1803.03148
N2  - In this paper, we propose generating artificial data that retain statistical properties of real data as the means of providing privacy with respect to the original dataset. We use generative adversarial network to draw privacy-preserving artificial data samples and derive an empirical method to assess the risk of information disclosure in a differential-privacy-like way. Our experiments show that we are able to generate artificial data of high quality and successfully train and validate machine learning models on this data while limiting potential privacy loss.
ER  -


TY  - Preprint
T1  - Learning Effective Binary Visual Representations with Deep Networks
A1  - Jianxin Wu
A1  - Jian-Hao Luo
JO  - ArXiv e-prints
Y1  - 8 March, 2018
UR  - https://arxiv.org/abs/1803.03004
N2  - Although traditionally binary visual representations are mainly designed to reduce computational and storage costs in the image retrieval research, this paper argues that binary visual representations can be applied to large scale recognition and detection problems in addition to hashing in retrieval. Furthermore, the binary nature may make it generalize better than its real-valued counterparts. Existing binary hashing methods are either two-stage or hinging on loss term regularization or saturated functions, hence converge slowly and only emit soft binary values. This paper proposes Approximately Binary Clamping (ABC), which is non-saturating, end-to-end trainable, with fast convergence and can output true binary visual representations. ABC achieves comparable accuracy in ImageNet classification as its real-valued counterpart, and even generalizes better in object detection. On benchmark image retrieval datasets, ABC also outperforms existing hashing methods.
ER  -


TY  - Preprint
T1  - DeepCAS: A Deep Reinforcement Learning Algorithm for Control-Aware Scheduling
A1  - Burak Demirel
A1  - Arunselvan Ramaswamy
A1  - Daniel E. Quevedo
A1  - Holger Karl
JO  - ArXiv e-prints
Y1  - 13 June, 2018
UR  - https://arxiv.org/abs/1803.02998
N2  - We consider networked control systems consisting of multiple independent controlled subsystems, operating over a shared communication network. Such systems are ubiquitous in cyber-physical systems, Internet of Things, and large-scale industrial systems. In many large-scale settings, the size of the communication network is smaller than the size of the system. In consequence, scheduling issues arise. The main contribution of this paper is to develop a deep reinforcement learning-based \emph{control-aware} scheduling (\textsc{DeepCAS}) algorithm to tackle these issues. We use the following (optimal) design strategy: First, we synthesize an optimal controller for each subsystem; next, we design a learning algorithm that adapts to the chosen subsystems (plants) and controllers. As a consequence of this adaptation, our algorithm finds a schedule that minimizes the \emph{control loss}. We present empirical results to show that \textsc{DeepCAS} finds schedules with better performance than periodic ones.
ER  -


TY  - Preprint
T1  - A Multi-Objective Deep Reinforcement Learning Framework
A1  - Thanh Thi Nguyen
JO  - ArXiv e-prints
Y1  - 27 June, 2018
UR  - https://arxiv.org/abs/1803.02965
N2  - This paper presents a new multi-objective deep reinforcement learning (MODRL) framework based on deep Q-networks. We propose the use of linear and non-linear methods to develop the MODRL framework that includes both single-policy and multi-policy strategies. The experimental results on two benchmark problems including the two-objective deep sea treasure environment and the three-objective mountain car problem indicate that the proposed framework is able to converge to the optimal Pareto solutions effectively. The proposed framework is generic, which allows implementation of different deep reinforcement learning algorithms in different complex environments. This therefore overcomes many difficulties involved with standard multi-objective reinforcement learning (MORL) methods existing in the current literature. The framework creates a platform as a testbed environment to develop methods for solving various problems associated with the current MORL. Details of the framework implementation can be referred to http://www.deakin.edu.au/~thanhthi/drl.htm.
ER  -


TY  - Preprint
T1  - The Advantage of Doubling: A Deep Reinforcement Learning Approach to Studying the Double Team in the NBA
A1  - Jiaxuan Wang
A1  - Ian Fox
A1  - Jonathan Skaza
A1  - Nick Linck
A1  - Satinder Singh
A1  - Jenna Wiens
JO  - ArXiv e-prints
Y1  - 7 March, 2018
UR  - https://arxiv.org/abs/1803.02940
N2  - During the 2017 NBA playoffs, Celtics coach Brad Stevens was faced with a difficult decision when defending against the Cavaliers: &#34;Do you double and risk giving up easy shots, or stay at home and do the best you can?&#34; It&#39;s a tough call, but finding a good defensive strategy that effectively incorporates doubling can make all the difference in the NBA. In this paper, we analyze double teaming in the NBA, quantifying the trade-off between risk and reward. Using player trajectory data pertaining to over 643,000 possessions, we identified when the ball handler was double teamed. Given these data and the corresponding outcome (i.e., was the defense successful), we used deep reinforcement learning to estimate the quality of the defensive actions. We present qualitative and quantitative results summarizing our learned defensive strategy for defending. We show that our policy value estimates are predictive of points per possession and win percentage. Overall, the proposed framework represents a step toward a more comprehensive understanding of defensive strategies in the NBA.
ER  -


TY  - Preprint
T1  - Accelerated Methods for Deep Reinforcement Learning
A1  - Adam Stooke
A1  - Pieter Abbeel
JO  - ArXiv e-prints
Y1  - 7 March, 2018
UR  - https://arxiv.org/abs/1803.02811
N2  - Deep reinforcement learning (RL) has achieved many recent successes, yet experiment turn-around time remains a key bottleneck in research and in practice. We investigate how to optimize existing deep RL algorithms for modern computers, specifically for a combination of CPUs and GPUs. We confirm that both policy gradient and Q-value learning algorithms can be adapted to learn using many parallel simulator instances. We further find it possible to train using batch sizes considerably larger than are standard, without negatively affecting sample complexity or final performance. We leverage these facts to build a unified framework for parallelization that dramatically hastens experiments in both classes of algorithm. All neural network computations use GPUs, accelerating both data collection and training. Our results include using an entire NVIDIA DGX-1 to learn successful strategies in Atari games in single-digit minutes, using both synchronous and asynchronous algorithms.
ER  -


TY  - Preprint
T1  - A Deep Learning Algorithm for One-step Contour Aware Nuclei Segmentation of Histopathological Images
A1  - Yuxin Cui
A1  - Guiying Zhang
A1  - Zhonghao Liu
A1  - Zheng Xiong
A1  - Jianjun Hu
JO  - ArXiv e-prints
Y1  - 7 March, 2018
UR  - https://arxiv.org/abs/1803.02786
N2  - This paper addresses the task of nuclei segmentation in high-resolution histopathological images. We propose an auto- matic end-to-end deep neural network algorithm for segmenta- tion of individual nuclei. A nucleus-boundary model is introduced to predict nuclei and their boundaries simultaneously using a fully convolutional neural network. Given a color normalized image, the model directly outputs an estimated nuclei map and a boundary map. A simple, fast and parameter-free post-processing procedure is performed on the estimated nuclei map to produce the final segmented nuclei. An overlapped patch extraction and assembling method is also designed for seamless prediction of nuclei in large whole-slide images. We also show the effectiveness of data augmentation methods for nuclei segmentation task. Our experiments showed our method outperforms prior state-of-the- art methods. Moreover, it is efficient that one 1000X1000 image can be segmented in less than 5 seconds. This makes it possible to precisely segment the whole-slide image in acceptable time
ER  -


TY  - Preprint
T1  - Extracting Action Sequences from Texts Based on Deep Reinforcement Learning
A1  - Wenfeng Feng
A1  - Hankz Hankui Zhuo
A1  - Subbarao Kambhampati
JO  - ArXiv e-prints
Y1  - 11 May, 2018
UR  - https://arxiv.org/abs/1803.02632
N2  - Extracting action sequences from natural language texts is challenging, as it requires commonsense inferences based on world knowledge. Although there has been work on extracting action scripts, instructions, navigation actions, etc., they require that either the set of candidate actions be provided in advance, or that action descriptions are restricted to a specific form, e.g., description templates. In this paper, we aim to extract action sequences from texts in free natural language, i.e., without any restricted templates, provided the candidate set of actions is unknown. We propose to extract action sequences from texts based on the deep reinforcement learning framework. Specifically, we view &#34;selecting&#34; or &#34;eliminating&#34; words from texts as &#34;actions&#34;, and the texts associated with actions as &#34;states&#34;. We then build Q-networks to learn the policy of extracting actions and extract plans from the labeled texts. We demonstrate the effectiveness of our approach on several datasets with comparison to state-of-the-art approaches, including online experiments interacting with humans.
ER  -


TY  - Preprint
T1  - Generating Goal-Directed Visuomotor Plans Based on Learning Using a Predictive Coding-type Deep Visuomotor Recurrent Neural Network Model
A1  - Minkyu Choi
A1  - Takazumi Matsumoto
A1  - Minju Jung
A1  - Jun Tani
JO  - ArXiv e-prints
Y1  - 5 June, 2018
UR  - https://arxiv.org/abs/1803.02578
N2  - The current paper presents how a predictive coding type deep recurrent neural networks can generate vision-based goal-directed plans based on prior learning experience by examining experiment results using a real arm robot. The proposed deep recurrent neural network learns to predict visuo-proprioceptive sequences by extracting an adequate predictive model from various visuomotor experiences related to object-directed behaviors. The predictive model was developed in terms of mapping from intention state space to expected visuo-proprioceptive sequences space through iterative learning. Our arm robot experiments adopted with three different tasks with different levels of difficulty showed that the error minimization principle in the predictive coding framework applied to inference of the optimal intention states for given goal states can generate goal-directed plans even for unlearned goal states with generalization. It was, however, shown that sufficient generalization requires relatively large number of learning trajectories. The paper discusses possible countermeasure to overcome this problem.
ER  -


TY  - Preprint
T1  - Exponential Discriminative Metric Embedding in Deep Learning
A1  - Bowen Wu
A1  - Zhangling Chen
A1  - Jun Wang
A1  - Huaming Wu
JO  - ArXiv e-prints
Y1  - 6 March, 2018
UR  - https://arxiv.org/abs/1803.02504
N2  - With the remarkable success achieved by the Convolutional Neural Networks (CNNs) in object recognition recently, deep learning is being widely used in the computer vision community. Deep Metric Learning (DML), integrating deep learning with conventional metric learning, has set new records in many fields, especially in classification task. In this paper, we propose a replicable DML method, called Include and Exclude (IE) loss, to force the distance between a sample and its designated class center away from the mean distance of this sample to other class centers with a large margin in the exponential feature projection space. With the supervision of IE loss, we can train CNNs to enhance the intra-class compactness and inter-class separability, leading to great improvements on several public datasets ranging from object recognition to face verification. We conduct a comparative study of our algorithm with several typical DML methods on three kinds of networks with different capacity. Extensive experiments on three object recognition datasets and two face recognition datasets demonstrate that IE loss is always superior to other mainstream DML methods and approach the state-of-the-art results.
ER  -


TY  - Preprint
T1  - Comparison of Deep Learning Approaches for Multi-Label Chest X-Ray Classification
A1  - Ivo M. Baltruschat
A1  - Hannes Nickisch
A1  - Michael Grass
A1  - Tobias Knopp
A1  - Axel Saalbach
JO  - ArXiv e-prints
Y1  - 6 March, 2018
UR  - https://arxiv.org/abs/1803.02315
N2  - The increased availability of X-ray image archives (e.g. the ChestX-ray14 dataset from the NIH Clinical Center) has triggered a growing interest in deep learning techniques. To provide better insight into the different approaches, and their applications to chest X-ray classification, we investigate a powerful network architecture in detail: the ResNet-50. Building on prior work in this domain, we consider transfer learning with and without fine-tuning as well as the training of a dedicated X-ray network from scratch. To leverage the high spatial resolutions of X-ray data, we also include an extended ResNet-50 architecture, and a network integrating non-image data (patient age, gender and acquisition type) in the classification process. In a systematic evaluation, using 5-fold re-sampling and a multi-label loss function, we evaluate the performance of the different approaches for pathology classification by ROC statistics and analyze differences between the classifiers using rank correlation. We observe a considerable spread in the achieved performance and conclude that the X-ray-specific ResNet-50, integrating non-image data yields the best overall results.
ER  -


TY  - Preprint
T1  - Deep Thermal Imaging: Proximate Material Type Recognition in the Wild through Deep Learning of Spatial Surface Temperature Patterns
A1  - Youngjun Cho
A1  - Nadia Bianchi-Berthouze
A1  - Nicolai Marquardt
A1  - Simon J. Julier
JO  - ArXiv e-prints
Y1  - 6 March, 2018
UR  - https://arxiv.org/abs/1803.02310
N2  - We introduce Deep Thermal Imaging, a new approach for close-range automatic recognition of materials to enhance the understanding of people and ubiquitous technologies of their proximal environment. Our approach uses a low-cost mobile thermal camera integrated into a smartphone to capture thermal textures. A deep neural network classifies these textures into material types. This approach works effectively without the need for ambient light sources or direct contact with materials. Furthermore, the use of a deep learning network removes the need to handcraft the set of features for different materials. We evaluated the performance of the system by training it to recognise 32 material types in both indoor and outdoor environments. Our approach produced recognition accuracies above 98% in 14,860 images of 15 indoor materials and above 89% in 26,584 images of 17 outdoor materials. We conclude by discussing its potentials for real-time use in HCI applications and future directions.
ER  -


TY  - Preprint
T1  - A Hybrid Method for Traffic Flow Forecasting Using Multimodal Deep Learning
A1  - Shengdong Du
A1  - Tianrui Li
A1  - Xun Gong
A1  - Zeng Yu
A1  - Shi-Jinn Horng
JO  - ArXiv e-prints
Y1  - 6 July, 2018
UR  - https://arxiv.org/abs/1803.02099
N2  - Traffic flow forecasting has been regarded as a key problem of intelligent transport systems. In this work, we propose a hybrid multimodal deep learning method for short-term traffic flow forecasting, which jointly learns the spatial-temporal correlation features and interdependence of multi-modality traffic data by multimodal deep learning architecture. According to the highly nonlinear characteristics of multi-modality traffic data, the base module of our method consists of one-dimensional Convolutional Neural Networks (CNN) and Gated Recurrent Units (GRU). The former is to capture the local trend features and the latter is to capture long temporal dependencies. Then, we design a hybrid multimodal deep learning framework (HMDLF) for fusing share representation features of different modality traffic data based on multiple CNN-GRU modules. The experiment results indicate that the proposed multimodal deep learning framework is capable of dealing with complex nonlinear urban traffic flow forecasting with satisfying accuracy and effectiveness.
ER  -


TY  - Preprint
T1  - Online Deep Learning: Growing RBM on the fly
A1  - Savitha Ramasamy
A1  - Kanagasabai Rajaraman
A1  - Pavitra Krishnaswamy
A1  - Vijay Chandrasekhar
JO  - ArXiv e-prints
Y1  - 6 March, 2018
UR  - https://arxiv.org/abs/1803.02043
N2  - We propose a novel online learning algorithm for Restricted Boltzmann Machines (RBM), namely, the Online Generative Discriminative Restricted Boltzmann Machine (OGD-RBM), that provides the ability to build and adapt the network architecture of RBM according to the statistics of streaming data. The OGD-RBM is trained in two phases: (1) an online generative phase for unsupervised feature representation at the hidden layer and (2) a discriminative phase for classification. The online generative training begins with zero neurons in the hidden layer, adds and updates the neurons to adapt to statistics of streaming data in a single pass unsupervised manner, resulting in a feature representation best suited to the data. The discriminative phase is based on stochastic gradient descent and associates the represented features to the class labels. We demonstrate the OGD-RBM on a set of multi-category and binary classification problems for data sets having varying degrees of class-imbalance. We first apply the OGD-RBM algorithm on the multi-class MNIST dataset to characterize the network evolution. We demonstrate that the online generative phase converges to a stable, concise network architecture, wherein individual neurons are inherently discriminative to the class labels despite unsupervised training. We then benchmark OGD-RBM performance to other machine learning, neural network and ClassRBM techniques for credit scoring applications using 3 public non-stationary two-class credit datasets with varying degrees of class-imbalance. We report that OGD-RBM improves accuracy by 2.5-3% over batch learning techniques while requiring at least 24%-70% fewer neurons and fewer training samples. This online generative training approach can be extended greedily to multiple layers for training Deep Belief Networks in non-stationary data mining applications without the need for a priori fixed architectures.
ER  -


TY  - Preprint
T1  - M3Fusion: A Deep Learning Architecture for Multi-{Scale/Modal/Temporal} satellite data fusion
A1  - P. Benedetti
A1  - D. Ienco
A1  - R. Gaetano
A1  - K. OsÃ©
A1  - R. Pensa
A1  - S. Dupuy
JO  - ArXiv e-prints
Y1  - 5 March, 2018
UR  - https://arxiv.org/abs/1803.01945
N2  - Modern Earth Observation systems provide sensing data at different temporal and spatial resolutions. Among optical sensors, today the Sentinel-2 program supplies high-resolution temporal (every 5 days) and high spatial resolution (10m) images that can be useful to monitor land cover dynamics. On the other hand, Very High Spatial Resolution images (VHSR) are still an essential tool to figure out land cover mapping characterized by fine spatial patterns. Understand how to efficiently leverage these complementary sources of information together to deal with land cover mapping is still challenging. With the aim to tackle land cover mapping through the fusion of multi-temporal High Spatial Resolution and Very High Spatial Resolution satellite images, we propose an End-to-End Deep Learning framework, named M3Fusion, able to leverage simultaneously the temporal knowledge contained in time series data as well as the fine spatial information available in VHSR information. Experiments carried out on the Reunion Island study area asses the quality of our proposal considering both quantitative and qualitative aspects.
ER  -


TY  - Preprint
T1  - Resampling Forgery Detection Using Deep Learning and A-Contrario Analysis
A1  - Arjuna Flenner
A1  - Lawrence Peterson
A1  - Jason Bunk
A1  - Tajuddin Manhar Mohammed
A1  - Lakshmanan Nataraj
A1  - B. S. Manjunath
JO  - ArXiv e-prints
Y1  - 1 March, 2018
UR  - https://arxiv.org/abs/1803.01711
N2  - The amount of digital imagery recorded has recently grown exponentially, and with the advancement of software, such as Photoshop or Gimp, it has become easier to manipulate images. However, most images on the internet have not been manipulated and any automated manipulation detection algorithm must carefully control the false alarm rate. In this paper we discuss a method to automatically detect local resampling using deep learning while controlling the false alarm rate using a-contrario analysis. The automated procedure consists of three primary steps. First, resampling features are calculated for image blocks. A deep learning classifier is then used to generate a heatmap that indicates if the image block has been resampled. We expect some of these blocks to be falsely identified as resampled. We use a-contrario hypothesis testing to both identify if the patterns of the manipulated blocks indicate if the image has been tampered with and to localize the manipulation. We demonstrate that this strategy is effective in indicating if an image has been manipulated and localizing the manipulations.
ER  -


TY  - Preprint
T1  - Data Curation with Deep Learning [Vision]: Towards Self Driving Data Curation
A1  - Saravanan Thirumuruganathan
A1  - Nan Tang
A1  - Mourad Ouzzani
JO  - ArXiv e-prints
Y1  - 4 March, 2018
UR  - https://arxiv.org/abs/1803.01384
N2  - Past. Data curation - the process of discovering, integrating, and cleaning data - is one of the oldest data management problems. Unfortunately, it is still the most time consuming and least enjoyable work of data scientists. So far, successful data curation stories are mainly ad-hoc solutions that are either domain-specific (for example, ETL rules) or task-specific (for example, entity resolution).
ER  -


TY  - Preprint
T1  - Training deep learning based denoisers without ground truth data
A1  - Shakarim Soltanayev
A1  - Se Young Chun
JO  - ArXiv e-prints
Y1  - 23 May, 2018
UR  - https://arxiv.org/abs/1803.01314
N2  - Recent deep learning based denoisers often outperform state-of-the-art conventional denoisers such as BM3D. They are typically trained to minimize the mean squared error (MSE) between the output of a deep neural network and the ground truth image. In deep learning based denoisers, it is important to use high quality noiseless ground truth for high performance, but it is often challenging or even infeasible to obtain such a clean image in application areas such as hyperspectral remote sensing and medical imaging. We propose a Stein&#39;s Unbiased Risk Estimator (SURE) based method for training deep neural network denoisers only with noisy images. We demonstrated that our SURE based method without ground truth was able to train deep neural network denoisers to yield performance close to deep learning denoisers trained with ground truth and to outperform state-of-the-art BM3D. Further improvements were achieved by including noisy test images for training denoiser networks using our proposed SURE based method.
ER  -


TY  - Preprint
T1  - An Optimal Control Approach to Deep Learning and Applications to Discrete-Weight Neural Networks
A1  - Qianxiao Li
A1  - Shuji Hao
JO  - ArXiv e-prints
Y1  - 2 June, 2018
UR  - https://arxiv.org/abs/1803.01299
N2  - Deep learning is formulated as a discrete-time optimal control problem. This allows one to characterize necessary conditions for optimality and develop training algorithms that do not rely on gradients with respect to the trainable parameters. In particular, we introduce the discrete-time method of successive approximations (MSA), which is based on the Pontryagin&#39;s maximum principle, for training neural networks. A rigorous error estimate for the discrete MSA is obtained, which sheds light on its dynamics and the means to stabilize the algorithm. The developed methods are applied to train, in a rather principled way, neural networks with weights that are constrained to take values in a discrete set. We obtain competitive performance and interestingly, very sparse weights in the case of ternary networks, which may be useful in model deployment in low-memory devices.
ER  -


TY  - Preprint
T1  - A Benchmark for Iris Location and a Deep Learning Detector Evaluation
A1  - Evair Severo
A1  - Rayson Laroca
A1  - Cides S. Bezerra
A1  - Luiz A. Zanlorensi
A1  - Daniel Weingaertner
A1  - Gladston Moreira
A1  - David Menotti
JO  - ArXiv e-prints
Y1  - 30 April, 2018
UR  - https://arxiv.org/abs/1803.01250
N2  - The iris is considered as the biometric trait with the highest unique probability. The iris location is an important task for biometrics systems, affecting directly the results obtained in specific applications such as iris recognition, spoofing and contact lenses detection, among others. This work defines the iris location problem as the delimitation of the smallest squared window that encompasses the iris region. In order to build a benchmark for iris location we annotate (iris squared bounding boxes) four databases from different biometric applications and make them publicly available to the community. Besides these 4 annotated databases, we include 2 others from the literature. We perform experiments on these six databases, five obtained with near infra-red sensors and one with visible light sensor. We compare the classical and outstanding Daugman iris location approach with two window based detectors: 1) a sliding window detector based on features from Histogram of Oriented Gradients (HOG) and a linear Support Vector Machines (SVM) classifier; 2) a deep learning based detector fine-tuned from YOLO object detector. Experimental results showed that the deep learning based detector outperforms the other ones in terms of accuracy and runtime (GPUs version) and should be chosen whenever possible.
ER  -


TY  - Preprint
T1  - Deep Bayesian Active Semi-Supervised Learning
A1  - Matthias Rottmann
A1  - Karsten Kahl
A1  - Hanno Gottschalk
JO  - ArXiv e-prints
Y1  - 3 March, 2018
UR  - https://arxiv.org/abs/1803.01216
N2  - In many applications the process of generating label information is expensive and time consuming. We present a new method that combines active and semi-supervised deep learning to achieve high generalization performance from a deep convolutional neural network with as few known labels as possible. In a setting where a small amount of labeled data as well as a large amount of unlabeled data is available, our method first learns the labeled data set. This initialization is followed by an expectation maximization algorithm, where further training reduces classification entropy on the unlabeled data by targeting a low entropy fit which is consistent with the labeled data. In addition the algorithm asks at a specified frequency an oracle for labels of data with entropy above a certain entropy quantile. Using this active learning component we obtain an agile labeling process that achieves high accuracy, but requires only a small amount of known labels. For the MNIST dataset we report an error rate of 2.06% using only 300 labels and 1.06% for 1000 labels. These results are obtained without employing any special network architecture or data augmentation.
ER  -


TY  - Preprint
T1  - Automatic Instrument Segmentation in Robot-Assisted Surgery Using Deep Learning
A1  - Alexey Shvets
A1  - Alexander Rakhlin
A1  - Alexandr A. Kalinin
A1  - Vladimir Iglovikov
JO  - ArXiv e-prints
Y1  - 19 June, 2018
UR  - https://arxiv.org/abs/1803.01207
N2  - Semantic segmentation of robotic instruments is an important problem for the robot-assisted surgery. One of the main challenges is to correctly detect an instrument&#39;s position for the tracking and pose estimation in the vicinity of surgical scenes. Accurate pixel-wise instrument segmentation is needed to address this challenge. In this paper we describe our winning solution for MICCAI 2017 Endoscopic Vision SubChallenge: Robotic Instrument Segmentation. Our approach demonstrates an improvement over the state-of-the-art results using several novel deep neural network architectures. It addressed the binary segmentation problem, where every pixel in an image is labeled as an instrument or background from the surgery video feed. In addition, we solve a multi-class segmentation problem, where we distinguish different instruments or different parts of an instrument from the background. In this setting, our approach outperforms other methods in every task subcategory for automatic instrument segmentation thereby providing state-of-the-art solution for this problem. The source code for our solution is made publicly available at https://github.com/ternaus/robot-surgery-segmentation
ER  -


TY  - Preprint
T1  - Chest X-Ray Analysis of Tuberculosis by Deep Learning with Segmentation and Augmentation
A1  - Sergii Stirenko
A1  - Yuriy Kochura
A1  - Oleg Alienin
A1  - Oleksandr Rokovyi
A1  - Peng Gang
A1  - Wei Zeng
A1  - Yuri Gordienko
JO  - ArXiv e-prints
Y1  - 3 March, 2018
UR  - https://arxiv.org/abs/1803.01199
N2  - The results of chest X-ray (CXR) analysis of 2D images to get the statistically reliable predictions (availability of tuberculosis) by computer-aided diagnosis (CADx) on the basis of deep learning are presented. They demonstrate the efficiency of lung segmentation, lossless and lossy data augmentation for CADx of tuberculosis by deep convolutional neural network (CNN) applied to the small and not well-balanced dataset even. CNN demonstrates ability to train (despite overfitting) on the pre-processed dataset obtained after lung segmentation in contrast to the original not-segmented dataset. Lossless data augmentation of the segmented dataset leads to the lowest validation loss (without overfitting) and nearly the same accuracy (within the limits of standard deviation) in comparison to the original and other pre-processed datasets after lossy data augmentation. The additional limited lossy data augmentation results in the lower validation loss, but with a decrease of the validation accuracy. In conclusion, besides the more complex deep CNNs and bigger datasets, the better progress of CADx for the small and not well-balanced datasets even could be obtained by better segmentation, data augmentation, dataset stratification, and exclusion of non-evident outliers.
ER  -


TY  - Preprint
T1  - The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches
A1  - Md Zahangir Alom
A1  - Tarek M. Taha
A1  - Christopher Yakopcic
A1  - Stefan Westberg
A1  - Paheding Sidike
A1  - Mst Shamima Nasrin
A1  - Brian C Van Esesn
A1  - Abdul A S. Awwal
A1  - Vijayan K. Asari
JO  - ArXiv e-prints
Y1  - 12 September, 2018
UR  - https://arxiv.org/abs/1803.01164
N2  - Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].
ER  -


TY  - Preprint
T1  - Real-Time Deep Learning Method for Abandoned Luggage Detection in Video
A1  - Sorina Smeureanu
A1  - Radu Tudor Ionescu
JO  - ArXiv e-prints
Y1  - 15 June, 2018
UR  - https://arxiv.org/abs/1803.01160
N2  - Recent terrorist attacks in major cities around the world have brought many casualties among innocent citizens. One potential threat is represented by abandoned luggage items (that could contain bombs or biological warfare) in public areas. In this paper, we describe an approach for real-time automatic detection of abandoned luggage in video captured by surveillance cameras. The approach is comprised of two stages: (i) static object detection based on background subtraction and motion estimation and (ii) abandoned luggage recognition based on a cascade of convolutional neural networks (CNN). To train our neural networks we provide two types of examples: images collected from the Internet and realistic examples generated by imposing various suitcases and bags over the scene&#39;s background. We present empirical results demonstrating that our approach yields better performance than a strong CNN baseline method.
ER  -


TY  - Preprint
T1  - Model-Free Control for Distributed Stream Data Processing using Deep Reinforcement Learning
A1  - Teng Li
A1  - Zhiyuan Xu
A1  - Jian Tang
A1  - Yanzhi Wang
JO  - ArXiv e-prints
Y1  - 2 March, 2018
UR  - https://arxiv.org/abs/1803.01016
N2  - In this paper, we focus on general-purpose Distributed Stream Data Processing Systems (DSDPSs), which deal with processing of unbounded streams of continuous data at scale distributedly in real or near-real time. A fundamental problem in a DSDPS is the scheduling problem with the objective of minimizing average end-to-end tuple processing time. A widely-used solution is to distribute workload evenly over machines in the cluster in a round-robin manner, which is obviously not efficient due to lack of consideration for communication delay. Model-based approaches do not work well either due to the high complexity of the system environment. We aim to develop a novel model-free approach that can learn to well control a DSDPS from its experience rather than accurate and mathematically solvable system models, just as a human learns a skill (such as cooking, driving, swimming, etc). Specifically, we, for the first time, propose to leverage emerging Deep Reinforcement Learning (DRL) for enabling model-free control in DSDPSs; and present design, implementation and evaluation of a novel and highly effective DRL-based control framework, which minimizes average end-to-end tuple processing time by jointly learning the system environment via collecting very limited runtime statistics data and making decisions under the guidance of powerful Deep Neural Networks. To validate and evaluate the proposed framework, we implemented it based on a widely-used DSDPS, Apache Storm, and tested it with three representative applications. Extensive experimental results show 1) Compared to Storm&#39;s default scheduler and the state-of-the-art model-based method, the proposed framework reduces average tuple processing by 33.5% and 14.0% respectively on average. 2) The proposed framework can quickly reach a good scheduling solution during online learning, which justifies its practicability for online control in DSDPSs.
ER  -


TY  - Preprint
T1  - Not All Samples Are Created Equal: Deep Learning with Importance Sampling
A1  - Angelos Katharopoulos
A1  - FranÃ§ois Fleuret
JO  - ArXiv e-prints
Y1  - 9 June, 2018
UR  - https://arxiv.org/abs/1803.00942
N2  - Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on &#34;informative&#34; examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5% and 17%.
ER  -


TY  - Preprint
T1  - Deep Learning for Signal Authentication and Security in Massive Internet of Things Systems
A1  - Aidin Ferdowsi
A1  - Walid Saad
JO  - ArXiv e-prints
Y1  - 28 February, 2018
UR  - https://arxiv.org/abs/1803.00916
N2  - Secure signal authentication is arguably one of the most challenging problems in the Internet of Things (IoT) environment, due to the large-scale nature of the system and its susceptibility to man-in-the-middle and eavesdropping attacks. In this paper, a novel deep learning method is proposed for dynamic authentication of IoT signals to detect cyber attacks. The proposed learning framework, based on a long short-term memory (LSTM) structure, enables the IoT devices (IoTDs) to extract a set of stochastic features from their generated signal and dynamically watermark these features into the signal. This method enables the cloud, which collects signals from the IoT devices, to effectively authenticate the reliability of the signals. Moreover, in massive IoT scenarios, since the cloud cannot authenticate all the IoTDs simultaneously due to computational limitations, a game-theoretic framework is proposed to improve the cloud&#39;s decision making process by predicting vulnerable IoTDs. The mixed-strategy Nash equilibrium (MSNE) for this game is derived and the uniqueness of the expected utility at the equilibrium is proven. In the massive IoT system, due to a large set of available actions for the cloud, it is shown that analytically deriving the MSNE is challenging and, thus, a learning algorithm proposed that converges to the MSNE. Moreover, in order to cope with the incomplete information case in which the cloud cannot access the state of the unauthenticated IoTDs, a deep reinforcement learning algorithm is proposed to dynamically predict the state of unauthenticated IoTDs and allow the cloud to decide on which IoTDs to authenticate. Simulation results show that, with an attack detection delay of under 1 second the messages can be transmitted from IoT devices with an almost 100% reliability.
ER  -


TY  - Preprint
T1  - Unravelling Robustness of Deep Learning based Face Recognition Against Adversarial Attacks
A1  - Gaurav Goswami
A1  - Nalini Ratha
A1  - Akshay Agarwal
A1  - Richa Singh
A1  - Mayank Vatsa
JO  - ArXiv e-prints
Y1  - 22 February, 2018
UR  - https://arxiv.org/abs/1803.00401
N2  - Deep neural network (DNN) architecture based models have high expressive power and learning capacity. However, they are essentially a black box method since it is not easy to mathematically formulate the functions that are learned within its many layers of representation. Realizing this, many researchers have started to design methods to exploit the drawbacks of deep learning based algorithms questioning their robustness and exposing their singularities. In this paper, we attempt to unravel three aspects related to the robustness of DNNs for face recognition: (i) assessing the impact of deep architectures for face recognition in terms of vulnerabilities to attacks inspired by commonly observed distortions in the real world that are well handled by shallow learning methods along with learning based adversaries; (ii) detecting the singularities by characterizing abnormal filter response behavior in the hidden layers of deep networks; and (iii) making corrections to the processing pipeline to alleviate the problem. Our experimental evaluation using multiple open-source DNN-based face recognition networks, including OpenFace and VGG-Face, and two publicly available databases (MEDS and PaSC) demonstrates that the performance of deep learning based face recognition algorithms can suffer greatly in the presence of such distortions. The proposed method is also compared with existing detection algorithms and the results show that it is able to detect the attacks with very high accuracy by suitably designing a classifier using the response of the hidden layers in the network. Finally, we present several effective countermeasures to mitigate the impact of adversarial attacks and improve the overall robustness of DNN-based face recognition.
ER  -


TY  - Preprint
T1  - Detecting Volcano Deformation in InSAR using Deep learning
A1  - N. Anantrasirichai
A1  - F. Albino
A1  - P. Hill
A1  - D. Bull
A1  - J. Biggs
JO  - ArXiv e-prints
Y1  - 21 January, 2018
UR  - https://arxiv.org/abs/1803.00380
N2  - Globally 800 million people live within 100 km of a volcano and currently 1500 volcanoes are considered active, but half of these have no ground-based monitoring. Alternatively, satellite radar (InSAR) can be employed to observe volcanic ground deformation, which has shown a significant statistical link to eruptions. Modern satellites provide large coverage with high resolution signals, leading to huge amounts of data. The explosion in data has brought major challenges associated with timely dissemination of information and distinguishing volcano deformation patterns from noise, which currently relies on manual inspection. Moreover, volcano observatories still lack expertise to exploit satellite datasets, particularly in developing countries. This paper presents a novel approach to detect volcanic ground deformation automatically from wrapped-phase InSAR images. Convolutional neural networks (CNN) are employed to detect unusual patterns within the radar data.
ER  -


TY  - Preprint
T1  - A Deep Learning Approach for Multimodal Deception Detection
A1  - Gangeshwar Krishnamurthy
A1  - Navonil Majumder
A1  - Soujanya Poria
A1  - Erik Cambria
JO  - ArXiv e-prints
Y1  - 1 March, 2018
UR  - https://arxiv.org/abs/1803.00344
N2  - Automatic deception detection is an important task that has gained momentum in computational linguistics due to its potential applications. In this paper, we propose a simple yet tough to beat multi-modal neural model for deception detection. By combining features from different modalities such as video, audio, and text along with Micro-Expression features, we show that detecting deception in real life videos can be more accurate. Experimental results on a dataset of real-life deception videos show that our model outperforms existing techniques for deception detection with an accuracy of 96.14% and ROC-AUC of 0.9799.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Sponsored Search Real-time Bidding
A1  - Jun Zhao
A1  - Guang Qiu
A1  - Ziyu Guan
A1  - Wei Zhao
A1  - Xiaofei He
JO  - ArXiv e-prints
Y1  - 1 March, 2018
UR  - https://arxiv.org/abs/1803.00259
N2  - Bidding optimization is one of the most critical problems in online advertising. Sponsored search (SS) auction, due to the randomness of user query behavior and platform nature, usually adopts keyword-level bidding strategies. In contrast, the display advertising (DA), as a relatively simpler scenario for auction, has taken advantage of real-time bidding (RTB) to boost the performance for advertisers. In this paper, we consider the RTB problem in sponsored search auction, named SS-RTB. SS-RTB has a much more complex dynamic environment, due to stochastic user query behavior and more complex bidding policies based on multiple keywords of an ad. Most previous methods for DA cannot be applied. We propose a reinforcement learning (RL) solution for handling the complex dynamic environment. Although some RL methods have been proposed for online advertising, they all fail to address the &#34;environment changing&#34; problem: the state transition probabilities vary between two days. Motivated by the observation that auction sequences of two days share similar transition patterns at a proper aggregation level, we formulate a robust MDP model at hour-aggregation level of the auction data and propose a control-by-model framework for SS-RTB. Rather than generating bid prices directly, we decide a bidding model for impressions of each hour and perform real-time bidding accordingly. We also extend the method to handle the multi-agent problem. We deployed the SS-RTB system in the e-commerce search auction platform of Alibaba. Empirical experiments of offline evaluation and online A/B test demonstrate the effectiveness of our method.
ER  -


TY  - Preprint
T1  - DRUNET: A Dilated-Residual U-Net Deep Learning Network to Digitally Stain Optic Nerve Head Tissues in Optical Coherence Tomography Images
A1  - Sripad Krishna Devalla
A1  - Prajwal K. Renukanand
A1  - Bharathwaj K. Sreedhar
A1  - Shamira Perera
A1  - Jean-Martial Mari
A1  - Khai Sing Chin
A1  - Tin A. Tun
A1  - Nicholas G. Strouthidis
A1  - Tin Aung
A1  - Alexandre H. Thiery
A1  - Michael J. A. Girard
JO  - ArXiv e-prints
Y1  - 1 March, 2018
UR  - https://arxiv.org/abs/1803.00232
N2  - Given that the neural and connective tissues of the optic nerve head (ONH) exhibit complex morphological changes with the development and progression of glaucoma, their simultaneous isolation from optical coherence tomography (OCT) images may be of great interest for the clinical diagnosis and management of this pathology. A deep learning algorithm was designed and trained to digitally stain (i.e. highlight) 6 ONH tissue layers by capturing both the local (tissue texture) and contextual information (spatial arrangement of tissues). The overall dice coefficient (mean of all tissues) was $0.91 \pm 0.05$ when assessed against manual segmentations performed by an expert observer. We offer here a robust segmentation framework that could be extended for the automated parametric study of the ONH tissues.
ER  -


TY  - Preprint
T1  - Towards Cooperation in Sequential Prisoner&#39;s Dilemmas: a Deep Multiagent Reinforcement Learning Approach
A1  - Weixun Wang
A1  - Jianye Hao
A1  - Yixi Wang
A1  - Matthew Taylor
JO  - ArXiv e-prints
Y1  - 28 February, 2018
UR  - https://arxiv.org/abs/1803.00162
N2  - The Iterated Prisoner&#39;s Dilemma has guided research on social dilemmas for decades. However, it distinguishes between only two atomic actions: cooperate and defect. In real-world prisoner&#39;s dilemmas, these choices are temporally extended and different strategies may correspond to sequences of actions, reflecting grades of cooperation. We introduce a Sequential Prisoner&#39;s Dilemma (SPD) game to better capture the aforementioned characteristics. In this work, we propose a deep multiagent reinforcement learning approach that investigates the evolution of mutual cooperation in SPD games. Our approach consists of two phases. The first phase is offline: it synthesizes policies with different cooperation degrees and then trains a cooperation degree detection network. The second phase is online: an agent adaptively selects its policy based on the detected degree of opponent cooperation. The effectiveness of our approach is demonstrated in two representative SPD 2D games: the Apple-Pear game and the Fruit Gathering game. Experimental results show that our strategy can avoid being exploited by exploitative opponents and achieve cooperation with cooperative opponents.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Join Order Enumeration
A1  - Ryan Marcus
A1  - Olga Papaemmanouil
JO  - ArXiv e-prints
Y1  - 12 March, 2018
UR  - https://arxiv.org/abs/1803.00055
N2  - Join order selection plays a significant role in query performance. However, modern query optimizers typically employ static join enumeration algorithms that do not receive any feedback about the quality of the resulting plan. Hence, optimizers often repeatedly choose the same bad plan, as they do not have a mechanism for &#34;learning from their mistakes&#34;. In this paper, we argue that existing deep reinforcement learning techniques can be applied to address this challenge. These techniques, powered by artificial neural networks, can automatically improve decision making by incorporating feedback from their successes and failures. Towards this goal, we present ReJOIN, a proof-of-concept join enumerator, and present preliminary results indicating that ReJOIN can match or outperform the PostgreSQL optimizer in terms of plan quality and join enumeration efficiency.
ER  -


TY  - Preprint
T1  - Using Deep Learning for Segmentation and Counting within Microscopy Data
A1  - Carlos X. HernÃ¡ndez
A1  - Mohammad M. Sultan
A1  - Vijay S. Pande
JO  - ArXiv e-prints
Y1  - 28 February, 2018
UR  - https://arxiv.org/abs/1802.10548
N2  - Cell counting is a ubiquitous, yet tedious task that would greatly benefit from automation. From basic biological questions to clinical trials, cell counts provide key quantitative feedback that drive research. Unfortunately, cell counting is most commonly a manual task and can be time-intensive. The task is made even more difficult due to overlapping cells, existence of multiple focal planes, and poor imaging quality, among other factors. Here, we describe a convolutional neural network approach, using a recently described feature pyramid network combined with a VGG-style neural network, for segmenting and subsequent counting of cells in a given microscopy image.
ER  -


TY  - Preprint
T1  - Precision medicine as a control problem: Using simulation and deep reinforcement learning to discover adaptive, personalized multi-cytokine therapy for sepsis
A1  - Brenden K. Petersen
A1  - Jiachen Yang
A1  - Will S. Grathwohl
A1  - Chase Cockrell
A1  - Claudio Santiago
A1  - Gary An
A1  - Daniel M. Faissol
JO  - ArXiv e-prints
Y1  - 8 February, 2018
UR  - https://arxiv.org/abs/1802.10440
N2  - Sepsis is a life-threatening condition affecting one million people per year in the US in which dysregulation of the body&#39;s own immune system causes damage to its tissues, resulting in a 28 - 50% mortality rate. Clinical trials for sepsis treatment over the last 20 years have failed to produce a single currently FDA approved drug treatment. In this study, we attempt to discover an effective cytokine mediation treatment strategy for sepsis using a previously developed agent-based model that simulates the innate immune response to infection: the Innate Immune Response agent-based model (IIRABM). Previous attempts at reducing mortality with multi-cytokine mediation using the IIRABM have failed to reduce mortality across all patient parameterizations and motivated us to investigate whether adaptive, personalized multi-cytokine mediation can control the trajectory of sepsis and lower patient mortality. We used the IIRABM to compute a treatment policy in which systemic patient measurements are used in a feedback loop to inform future treatment. Using deep reinforcement learning, we identified a policy that achieves 0% mortality on the patient parameterization on which it was trained. More importantly, this policy also achieves 0.8% mortality over 500 randomly selected patient parameterizations with baseline mortalities ranging from 1 - 99% (with an average of 49%) spanning the entire clinically plausible parameter space of the IIRABM. These results suggest that adaptive, personalized multi-cytokine mediation therapy could be a promising approach for treating sepsis. We hope that this work motivates researchers to consider such an approach as part of future clinical trials. To the best of our knowledge, this work is the first to consider adaptive, personalized multi-cytokine mediation therapy for sepsis, and is the first to exploit deep reinforcement learning on a biological simulation.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Vision-Based Robotic Grasping: A Simulated Comparative Evaluation of Off-Policy Methods
A1  - Deirdre Quillen
A1  - Eric Jang
A1  - Ofir Nachum
A1  - Chelsea Finn
A1  - Julian Ibarz
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 28 March, 2018
UR  - https://arxiv.org/abs/1802.10264
N2  - In this paper, we explore deep reinforcement learning algorithms for vision-based robotic grasping. Model-free deep reinforcement learning (RL) has been successfully applied to a range of challenging environments, but the proliferation of algorithms makes it difficult to discern which particular approach would be best suited for a rich, diverse task like grasping. To answer this question, we propose a simulated benchmark for robotic grasping that emphasizes off-policy learning and generalization to unseen objects. Off-policy learning enables utilization of grasping data over a wide variety of objects, and diversity is important to enable the method to generalize to new objects that were not seen during training. We evaluate the benchmark tasks against a variety of Q-function estimation methods, a method previously proposed for robotic grasping with deep neural network models, and a novel approach based on a combination of Monte Carlo return estimation and an off-policy correction. Our results indicate that several simple methods provide a surprisingly strong competitor to popular algorithms such as double Q-learning, and our analysis of stability sheds light on the relative tradeoffs between the algorithms.
ER  -


TY  - Preprint
T1  - DeepSOFA: A Continuous Acuity Score for Critically Ill Patients using Clinically Interpretable Deep Learning
A1  - Benjamin Shickel
A1  - Tyler J. Loftus
A1  - Lasith Adhikari
A1  - Tezcan Ozrazgat-Baslanti
A1  - Azra Bihorac
A1  - Parisa Rashidi
JO  - ArXiv e-prints
Y1  - 29 August, 2018
UR  - https://arxiv.org/abs/1802.10238
N2  - Traditional methods for assessing illness severity and predicting in-hospital mortality among critically ill patients require time-consuming, error-prone calculations using static variable thresholds. These methods do not capitalize on the emerging availability of streaming electronic health record data or capture time-sensitive individual physiological patterns, a critical task in the intensive care unit. We propose a novel acuity score framework (DeepSOFA) that leverages temporal measurements and interpretable deep learning models to assess illness severity at any point during an ICU stay. We compare DeepSOFA with SOFA (Sequential Organ Failure Assessment) baseline models using the same model inputs and find that at any point during an ICU admission, DeepSOFA yields significantly more accurate predictions of in-hospital mortality. A DeepSOFA model developed in a public database and validated in a single institutional cohort had a mean AUC for the entire ICU stay of 0.90 (95% CI 0.90-0.91) compared with baseline SOFA models with mean AUC 0.79 (95% CI 0.79-0.80) and 0.85 (95% CI 0.85-0.86). Deep models are well-suited to identify ICU patients in need of life-saving interventions prior to the occurrence of an unexpected adverse event and inform shared decision-making processes among patients, providers, and families regarding goals of care and optimal resource utilization.
ER  -


TY  - Preprint
T1  - Cluster Naturalistic Driving Encounters Using Deep Unsupervised Learning
A1  - Sisi Li
A1  - Wenshuo Wang
A1  - Zhaobin Mo
A1  - Ding Zhao
JO  - ArXiv e-prints
Y1  - 5 June, 2018
UR  - https://arxiv.org/abs/1802.10214
N2  - Learning knowledge from driving encounters could help self-driving cars make appropriate decisions when driving in complex settings with nearby vehicles engaged. This paper develops an unsupervised classifier to group naturalistic driving encounters into distinguishable clusters by combining an auto-encoder with k-means clustering (AE-kMC). The effectiveness of AE-kMC was validated using the data of 10,000 naturalistic driving encounters which were collected by the University of Michigan, Ann Arbor in the past five years. We compare our developed method with the $k$-means clustering methods and experimental results demonstrate that the AE-kMC method outperforms the original k-means clustering method.
ER  -


TY  - Preprint
T1  - Semi-Supervised Learning Enabled by Multiscale Deep Neural Network Inversion
A1  - Randall Balestriero
A1  - Herve Glotin
A1  - Richard Baraniuk
JO  - ArXiv e-prints
Y1  - 27 February, 2018
UR  - https://arxiv.org/abs/1802.10172
N2  - Deep Neural Networks (DNNs) provide state-of-the-art solutions in several difficult machine perceptual tasks. However, their performance relies on the availability of a large set of labeled training data, which limits the breadth of their applicability. Hence, there is a need for new {\em semi-supervised learning} methods for DNNs that can leverage both (a small amount of) labeled and unlabeled training data. In this paper, we develop a general loss function enabling DNNs of any topology to be trained in a semi-supervised manner without extra hyper-parameters. As opposed to current semi-supervised techniques based on topology-specific or unstable approaches, ours is both robust and general. We demonstrate that our approach reaches state-of-the-art performance on the SVHN ($9.82\%$ test error, with $500$ labels and wide Resnet) and CIFAR10 (16.38% test error, with 8000 labels and sigmoid convolutional neural network) data sets.
ER  -


TY  - Preprint
T1  - A Fast Deep Learning Model for Textual Relevance in Biomedical Information Retrieval
A1  - Sunil Mohan
A1  - Nicolas Fiorini
A1  - Sun Kim
A1  - Zhiyong Lu
JO  - ArXiv e-prints
Y1  - 26 February, 2018
UR  - https://arxiv.org/abs/1802.10078
N2  - Publications in the life sciences are characterized by a large technical vocabulary, with many lexical and semantic variations for expressing the same concept. Towards addressing the problem of relevance in biomedical literature search, we introduce a deep learning model for the relevance of a document&#39;s text to a keyword style query. Limited by a relatively small amount of training data, the model uses pre-trained word embeddings. With these, the model first computes a variable-length Delta matrix between the query and document, representing a difference between the two texts, which is then passed through a deep convolution stage followed by a deep feed-forward network to compute a relevance score. This results in a fast model suitable for use in an online search engine. The model is robust and outperforms comparable state-of-the-art deep learning approaches.
ER  -


TY  - Preprint
T1  - Deep Learning Architectures for Face Recognition in Video Surveillance
A1  - Saman Bashbaghi
A1  - Eric Granger
A1  - Robert Sabourin
A1  - Mostafa Parchami
JO  - ArXiv e-prints
Y1  - 27 June, 2018
UR  - https://arxiv.org/abs/1802.09990
N2  - Face recognition (FR) systems for video surveillance (VS) applications attempt to accurately detect the presence of target individuals over a distributed network of cameras. In video-based FR systems, facial models of target individuals are designed a priori during enrollment using a limited number of reference still images or video data. These facial models are not typically representative of faces being observed during operations due to large variations in illumination, pose, scale, occlusion, blur, and to camera inter-operability. Specifically, in still-to-video FR application, a single high-quality reference still image captured with still camera under controlled conditions is employed to generate a facial model to be matched later against lower-quality faces captured with video cameras under uncontrolled conditions. Current video-based FR systems can perform well on controlled scenarios, while their performance is not satisfactory in uncontrolled scenarios mainly because of the differences between the source (enrollment) and the target (operational) domains. Most of the efforts in this area have been toward the design of robust video-based FR systems in unconstrained surveillance environments. This chapter presents an overview of recent advances in still-to-video FR scenario through deep convolutional neural networks (CNNs). In particular, deep learning architectures proposed in the literature based on triplet-loss function (e.g., cross-correlation matching CNN, trunk-branch ensemble CNN and HaarNet) and supervised autoencoders (e.g., canonical face representation CNN) are reviewed and compared in terms of accuracy and computational complexity.
ER  -


TY  - Preprint
T1  - Mono-Camera 3D Multi-Object Tracking Using Deep Learning Detections and PMBM Filtering
A1  - Samuel Scheidegger
A1  - Joachim Benjaminsson
A1  - Emil Rosenberg
A1  - Amrit Krishnan
A1  - Karl Granstrom
JO  - ArXiv e-prints
Y1  - 27 February, 2018
UR  - https://arxiv.org/abs/1802.09975
N2  - Monocular cameras are one of the most commonly used sensors in the automotive industry for autonomous vehicles. One major drawback using a monocular camera is that it only makes observations in the two dimensional image plane and can not directly measure the distance to objects. In this paper, we aim at filling this gap by developing a multi-object tracking algorithm that takes an image as input and produces trajectories of detected objects in a world coordinate system. We solve this by using a deep neural network trained to detect and estimate the distance to objects from a single input image. The detections from a sequence of images are fed in to a state-of-the art Poisson multi-Bernoulli mixture tracking filter. The combination of the learned detector and the PMBM filter results in an algorithm that achieves 3D tracking using only mono-camera images as input. The performance of the algorithm is evaluated both in 3D world coordinates, and 2D image coordinates, using the publicly available KITTI object tracking dataset. The algorithm shows the ability to accurately track objects, correctly handle data associations, even when there is a big overlap of the objects in the image, and is one of the top performing algorithms on the KITTI object tracking benchmark. Furthermore, the algorithm is efficient, running on average close to 20 frames per second.
ER  -


TY  - Preprint
T1  - Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis
A1  - Tal Ben-Nun
A1  - Torsten Hoefler
JO  - ArXiv e-prints
Y1  - 15 September, 2018
UR  - https://arxiv.org/abs/1802.09941
N2  - Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.
ER  -


TY  - Preprint
T1  - Adversarial Active Learning for Deep Networks: a Margin Based Approach
A1  - Melanie Ducoffe
A1  - Frederic Precioso
JO  - ArXiv e-prints
Y1  - 27 February, 2018
UR  - https://arxiv.org/abs/1802.09841
N2  - We propose a new active learning strategy designed for deep neural networks. The goal is to minimize the number of data annotation queried from an oracle during training. Previous active learning strategies scalable for deep networks were mostly based on uncertain sample selection. In this work, we focus on examples lying close to the decision boundary. Based on theoretical works on margin theory for active learning, we know that such examples may help to considerably decrease the number of annotations. While measuring the exact distance to the decision boundaries is intractable, we propose to rely on adversarial examples. We do not consider anymore them as a threat instead we exploit the information they provide on the distribution of the input space in order to approximate the distance to decision boundaries. We demonstrate empirically that adversarial active queries yield faster convergence of CNNs trained on MNIST, the Shoe-Bag and the Quick-Draw datasets.
ER  -


TY  - Preprint
T1  - Bioinformatics and Medicine in the Era of Deep Learning
A1  - Davide Bacciu
A1  - Paulo J. G. Lisboa
A1  - JosÃ© D. MartÃ­n
A1  - Ruxandra Stoean
A1  - Alfredo Vellido
JO  - ArXiv e-prints
Y1  - 27 February, 2018
UR  - https://arxiv.org/abs/1802.09791
N2  - Many of the current scientific advances in the life sciences have their origin in the intensive use of data for knowledge discovery. In no area this is so clear as in bioinformatics, led by technological breakthroughs in data acquisition technologies. It has been argued that bioinformatics could quickly become the field of research generating the largest data repositories, beating other data-intensive areas such as high-energy physics or astroinformatics. Over the last decade, deep learning has become a disruptive advance in machine learning, giving new live to the long-standing connectionist paradigm in artificial intelligence. Deep learning methods are ideally suited to large-scale data and, therefore, they should be ideally suited to knowledge discovery in bioinformatics and biomedicine at large. In this brief paper, we review key aspects of the application of deep learning in bioinformatics and medicine, drawing from the themes covered by the contributions to an ESANN 2018 special session devoted to this topic.
ER  -


TY  - Preprint
T1  - Directional Statistics-based Deep Metric Learning for Image Classification and Retrieval
A1  - Xuefei Zhe
A1  - Shifeng Chen
A1  - Hong Yan
JO  - ArXiv e-prints
Y1  - 27 March, 2018
UR  - https://arxiv.org/abs/1802.09662
N2  - Deep distance metric learning (DDML), which is proposed to learn image similarity metrics in an end-to-end manner based on the convolution neural network, has achieved encouraging results in many computer vision tasks.$L2$-normalization in the embedding space has been used to improve the performance of several DDML methods. However, the commonly used Euclidean distance is no longer an accurate metric for $L2$-normalized embedding space, i.e., a hyper-sphere. Another challenge of current DDML methods is that their loss functions are usually based on rigid data formats, such as the triplet tuple. Thus, an extra process is needed to prepare data in specific formats. In addition, their losses are obtained from a limited number of samples, which leads to a lack of the global view of the embedding space. In this paper, we replace the Euclidean distance with the cosine similarity to better utilize the $L2$-normalization, which is able to attenuate the curse of dimensionality. More specifically, a novel loss function based on the von Mises-Fisher distribution is proposed to learn a compact hyper-spherical embedding space. Moreover, a new efficient learning algorithm is developed to better capture the global structure of the embedding space. Experiments for both classification and retrieval tasks on several standard datasets show that our method achieves state-of-the-art performance with a simpler training procedure. Furthermore, we demonstrate that, even with a small number of convolutional layers, our model can still obtain significantly better classification performance than the widely used softmax loss.
ER  -


TY  - Preprint
T1  - 2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning
A1  - Diogo C. Luvizon
A1  - David Picard
A1  - Hedi Tabia
JO  - ArXiv e-prints
Y1  - 21 March, 2018
UR  - https://arxiv.org/abs/1802.09232
N2  - Action recognition and human pose estimation are closely related but both problems are generally handled as distinct tasks in the literature. In this work, we propose a multitask framework for jointly 2D and 3D pose estimation from still images and human action recognition from video sequences. We show that a single architecture can be used to solve the two problems in an efficient way and still achieves state-of-the-art results. Additionally, we demonstrate that optimization from end-to-end leads to significantly higher accuracy than separated learning. The proposed architecture can be trained with data from different categories simultaneously in a seamlessly way. The reported results on four datasets (MPII, Human3.6M, Penn Action and NTU) demonstrate the effectiveness of our method on the targeted tasks.
ER  -


TY  - Preprint
T1  - Deep Neural Network for Learning to Rank Query-Text Pairs
A1  - Baoyang Song
JO  - ArXiv e-prints
Y1  - 25 February, 2018
UR  - https://arxiv.org/abs/1802.08988
N2  - This paper considers the problem of document ranking in information retrieval systems by Learning to Rank. We propose ConvRankNet combining a Siamese Convolutional Neural Network encoder and the RankNet ranking model which could be trained in an end-to-end fashion. We prove a general result justifying the linear test-time complexity of pairwise Learning to Rank approach. Experiments on the OHSUMED dataset show that ConvRankNet outperforms systematically existing feature-based models.
ER  -


TY  - Preprint
T1  - Deep learning for conifer/deciduous classification of airborne LiDAR 3D point clouds representing individual trees
A1  - Hamid Hamraz
A1  - Nathan B. Jacobs
A1  - Marco A. Contreras
A1  - Chase H. Clark
JO  - ArXiv e-prints
Y1  - 24 February, 2018
UR  - https://arxiv.org/abs/1802.08872
N2  - The purpose of this study was to investigate the use of deep learning for coniferous/deciduous classification of individual trees from airborne LiDAR data. To enable efficient processing by a deep convolutional neural network (CNN), we designed two discrete representations using leaf-off and leaf-on LiDAR data: a digital surface model with four channels (DSMx4) and a set of four 2D views (4x2D). A training dataset of labeled tree crowns was generated via segmentation of tree crowns, followed by co-registration with field data. Potential mislabels due to GPS error or tree leaning were corrected using a statistical ensemble filtering procedure. Because the training data was heavily unbalanced (~8% conifers), we trained an ensemble of CNNs on random balanced sub-samples of augmented data (180 rotational variations per instance). The 4x2D representation yielded similar classification accuracies to the DSMx4 representation (~82% coniferous and ~90% deciduous) while converging faster. The data augmentation improved the classification accuracies, but more real training instances (especially coniferous) likely results in much stronger improvements. Leaf-off LiDAR data were the primary source of useful information, which is likely due to the perennial nature of coniferous foliage. LiDAR intensity values also proved to be useful, but normalization yielded no significant improvements. Lastly, the classification accuracies of overstory trees (~90%) were more balanced than those of understory trees (~90% deciduous and ~65% coniferous), which is likely due to the incomplete capture of understory tree crowns via airborne LiDAR. Automatic derivation of optimal features via deep learning provide the opportunity for remarkable improvements in prediction tasks where captured data are not friendly to human visual system - likely yielding sub-optimal human-designed features.
ER  -


TY  - Preprint
T1  - Euler angles based loss function for camera relocalization with Deep learning
A1  - Qiang Fang
A1  - Tianjiang Hu
JO  - ArXiv e-prints
Y1  - 24 February, 2018
UR  - https://arxiv.org/abs/1802.08851
N2  - Deep learning has been applied to camera relocalization, in particular, PoseNet and its extended work are the convolutional neural networks which regress the camera pose from a single image. However there are many problems, one of them is expensive parameter selection. In this paper, we directly explore the three Euler angles as the orientation representation in the camera pose regressor. There is no need to select the parameter, which is not tolerant in the previous works. Experimental results on the 7 Scenes datasets and the King&#39;s College dataset demonstrate that it has competitive performances.
ER  -


TY  - Preprint
T1  - Adaptive Deep Learning through Visual Domain Localization
A1  - Gabriele Angeletti
A1  - Barbara Caputo
A1  - Tatiana Tommasi
JO  - ArXiv e-prints
Y1  - 24 February, 2018
UR  - https://arxiv.org/abs/1802.08833
N2  - A commercial robot, trained by its manufacturer to recognize a predefined number and type of objects, might be used in many settings, that will in general differ in their illumination conditions, background, type and degree of clutter, and so on. Recent computer vision works tackle this generalization issue through domain adaptation methods, assuming as source the visual domain where the system is trained and as target the domain of deployment. All approaches assume to have access to images from all classes of the target during training, an unrealistic condition in robotics applications. We address this issue proposing an algorithm that takes into account the specific needs of robot vision. Our intuition is that the nature of the domain shift experienced mostly in robotics is local. We exploit this through the learning of maps that spatially ground the domain and quantify the degree of shift, embedded into an end-to-end deep domain adaptation architecture. By explicitly localizing the roots of the domain shift we significantly reduce the number of parameters of the architecture to tune, we gain the flexibility necessary to deal with subset of categories in the target domain at training time, and we provide a clear feedback on the rationale behind any classification decision, which can be exploited in human-robot interactions. Experiments on two different settings of the iCub World database confirm the suitability of our method for robot vision.
ER  -


TY  - Preprint
T1  - The AdobeIndoorNav Dataset: Towards Deep Reinforcement Learning based Real-world Indoor Robot Visual Navigation
A1  - Kaichun Mo
A1  - Haoxiang Li
A1  - Zhe Lin
A1  - Joon-Young Lee
JO  - ArXiv e-prints
Y1  - 24 February, 2018
UR  - https://arxiv.org/abs/1802.08824
N2  - Deep reinforcement learning (DRL) demonstrates its potential in learning a model-free navigation policy for robot visual navigation. However, the data-demanding algorithm relies on a large number of navigation trajectories in training. Existing datasets supporting training such robot navigation algorithms consist of either 3D synthetic scenes or reconstructed scenes. Synthetic data suffers from domain gap to the real-world scenes while visual inputs rendered from 3D reconstructed scenes have undesired holes and artifacts. In this paper, we present a new dataset collected in real-world to facilitate the research in DRL based visual navigation. Our dataset includes 3D reconstruction for real-world scenes as well as densely captured real 2D images from the scenes. It provides high-quality visual inputs with real-world scene complexity to the robot at dense grid locations. We further study and benchmark one recent DRL based navigation algorithm and present our attempts and thoughts on improving its generalizability to unseen test targets in the scenes.
ER  -


TY  - Preprint
T1  - Longitudinal Face Aging in the Wild - Recent Deep Learning Approaches
A1  - Chi Nhan Duong
A1  - Khoa Luu
A1  - Kha Gia Quach
A1  - Tien D. Bui
JO  - ArXiv e-prints
Y1  - 23 February, 2018
UR  - https://arxiv.org/abs/1802.08726
N2  - Face Aging has raised considerable attentions and interest from the computer vision community in recent years. Numerous approaches ranging from purely image processing techniques to deep learning structures have been proposed in literature. In this paper, we aim to give a review of recent developments of modern deep learning based approaches, i.e. Deep Generative Models, for Face Aging task. Their structures, formulation, learning algorithms as well as synthesized results are also provided with systematic discussions. Moreover, the aging databases used in most methods to learn the aging process are also reviewed.
ER  -


TY  - Preprint
T1  - Deep learning in radiology: an overview of the concepts and a survey of the state of the art
A1  - Maciej A. Mazurowski
A1  - Mateusz Buda
A1  - Ashirbani Saha
A1  - Mustafa R. Bashir
JO  - ArXiv e-prints
Y1  - 9 February, 2018
UR  - https://arxiv.org/abs/1802.08717
N2  - Deep learning is a branch of artificial intelligence where networks of simple interconnected units are used to extract patterns from data in order to solve complex problems. Deep learning algorithms have shown groundbreaking performance in a variety of sophisticated tasks, especially those related to images. They have often matched or exceeded human performance. Since the medical field of radiology mostly relies on extracting useful information from images, it is a very natural application area for deep learning, and research in this area has rapidly grown in recent years. In this article, we review the clinical reality of radiology and discuss the opportunities for application of deep learning algorithms. We also introduce basic concepts of deep learning including convolutional neural networks. Then, we present a survey of the research in deep learning applied to radiology. We organize the studies by the types of specific tasks that they attempt to solve and review the broad range of utilized deep learning algorithms. Finally, we briefly discuss opportunities and challenges for incorporating deep learning in the radiology practice of the future.
ER  -


TY  - Preprint
T1  - Deep Unsupervised Learning of Visual Similarities
A1  - Artsiom Sanakoyeu
A1  - Miguel A. Bautista
A1  - BjÃ¶rn Ommer
JO  - ArXiv e-prints
Y1  - 21 February, 2018
UR  - https://arxiv.org/abs/1802.08562
N2  - Exemplar learning of visual similarities in an unsupervised manner is a problem of paramount importance to Computer Vision. In this context, however, the recent breakthrough in deep learning could not yet unfold its full potential. With only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of Convolutional Neural networks is impaired. In this paper we use weak estimates of local similarities and propose a single optimization problem to extract batches of samples with mutually consistent relations. Conflicting relations are distributed over different batches and similar samples are grouped into compact groups. Learning visual similarities is then framed as a sequence of categorization tasks. The CNN then consolidates transitivity relations within and between groups and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification.
ER  -


TY  - Preprint
T1  - Weighted Double Deep Multiagent Reinforcement Learning in Stochastic Cooperative Environments
A1  - Yan Zheng
A1  - Jianye Hao
A1  - Zongzhang Zhang
JO  - ArXiv e-prints
Y1  - 14 April, 2018
UR  - https://arxiv.org/abs/1802.08534
N2  - Recently, multiagent deep reinforcement learning (DRL) has received increasingly wide attention. Existing multiagent DRL algorithms are inefficient when facing with the non-stationarity due to agents update their policies simultaneously in stochastic cooperative environments. This paper extends the recently proposed weighted double estimator to the multiagent domain and propose a multiagent DRL framework, named weighted double deep Q-network (WDDQN). By utilizing the weighted double estimator and the deep neural network, WDDQN can not only reduce the bias effectively but also be extended to scenarios with raw visual inputs. To achieve efficient cooperation in the multiagent domain, we introduce the lenient reward network and the scheduled replay strategy. Experiments show that the WDDQN outperforms the existing DRL and multiaent DRL algorithms, i.e., double DQN and lenient Q-learning, in terms of the average reward and the convergence rate in stochastic cooperative environments.
ER  -


TY  - Preprint
T1  - Deep Multimodal Learning for Emotion Recognition in Spoken Language
A1  - Yue Gu
A1  - Shuhong Chen
A1  - Ivan Marsic
JO  - ArXiv e-prints
Y1  - 22 February, 2018
UR  - https://arxiv.org/abs/1802.08332
N2  - In this paper, we present a novel deep multimodal framework to predict human emotions based on sentence-level spoken language. Our architecture has two distinctive characteristics. First, it extracts the high-level features from both text and audio via a hybrid deep multimodal structure, which considers the spatial information from text, temporal information from audio, and high-level associations from low-level handcrafted features. Second, we fuse all features by using a three-layer deep neural network to learn the correlations across modalities and train the feature extraction and fusion modules together, allowing optimal global fine-tuning of the entire structure. We evaluated the proposed framework on the IEMOCAP dataset. Our result shows promising performance, achieving 60.4% in weighted accuracy for five emotion categories.
ER  -


TY  - Preprint
T1  - Structured Control Nets for Deep Reinforcement Learning
A1  - Mario Srouji
A1  - Jian Zhang
A1  - Ruslan Salakhutdinov
JO  - ArXiv e-prints
Y1  - 22 February, 2018
UR  - https://arxiv.org/abs/1802.08311
N2  - In recent years, Deep Reinforcement Learning has made impressive advances in solving several important benchmark problems for sequential decision making. Many control applications use a generic multilayer perceptron (MLP) for non-vision parts of the policy network. In this work, we propose a new neural network architecture for the policy network representation that is simple yet effective. The proposed Structured Control Net (SCN) splits the generic MLP into two separate sub-modules: a nonlinear control module and a linear control module. Intuitively, the nonlinear control is for forward-looking and global control, while the linear control stabilizes the local dynamics around the residual of global control. We hypothesize that this will bring together the benefits of both linear and nonlinear policies: improve training sample efficiency, final episodic reward, and generalization of learned policy, while requiring a smaller network and being generally applicable to different training methods. We validated our hypothesis with competitive results on simulations from OpenAI MuJoCo, Roboschool, Atari, and a custom 2D urban driving environment, with various ablation and generalization tests, trained with multiple black-box and policy gradient training methods. The proposed architecture has the potential to improve upon broader control tasks by incorporating problem specific priors into the architecture. As a case study, we demonstrate much improved performance for locomotion tasks by emulating the biological central pattern generators (CPGs) as the nonlinear part of the architecture.
ER  -


TY  - Preprint
T1  - Classification of Breast Cancer Histology using Deep Learning
A1  - Aditya Golatkar
A1  - Deepak Anand
A1  - Amit Sethi
JO  - ArXiv e-prints
Y1  - 25 July, 2018
UR  - https://arxiv.org/abs/1802.08080
N2  - Breast Cancer is a major cause of death worldwide among women. Hematoxylin and Eosin (H&amp;E) stained breast tissue samples from biopsies are observed under microscopes for the primary diagnosis of breast cancer. In this paper, we propose a deep learning-based method for classification of H&amp;E stained breast tissue images released for BACH challenge 2018 by fine-tuning Inception-v3 convolutional neural network (CNN) proposed by Szegedy et al. These images are to be classified into four classes namely, i) normal tissue, ii) benign tumor, iii) in-situ carcinoma and iv) invasive carcinoma. Our strategy is to extract patches based on nuclei density instead of random or grid sampling, along with rejection of patches that are not rich in nuclei (non-epithelial) regions for training and testing. Every patch (nuclei-dense region) in an image is classified in one of the four above mentioned categories. The class of the entire image is determined using majority voting over the nuclear classes. We obtained an average four class accuracy of 85% and an average two class (non-cancer vs. carcinoma) accuracy of 93%, which improves upon a previous benchmark by Araujo et al.
ER  -


TY  - Preprint
T1  - Learning Multiple Categories on Deep Convolution Networks
A1  - Mohamed Hajaj
A1  - Duncan Gillies
JO  - ArXiv e-prints
Y1  - 21 February, 2018
UR  - https://arxiv.org/abs/1802.07672
N2  - Deep convolution networks have proved very successful with big datasets such as the 1000-classes ImageNet. Results show that the error rate increases slowly as the size of the dataset increases. Experiments presented here may explain why these networks are very effective in solving big recognition problems. If the big task is made up of multiple smaller tasks, then the results show the ability of deep convolution networks to decompose the complex task into a number of smaller tasks and to learn them simultaneously. The results show that the performance of solving the big task on a single network is very close to the average performance of solving each of the smaller tasks on a separate network. Experiments also show the advantage of using task specific or category labels in combination with class labels.
ER  -


TY  - Preprint
T1  - Towards Deep Representation Learning with Genetic Programming
A1  - Lino Rodriguez-Coayahuitl
A1  - Alicia Morales-Reyes
A1  - Hugo Jair Escalante
JO  - ArXiv e-prints
Y1  - 20 February, 2018
UR  - https://arxiv.org/abs/1802.07133
N2  - Genetic Programming (GP) is an evolutionary algorithm commonly used for machine learning tasks. In this paper we present a method that allows GP to transform the representation of a large-scale machine learning dataset into a more compact representation, by means of processing features from the original representation at individual level. We develop as a proof of concept of this method an autoencoder. We tested a preliminary version of our approach in a variety of well-known machine learning image datasets. We speculate that this method, used in an iterative manner, can produce results competitive with state-of-art deep neural networks.
ER  -


TY  - Preprint
T1  - The Description Length of Deep Learning Models
A1  - LÃ©onard Blier
A1  - Yann Ollivier
JO  - ArXiv e-prints
Y1  - 23 May, 2018
UR  - https://arxiv.org/abs/1802.07044
N2  - Solomonoff&#39;s general theory of inference and the Minimum Description Length principle formalize Occam&#39;s razor, and hold that a good model of data is a model that is good at losslessly compressing the data, including the cost of describing the model itself. Deep neural networks might seem to go against this principle given the large number of parameters to be encoded.
ER  -


TY  - Preprint
T1  - High-Order Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting
A1  - Zhiyong Cui
A1  - Kristian Henrickson
A1  - Ruimin Ke
A1  - Yinhai Wang
JO  - ArXiv e-prints
Y1  - 20 February, 2018
UR  - https://arxiv.org/abs/1802.07007
N2  - Traffic forecasting is a challenging task, due to the complicated spatial dependencies on roadway networks and the time-varying traffic patterns. To address this challenge, we learn the traffic network as a graph and propose a novel deep learning framework, High-Order Graph Convolutional Long Short-Term Memory Neural Network (HGC-LSTM), to learn the interactions between links in the traffic network and forecast the network-wide traffic state. We define the high-order traffic graph convolution based on the physical network topology. The proposed framework employs L1-norms on the graph convolution weights and L2-norms on the graph convolution features to identify the most influential links in the traffic network. We propose a novel Real-Time Branching Learning (RTBL) algorithm for the HGC-LSTM framework to accelerate the training process for spatio-temporal data. Experiments show that our HGC-LSTM network is able to capture the complex spatio-temporal dependencies efficiently present in the traffic network and consistently outperforms state-of-the-art baseline methods on two heterogeneous real-world traffic datasets. The visualization of graph convolution weights shows that the proposed framework can accurately recognize the most influential roadway segments in real-world traffic networks.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Dynamic Multichannel Access in Wireless Networks
A1  - Shangxing Wang
A1  - Hanpeng Liu
A1  - Pedro Henrique Gomes
A1  - Bhaskar Krishnamachari
JO  - ArXiv e-prints
Y1  - 19 February, 2018
UR  - https://arxiv.org/abs/1802.06958
N2  - We consider a dynamic multichannel access problem, where multiple correlated channels follow an unknown joint Markov model. A user at each time slot selects a channel to transmit data and receives a reward based on the success or failure of the transmission. The objective is to find a policy that maximizes the expected long-term reward. The problem is formulated as a partially observable Markov decision process (POMDP) with unknown system dynamics. To overcome the challenges of unknown system dynamics as well as prohibitive computation, we apply the concept of reinforcement learning and implement a Deep Q-Network (DQN) that can deal with large state space without any prior knowledge of the system dynamics. We provide an analytical study on the optimal policy for fixed-pattern channel switching with known system dynamics and show through simulations that DQN can achieve the same optimal performance without knowing the system statistics. We compare the performance of DQN with a Myopic policy and a Whittle Index-based heuristic through both simulations as well as real-data trace and show that DQN achieves near-optimal performance in more complex situations. Finally, we propose an adaptive DQN approach with the capability to adapt its learning in time-varying, dynamic scenarios.
ER  -


TY  - Preprint
T1  - Efficient Embedding of MPI Collectives in MXNET DAGs for scaling Deep Learning
A1  - Amith R Mamidala
JO  - ArXiv e-prints
Y1  - 19 February, 2018
UR  - https://arxiv.org/abs/1802.06949
N2  - Availability of high performance computing infrastructures such as clusters of GPUs and CPUs have fueled the growth of distributed learning systems. Deep Learning frameworks express neural nets as DAGs and execute these DAGs on computation resources such as GPUs. In this paper, we propose efficient designs of embedding MPI collective operations into data parallel DAGs. Incorrect designs can easily lead to deadlocks or program crashes. In particular, we demonstrate three designs: Funneled, Concurrent communication and Dependency chaining of using MPI collectives with DAGs. These designs automatically enable overlap of computation with communication by allowing for concurrent execution with the other tasks. We directly implement these designs into the KVStore API of the MXNET. This allows us to directly leverage the rest of the infrastructure. Using ImageNet and CIFAR data sets, we show the potential of our designs. In particular, our designs scale to 256 GPUs with as low as 50 seconds of epoch times for ImageNet 1K datasets.
ER  -


TY  - Preprint
T1  - Automated soft tissue lesion detection and segmentation in digital mammography using a u-net deep learning network
A1  - Timothy de Moor
A1  - Alejandro Rodriguez-Ruiz
A1  - Albert Gubern MÃ©rida
A1  - Ritse Mann
A1  - Jonas Teuwen
JO  - ArXiv e-prints
Y1  - 8 March, 2018
UR  - https://arxiv.org/abs/1802.06865
N2  - Computer-aided detection or decision support systems aim to improve breast cancer screening programs by helping radiologists to evaluate digital mammography (DM) exams. Commonly such methods proceed in two steps: selection of candidate regions for malignancy, and later classification as either malignant or not. In this study, we present a candidate detection method based on deep learning to automatically detect and additionally segment soft tissue lesions in DM. A database of DM exams (mostly bilateral and two views) was collected from our institutional archive. In total, 7196 DM exams (28294 DM images) acquired with systems from three different vendors (General Electric, Siemens, Hologic) were collected, of which 2883 contained malignant lesions verified with histopathology. Data was randomly split on an exam level into training (50\%), validation (10\%) and testing (40\%) of deep neural network with u-net architecture. The u-net classifies the image but also provides lesion segmentation. Free receiver operating characteristic (FROC) analysis was used to evaluate the model, on an image and on an exam level. On an image level, a maximum sensitivity of 0.94 at 7.93 false positives (FP) per image was achieved. Similarly, per exam a maximum sensitivity of 0.98 at 7.81 FP per image was achieved. In conclusion, the method could be used as a candidate selection model with high accuracy and with the additional information of lesion segmentation.
ER  -


TY  - Preprint
T1  - Deep Learning for Joint Source-Channel Coding of Text
A1  - Nariman Farsad
A1  - Milind Rao
A1  - Andrea Goldsmith
JO  - ArXiv e-prints
Y1  - 19 February, 2018
UR  - https://arxiv.org/abs/1802.06832
N2  - We consider the problem of joint source and channel coding of structured data such as natural language over a noisy channel. The typical approach to this problem in both theory and practice involves performing source coding to first compress the text and then channel coding to add robustness for the transmission across the channel. This approach is optimal in terms of minimizing end-to-end distortion with arbitrarily large block lengths of both the source and channel codes when transmission is over discrete memoryless channels. However, the optimality of this approach is no longer ensured for documents of finite length and limitations on the length of the encoding. We will show in this scenario that we can achieve lower word error rates by developing a deep learning based encoder and decoder. While the approach of separate source and channel coding would minimize bit error rates, our approach preserves semantic information of sentences by first embedding sentences in a semantic space where sentences closer in meaning are located closer together, and then performing joint source and channel coding on these embeddings.
ER  -


TY  - Preprint
T1  - Shield: Fast, Practical Defense and Vaccination for Deep Learning using JPEG Compression
A1  - Nilaksh Das
A1  - Madhuri Shanbhogue
A1  - Shang-Tse Chen
A1  - Fred Hohman
A1  - Siwei Li
A1  - Li Chen
A1  - Michael E. Kounavis
A1  - Duen Horng Chau
JO  - ArXiv e-prints
Y1  - 19 February, 2018
UR  - https://arxiv.org/abs/1802.06816
N2  - The rapidly growing body of research in adversarial machine learning has demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarially generated images. This underscores the urgent need for practical defense that can be readily deployed to combat attacks in real-time. Observing that many attack strategies aim to perturb image pixels in ways that are visually imperceptible, we place JPEG compression at the core of our proposed Shield defense framework, utilizing its capability to effectively &#34;compress away&#34; such pixel manipulation. To immunize a DNN model from artifacts introduced by compression, Shield &#34;vaccinates&#34; a model by re-training it with compressed images, where different compression levels are applied to generate multiple vaccinated models that are ultimately used together in an ensemble defense. On top of that, Shield adds an additional layer of protection by employing randomization at test time that compresses different regions of an image using random compression levels, making it harder for an adversary to estimate the transformation performed. This novel combination of vaccination, ensembling, and randomization makes Shield a fortified multi-pronged protection. We conducted extensive, large-scale experiments using the ImageNet dataset, and show that our approaches eliminate up to 94% of black-box attacks and 98% of gray-box attacks delivered by the recent, strongest attacks, such as Carlini-Wagner&#39;s L2 and DeepFool. Our approaches are fast and work without requiring knowledge about the model.
ER  -


TY  - Preprint
T1  - Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning
A1  - Xiangyu Zhao
A1  - Liang Zhang
A1  - Zhuoye Ding
A1  - Long Xia
A1  - Jiliang Tang
A1  - Dawei Yin
JO  - ArXiv e-prints
Y1  - 9 August, 2018
UR  - https://arxiv.org/abs/1802.06501
N2  - Recommender systems play a crucial role in mitigating the problem of information overload by suggesting users&#39; personalized items or services. The vast majority of traditional recommender systems consider the recommendation procedure as a static process and make recommendations following a fixed strategy. In this paper, we propose a novel recommender system with the capability of continuously improving its strategies during the interactions with users. We model the sequential interactions between users and a recommender system as a Markov Decision Process (MDP) and leverage Reinforcement Learning (RL) to automatically learn the optimal strategies via recommending trial-and-error items and receiving reinforcements of these items from users&#39; feedback. Users&#39; feedback can be positive and negative and both types of feedback have great potentials to boost recommendations. However, the number of negative feedback is much larger than that of positive one; thus incorporating them simultaneously is challenging since positive feedback could be buried by negative one. In this paper, we develop a novel approach to incorporate them into the proposed deep recommender system (DEERS) framework. The experimental results based on real-world e-commerce data demonstrate the effectiveness of the proposed framework. Further experiments have been conducted to understand the importance of both positive and negative feedback in recommendations.
ER  -


TY  - Preprint
T1  - D-Sempre: Learning Deep Semantic-Preserving Embeddings for User interests-Social Contents Modeling
A1  - Shuang Ma
A1  - Chang Wen Chen
JO  - ArXiv e-prints
Y1  - 18 February, 2018
UR  - https://arxiv.org/abs/1802.06451
N2  - Exponential growth of social media consumption demands effective user interests-social contents modeling for more personalized recommendation and social media summarization. However, due to the heterogeneous nature of social contents, traditional approaches lack the ability of capturing the hidden semantic correlations across these multi-modal data, which leads to semantic gaps between social content understanding and user interests. To effectively bridge the semantic gaps, we propose a novel deep learning framework for user interests-social contents modeling. We first mine and parse data, i.e. textual content, visual content, social context and social relation, from heterogeneous social media feeds. Then, we design a two-branch network to map the social contents and users into a same latent space. Particularly, the network is trained by a large margin objective that combines a cross-instance distance constraint with a within-instance semantic-preserving constraint in an end-to- end manner. At last, a Deep Semantic-Preserving Embedding (D-Sempre) is learned, and the ranking results can be given by calculating distances between social contents and users. To demonstrate the effectiveness of D-Sempre in user interests-social contents modeling, we construct a Twitter dataset and conduct extensive experiments on it. As a result, D-Sempre effectively integrates the multi-modal data from heterogeneous social media feeds and captures the hidden semantic correlations between users&#39; interests and social contents.
ER  -


TY  - Preprint
T1  - Efficient Large-Scale Fleet Management via Multi-Agent Deep Reinforcement Learning
A1  - Kaixiang Lin
A1  - Renyu Zhao
A1  - Zhe Xu
A1  - Jiayu Zhou
JO  - ArXiv e-prints
Y1  - 18 February, 2018
UR  - https://arxiv.org/abs/1802.06444
N2  - Large-scale online ride-sharing platforms have substantially transformed our lives by reallocating transportation resources to alleviate traffic congestion and promote transportation efficiency. An efficient fleet management strategy not only can significantly improve the utilization of transportation resources but also increase the revenue and customer satisfaction. It is a challenging task to design an effective fleet management strategy that can adapt to an environment involving complex dynamics between demand and supply. Existing studies usually work on a simplified problem setting that can hardly capture the complicated stochastic demand-supply variations in high-dimensional space. In this paper we propose to tackle the large-scale fleet management problem using reinforcement learning, and propose a contextual multi-agent reinforcement learning framework including two concrete algorithms, namely contextual deep Q-learning and contextual multi-agent actor-critic, to achieve explicit coordination among a large number of agents adaptive to different contexts. We show significant improvements of the proposed framework over state-of-the-art approaches through extensive empirical studies.
ER  -


TY  - Preprint
T1  - Sim-to-Real Optimization of Complex Real World Mobile Network with Imperfect Information via Deep Reinforcement Learning from Self-play
A1  - Yongxi Tan
A1  - Jin Yang
A1  - Xin Chen
A1  - Qitao Song
A1  - Yunjun Chen
A1  - Zhangxiang Ye
A1  - Zhenqiang Su
JO  - ArXiv e-prints
Y1  - 17 April, 2018
UR  - https://arxiv.org/abs/1802.06416
N2  - Mobile network that millions of people use every day is one of the most complex systems in real world. Optimization of mobile network to meet exploding customer demand and reduce CAPEX/OPEX poses greater challenges than in prior works. Actually, learning to solve complex problems in real world to benefit everyone and make the world better has long been ultimate goal of AI. However, application of deep reinforcement learning (DRL) to complex problems in real world still remains unsolved, due to imperfect information, data scarcity and complex rules in real world, potential negative impact to real world, etc. To bridge this reality gap, we propose a sim-to-real framework to direct transfer learning from simulation to real world without any training in real world. First, we distill temporal-spatial relationships between cells and mobile users to scalable 3D image-like tensor to best characterize partially observed mobile network. Second, inspired by AlphaGo, we introduce a novel self-play mechanism to empower DRL agents to gradually improve intelligence by competing for best record on multiple tasks, just like athletes compete for world record in decathlon. Third, a decentralized DRL method is proposed to coordinate multi-agents to compete and cooperate as a team to maximize global reward and minimize potential negative impact. Using 7693 unseen test tasks over 160 unseen mobile networks in another simulator as well as 6 field trials on 4 commercial mobile networks in real world, we demonstrate the capability of this sim-to-real framework to direct transfer the learning not only from one simulator to another simulator, but also from simulation to real world. This is the first time that a DRL agent successfully transfers its learning directly from simulation to very complex real world problems with imperfect information, complex rules, huge state/action space, and multi-agent interactions.
ER  -


TY  - Preprint
T1  - Towards Ultra-High Performance and Energy Efficiency of Deep Learning Systems: An Algorithm-Hardware Co-Optimization Framework
A1  - Yanzhi Wang
A1  - Caiwen Ding
A1  - Zhe Li
A1  - Geng Yuan
A1  - Siyu Liao
A1  - Xiaolong Ma
A1  - Bo Yuan
A1  - Xuehai Qian
A1  - Jian Tang
A1  - Qinru Qiu
A1  - Xue Lin
JO  - ArXiv e-prints
Y1  - 18 February, 2018
UR  - https://arxiv.org/abs/1802.06402
N2  - Hardware accelerations of deep learning systems have been extensively investigated in industry and academia. The aim of this paper is to achieve ultra-high energy efficiency and performance for hardware implementations of deep neural networks (DNNs). An algorithm-hardware co-optimization framework is developed, which is applicable to different DNN types, sizes, and application scenarios. The algorithm part adopts the general block-circulant matrices to achieve a fine-grained tradeoff between accuracy and compression ratio. It applies to both fully-connected and convolutional layers and contains a mathematically rigorous proof of the effectiveness of the method. The proposed algorithm reduces computational complexity per layer from O($n^2$) to O($n\log n$) and storage complexity from O($n^2$) to O($n$), both for training and inference. The hardware part consists of highly efficient Field Programmable Gate Array (FPGA)-based implementations using effective reconfiguration, batch processing, deep pipelining, resource re-using, and hierarchical control. Experimental results demonstrate that the proposed framework achieves at least 152X speedup and 71X energy efficiency gain compared with IBM TrueNorth processor under the same test accuracy. It achieves at least 31X energy efficiency gain compared with the reference FPGA-based work.
ER  -


TY  - Preprint
T1  - A Collaborative Computer Aided Diagnosis (C-CAD) System with Eye-Tracking, Sparse Attentional Model, and Deep Learning
A1  - Naji Khosravan
A1  - Haydar Celik
A1  - Baris Turkbey
A1  - Elizabeth Jones
A1  - Bradford Wood
A1  - Ulas Bagci
JO  - ArXiv e-prints
Y1  - 28 April, 2018
UR  - https://arxiv.org/abs/1802.06260
N2  - There are at least two categories of errors in radiology screening that can lead to suboptimal diagnostic decisions and interventions:(i)human fallibility and (ii)complexity of visual search. Computer aided diagnostic (CAD) tools are developed to help radiologists to compensate for some of these errors. However, despite their significant improvements over conventional screening strategies, most CAD systems do not go beyond their use as second opinion tools due to producing a high number of false positives, which human interpreters need to correct. In parallel with efforts in computerized analysis of radiology scans, several researchers have examined behaviors of radiologists while screening medical images to better understand how and why they miss tumors, how they interact with the information in an image, and how they search for unknown pathology in the images. Eye-tracking tools have been instrumental in exploring answers to these fundamental questions. In this paper, we aim to develop a paradigm shift CAD system, called collaborative CAD (C-CAD), that unifies both of the above mentioned research lines: CAD and eye-tracking. We design an eye-tracking interface providing radiologists with a real radiology reading room experience. Then, we propose a novel algorithm that unifies eye-tracking data and a CAD system. Specifically, we present a new graph based clustering and sparsification algorithm to transform eye-tracking data (gaze) into a signal model to interpret gaze patterns quantitatively and qualitatively. The proposed C-CAD collaborates with radiologists via eye-tracking technology and helps them to improve diagnostic decisions. The C-CAD learns radiologists&#39; search efficiency by processing their gaze patterns. To do this, the C-CAD uses a deep learning algorithm in a newly designed multi-task learning platform to segment and diagnose cancers simultaneously.
ER  -


TY  - Preprint
T1  - A Deep Q-Learning Agent for the L-Game with Variable Batch Training
A1  - Petros Giannakopoulos
A1  - Yannis Cotronis
JO  - ArXiv e-prints
Y1  - 17 February, 2018
UR  - https://arxiv.org/abs/1802.06225
N2  - We employ the Deep Q-Learning algorithm with Experience Replay to train an agent capable of achieving a high-level of play in the L-Game while self-learning from low-dimensional states. We also employ variable batch size for training in order to mitigate the loss of the rare reward signal and significantly accelerate training. Despite the large action space due to the number of possible moves, the low-dimensional state space and the rarity of rewards, which only come at the end of a game, DQL is successful in training an agent capable of strong play without the use of any search methods or domain knowledge.
ER  -


TY  - Preprint
T1  - Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks
A1  - Peter L. Bartlett
A1  - David P. Helmbold
A1  - Philip M. Long
JO  - ArXiv e-prints
Y1  - 18 June, 2018
UR  - https://arxiv.org/abs/1802.06093
N2  - We analyze algorithms for approximating a function $f(x) = Î¦x$ mapping $\Re^d$ to $\Re^d$ using deep linear neural networks, i.e. that learn a function $h$ parameterized by matrices $Î_1,...,Î_L$ and defined by $h(x) = Î_L Î_{L-1} ... Î_1 x$. We focus on algorithms that learn through gradient descent on the population quadratic loss in the case that the distribution over the inputs is isotropic.
ER  -


TY  - Preprint
T1  - Variance-based Gradient Compression for Efficient Distributed Deep Learning
A1  - Yusuke Tsuzuku
A1  - Hiroto Imachi
A1  - Takuya Akiba
JO  - ArXiv e-prints
Y1  - 19 February, 2018
UR  - https://arxiv.org/abs/1802.06058
N2  - Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers. However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections. A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when applied to convolutional neural networks. To address these issues, we propose a method to reduce the communication overhead of distributed deep learning. Our key observation is that gradient updates can be delayed until an unambiguous (high amplitude, low variance) gradient has been calculated. We also present an efficient algorithm to compute the variance with negligible additional cost. We experimentally show that our method can achieve very high compression ratio while maintaining the result model accuracy. We also analyze the efficiency using computation and communication cost models and provide the evidence that this method enables distributed deep learning for many scenarios with commodity environments.
ER  -


TY  - Preprint
T1  - Improved GQ-CNN: Deep Learning Model for Planning Robust Grasps
A1  - Maciej JaÅkowski
A1  - Jakub ÅwiÄtkowski
A1  - MichaÅ ZajÄc
A1  - Maciej Klimek
A1  - Jarek Potiuk
A1  - Piotr Rybicki
A1  - Piotr Polatowski
A1  - PrzemysÅaw Walczyk
A1  - Kacper Nowicki
A1  - Marek Cygan
JO  - ArXiv e-prints
Y1  - 16 February, 2018
UR  - https://arxiv.org/abs/1802.05992
N2  - Recent developments in the field of robot grasping have shown great improvements in the grasp success rates when dealing with unknown objects. In this work we improve on one of the most promising approaches, the Grasp Quality Convolutional Neural Network (GQ-CNN) trained on the DexNet 2.0 dataset. We propose a new architecture for the GQ-CNN and describe practical improvements that increase the model validation accuracy from 92.2% to 95.8% and from 85.9% to 88.0% on respectively image-wise and object-wise training and validation splits.
ER  -


TY  - Preprint
T1  - Instance-based Inductive Deep Transfer Learning by Cross-Dataset Querying with Locality Sensitive Hashing
A1  - Somnath Basu Roy Chowdhury
A1  - K M Annervaz
A1  - Ambedkar Dukkipati
JO  - ArXiv e-prints
Y1  - 16 February, 2018
UR  - https://arxiv.org/abs/1802.05934
N2  - Supervised learning models are typically trained on a single dataset and the performance of these models rely heavily on the size of the dataset, i.e., amount of data available with the ground truth. Learning algorithms try to generalize solely based on the data that is presented with during the training. In this work, we propose an inductive transfer learning method that can augment learning models by infusing similar instances from different learning tasks in the Natural Language Processing (NLP) domain. We propose to use instance representations from a source dataset, \textit{without inheriting anything} from the source learning model. Representations of the instances of \textit{source} \&amp; \textit{target} datasets are learned, retrieval of relevant source instances is performed using soft-attention mechanism and \textit{locality sensitive hashing}, and then, augmented into the model during training on the target dataset. Our approach simultaneously exploits the local \textit{instance level information} as well as the macro statistical viewpoint of the dataset. Using this approach we have shown significant improvements for three major news classification datasets over the baseline. Experimental evaluations also show that the proposed approach reduces dependency on labeled data by a significant margin for comparable performance. With our proposed cross dataset learning procedure we show that one can achieve competitive/better performance than learning from a single dataset.
ER  -


TY  - Preprint
T1  - Tree-CNN: A Hierarchical Deep Convolutional Neural Network for Incremental Learning
A1  - Deboleena Roy
A1  - Priyadarshini Panda
A1  - Kaushik Roy
JO  - ArXiv e-prints
Y1  - 23 May, 2018
UR  - https://arxiv.org/abs/1802.05800
N2  - In recent years, Convolutional Neural Networks (CNNs) have shown remarkable performance in many computer vision tasks such as object recognition and detection. However, complex training issues, such as `catastrophic forgetting&#39; and hyper-parameter tuning, make incremental learning in CNNs a difficult challenge. In this paper, we propose a hierarchical deep neural network, with CNNs at multiple levels, and a corresponding training method for incremental learning. The network grows in a tree-like manner to accommodate the new classes of data without losing the ability to identify the previously trained classes. The proposed network was tested on CIFAR-100 and reported 60.46% accuracy and 20% reduction in training effort as compared to retraining final layers of a deep network. The network organizes the incoming classes of data into feature-driven super-classes and improves upon existing hierarchical CNN models by adding the capability of self-growth.
ER  -


TY  - Preprint
T1  - Horovod: fast and easy distributed deep learning in TensorFlow
A1  - Alexander Sergeev
A1  - Mike Del Balso
JO  - ArXiv e-prints
Y1  - 20 February, 2018
UR  - https://arxiv.org/abs/1802.05799
N2  - Training modern deep learning models requires large amounts of computation, often provided by GPUs. Scaling computation from one GPU to many can enable much faster training and research progress but entails two complications. First, the training library must support inter-GPU communication. Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead. Second, the user must modify his or her training code to take advantage of inter-GPU communication. Depending on the training library&#39;s API, the modification required may be either significant or minimal.
ER  -


TY  - Preprint
T1  - Deep Learning for Lip Reading using Audio-Visual Information for Urdu Language
A1  - M Faisal
A1  - Sanaullah Manzoor
JO  - ArXiv e-prints
Y1  - 15 February, 2018
UR  - https://arxiv.org/abs/1802.05521
N2  - Human lip-reading is a challenging task. It requires not only knowledge of underlying language but also visual clues to predict spoken words. Experts need certain level of experience and understanding of visual expressions learning to decode spoken words. Now-a-days, with the help of deep learning it is possible to translate lip sequences into meaningful words. The speech recognition in the noisy environments can be increased with the visual information [1]. To demonstrate this, in this project, we have tried to train two different deep-learning models for lip-reading: first one for video sequences using spatiotemporal convolution neural network, Bi-gated recurrent neural network and Connectionist Temporal Classification Loss, and second for audio that inputs the MFCC features to a layer of LSTM cells and output the sequence. We have also collected a small audio-visual dataset to train and test our model. Our target is to integrate our both models to improve the speech recognition in the noisy environment
ER  -


TY  - Preprint
T1  - Deep Learning Based Speech Beamforming
A1  - Kaizhi Qian
A1  - Yang Zhang
A1  - Shiyu Chang
A1  - Xuesong Yang
A1  - Dinei Florencio
A1  - Mark Hasegawa-Johnson
JO  - ArXiv e-prints
Y1  - 14 February, 2018
UR  - https://arxiv.org/abs/1802.05383
N2  - Multi-channel speech enhancement with ad-hoc sensors has been a challenging task. Speech model guided beamforming algorithms are able to recover natural sounding speech, but the speech models tend to be oversimplified or the inference would otherwise be too complicated. On the other hand, deep learning based enhancement approaches are able to learn complicated speech distributions and perform efficient inference, but they are unable to deal with variable number of input channels. Also, deep learning approaches introduce a lot of errors, particularly in the presence of unseen noise types and settings. We have therefore proposed an enhancement framework called DEEPBEAM, which combines the two complementary classes of algorithms. DEEPBEAM introduces a beamforming filter to produce natural sounding speech, but the filter coefficients are determined with the help of a monaural speech enhancement neural network. Experiments on synthetic and real-world data show that DEEPBEAM is able to produce clean, dry and natural sounding speech, and is robust against unseen noise.
ER  -


TY  - Preprint
T1  - 500+ Times Faster Than Deep Learning (A Case Study Exploring Faster Methods for Text Mining StackOverflow)
A1  - Suvodeep Majumder
A1  - Nikhila Balaji
A1  - Katie Brey
A1  - Wei Fu
A1  - Tim Menzies
JO  - ArXiv e-prints
Y1  - 14 February, 2018
UR  - https://arxiv.org/abs/1802.05319
N2  - Deep learning methods are useful for high-dimensional data and are becoming widely used in many areas of software engineering. Deep learners utilizes extensive computational power and can take a long time to train-- making it difficult to widely validate and repeat and improve their results. Further, they are not the best solution in all domains. For example, recent results show that for finding related Stack Overflow posts, a tuned SVM performs similarly to a deep learner, but is significantly faster to train. This paper extends that recent result by clustering the dataset, then tuning very learners within each cluster. This approach is over 500 times faster than deep learning (and over 900 times faster if we use all the cores on a standard laptop computer). Significantly, this faster approach generates classifiers nearly as good (within 2\% F1 Score) as the much slower deep learning method. Hence we recommend this faster methods since it is much easier to reproduce and utilizes far fewer CPU resources. More generally, we recommend that before researchers release research results, that they compare their supposedly sophisticated methods against simpler alternatives (e.g applying simpler learners to build local models).
ER  -


TY  - Preprint
T1  - Sharkzor: Interactive Deep Learning for Image Triage, Sort and Summary
A1  - Meg Pirrung
A1  - Nathan Hilliard
A1  - ArtÃ«m Yankov
A1  - Nancy O&#39;Brien
A1  - Paul Weidert
A1  - Courtney D Corley
A1  - Nathan O Hodas
JO  - ArXiv e-prints
Y1  - 14 February, 2018
UR  - https://arxiv.org/abs/1802.05316
N2  - Sharkzor is a web application for machine-learning assisted image sort and summary. Deep learning algorithms are leveraged to infer, augment, and automate the user&#39;s mental model. Initially, images uploaded by the user are spread out on a canvas. The user then interacts with the images to impute their mental model into the application&#39;s algorithmic underpinnings. Methods of interaction within Sharkzor&#39;s user interface and user experience support three primary user tasks; triage, organize and automate. The user triages the large pile of overlapping images by moving images of interest into proximity. The user then organizes said images into meaningful groups. After interacting with the images and groups, deep learning helps to automate the user&#39;s interactions. The loop of interaction, automation, and response by the user allows the system to quickly make sense of large amounts of data.
ER  -


TY  - Preprint
T1  - Learning Deep Disentangled Embeddings with the F-Statistic Loss
A1  - Karl Ridgeway
A1  - Michael C. Mozer
JO  - ArXiv e-prints
Y1  - 19 May, 2018
UR  - https://arxiv.org/abs/1802.05312
N2  - Deep-embedding methods aim to discover representations of a domain that make explicit the domain&#39;s class structure and thereby support few-shot learning. Disentangling methods aim to make explicit compositional or factorial structure. We combine these two active but independent lines of research and propose a new paradigm suitable for both goals. We propose and evaluate a novel loss function based on the $F$ statistic, which describes the separation of two or more distributions. By ensuring that distinct classes are well separated on a subset of embedding dimensions, we obtain embeddings that are useful for few-shot learning. By not requiring separation on all dimensions, we encourage the discovery of disentangled representations. Our embedding method matches or beats state-of-the-art, as evaluated by performance on recall@$k$ and few-shot learning tasks. Our method also obtains performance superior to a variety of alternatives on disentangling, as evaluated by two key properties of a disentangled representation: modularity and explicitness. The goal of our work is to obtain more interpretable, manipulable, and generalizable deep representations of concepts and categories.
ER  -


TY  - Preprint
T1  - Security Analysis and Enhancement of Model Compressed Deep Learning Systems under Adversarial Attacks
A1  - Qi Liu
A1  - Tao Liu
A1  - Zihao Liu
A1  - Yanzhi Wang
A1  - Yier Jin
A1  - Wujie Wen
JO  - ArXiv e-prints
Y1  - 19 March, 2018
UR  - https://arxiv.org/abs/1802.05193
N2  - DNN is presenting human-level performance for many complex intelligent tasks in real-world applications. However, it also introduces ever-increasing security concerns. For example, the emerging adversarial attacks indicate that even very small and often imperceptible adversarial input perturbations can easily mislead the cognitive function of deep learning systems (DLS). Existing DNN adversarial studies are narrowly performed on the ideal software-level DNN models with a focus on single uncertainty factor, i.e. input perturbations, however, the impact of DNN model reshaping on adversarial attacks, which is introduced by various hardware-favorable techniques such as hash-based weight compression during modern DNN hardware implementation, has never been discussed. In this work, we for the first time investigate the multi-factor adversarial attack problem in practical model optimized deep learning systems by jointly considering the DNN model-reshaping (e.g. HashNet based deep compression) and the input perturbations. We first augment adversarial example generating method dedicated to the compressed DNN models by incorporating the software-based approaches and mathematical modeled DNN reshaping. We then conduct a comprehensive robustness and vulnerability analysis of deep compressed DNN models under derived adversarial attacks. A defense technique named &#34;gradient inhibition&#34; is further developed to ease the generating of adversarial examples thus to effectively mitigate adversarial attacks towards both software and hardware-oriented DNNs. Simulation results show that &#34;gradient inhibition&#34; can decrease the average success rate of adversarial attacks from 87.99% to 4.77% (from 86.74% to 4.64%) on MNIST (CIFAR-10) benchmark with marginal accuracy degradation across various DNNs.
ER  -


TY  - Preprint
T1  - Deep Learning and Data Assimilation for Real-Time Production Prediction in Natural Gas Wells
A1  - Kelvin Loh
A1  - Pejman Shoeibi Omrani
A1  - Ruud van der Linden
JO  - ArXiv e-prints
Y1  - 14 February, 2018
UR  - https://arxiv.org/abs/1802.05141
N2  - The prediction of the gas production from mature gas wells, due to their complex end-of-life behavior, is challenging and crucial for operational decision making. In this paper, we apply a modified deep LSTM model for prediction of the gas flow rates in mature gas wells, including the uncertainties in input parameters. Additionally, due to changes in the system in time and in order to increase the accuracy and robustness of the prediction, the Ensemble Kalman Filter (EnKF) is used to update the flow rate predictions based on new observations. The developed approach was tested on the data from two mature gas production wells in which their production is highly dynamic and suffering from salt deposition. The results show that the flow predictions using the EnKF updated model leads to better Jeffreys&#39; J-divergences than the predictions without the EnKF model updating scheme.
ER  -


TY  - Preprint
T1  - L4: Practical loss-based stepsize adaptation for deep learning
A1  - Michal Rolinek
A1  - Georg Martius
JO  - ArXiv e-prints
Y1  - 5 June, 2018
UR  - https://arxiv.org/abs/1802.05074
N2  - We propose a stepsize adaptation scheme for stochastic gradient descent. It operates directly with the loss function and rescales the gradient in order to make fixed predicted progress on the loss. We demonstrate its capabilities by conclusively improving the performance of Adam and Momentum optimizers. The enhanced optimizers with default hyperparameters consistently outperform their constant stepsize counterparts, even the best ones, without a measurable increase in computational cost. The performance is validated on multiple architectures including dense nets, CNNs, ResNets, and the recurrent Differential Neural Computer on classical datasets MNIST, fashion MNIST, CIFAR10 and others.
ER  -


TY  - Preprint
T1  - GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms
A1  - CÃ©dric Colas
A1  - Olivier Sigaud
A1  - Pierre-Yves Oudeyer
JO  - ArXiv e-prints
Y1  - 20 September, 2018
UR  - https://arxiv.org/abs/1802.05054
N2  - In continuous action domains, standard deep reinforcement learning algorithms like DDPG suffer from inefficient exploration when facing sparse or deceptive reward problems. Conversely, evolutionary and developmental methods focusing on exploration like Novelty Search, Quality-Diversity or Goal Exploration Processes explore more robustly but are less efficient at fine-tuning policies using gradient descent. In this paper, we present the GEP-PG approach, taking the best of both worlds by sequentially combining a Goal Exploration Process and two variants of DDPG. We study the learning performance of these components and their combination on a low dimensional deceptive reward problem and on the larger Half-Cheetah benchmark. We show that DDPG fails on the former and that GEP-PG improves over the best DDPG variant in both environments. Supplementary videos and discussion can be found at http://frama.link/gep_pg, the code at http://github.com/flowersteam/geppg.
ER  -


TY  - Preprint
T1  - Molecular Structure Extraction From Documents Using Deep Learning
A1  - Joshua Staker
A1  - Kyle Marshall
A1  - Robert Abel
A1  - Carolyn McQuaw
JO  - ArXiv e-prints
Y1  - 13 February, 2018
UR  - https://arxiv.org/abs/1802.04903
N2  - Chemical structure extraction from documents remains a hard problem due to both false positive identification of structures during segmentation and errors in the predicted structures. Current approaches rely on handcrafted rules and subroutines that perform reasonably well generally, but still routinely encounter situations where recognition rates are not yet satisfactory and systematic improvement is challenging. Complications impacting performance of current approaches include the diversity in visual styles used by various software to render structures, the frequent use of ad hoc annotations, and other challenges related to image quality, including resolution and noise. We here present end-to-end deep learning solutions for both segmenting molecular structures from documents and for predicting chemical structures from these segmented images. This deep learning-based approach does not require any handcrafted features, is learned directly from data, and is robust against variations in image quality and style. Using the deep-learning approach described herein we show that it is possible to perform well on both segmentation and prediction of low resolution images containing moderately sized molecules found in journal articles and patents.
ER  -


TY  - Preprint
T1  - Field-Programmable Deep Neural Network (DNN) Learning and Inference accelerator: a concept
A1  - Luiz M Franca-Neto
JO  - ArXiv e-prints
Y1  - 22 March, 2018
UR  - https://arxiv.org/abs/1802.04899
N2  - An accelerator is a specialized integrated circuit designed to perform specific computations faster than if those were performed by CPU or GPU. A Field-Programmable DNN learning and inference accelerator (FProg-DNN) using hybrid systolic and non-systolic techniques, distributed information-control and deep pipelined structure is proposed and its microarchitecture and operation presented here. Reconfigurability attends diverse DNN designs and allows for different number of workers to be assigned to different layers as a function of the relative difference in computational load among layers. The computational delay per layer is made roughly the same along pipelined accelerator structure. VGG-16 and recently proposed Inception Modules are used for showing the flexibility of the FProg-DNN reconfigurability. Special structures were also added for a combination of convolution layer, map coincidence and feedback for state of the art learning with small set of examples, which is the focus of a companion paper by the author (Franca-Neto, 2018). The accelerator described is able to reconfigure from (1) allocating all a DNN computations to a single worker in one extreme of sub-optimal performance to (2) optimally allocating workers per layer according to computational load in each DNN layer to be realized. Due the pipelined architecture, more than 50x speedup is achieved relative to GPUs or TPUs. This speed-up is consequence of hiding the delay in transporting activation outputs from one layer to the next in a DNN behind the computations in the receiving layer. This FProg-DNN concept has been simulated and validated at behavioral-functional level.
ER  -


TY  - Preprint
T1  - Learning via social awareness: Improving a deep generative sketching model with facial feedback
A1  - Natasha Jaques
A1  - Jennifer McCleary
A1  - Jesse Engel
A1  - David Ha
A1  - Fred Bertsch
A1  - Rosalind Picard
A1  - Douglas Eck
JO  - ArXiv e-prints
Y1  - 27 August, 2018
UR  - https://arxiv.org/abs/1802.04877
N2  - In the quest towards general artificial intelligence (AI), researchers have explored developing loss functions that act as intrinsic motivators in the absence of external rewards. This paper argues that such research has overlooked an important and useful intrinsic motivator: social interaction. We posit that making an AI agent aware of implicit social feedback from humans can allow for faster learning of more generalizable and useful representations, and could potentially impact AI safety. We collect social feedback in the form of facial expression reactions to samples from Sketch RNN, an LSTM-based variational autoencoder (VAE) designed to produce sketch drawings. We use a Latent Constraints GAN (LC-GAN) to learn from the facial feedback of a small group of viewers, by optimizing the model to produce sketches that it predicts will lead to more positive facial expressions. We show in multiple independent evaluations that the model trained with facial feedback produced sketches that are more highly rated, and induce significantly more positive facial expressions. Thus, we establish that implicit social feedback can improve the output of a deep learning model.
ER  -


TY  - Preprint
T1  - TVM: An Automated End-to-End Optimizing Compiler for Deep Learning
A1  - Tianqi Chen
A1  - Thierry Moreau
A1  - Ziheng Jiang
A1  - Lianmin Zheng
A1  - Eddie Yan
A1  - Meghan Cowan
A1  - Haichen Shen
A1  - Leyuan Wang
A1  - Yuwei Hu
A1  - Luis Ceze
A1  - Carlos Guestrin
A1  - Arvind Krishnamurthy
JO  - ArXiv e-prints
Y1  - 5 October, 2018
UR  - https://arxiv.org/abs/1802.04799
N2  - There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM&#39;s ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.
ER  -


TY  - Preprint
T1  - Quantifying Uncertainty in Discrete-Continuous and Skewed Data with Bayesian Deep Learning
A1  - Thomas Vandal
A1  - Evan Kodra
A1  - Jennifer Dy
A1  - Sangram Ganguly
A1  - Ramakrishna Nemani
A1  - Auroop R. Ganguly
JO  - ArXiv e-prints
Y1  - 24 May, 2018
UR  - https://arxiv.org/abs/1802.04742
N2  - Deep Learning (DL) methods have been transforming computer vision with innovative adaptations to other domains including climate change. For DL to pervade Science and Engineering (S&amp;E) applications where risk management is a core component, well-characterized uncertainty estimates must accompany predictions. However, S&amp;E observations and model-simulations often follow heavily skewed distributions and are not well modeled with DL approaches, since they usually optimize a Gaussian, or Euclidean, likelihood loss. Recent developments in Bayesian Deep Learning (BDL), which attempts to capture uncertainties from noisy observations, aleatoric, and from unknown model parameters, epistemic, provide us a foundation. Here we present a discrete-continuous BDL model with Gaussian and lognormal likelihoods for uncertainty quantification (UQ). We demonstrate the approach by developing UQ estimates on `DeepSD&#39;, a super-resolution based DL model for Statistical Downscaling (SD) in climate applied to precipitation, which follows an extremely skewed distribution. We find that the discrete-continuous models outperform a basic Gaussian distribution in terms of predictive accuracy and uncertainty calibration. Furthermore, we find that the lognormal distribution, which can handle skewed distributions, produces quality uncertainty estimates at the extremes. Such results may be important across S&amp;E, as well as other domains such as finance and economics, where extremes are often of significant interest. Furthermore, to our knowledge, this is the first UQ model in SD where both aleatoric and epistemic uncertainties are characterized.
ER  -


TY  - Preprint
T1  - Deep Learning for Decoding of Linear Codes - A Syndrome-Based Approach
A1  - Amir Bennatan
A1  - Yoni Choukroun
A1  - Pavel Kisilev
JO  - ArXiv e-prints
Y1  - 13 February, 2018
UR  - https://arxiv.org/abs/1802.04741
N2  - We present a novel framework for applying deep neural networks (DNN) to soft decoding of linear codes at arbitrary block lengths. Unlike other approaches, our framework allows unconstrained DNN design, enabling the free application of powerful designs that were developed in other contexts. Our method is robust to overfitting that inhibits many competing methods, which follows from the exponentially large number of codewords required for their training. We achieve this by transforming the channel output before feeding it to the network, extracting only the syndrome of the hard decisions and the channel output reliabilities. We prove analytically that this approach does not involve any intrinsic performance penalty, and guarantees the generalization of performance obtained during training. Our best results are obtained using a recurrent neural network (RNN) architecture combined with simple preprocessing by permutation. We provide simulation results that demonstrate performance that sometimes approaches that of the ordered statistics decoding (OSD) algorithm.
ER  -


TY  - Preprint
T1  - Attention-based Deep Multiple Instance Learning
A1  - Maximilian Ilse
A1  - Jakub M. Tomczak
A1  - Max Welling
JO  - ArXiv e-prints
Y1  - 28 June, 2018
UR  - https://arxiv.org/abs/1802.04712
N2  - Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.
ER  -


TY  - Preprint
T1  - Deep Learning with Apache SystemML
A1  - Niketan Pansare
A1  - Michael Dusenberry
A1  - Nakul Jindal
A1  - Matthias Boehm
A1  - Berthold Reinwald
A1  - Prithviraj Sen
JO  - ArXiv e-prints
Y1  - 8 February, 2018
UR  - https://arxiv.org/abs/1802.04647
N2  - Enterprises operate large data lakes using Hadoop and Spark frameworks that (1) run a plethora of tools to automate powerful data preparation/transformation pipelines, (2) run on shared, large clusters to (3) perform many different analytics tasks ranging from model preparation, building, evaluation, and tuning for both machine learning and deep learning. Developing machine/deep learning models on data in such shared environments is challenging. Apache SystemML provides a unified framework for implementing machine learning and deep learning algorithms in a variety of shared deployment scenarios. SystemML&#39;s novel compilation approach automatically generates runtime execution plans for machine/deep learning algorithms that are composed of single-node and distributed runtime operations depending on data and cluster characteristics such as data size, data sparsity, cluster size, and memory configurations, while still exploiting the capabilities of the underlying big data frameworks.
ER  -


TY  - Preprint
T1  - Diversity-Driven Exploration Strategy for Deep Reinforcement Learning
A1  - Zhang-Wei Hong
A1  - Tzu-Yun Shann
A1  - Shih-Yang Su
A1  - Yi-Hsiang Chang
A1  - Chun-Yi Lee
JO  - ArXiv e-prints
Y1  - 13 February, 2018
UR  - https://arxiv.org/abs/1802.04564
N2  - Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive local optima, or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure to the loss function, the proposed methodology significantly enhances an agent&#39;s exploratory behaviors, and thus preventing the policy from being trapped in local optima. We further propose an adaptive scaling method for stabilizing the learning process. Our experimental results in Atari 2600 show that our method outperforms baseline approaches in several tasks in terms of mean scores and exploration efficiency.
ER  -


TY  - Preprint
T1  - Deceiving End-to-End Deep Learning Malware Detectors using Adversarial Examples
A1  - Felix Kreuk
A1  - Assi Barak
A1  - Shir Aviv-Reuven
A1  - Moran Baruch
A1  - Benny Pinkas
A1  - Joseph Keshet
JO  - ArXiv e-prints
Y1  - 13 May, 2018
UR  - https://arxiv.org/abs/1802.04528
N2  - In recent years, deep learning has shown performance breakthroughs in many applications, such as image detection, image segmentation, pose estimation, and speech recognition. However, this comes with a major concern: deep networks have been found to be vulnerable to adversarial examples. Adversarial examples are slightly modified inputs that are intentionally designed to cause a misclassification by the model. In the domains of images and speech, the modifications are so small that they are not seen or heard by humans, but nevertheless greatly affect the classification of the model.
ER  -


TY  - Preprint
T1  - Deep Learning Models Delineates Multiple Nuclear Phenotypes in H&amp;E Stained Histology Sections
A1  - Mina Khoshdeli
A1  - Bahram Parvin
JO  - ArXiv e-prints
Y1  - 14 February, 2018
UR  - https://arxiv.org/abs/1802.04427
N2  - Nuclear segmentation is an important step for profiling aberrant regions of histology sections. However, segmentation is a complex problem as a result of variations in nuclear geometry (e.g., size, shape), nuclear type (e.g., epithelial, fibroblast), and nuclear phenotypes (e.g., vesicular, aneuploidy). The problem is further complicated as a result of variations in sample preparation. It is shown and validated that fusion of very deep convolutional networks overcomes (i) complexities associated with multiple nuclear phenotypes, and (ii) separation of overlapping nuclei. The fusion relies on integrating of networks that learn region- and boundary-based representations. The system has been validated on a diverse set of nuclear phenotypes that correspond to the breast and brain histology sections.
ER  -


TY  - Preprint
T1  - Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation
A1  - Dane Corneil
A1  - Wulfram Gerstner
A1  - Johanni Brea
JO  - ArXiv e-prints
Y1  - 11 June, 2018
UR  - https://arxiv.org/abs/1802.04325
N2  - Modern reinforcement learning algorithms reach super-human performance on many board and video games, but they are sample inefficient, i.e. they typically require significantly more playing experience than humans to reach an equal performance level. To improve sample efficiency, an agent may build a model of the environment and use planning methods to update its policy. In this article we introduce Variational State Tabulation (VaST), which maps an environment with a high-dimensional state space (e.g. the space of visual inputs) to an abstract tabular model. Prioritized sweeping with small backups, a highly efficient planning method, can then be used to update state-action values. We show how VaST can rapidly learn to maximize reward in tasks like 3D navigation and efficiently adapt to sudden changes in rewards or transition probabilities.
ER  -


TY  - Preprint
T1  - One Deep Music Representation to Rule Them All? : A comparative analysis of different representation learning strategies
A1  - Jaehun Kim
A1  - JuliÃ¡n Urbano
A1  - Cynthia C. S. Liem
A1  - Alan Hanjalic
JO  - ArXiv e-prints
Y1  - 13 February, 2018
UR  - https://arxiv.org/abs/1802.04051
N2  - Inspired by the success of deploying deep learning in the fields of Computer Vision and Natural Language Processing, this learning paradigm has also found its way into the field of Music Information Retrieval. In order to benefit from deep learning in an effective, but also efficient manner, deep transfer learning has become a common approach. In this approach, it is possible to reuse the output of a pre-trained neural network as the basis for a new, yet unseen learning task. The underlying hypothesis is that if the initial and new learning tasks show commonalities and are applied to the same type of data (e.g. music audio), the generated deep representation of the data is also informative for the new task. Since, however, most of the networks used to generate deep representations are trained using a single initial learning task, the validity of the above hypothesis is questionable for an arbitrary new learning task. In this paper we present the results of our investigation of what the best ways are to generate deep representations for the data and learning tasks in the music domain. We conducted this investigation via an extensive empirical study that involves multiple learning tasks, as well as multiple deep learning architectures with varying levels of information sharing between tasks, in order to learn music representations. We then validate these representations considering multiple unseen learning tasks for evaluation. The results of our experiments yield several insights on how to approach the design of methods for learning widely deployable deep data representations in the music domain.
ER  -


TY  - Preprint
T1  - Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks
A1  - Daphna Weinshall
A1  - Gad Cohen
A1  - Dan Amir
JO  - ArXiv e-prints
Y1  - 8 June, 2018
UR  - https://arxiv.org/abs/1802.03796
N2  - We provide theoretical investigation of curriculum learning in the context of stochastic gradient descent when optimizing the convex linear regression loss. We prove that the rate of convergence of an ideal curriculum learning method is monotonically increasing with the difficulty of the examples. Moreover, among all equally difficult points, convergence is faster when using points which incur higher loss with respect to the current hypothesis. We then analyze curriculum learning in the context of training a CNN. We describe a method which infers the curriculum by way of transfer learning from another network, pre-trained on a different task. While this approach can only approximate the ideal curriculum, we observe empirically similar behavior to the one predicted by the theory, namely, a significant boost in convergence speed at the beginning of training. When the task is made more difficult, improvement in generalization performance is also observed. Finally, curriculum learning exhibits robustness against unfavorable conditions such as excessive regularization.
ER  -


TY  - Preprint
T1  - Learning Deep Convolutional Networks for Demosaicing
A1  - Nai-Sheng Syu
A1  - Yu-Sheng Chen
A1  - Yung-Yu Chuang
JO  - ArXiv e-prints
Y1  - 11 February, 2018
UR  - https://arxiv.org/abs/1802.03769
N2  - This paper presents a comprehensive study of applying the convolutional neural network (CNN) to solving the demosaicing problem. The paper presents two CNN models that learn end-to-end mappings between the mosaic samples and the original image patches with full information. In the case the Bayer color filter array (CFA) is used, an evaluation with ten competitive methods on popular benchmarks confirms that the data-driven, automatically learned features by the CNN models are very effective. Experiments show that the proposed CNN models can perform equally well in both the sRGB space and the linear space. It is also demonstrated that the CNN model can perform joint denoising and demosaicing. The CNN model is very flexible and can be easily adopted for demosaicing with any CFA design. We train CNN models for demosaicing with three different CFAs and obtain better results than existing methods. With the great flexibility to be coupled with any CFA, we present the first data-driven joint optimization of the CFA design and the demosaicing method using CNN. Experiments show that the combination of the automatically discovered CFA pattern and the automatically devised demosaicing method significantly outperforms the current best demosaicing results. Visual comparisons confirm that the proposed methods reduce more visual artifacts than existing methods. Finally, we show that the CNN model is also effective for the more general demosaicing problem with spatially varying exposure and color and can be used for taking images of higher dynamic ranges with a single shot. The proposed models and the thorough experiments together demonstrate that CNN is an effective and versatile tool for solving the demosaicing problem.
ER  -


TY  - Preprint
T1  - Sample Efficient Deep Reinforcement Learning for Dialogue Systems with Large Action Spaces
A1  - GellÃ©rt Weisz
A1  - PaweÅ Budzianowski
A1  - Pei-Hao Su
A1  - Milica GaÅ¡iÄ
JO  - ArXiv e-prints
Y1  - 11 February, 2018
UR  - https://arxiv.org/abs/1802.03753
N2  - In spoken dialogue systems, we aim to deploy artificial intelligence to build automated dialogue agents that can converse with humans. A part of this effort is the policy optimisation task, which attempts to find a policy describing how to respond to humans, in the form of a function taking the current state of the dialogue and returning the response of the system. In this paper, we investigate deep reinforcement learning approaches to solve this problem. Particular attention is given to actor-critic methods, off-policy reinforcement learning with experience replay, and various methods aimed at reducing the bias and variance of estimators. When combined, these methods result in the previously proposed ACER algorithm that gave competitive results in gaming environments. These environments however are fully observable and have a relatively small action set so in this paper we examine the application of ACER to dialogue policy optimisation. We show that this method beats the current state-of-the-art in deep learning approaches for spoken dialogue systems. This not only leads to a more sample efficient algorithm that can train faster, but also allows us to apply the algorithm in more difficult environments than before. We thus experiment with learning in a very large action space, which has two orders of magnitude more actions than previously considered. We find that ACER trains significantly faster than the current state-of-the-art.
ER  -


TY  - Preprint
T1  - Deep learning with t-exponential Bayesian kitchen sinks
A1  - Harris Partaourides
A1  - Sotirios Chatzis
JO  - ArXiv e-prints
Y1  - 10 February, 2018
UR  - https://arxiv.org/abs/1802.03651
N2  - Bayesian learning has been recently considered as an effective means of accounting for uncertainty in trained deep network parameters. This is of crucial importance when dealing with small or sparse training datasets. On the other hand, shallow models that compute weighted sums of their inputs, after passing them through a bank of arbitrary randomized nonlinearities, have been recently shown to enjoy good test error bounds that depend on the number of nonlinearities. Inspired from these advances, in this paper we examine novel deep network architectures, where each layer comprises a bank of arbitrary nonlinearities, linearly combined using multiple alternative sets of weights. We effect model training by means of approximate inference based on a t-divergence measure; this generalizes the Kullback-Leibler divergence in the context of the t-exponential family of distributions. We adopt the t-exponential family since it can more flexibly accommodate real-world data, that entail outliers and distributions with fat tails, compared to conventional Gaussian model assumptions. We extensively evaluate our approach using several challenging benchmarks, and provide comparative results to related state-of-the-art techniques.
ER  -


TY  - Preprint
T1  - Deep Meta-Learning: Learning to Learn in the Concept Space
A1  - Fengwei Zhou
A1  - Bin Wu
A1  - Zhenguo Li
JO  - ArXiv e-prints
Y1  - 10 February, 2018
UR  - https://arxiv.org/abs/1802.03596
N2  - Few-shot learning remains challenging for meta-learning that learns a learning algorithm (meta-learner) from many related tasks. In this work, we argue that this is due to the lack of a good representation for meta-learning, and propose deep meta-learning to integrate the representation power of deep learning into meta-learning. The framework is composed of three modules, a concept generator, a meta-learner, and a concept discriminator, which are learned jointly. The concept generator, e.g. a deep residual net, extracts a representation for each instance that captures its high-level concept, on which the meta-learner performs few-shot learning, and the concept discriminator recognizes the concepts. By learning to learn in the concept space rather than in the complicated instance space, deep meta-learning can substantially improve vanilla meta-learning, which is demonstrated on various few-shot image recognition problems. For example, on 5-way-1-shot image recognition on CIFAR-100 and CUB-200, it improves Matching Nets from 50.53% and 56.53% to 58.18% and 63.47%, improves MAML from 49.28% and 50.45% to 56.65% and 64.63%, and improves Meta-SGD from 53.83% and 53.34% to 61.62% and 66.95%, respectively.
ER  -


TY  - Preprint
T1  - Generative ScatterNet Hybrid Deep Learning (G-SHDL) Network with Structural Priors for Semantic Image Segmentation
A1  - Amarjot Singh
A1  - Nick Kingsbury
JO  - ArXiv e-prints
Y1  - 13 February, 2018
UR  - https://arxiv.org/abs/1802.03374
N2  - This paper proposes a generative ScatterNet hybrid deep learning (G-SHDL) network for semantic image segmentation. The proposed generative architecture is able to train rapidly from relatively small labeled datasets using the introduced structural priors. In addition, the number of filters in each layer of the architecture is optimized resulting in a computationally efficient architecture. The G-SHDL network produces state-of-the-art classification performance against unsupervised and semi-supervised learning on two image datasets. Advantages of the G-SHDL network over supervised methods are demonstrated with experiments performed on training datasets of reduced size.
ER  -


TY  - Preprint
T1  - Deep Learning for Malicious Flow Detection
A1  - Yun-Chun Chen
A1  - Yu-Jhe Li
A1  - Aragorn Tseng
A1  - Tsungnan Lin
JO  - ArXiv e-prints
Y1  - 9 February, 2018
UR  - https://arxiv.org/abs/1802.03358
N2  - Cyber security has grown up to be a hot issue in recent years. How to identify potential malware becomes a challenging task. To tackle this challenge, we adopt deep learning approaches and perform flow detection on real data. However, real data often encounters an issue of imbalanced data distribution which will lead to a gradient dilution issue. When training a neural network, this problem will not only result in a bias toward the majority class but show the inability to learn from the minority classes. In this paper, we propose an end-to-end trainable Tree-Shaped Deep Neural Network (TSDNN) which classifies the data in a layer-wise manner. To better learn from the minority classes, we propose a Quantity Dependent Backpropagation (QDBP) algorithm which incorporates the knowledge of the disparity between classes. We evaluate our method on an imbalanced data set. Experimental result demonstrates that our approach outperforms the state-of-the-art methods and justifies that the proposed method is able to overcome the difficulty of imbalanced learning. We also conduct a partial flow experiment which shows the feasibility of real-time detection and a zero-shot learning experiment which justifies the generalization capability of deep learning in cyber security.
ER  -


TY  - Preprint
T1  - Triplet-based Deep Similarity Learning for Person Re-Identification
A1  - Wentong Liao
A1  - Michael Ying Yang
A1  - Ni Zhan
A1  - Bodo Rosenhahn
JO  - ArXiv e-prints
Y1  - 9 February, 2018
UR  - https://arxiv.org/abs/1802.03254
N2  - In recent years, person re-identification (re-id) catches great attention in both computer vision community and industry. In this paper, we propose a new framework for person re-identification with a triplet-based deep similarity learning using convolutional neural networks (CNNs). The network is trained with triplet input: two of them have the same class labels and the other one is different. It aims to learn the deep feature representation, with which the distance within the same class is decreased, while the distance between the different classes is increased as much as possible. Moreover, we trained the model jointly on six different datasets, which differs from common practice - one model is just trained on one dataset and tested also on the same one. However, the enormous number of possible triplet data among the large number of training samples makes the training impossible. To address this challenge, a double-sampling scheme is proposed to generate triplets of images as effective as possible. The proposed framework is evaluated on several benchmark datasets. The experimental results show that, our method is effective for the task of person re-identification and it is comparable or even outperforms the state-of-the-art methods.
ER  -


TY  - Preprint
T1  - URLNet: Learning a URL Representation with Deep Learning for Malicious URL Detection
A1  - Hung Le
A1  - Quang Pham
A1  - Doyen Sahoo
A1  - Steven C. H. Hoi
JO  - ArXiv e-prints
Y1  - 1 March, 2018
UR  - https://arxiv.org/abs/1802.03162
N2  - Malicious URLs host unsolicited content and are used to perpetrate cybercrimes. It is imperative to detect them in a timely manner. Traditionally, this is done through the usage of blacklists, which cannot be exhaustive, and cannot detect newly generated malicious URLs. To address this, recent years have witnessed several efforts to perform Malicious URL Detection using Machine Learning. The most popular and scalable approaches use lexical properties of the URL string by extracting Bag-of-words like features, followed by applying machine learning models such as SVMs. There are also other features designed by experts to improve the prediction performance of the model. These approaches suffer from several limitations: (i) Inability to effectively capture semantic meaning and sequential patterns in URL strings; (ii) Requiring substantial manual feature engineering; and (iii) Inability to handle unseen features and generalize to test data. To address these challenges, we propose URLNet, an end-to-end deep learning framework to learn a nonlinear URL embedding for Malicious URL Detection directly from the URL. Specifically, we apply Convolutional Neural Networks to both characters and words of the URL String to learn the URL embedding in a jointly optimized framework. This approach allows the model to capture several types of semantic information, which was not possible by the existing models. We also propose advanced word-embeddings to solve the problem of too many rare words observed in this task. We conduct extensive experiments on a large-scale dataset and show a significant performance gain over existing methods. We also conduct ablation studies to evaluate the performance of various components of URLNet.
ER  -


TY  - Preprint
T1  - PoTrojan: powerful neural-level trojan designs in deep learning models
A1  - Minhui Zou
A1  - Yang Shi
A1  - Chengliang Wang
A1  - Fangyu Li
A1  - WenZhan Song
A1  - Yu Wang
JO  - ArXiv e-prints
Y1  - 8 February, 2018
UR  - https://arxiv.org/abs/1802.03043
N2  - With the popularity of deep learning (DL), artificial intelligence (AI) has been applied in many areas of human life. Neural network or artificial neural network (NN), the main technique behind DL, has been extensively studied to facilitate computer vision and natural language recognition. However, the more we rely on information technology, the more vulnerable we are. That is, malicious NNs could bring huge threat in the so-called coming AI era. In this paper, for the first time in the literature, we propose a novel approach to design and insert powerful neural-level trojans or PoTrojan in pre-trained NN models. Most of the time, PoTrojans remain inactive, not affecting the normal functions of their host NN models. PoTrojans could only be triggered in very rare conditions. Once activated, however, the PoTrojans could cause the host NN models to malfunction, either falsely predicting or classifying, which is a significant threat to human society of the AI era. We would explain the principles of PoTrojans and the easiness of designing and inserting them in pre-trained deep learning models. PoTrojans doesn&#39;t modify the existing architecture or parameters of the pre-trained models, without re-training. Hence, the proposed method is very efficient.
ER  -


TY  - Preprint
T1  - TSViz: Demystification of Deep Learning Models for Time-Series Analysis
A1  - Shoaib Ahmed Siddiqui
A1  - Dominik Mercier
A1  - Mohsin Munir
A1  - Andreas Dengel
A1  - Sheraz Ahmed
JO  - ArXiv e-prints
Y1  - 8 February, 2018
UR  - https://arxiv.org/abs/1802.02952
N2  - This paper presents a novel framework for demystification of convolutional deep learning models for time series analysis. This is a step towards making informed/explainable decisions in the domain of time series, powered by deep learning. There have been numerous efforts to increase the interpretability of image-centric deep neural network models, where the learned features are more intuitive to visualize. Visualization in time-series is much more complicated as there is no direct interpretation of the filters and inputs as compared to image modality. In addition, little or no concentration has been devoted for the development of such tools in the domain of time-series in the past. The visualization engine of the presented framework provides possibilities to explore and analyze a network from different dimensions at four different levels of abstraction. This enables the user to uncover different aspects of the model which includes important filters, filter clusters, and input saliency maps. These representations allow to understand the network features so that the acceptability of deep networks for time-series data can be enhanced. This is extremely important in domains like finance, industry 4.0, self-driving cars, health-care, counter-terrorism etc., where reasons for reaching a particular prediction are equally important as the prediction itself. The framework \footnote{Framework download link: https://hidden.for.blind.review} can also aid in discovery of the filters which are contributing nothing to the final prediction, hence, can be pruned without any significant loss in performance.
ER  -


TY  - Preprint
T1  - A Deep Unsupervised Learning Approach Toward MTBI Identification Using Diffusion MRI
A1  - Shervin Minaee
A1  - Yao Wang
A1  - Anna Choromanska
A1  - Sohae Chung
A1  - Xiuyuan Wang
A1  - Els Fieremans
A1  - Steven Flanagan
A1  - Joseph Rath
A1  - Yvonne W Lui
JO  - ArXiv e-prints
Y1  - 11 April, 2018
UR  - https://arxiv.org/abs/1802.02925
N2  - Mild traumatic brain injury is a growing public health problem with an estimated incidence of over 1.7 million people annually in US. Diagnosis is based on clinical history and symptoms, and accurate, concrete measures of injury are lacking. This work aims to directly use diffusion MR images obtained within one month of trauma to detect injury, by incorporating deep learning techniques. To overcome the challenge due to limited training data, we describe each brain region using the bag of word representation, which specifies the distribution of representative patch patterns. We apply a convolutional auto-encoder to learn the patch-level features, from overlapping image patches extracted from the MR images, to learn features from diffusion MR images of brain using an unsupervised approach. Our experimental results show that the bag of word representation using patch level features learnt by the auto encoder provides similar performance as that using the raw patch patterns, both significantly outperform earlier work relying on the mean values of MR metrics in selected brain regions.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Image Hashing
A1  - Jian Zhang
A1  - Yuxin Peng
A1  - Zhaoda Ye
JO  - ArXiv e-prints
Y1  - 7 February, 2018
UR  - https://arxiv.org/abs/1802.02904
N2  - Deep hashing methods have received much attention recently, which achieve promising results by taking advantage of the strong representation power of deep networks. However, most existing deep hashing methods learn a whole set of hashing functions independently and directly, while ignore the correlation between different hashing functions that can promote the retrieval accuracy greatly. Inspired by the sequential decision ability of deep reinforcement learning, in this paper, we propose a new Deep Reinforcement Learning approach for Image Hashing (DRLIH). Our proposed DRLIH models the hashing learning problem as a Markov Decision Process (MDP), which learns each hashing function by correcting the errors imposed by previous ones and promotes retrieval accuracy. To the best of our knowledge, this is the first work that tries to address hashing problem from deep reinforcement learning perspective. The main contributions of our proposed DRLIH approach can be summarized as follows: (1) We propose a deep reinforcement learning hashing network. In our proposed DRLIH approach, we utilize recurrent neural network (RNN) as agents to model the hashing functions, which take actions of projecting images into binary codes sequentially, so that current hashing function learning can take previous hashing functions&#39; error into account. (2) We propose a sequential learning strategy based on proposed DRLIH. We define the state as a tuple of internal features of RNN&#39;s hidden layers and image features, which can well reflect history decisions made by the agents. We also propose an action group method to enhance the correlation of the hash functions in the same group. Experiments on three widely-used datasets demonstrate the effectiveness of our proposed DRLIH approach.
ER  -


TY  - Preprint
T1  - Polisis: Automated Analysis and Presentation of Privacy Policies Using Deep Learning
A1  - Hamza Harkous
A1  - Kassem Fawaz
A1  - RÃ©mi Lebret
A1  - Florian Schaub
A1  - Kang G. Shin
A1  - Karl Aberer
JO  - ArXiv e-prints
Y1  - 29 June, 2018
UR  - https://arxiv.org/abs/1802.02561
N2  - Privacy policies are the primary channel through which companies inform users about their data collection and sharing practices. These policies are often long and difficult to comprehend. Short notices based on information extracted from privacy policies have been shown to be useful but face a significant scalability hurdle, given the number of policies and their evolution over time. Companies, users, researchers, and regulators still lack usable and scalable tools to cope with the breadth and depth of privacy policies. To address these hurdles, we propose an automated framework for privacy policy analysis (Polisis). It enables scalable, dynamic, and multi-dimensional queries on natural language privacy policies. At the core of Polisis is a privacy-centric language model, built with 130K privacy policies, and a novel hierarchy of neural-network classifiers that accounts for both high-level aspects and fine-grained details of privacy practices. We demonstrate Polisis&#39; modularity and utility with two applications supporting structured and free-form querying. The structured querying application is the automated assignment of privacy icons from privacy policies. With Polisis, we can achieve an accuracy of 88.4% on this task. The second application, PriBot, is the first freeform question-answering system for privacy policies. We show that PriBot can produce a correct answer among its top-3 results for 82% of the test questions. Using an MTurk user study with 700 participants, we show that at least one of PriBot&#39;s top-3 answers is relevant to users for 89% of the test questions.
ER  -


TY  - Preprint
T1  - A Spatial Mapping Algorithm with Applications in Deep Learning-Based Structure Classification
A1  - Thomas Corcoran
A1  - Rafael Zamora-Resendiz
A1  - Xinlian Liu
A1  - Silvia Crivelli
JO  - ArXiv e-prints
Y1  - 22 February, 2018
UR  - https://arxiv.org/abs/1802.02532
N2  - Convolutional Neural Network (CNN)-based machine learning systems have made breakthroughs in feature extraction and image recognition tasks in two dimensions (2D). Although there is significant ongoing work to apply CNN technology to domains involving complex 3D data, the success of such efforts has been constrained, in part, by limitations in data representation techniques. Most current approaches rely upon low-resolution 3D models, strategic limitation of scope in the 3D space, or the application of lossy projection techniques to allow for the use of 2D CNNs. To address this issue, we present a mapping algorithm that converts 3D structures to 2D and 1D data grids by mapping a traversal of a 3D space-filling curve to the traversal of corresponding 2D and 1D curves. We explore the performance of 2D and 1D CNNs trained on data encoded with our method versus comparable volumetric CNNs operating upon raw 3D data from a popular benchmarking dataset. Our experiments demonstrate that both 2D and 1D representations of 3D data generated via our method preserve a significant proportion of the 3D data&#39;s features in forms learnable by CNNs. Furthermore, we demonstrate that our method of encoding 3D data into lower-dimensional representations allows for decreased CNN training time cost, increased original 3D model rendering resolutions, and supports increased numbers of data channels when compared to purely volumetric approaches. This demonstration is accomplished in the context of a structural biology classification task wherein we train 3D, 2D, and 1D CNNs on examples of two homologous branches within the Ras protein family. The essential contribution of this paper is the introduction of a dimensionality-reduction method that may ease the application of powerful deep learning tools to domains characterized by complex structural data.
ER  -


TY  - Preprint
T1  - Evaluation of Deep Reinforcement Learning Methods for Modular Robots
A1  - Risto Kojcev
A1  - Nora Etxezarreta
A1  - Alejandro HernÃ¡ndez
A1  - VÃ­ctor Mayoral
JO  - ArXiv e-prints
Y1  - 7 February, 2018
UR  - https://arxiv.org/abs/1802.02395
N2  - We propose a novel framework for Deep Reinforcement Learning (DRL) in modular robotics using traditional robotic tools that extend state-of-the-art DRL implementations and provide an end-to-end approach which trains a robot directly from joint states. Moreover, we present a novel technique to transfer these DLR methods into the real robot, aiming to close the simulation-reality gap. We demonstrate the robustness of the performance of state-of-the-art DRL methods for continuous action spaces in modular robots, with an empirical study both in simulation and in the real robot where we also evaluate how accelerating the simulation time affects the robot&#39;s performance. Our results show that extending the modular robot from 3 degrees-of-freedom (DoF), to 4 DoF, does not affect the robot&#39;s learning. This paves the way towards training modular robots using DRL techniques.
ER  -


TY  - Preprint
T1  - A Novel Co-design Peta-scale Heterogeneous Cluster for Deep Learning Training
A1  - Xin Chen
A1  - Hua Zhou
A1  - Yuxiang Gao
A1  - Yu Zhu
JO  - ArXiv e-prints
Y1  - 18 May, 2018
UR  - https://arxiv.org/abs/1802.02326
N2  - Large scale deep Convolution Neural Networks (CNNs) increasingly demands the computing power. It is key for researchers to own a great powerful computing platform to leverage deep learning (DL) advancing.On the other hand, as the commonly-used accelerator, the commodity GPUs cards of new generations are more and more expensive. Consequently, it is of importance to design an affordable distributed heterogeneous system that provides powerful computational capacity and develop a well-suited software that efficiently utilizes its computational capacity. In this paper, we present our co-design distributed system including a peta-scale GPU cluster, called &#34;Manoa&#34;. Based on properties and topology of Manoa, we first propose job server framework and implement it, named &#34;MiMatrix&#34;. The central node of MiMatrix, referred to as the job server, undertakes all of controlling, scheduling and monitoring, and I/O tasks without weight data transfer for AllReduce processing in each iteration. Therefore, MiMatrix intrinsically solves the bandwidth bottleneck of central node in parameter server framework that is widely used in distributed DL tasks. Meanwhile, we also propose a new AllReduce algorithm, GPUDirect RDMA-Aware AllReduce~(GDRAA), in which both computation and handshake message are O(1) and the number of synchronization is two in each iteration that is a theoretical minimum number. Owe to the dedicated co-design distributed system, MiMatrix efficiently makes use of the Manoa&#39;s computational capacity and bandwidth. We benchmark Manoa Resnet50 and Resenet101 on Imagenet-1K dataset. Some of results have demonstrated state-of-the-art.
ER  -


TY  - Preprint
T1  - An Empirical Evaluation of Deep Learning for ICD-9 Code Assignment using MIMIC-III Clinical Notes
A1  - Jinmiao Huang
A1  - Cesar Osorio
A1  - Luke Wicent Sy
JO  - ArXiv e-prints
Y1  - 7 February, 2018
UR  - https://arxiv.org/abs/1802.02311
N2  - Code assignment is important on many levels in the modern hospital, from ensuring accurate billing process to creating a valid record of patient care history. However, the coding process is tedious, subjective, and requires medical coders with extensive training. The objective of this study is to evaluate the performance of deep learning based systems to automatically map clinical notes to medical codes. We applied the state-of-the-art deep learning methods such as Recurrent Neural Networks and Convolution Neural Networks on MIMIC-III dataset. Experiments show that the deep-learning-based methods outperform other conventional machine learning methods. Our evaluations are focused on end-to-end learning methods without manually defined rules. From our evaluations, the best models are able to predict the top 10 ICD-9 codes with 69.57% F1 and 89.67% accuracy; the top 10 ICD-9 categories with 72.33% F1 and 85.88% accuracy.
ER  -


TY  - Preprint
T1  - A Critical Investigation of Deep Reinforcement Learning for Navigation
A1  - Vikas Dhiman
A1  - Shurjo Banerjee
A1  - Brent Griffin
A1  - Jeffrey M Siskind
A1  - Jason J Corso
JO  - ArXiv e-prints
Y1  - 6 February, 2018
UR  - https://arxiv.org/abs/1802.02274
N2  - The navigation problem is classically approached in two steps: an exploration step, where map-information about the environment is gathered; and an exploitation step, where this information is used to navigate efficiently. Deep reinforcement learning (DRL) algorithms, alternatively, approach the problem of navigation in an end-to-end fashion. Inspired by the classical approach, we ask whether DRL algorithms are able to inherently explore, gather and exploit map-information over the course of navigation. We build upon Mirowski et al. [2017] work and introduce a systematic suite of experiments that vary three parameters: the agent&#39;s starting location, the agent&#39;s target location, and the maze structure. We choose evaluation metrics that explicitly measure the algorithm&#39;s ability to gather and exploit map-information. Our experiments show that when trained and tested on the same maps, the algorithm successfully gathers and exploits map-information. However, when trained and tested on different sets of maps, the algorithm fails to transfer the ability to gather and exploit map-information to unseen maps. Furthermore, we find that when the goal location is randomized and the map is kept static, the algorithm is able to gather and exploit map-information but the exploitation is far from optimal. We open-source our experimental suite in the hopes that it serves as a framework for the comparison of future algorithms and leads to the discovery of robust alternatives to classical navigation methods.
ER  -


TY  - Preprint
T1  - Object Detection on Dynamic Occupancy Grid Maps Using Deep Learning and Automatic Label Generation
A1  - Stefan Hoermann
A1  - Philipp Henzler
A1  - Martin Bach
A1  - Klaus Dietmayer
JO  - ArXiv e-prints
Y1  - 30 January, 2018
UR  - https://arxiv.org/abs/1802.02202
N2  - We tackle the problem of object detection and pose estimation in a shared space downtown environment. For perception multiple laser scanners with 360Â° coverage were fused in a dynamic occupancy grid map (DOGMa). A single-stage deep convolutional neural network is trained to provide object hypotheses comprising of shape, position, orientation and an existence score from a single input DOGMa. Furthermore, an algorithm for offline object extraction was developed to automatically label several hours of training data. The algorithm is based on a two-pass trajectory extraction, forward and backward in time. Typical for engineered algorithms, the automatic label generation suffers from misdetections, which makes hard negative mining impractical. Therefore, we propose a loss function counteracting the high imbalance between mostly static background and extremely rare dynamic grid cells. Experiments indicate, that the trained network has good generalization capabilities since it detects objects occasionally lost by the label algorithm. Evaluation reaches an average precision (AP) of 75.9%
ER  -


TY  - Preprint
T1  - A Deep Reinforcement Learning Based Approach for Cost- and Energy-Aware Multi-Flow Mobile Data Offloading
A1  - Cheng Zhang
A1  - Zhi Liu
A1  - Bo Gu
A1  - Kyoko Yamori
A1  - Yoshiaki Tanaka
JO  - ArXiv e-prints
Y1  - 29 January, 2018
UR  - https://arxiv.org/abs/1802.01970
N2  - With the rapid increase in demand for mobile data, mobile network operators are trying to expand wireless network capacity by deploying wireless local area network (LAN) hotspots on to which they can offload their mobile traffic. However, these network-centric methods usually do not fulfill the interests of mobile users (MUs). Taking into consideration many issues such as different applications&#39; deadlines, monetary cost and energy consumption, how the MU decides whether to offload their traffic to a complementary wireless LAN is an important issue. Previous studies assume the MU&#39;s mobility pattern is known in advance, which is not always true. In this paper, we study the MU&#39;s policy to minimize his monetary cost and energy consumption without known MU mobility pattern. We propose to use a kind of reinforcement learning technique called deep Q-network (DQN) for MU to learn the optimal offloading policy from past experiences. In the proposed DQN based offloading algorithm, MU&#39;s mobility pattern is no longer needed. Furthermore, MU&#39;s state of remaining data is directly fed into the convolution neural network in DQN without discretization. Therefore, not only does the discretization error present in previous work disappear, but also it makes the proposed algorithm has the ability to generalize the past experiences, which is especially effective when the number of states is large. Extensive simulations are conducted to validate our proposed offloading algorithms.
ER  -


TY  - Preprint
T1  - Shared Autonomy via Deep Reinforcement Learning
A1  - Siddharth Reddy
A1  - Anca D. Dragan
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 22 May, 2018
UR  - https://arxiv.org/abs/1802.01744
N2  - In shared autonomy, user input is combined with semi-autonomous control to achieve a common goal. The goal is often unknown ex-ante, so prior work enables agents to infer the goal from user input and assist with the task. Such methods tend to assume some combination of knowledge of the dynamics of the environment, the user&#39;s policy given their goal, and the set of possible goals the user might target, which limits their application to real-world scenarios. We propose a deep reinforcement learning framework for model-free shared autonomy that lifts these assumptions. We use human-in-the-loop reinforcement learning with neural network function approximation to learn an end-to-end mapping from environmental observation and user input to agent action values, with task reward as the only form of supervision. This approach poses the challenge of following user commands closely enough to provide the user with real-time action feedback and thereby ensure high-quality user input, but also deviating from the user&#39;s actions when they are suboptimal. We balance these two needs by discarding actions whose values fall below some threshold, then selecting the remaining action closest to the user&#39;s input. Controlled studies with users (n = 12) and synthetic pilots playing a video game, and a pilot study with users (n = 4) flying a real quadrotor, demonstrate the ability of our algorithm to assist users with real-time control tasks in which the agent cannot directly access the user&#39;s private information through observations, but receives a reward signal and user input that both depend on the user&#39;s intent. The agent learns to assist the user without access to this private information, implicitly inferring it from the user&#39;s input. This paper is a proof of concept that illustrates the potential for deep reinforcement learning to enable flexible and practical assistive systems.
ER  -


TY  - Preprint
T1  - Compressive Light Field Reconstructions using Deep Learning
A1  - Mayank Gupta
A1  - Arjun Jauhari
A1  - Kuldeep Kulkarni
A1  - Suren Jayasuriya
A1  - Alyosha Molnar
A1  - Pavan Turaga
JO  - ArXiv e-prints
Y1  - 5 February, 2018
UR  - https://arxiv.org/abs/1802.01722
N2  - Light field imaging is limited in its computational processing demands of high sampling for both spatial and angular dimensions. Single-shot light field cameras sacrifice spatial resolution to sample angular viewpoints, typically by multiplexing incoming rays onto a 2D sensor array. While this resolution can be recovered using compressive sensing, these iterative solutions are slow in processing a light field. We present a deep learning approach using a new, two branch network architecture, consisting jointly of an autoencoder and a 4D CNN, to recover a high resolution 4D light field from a single coded 2D image. This network decreases reconstruction time significantly while achieving average PSNR values of 26-32 dB on a variety of light fields. In particular, reconstruction time is decreased from 35 minutes to 6.7 minutes as compared to the dictionary method for equivalent visual quality. These reconstructions are performed at small sampling/compression ratios as low as 8%, allowing for cheaper coded light field cameras. We test our network reconstructions on synthetic light fields, simulated coded measurements of real light fields captured from a Lytro Illum camera, and real coded images from a custom CMOS diffractive light field camera. The combination of compressive light field capture with deep learning allows the potential for real-time light field video acquisition systems in the future.
ER  -


TY  - Preprint
T1  - Deep Learning with a Rethinking Structure for Multi-label Classification
A1  - Yao-Yuan Yang
A1  - Yi-An Lin
A1  - Hong-Min Chu
A1  - Hsuan-Tien Lin
JO  - ArXiv e-prints
Y1  - 5 February, 2018
UR  - https://arxiv.org/abs/1802.01697
N2  - Multi-label classification (MLC) is an important learning problem that expects the learning algorithm to take the hidden correlation of the labels into account. Extracting the hidden correlation is generally a challenging task. In this work, we propose a novel deep learning framework to better extract the hidden correlation with the help of the memory structure within recurrent neural networks. The memory stores the temporary guesses on the labels and effectively allows the framework to rethink about the goodness and correlation of the guesses before making the final prediction. Furthermore, the rethinking process makes it easy to adapt to different evaluation criterion to match real-world application needs. Experimental results across many real-world data sets justify that the rethinking process indeed improves MLC performance across different evaluation criteria and leads to superior performance over state-of-the-art MLC algorithms.
ER  -


TY  - Preprint
T1  - Re-Weighted Learning for Sparsifying Deep Neural Networks
A1  - Igor Fedorov
A1  - Bhaskar D. Rao
JO  - ArXiv e-prints
Y1  - 5 February, 2018
UR  - https://arxiv.org/abs/1802.01616
N2  - This paper addresses the topic of sparsifying deep neural networks (DNN&#39;s). While DNN&#39;s are powerful models that achieve state-of-the-art performance on a large number of tasks, the large number of model parameters poses serious storage and computational challenges. To combat these difficulties, a growing line of work focuses on pruning network weights without sacrificing performance. We propose a general affine scaling transformation (AST) algorithm to sparsify DNN&#39;s. Our approach follows in the footsteps of popular sparse recovery techniques, which have yet to be explored in the context of DNN&#39;s. We describe a principled framework for transforming densely connected DNN&#39;s into sparsely connected ones without sacrificing network performance. Unlike existing methods, our approach is able to learn sparse connections at each layer simultaneously, and achieves comparable pruning results on the architecture tested.
ER  -


TY  - Preprint
T1  - The Matrix Calculus You Need For Deep Learning
A1  - Terence Parr
A1  - Jeremy Howard
JO  - ArXiv e-prints
Y1  - 2 July, 2018
UR  - https://arxiv.org/abs/1802.01528
N2  - This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don&#39;t worry if you get stuck at some point along the way---just go back and reread the previous section, and try writing down and working through some examples. And if you&#39;re still stuck, we&#39;re happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here. See related articles at http://explained.ai
ER  -


TY  - Preprint
T1  - Deep Learning-based Channel Estimation for Beamspace mmWave Massive MIMO Systems
A1  - Hengtao He
A1  - Chao-Kai Wen
A1  - Shi Jin
A1  - Geoffrey Ye Li
JO  - ArXiv e-prints
Y1  - 5 February, 2018
UR  - https://arxiv.org/abs/1802.01290
N2  - Channel estimation is very challenging when the receiver is equipped with a limited number of radio-frequency (RF) chains in beamspace millimeter-wave (mmWave) massive multiple-input and multiple-output systems. To solve this problem, we exploit a learned denoising-based approximate message passing (LDAMP) network. This neural network can learn channel structure and estimate channel from a large number of training data. Furthermore, we provide an analytical framework on the asymptotic performance of the channel estimator. Based on our analysis and simulation results, the LDAMP neural network significantly outperforms state-of-the-art compressed sensingbased algorithms even when the receiver is equipped with a small number of RF chains. Therefore, deep learning is a powerful tool for channel estimation in mmWave communications.
ER  -


TY  - Preprint
T1  - Dream Formulations and Deep Neural Networks: Humanistic Themes in the Iconology of the Machine-Learned Image
A1  - Emily L. Spratt
JO  - ArXiv e-prints
Y1  - 5 February, 2018
UR  - https://arxiv.org/abs/1802.01274
N2  - This paper addresses the interpretability of deep learning-enabled image recognition processes in computer vision science in relation to theories in art history and cognitive psychology on the vision-related perceptual capabilities of humans. Examination of what is determinable about the machine-learned image in comparison to humanistic theories of visual perception, particularly in regard to art historian Erwin Panofsky&#39;s methodology for image analysis and psychologist Eleanor Rosch&#39;s theory of graded categorization according to prototypes, finds that there are surprising similarities between the two that suggest that researchers in the arts and the sciences would have much to benefit from closer collaborations. Utilizing the examples of Google&#39;s DeepDream and the Machine Learning and Perception Lab at Georgia Tech&#39;s Grad-CAM: Gradient-weighted Class Activation Mapping programs, this study suggests that a revival of art historical research in iconography and formalism in the age of AI is essential for shaping the future navigation and interpretation of all machine-learned images, given the rapid developments in image recognition technologies.
ER  -


TY  - Preprint
T1  - Deep Temporal Clustering : Fully Unsupervised Learning of Time-Domain Features
A1  - Naveen Sai Madiraju
A1  - Seid M. Sadat
A1  - Dimitry Fisher
A1  - Homa Karimabadi
JO  - ArXiv e-prints
Y1  - 3 February, 2018
UR  - https://arxiv.org/abs/1802.01059
N2  - Unsupervised learning of time series data, also known as temporal clustering, is a challenging problem in machine learning. Here we propose a novel algorithm, Deep Temporal Clustering (DTC), to naturally integrate dimensionality reduction and temporal clustering into a single end-to-end learning framework, fully unsupervised. The algorithm utilizes an autoencoder for temporal dimensionality reduction and a novel temporal clustering layer for cluster assignment. Then it jointly optimizes the clustering objective and the dimensionality reduction objec tive. Based on requirement and application, the temporal clustering layer can be customized with any temporal similarity metric. Several similarity metrics and state-of-the-art algorithms are considered and compared. To gain insight into temporal features that the network has learned for its clustering, we apply a visualization method that generates a region of interest heatmap for the time series. The viability of the algorithm is demonstrated using time series data from diverse domains, ranging from earthquakes to spacecraft sensor data. In each case, we show that the proposed algorithm outperforms traditional methods. The superior performance is attributed to the fully integrated temporal dimensionality reduction and clustering criterion.
ER  -


TY  - Preprint
T1  - Deep Learning Framework for Multi-class Breast Cancer Histology Image Classification
A1  - Yeeleng S. Vang
A1  - Zhen Chen
A1  - Xiaohui Xie
JO  - ArXiv e-prints
Y1  - 3 February, 2018
UR  - https://arxiv.org/abs/1802.00931
N2  - In this work, we present a deep learning framework for multi-class breast cancer image classification as our submission to the International Conference on Image Analysis and Recognition (ICIAR) 2018 Grand Challenge on BreAst Cancer Histology images (BACH). As these histology images are too large to fit into GPU memory, we first propose using Inception V3 to perform patch level classification. The patch level predictions are then passed through an ensemble fusion framework involving majority voting, gradient boosting machine (GBM), and logistic regression to obtain the image level prediction. We improve the sensitivity of the Normal and Benign predicted classes by designing a Dual Path Network (DPN) to be used as a feature extractor where these extracted features are further sent to a second layer of ensemble prediction fusion using GBM, logistic regression, and support vector machine (SVM) to refine predictions. Experimental results demonstrate our framework shows a 12.5$\%$ improvement over the state-of-the-art model.
ER  -


TY  - Preprint
T1  - Handwritten Isolated Bangla Compound Character Recognition: a new benchmark using a novel deep learning approach
A1  - Saikat Roy
A1  - Nibaran Das
A1  - Mahantapas Kundu
A1  - Mita Nasipuri
JO  - ArXiv e-prints
Y1  - 2 February, 2018
UR  - https://arxiv.org/abs/1802.00671
N2  - In this work, a novel deep learning technique for the recognition of handwritten Bangla isolated compound character is presented and a new benchmark of recognition accuracy on the CMATERdb 3.1.3.3 dataset is reported. Greedy layer wise training of Deep Neural Network has helped to make significant strides in various pattern recognition problems. We employ layerwise training to Deep Convolutional Neural Networks (DCNN) in a supervised fashion and augment the training process with the RMSProp algorithm to achieve faster convergence. We compare results with those obtained from standard shallow learning methods with predefined features, as well as standard DCNNs. Supervised layerwise trained DCNNs are found to outperform standard shallow learning models such as Support Vector Machines as well as regular DCNNs of similar architecture by achieving error rate of 9.67% thereby setting a new benchmark on the CMATERdb 3.1.3.3 with recognition accuracy of 90.33%, representing an improvement of nearly 10%.
ER  -


TY  - Preprint
T1  - Visual Interpretability for Deep Learning: a Survey
A1  - Quanshi Zhang
A1  - Song-Chun Zhu
JO  - ArXiv e-prints
Y1  - 7 February, 2018
UR  - https://arxiv.org/abs/1802.00614
N2  - This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, the interpretability is always the Achilles&#39; heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of low interpretability of their black-box representations. We believe that high model interpretability may help people to break several bottlenecks of deep learning, e.g., learning from very few annotations, learning via human-computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and we revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.
ER  -


TY  - Preprint
T1  - Detecting Zones and Threat on 3D Body for Security in Airports using Deep Machine Learning
A1  - Abel Ag Rb Guimaraes
A1  - Ghassem Tofighi
JO  - ArXiv e-prints
Y1  - 9 February, 2018
UR  - https://arxiv.org/abs/1802.00565
N2  - In this research, it was used a segmentation and classification method to identify threat recognition in human scanner images of airport security. The Department of Homeland Security&#39;s (DHS) in USA has a higher false alarm, produced from theirs algorithms using today&#39;s scanners at the airports. To repair this problem they started a new competition at Kaggle site asking the science community to improve their detection with new algorithms. The dataset used in this research comes from DHS at https://www.kaggle.com/c/passenger-screening-algorithm-challenge/data According to DHS: &#34;This dataset contains a large number of body scans acquired by a new generation of millimeter wave scanner called the High Definition-Advanced Imaging Technology (HD-AIT) system. They are comprised of volunteers wearing different clothing types (from light summer clothes to heavy winter clothes), different body mass indices, different genders, different numbers of threats, and different types of threats&#34;. Using Python as a principal language, the preprocessed of the dataset images extracted features from 200 bodies using: intensity, intensity differences and local neighbourhood to detect, to produce segmentation regions and label those regions to be used as a truth in a training and test dataset. The regions are subsequently give to a CNN deep learning classifier to predict 17 classes (that represents the body zones): zone1, zone2, ... zone17 and zones with threat in a total of 34 zones. The analysis showed the results of the classifier an accuracy of 98.2863% and a loss of 0.091319, as well as an average of 100% for recall and precision.
ER  -


TY  - Preprint
T1  - Interpretable Deep Convolutional Neural Networks via Meta-learning
A1  - Xuan Liu
A1  - Xiaoguang Wang
A1  - Stan Matwin
JO  - ArXiv e-prints
Y1  - 18 August, 2018
UR  - https://arxiv.org/abs/1802.00560
N2  - Model interpretability is a requirement in many applications in which crucial decisions are made by users relying on a model&#39;s outputs. The recent movement for &#34;algorithmic fairness&#34; also stipulates explainability, and therefore interpretability of learning models. And yet the most successful contemporary Machine Learning approaches, the Deep Neural Networks, produce models that are highly non-interpretable. We attempt to address this challenge by proposing a technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN) via meta-learning. In this work, we interpret a specific hidden layer of the deep CNN model on the MNIST image dataset. We use a clustering algorithm in a two-level structure to find the meta-level training data and Random Forest as base learning algorithms to generate the meta-level test data. The interpretation results are displayed visually via diagrams, which clearly indicates how a specific test instance is classified. Our method achieves global interpretation for all the test instances without sacrificing the accuracy obtained by the original deep CNN model. This means our model is faithful to the deep CNN model, which leads to reliable interpretations.
ER  -


TY  - Preprint
T1  - Causal Learning and Explanation of Deep Neural Networks via Autoencoded Activations
A1  - Michael Harradon
A1  - Jeff Druce
A1  - Brian Ruttenberg
JO  - ArXiv e-prints
Y1  - 1 February, 2018
UR  - https://arxiv.org/abs/1802.00541
N2  - Deep neural networks are complex and opaque. As they enter application in a variety of important and safety critical domains, users seek methods to explain their output predictions. We develop an approach to explaining deep neural networks by constructing causal models on salient concepts contained in a CNN. We develop methods to extract salient concepts throughout a target network by using autoencoders trained to extract human-understandable representations of network activations. We then build a bayesian causal model using these extracted concepts as variables in order to explain image classification. Finally, we use this causal model to identify and visualize features with significant causal influence on final classification.
ER  -


TY  - Preprint
T1  - Cross-City Transfer Learning for Deep Spatio-Temporal Prediction
A1  - Leye Wang
A1  - Xu Geng
A1  - Xiaojuan Ma
A1  - Feng Liu
A1  - Qiang Yang
JO  - ArXiv e-prints
Y1  - 19 May, 2018
UR  - https://arxiv.org/abs/1802.00386
N2  - Spatio-temporal prediction is a key type of tasks in urban computing, e.g., traffic flow and air quality. Adequate data is usually a prerequisite, especially when deep learning is adopted. However, the development levels of different cities are unbalanced, and still many cities suffer from data scarcity. To address the problem, we propose a novel cross-city transfer learning method for deep spatio-temporal prediction tasks, called RegionTrans. RegionTrans aims to effectively transfer knowledge from a data-rich source city to a data-scarce target city. More specifically, we first learn an inter-city region matching function to match each target city region to a similar source city region. A neural network is designed to effectively extract region-level representation for spatio-temporal prediction. Finally, an optimization algorithm is proposed to transfer learned features from the source city to the target city with the region matching function. Using citywide crowd flow prediction as a demonstration experiment, we verify the effectiveness of RegionTrans. Results show that RegionTrans can outperform the state-of-the-art fine-tuning deep spatio-temporal prediction models by reducing up to 10.7% prediction error.
ER  -


TY  - Preprint
T1  - A Unified Deep Learning Architecture for Abuse Detection
A1  - Antigoni-Maria Founta
A1  - Despoina Chatzakou
A1  - Nicolas Kourtellis
A1  - Jeremy Blackburn
A1  - Athena Vakali
A1  - Ilias Leontiadis
JO  - ArXiv e-prints
Y1  - 21 February, 2018
UR  - https://arxiv.org/abs/1802.00385
N2  - Hate speech, offensive language, sexism, racism and other types of abusive behavior have become a common phenomenon in many online social media platforms. In recent years, such diverse abusive behaviors have been manifesting with increased frequency and levels of intensity. This is due to the openness and willingness of popular media platforms, such as Twitter and Facebook, to host content of sensitive or controversial topics. However, these platforms have not adequately addressed the problem of online abusive behavior, and their responsiveness to the effective detection and blocking of such inappropriate behavior remains limited.
ER  -


TY  - Preprint
T1  - Elements of Effective Deep Reinforcement Learning towards Tactical Driving Decision Making
A1  - Jingchu Liu
A1  - Pengfei Hou
A1  - Lisen Mu
A1  - Yinan Yu
A1  - Chang Huang
JO  - ArXiv e-prints
Y1  - 1 February, 2018
UR  - https://arxiv.org/abs/1802.00332
N2  - Tactical driving decision making is crucial for autonomous driving systems and has attracted considerable interest in recent years. In this paper, we propose several practical components that can speed up deep reinforcement learning algorithms towards tactical decision making tasks: 1) non-uniform action skipping as a more stable alternative to action-repetition frame skipping, 2) a counter-based penalty for lanes on which ego vehicle has less right-of-road, and 3) heuristic inference-time action masking for apparently undesirable actions. We evaluate the proposed components in a realistic driving simulator and compare them with several baselines. Results show that the proposed scheme provides superior performance in terms of safety, efficiency, and comfort.
ER  -


TY  - Preprint
T1  - Fusarium Damaged Kernels Detection Using Transfer Learning on Deep Neural Network Architecture
A1  - MÃ¡rcio Nicolau
A1  - MÃ¡rcia Barrocas Moreira Pimentel
A1  - Casiane Salete Tibola
A1  - JosÃ© Mauricio Cunha Fernandes
A1  - Willingthon Pavan
JO  - ArXiv e-prints
Y1  - 31 January, 2018
UR  - https://arxiv.org/abs/1802.00030
N2  - The present work shows the application of transfer learning for a pre-trained deep neural network (DNN), using a small image dataset ($\approx$ 12,000) on a single workstation with enabled NVIDIA GPU card that takes up to 1 hour to complete the training task and archive an overall average accuracy of $94.7\%$. The DNN presents a $20\%$ score of misclassification for an external test dataset. The accuracy of the proposed methodology is equivalent to ones using HSI methodology $(81\%-91\%)$ used for the same task, but with the advantage of being independent on special equipment to classify wheat kernel for FHB symptoms.
ER  -


TY  - Preprint
T1  - Deep Learning of Constrained Autoencoders for Enhanced Understanding of Data
A1  - Babajide O. Ayinde
A1  - Jacek M. Zurada
JO  - ArXiv e-prints
Y1  - 3 February, 2018
UR  - https://arxiv.org/abs/1802.00003
N2  - Unsupervised feature extractors are known to perform an efficient and discriminative representation of data. Insight into the mappings they perform and human ability to understand them, however, remain very limited. This is especially prominent when multilayer deep learning architectures are used. This paper demonstrates how to remove these bottlenecks within the architecture of Nonnegativity Constrained Autoencoder (NCSAE). It is shown that by using both L1 and L2 regularization that induce nonnegativity of weights, most of the weights in the network become constrained to be nonnegative thereby resulting into a more understandable structure with minute deterioration in classification accuracy. Also, this proposed approach extracts features that are more sparse and produces additional output layer sparsification. The method is analyzed for accuracy and feature interpretation on the MNIST data, the NORB normalized uniform object data, and the Reuters text categorization dataset.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Programming Language Correction
A1  - Rahul Gupta
A1  - Aditya Kanade
A1  - Shirish Shevade
JO  - ArXiv e-prints
Y1  - 31 January, 2018
UR  - https://arxiv.org/abs/1801.10467
N2  - Novice programmers often struggle with the formal syntax of programming languages. To assist them, we design a novel programming language correction framework amenable to reinforcement learning. The framework allows an agent to mimic human actions for text navigation and editing. We demonstrate that the agent can be trained through self-exploration directly from the raw input, that is, program text itself, without any knowledge of the formal syntax of the programming language. We leverage expert demonstrations for one tenth of the training data to accelerate training. The proposed technique is evaluated on 6975 erroneous C programs with typographic errors, written by students during an introductory programming course. Our technique fixes 14% more programs and 29% more compiler error messages relative to those fixed by a state-of-the-art tool, DeepFix, which uses a fully supervised neural machine translation approach.
ER  -


TY  - Preprint
T1  - Pretraining Deep Actor-Critic Reinforcement Learning Algorithms With Expert Demonstrations
A1  - Xiaoqin Zhang
A1  - Huimin Ma
JO  - ArXiv e-prints
Y1  - 9 February, 2018
UR  - https://arxiv.org/abs/1801.10459
N2  - Pretraining with expert demonstrations have been found useful in speeding up the training process of deep reinforcement learning algorithms since less online simulation data is required. Some people use supervised learning to speed up the process of feature learning, others pretrain the policies by imitating expert demonstrations. However, these methods are unstable and not suitable for actor-critic reinforcement learning algorithms. Also, some existing methods rely on the global optimum assumption, which is not true in most scenarios. In this paper, we employ expert demonstrations in a actor-critic reinforcement learning framework, and meanwhile ensure that the performance is not affected by the fact that expert demonstrations are not global optimal. We theoretically derive a method for computing policy gradients and value estimators with only expert demonstrations. Our method is theoretically plausible for actor-critic reinforcement learning algorithms that pretrains both policy and value functions. We apply our method to two of the typical actor-critic reinforcement learning algorithms, DDPG and ACER, and demonstrate with experiments that our method not only outperforms the RL algorithms without pretraining process, but also is more simulation efficient.
ER  -


TY  - Preprint
T1  - Deep Learning Works in Practice. But Does it Work in Theory?
A1  - LÃª NguyÃªn Hoang
A1  - Rachid Guerraoui
JO  - ArXiv e-prints
Y1  - 31 January, 2018
UR  - https://arxiv.org/abs/1801.10437
N2  - Deep learning relies on a very specific kind of neural networks: those superposing several neural layers. In the last few years, deep learning achieved major breakthroughs in many tasks such as image analysis, speech recognition, natural language processing, and so on. Yet, there is no theoretical explanation of this success. In particular, it is not clear why the deeper the network, the better it actually performs.
ER  -


TY  - Preprint
T1  - A Deep Reinforcement Learning Approach for Dynamically Stable Inverse Kinematics of Humanoid Robots
A1  - S Phaniteja
A1  - Parijat Dewangan
A1  - Pooja Guhan
A1  - Abhishek Sarkar
A1  - K Madhava Krishna
JO  - ArXiv e-prints
Y1  - 31 January, 2018
UR  - https://arxiv.org/abs/1801.10425
N2  - Real time calculation of inverse kinematics (IK) with dynamically stable configuration is of high necessity in humanoid robots as they are highly susceptible to lose balance. This paper proposes a methodology to generate joint-space trajectories of stable configurations for solving inverse kinematics using Deep Reinforcement Learning (RL). Our approach is based on the idea of exploring the entire configuration space of the robot and learning the best possible solutions using Deep Deterministic Policy Gradient (DDPG). The proposed strategy was evaluated on the highly articulated upper body of a humanoid model with 27 degree of freedom (DoF). The trained model was able to solve inverse kinematics for the end effectors with 90% accuracy while maintaining the balance in double support phase.
ER  -


TY  - Preprint
T1  - Deep Multi-view Learning to Rank
A1  - Guanqun Cao
A1  - Alexandros Iosifidis
A1  - Moncef Gabbouj
A1  - Vijay Raghavan
A1  - Raju Gottumukkala
JO  - ArXiv e-prints
Y1  - 31 January, 2018
UR  - https://arxiv.org/abs/1801.10402
N2  - We study the problem of learning to rank from multiple sources. Though multi-view learning and learning to rank have been studied extensively leading to a wide range of applications, multi-view learning to rank as a synergy of both topics has received little attention. The aim of the paper is to propose a composite ranking method while keeping a close correlation with the individual rankings simultaneously. We propose a multi-objective solution to ranking by capturing the information of the feature mapping from both within each view as well as across views using autoencoder-like networks. Moreover, a novel end-to-end solution is introduced to enhance the joint ranking with minimum view-specific ranking loss, so that we can achieve the maximum global view agreements within a single optimization process. The proposed method is validated on a wide variety of ranking problems, including university ranking, multi-view lingual text ranking and image data ranking, providing superior results.
ER  -


TY  - Preprint
T1  - ConvCSNet: A Convolutional Compressive Sensing Framework Based on Deep Learning
A1  - Xiaotong Lu
A1  - Weisheng Dong
A1  - Peiyao Wang
A1  - Guangming Shi
A1  - Xuemei Xie
JO  - ArXiv e-prints
Y1  - 31 January, 2018
UR  - https://arxiv.org/abs/1801.10342
N2  - Compressive sensing (CS), aiming to reconstruct an image/signal from a small set of random measurements has attracted considerable attentions in recent years. Due to the high dimensionality of images, previous CS methods mainly work on image blocks to avoid the huge requirements of memory and computation, i.e., image blocks are measured with Gaussian random matrices, and the whole images are recovered from the reconstructed image blocks. Though efficient, such methods suffer from serious blocking artifacts. In this paper, we propose a convolutional CS framework that senses the whole image using a set of convolutional filters. Instead of reconstructing individual blocks, the whole image is reconstructed from the linear convolutional measurements. Specifically, the convolutional CS is implemented based on a convolutional neural network (CNN), which performs both the convolutional CS and nonlinear reconstruction. Through end-to-end training, the sensing filters and the reconstruction network can be jointly optimized. To facilitate the design of the CS reconstruction network, a novel two-branch CNN inspired from a sparsity-based CS reconstruction model is developed. Experimental results show that the proposed method substantially outperforms previous state-of-the-art CS methods in term of both PSNR and visual quality.
ER  -


TY  - Preprint
T1  - Cross-type Biomedical Named Entity Recognition with Deep Multi-Task Learning
A1  - Xuan Wang
A1  - Yu Zhang
A1  - Xiang Ren
A1  - Yuhao Zhang
A1  - Marinka Zitnik
A1  - Jingbo Shang
A1  - Curtis Langlotz
A1  - Jiawei Han
JO  - ArXiv e-prints
Y1  - 7 October, 2018
UR  - https://arxiv.org/abs/1801.09851
N2  - Motivation: State-of-the-art biomedical named entity recognition (BioNER) systems often require handcrafted features specific to each entity type, such as genes, chemicals and diseases. Although recent studies explored using neural network models for BioNER to free experts from manual feature engineering, the performance remains limited by the available training data for each entity type. Results: We propose a multi-task learning framework for BioNER to collectively use the training data of different types of entities and improve the performance on each of them. In experiments on 15 benchmark BioNER datasets, our multi-task model achieves substantially better performance compared with state-of-the-art BioNER systems and baseline neural sequence labeling models. Further analysis shows that the large performance gains come from sharing character- and word-level information among relevant biomedical entities across differently labeled corpora.
ER  -


TY  - Preprint
T1  - Deep Learning based Retinal OCT Segmentation
A1  - Mike Pekala
A1  - Neil Joshi
A1  - David E. Freund
A1  - Neil M. Bressler
A1  - Delia Cabrera DeBuc
A1  - Philippe M Burlina
JO  - ArXiv e-prints
Y1  - 29 January, 2018
UR  - https://arxiv.org/abs/1801.09749
N2  - Our objective is to evaluate the efficacy of methods that use deep learning (DL) for the automatic fine-grained segmentation of optical coherence tomography (OCT) images of the retina. OCT images from 10 patients with mild non-proliferative diabetic retinopathy were used from a public (U. of Miami) dataset. For each patient, five images were available: one image of the fovea center, two images of the perifovea, and two images of the parafovea. For each image, two expert graders each manually annotated five retinal surfaces (i.e. boundaries between pairs of retinal layers). The first grader&#39;s annotations were used as ground truth and the second grader&#39;s annotations to compute inter-operator agreement. The proposed automated approach segments images using fully convolutional networks (FCNs) together with Gaussian process (GP)-based regression as a post-processing step to improve the quality of the estimates. Using 10-fold cross validation, the performance of the algorithms is determined by computing the per-pixel unsigned error (distance) between the automated estimates and the ground truth annotations generated by the first manual grader. We compare the proposed method against five state of the art automatic segmentation techniques. The results show that the proposed methods compare favorably with state of the art techniques, resulting in the smallest mean unsigned error values and associated standard deviations, and performance is comparable with human annotation of retinal layers from OCT when there is only mild retinopathy. The results suggest that semantic segmentation using FCNs, coupled with regression-based post-processing, can effectively solve the OCT segmentation problem on par with human capabilities with mild retinopathy.
ER  -


TY  - Preprint
T1  - Denoising Arterial Spin Labeling Cerebral Blood Flow Images Using Deep Learning
A1  - Danfeng Xie
A1  - Li Bai
A1  - Ze Wang
JO  - ArXiv e-prints
Y1  - 29 January, 2018
UR  - https://arxiv.org/abs/1801.09672
N2  - Arterial spin labeling perfusion MRI is a noninvasive technique for measuring quantitative cerebral blood flow (CBF), but the measurement is subject to a low signal-to-noise-ratio(SNR). Various post-processing methods have been proposed to denoise ASL MRI but only provide moderate improvement. Deep learning (DL) is an emerging technique that can learn the most representative signal from data without prior modeling which can be highly complex and analytically indescribable. The purpose of this study was to assess whether the record breaking performance of DL can be translated into ASL MRI denoising. We used convolutional neural network (CNN) to build the DL ASL denosing model (DL-ASL) to inherently consider the inter-voxel correlations. To better guide DL-ASL training, we incorporated prior knowledge about ASL MRI: the structural similarity between ASL CBF map and grey matter probability map. A relatively large sample data were used to train the model which was subsequently applied to a new set of data for testing. Experimental results showed that DL-ASL achieved state-of-the-art denoising performance for ASL MRI as compared to current routine methods in terms of higher SNR, keeping CBF quantification quality while shorten the acquisition time by 75%, and automatic partial volume correction.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning using Capsules in Advanced Game Environments
A1  - Per-Arne Andersen
JO  - ArXiv e-prints
Y1  - 29 January, 2018
UR  - https://arxiv.org/abs/1801.09597
N2  - Reinforcement Learning (RL) is a research area that has blossomed tremendously in recent years and has shown remarkable potential for artificial intelligence based opponents in computer games. This success is primarily due to vast capabilities of Convolutional Neural Networks (ConvNet), enabling algorithms to extract useful information from noisy environments. Capsule Network (CapsNet) is a recent introduction to the Deep Learning algorithm group and has only barely begun to be explored. The network is an architecture for image classification, with superior performance for classification of the MNIST dataset. CapsNets have not been explored beyond image classification.
ER  -


TY  - Preprint
T1  - Deep Learning Approach for Very Similar Objects Recognition Application on Chihuahua and Muffin Problem
A1  - Enkhtogtokh Togootogtokh
A1  - Amarzaya Amartuvshin
JO  - ArXiv e-prints
Y1  - 29 January, 2018
UR  - https://arxiv.org/abs/1801.09573
N2  - We address the problem to tackle the very similar objects like Chihuahua or muffin problem to recognize at least in human vision level. Our regular deep structured machine learning still does not solve it. We saw many times for about year in our community the problem. Today we proposed the state-of-the-art solution for it. Our approach is quite tricky to get the very high accuracy. We propose the deep transfer learning method which could be tackled all this type of problems not limited to just Chihuahua or muffin problem. It is the best method to train with small data set not like require huge amount data.
ER  -


TY  - Preprint
T1  - Social Influence (Deep) Learning for Human Behavior Prediction
A1  - Luca Luceri
A1  - Torsten Braun
A1  - Silvia Giordano
JO  - ArXiv e-prints
Y1  - 29 January, 2018
UR  - https://arxiv.org/abs/1801.09471
N2  - Influence propagation in social networks has recently received large interest. In fact, the understanding of how influence propagates among subjects in a social network opens the way to a growing number of applications. Many efforts have been made to quantitatively measure the influence probability between pairs of subjects. Existing approaches have two main drawbacks: (i) they assume that the influence probabilities are independent of each other, and (ii) they do not consider the actions not performed by the subject (but performed by her/his friends) to learn these probabilities. In this paper, we propose to address these limitations by employing a deep learning approach. We introduce a Deep Neural Network (DNN) framework that has the capability for both modeling social influence and for predicting human behavior. To empirically validate the proposed framework, we conduct experiments on a real-life (offline) dataset of an Event-Based Social Network (EBSN). Results indicate that our approach outperforms existing solutions, by efficiently resolving the limitations previously described.
ER  -


TY  - Preprint
T1  - Using deep Q-learning to understand the tax evasion behavior of risk-averse firms
A1  - Nikolaos D. Goumagias
A1  - Dimitrios Hristu-Varsakelis
A1  - Yannis M. Assael
JO  - ArXiv e-prints
Y1  - 29 January, 2018
UR  - https://arxiv.org/abs/1801.09466
N2  - Designing tax policies that are effective in curbing tax evasion and maximize state revenues requires a rigorous understanding of taxpayer behavior. This work explores the problem of determining the strategy a self-interested, risk-averse tax entity is expected to follow, as it &#34;navigates&#34; - in the context of a Markov Decision Process - a government-controlled tax environment that includes random audits, penalties and occasional tax amnesties. Although simplified versions of this problem have been previously explored, the mere assumption of risk-aversion (as opposed to risk-neutrality) raises the complexity of finding the optimal policy well beyond the reach of analytical techniques. Here, we obtain approximate solutions via a combination of Q-learning and recent advances in Deep Reinforcement Learning. By doing so, we i) determine the tax evasion behavior expected of the taxpayer entity, ii) calculate the degree of risk aversion of the &#34;average&#34; entity given empirical estimates of tax evasion, and iii) evaluate sample tax policies, in terms of expected revenues. Our model can be useful as a testbed for &#34;in-vitro&#34; testing of tax policies, while our results lead to various policy recommendations.
ER  -


TY  - Preprint
T1  - Document Image Classification with Intra-Domain Transfer Learning and Stacked Generalization of Deep Convolutional Neural Networks
A1  - Arindam Das
A1  - Saikat Roy
A1  - Ujjwal Bhattacharya
A1  - Swapan Kumar Parui
JO  - ArXiv e-prints
Y1  - 30 August, 2018
UR  - https://arxiv.org/abs/1801.09321
N2  - In this work, a region-based Deep Convolutional Neural Network framework is proposed for document structure learning. The contribution of this work involves efficient training of region based classifiers and effective ensembling for document image classification. A primary level of `inter-domain&#39; transfer learning is used by exporting weights from a pre-trained VGG16 architecture on the ImageNet dataset to train a document classifier on whole document images. Exploiting the nature of region based influence modelling, a secondary level of `intra-domain&#39; transfer learning is used for rapid training of deep learning models for image segments. Finally, stacked generalization based ensembling is utilized for combining the predictions of the base deep neural network models. The proposed method achieves state-of-the-art accuracy of 92.2% on the popular RVL-CDIP document image dataset, exceeding benchmarks set by existing algorithms.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical Registry Data
A1  - Ning Liu
A1  - Ying Liu
A1  - Brent Logan
A1  - Zhiyuan Xu
A1  - Jian Tang
A1  - Yanzhi Wang
JO  - ArXiv e-prints
Y1  - 28 January, 2018
UR  - https://arxiv.org/abs/1801.09271
N2  - This paper presents the first deep reinforcement learning (DRL) framework to estimate the optimal Dynamic Treatment Regimes from observational medical data. This framework is more flexible and adaptive for high dimensional action and state spaces than existing reinforcement learning methods to model real-life complexity in heterogeneous disease progression and treatment choices, with the goal of providing doctor and patients the data-driven personalized decision recommendations. The proposed DRL framework comprises (i) a supervised learning step to predict the most possible expert actions, and (ii) a deep reinforcement learning step to estimate the long-term value function of Dynamic Treatment Regimes. Both steps depend on deep neural networks.
ER  -


TY  - Preprint
T1  - Deep LOGISMOS: Deep Learning Graph-based 3D Segmentation of Pancreatic Tumors on CT scans
A1  - Zhihui Guo
A1  - Ling Zhang
A1  - Le Lu
A1  - Mohammadhadi Bagheri
A1  - Ronald M. Summers
A1  - Milan Sonka
A1  - Jianhua Yao
JO  - ArXiv e-prints
Y1  - 25 January, 2018
UR  - https://arxiv.org/abs/1801.08599
N2  - This paper reports Deep LOGISMOS approach to 3D tumor segmentation by incorporating boundary information derived from deep contextual learning to LOGISMOS - layered optimal graph image segmentation of multiple objects and surfaces. Accurate and reliable tumor segmentation is essential to tumor growth analysis and treatment selection. A fully convolutional network (FCN), UNet, is first trained using three adjacent 2D patches centered at the tumor, providing contextual UNet segmentation and probability map for each 2D patch. The UNet segmentation is then refined by Gaussian Mixture Model (GMM) and morphological operations. The refined UNet segmentation is used to provide the initial shape boundary to build a segmentation graph. The cost for each node of the graph is determined by the UNet probability maps. Finally, a max-flow algorithm is employed to find the globally optimal solution thus obtaining the final segmentation. For evaluation, we applied the method to pancreatic tumor segmentation on a dataset of 51 CT scans, among which 30 scans were used for training and 21 for testing. With Deep LOGISMOS, DICE Similarity Coefficient (DSC) and Relative Volume Difference (RVD) reached 83.2+-7.8% and 18.6+-17.4% respectively, both are significantly improved (p&lt;0.05) compared with contextual UNet and/or LOGISMOS alone.
ER  -


TY  - Preprint
T1  - Deep Learning for End-to-End Automatic Target Recognition from Synthetic Aperture Radar Imagery
A1  - Hidetoshi Furukawa
JO  - ArXiv e-prints
Y1  - 25 January, 2018
UR  - https://arxiv.org/abs/1801.08558
N2  - The standard architecture of synthetic aperture radar (SAR) automatic target recognition (ATR) consists of three stages: detection, discrimination, and classification. In recent years, convolutional neural networks (CNNs) for SAR ATR have been proposed, but most of them classify target classes from a target chip extracted from SAR imagery, as a classification for the third stage of SAR ATR. In this report, we propose a novel CNN for end-to-end ATR from SAR imagery. The CNN named verification support network (VersNet) performs all three stages of SAR ATR end-to-end. VersNet inputs a SAR image of arbitrary sizes with multiple classes and multiple targets, and outputs a SAR ATR image representing the position, class, and pose of each detected target. This report describes the evaluation results of VersNet which trained to output scores of all 12 classes: 10 target classes, a target front class, and a background class, for each pixel using the moving and stationary target acquisition and recognition (MSTAR) public dataset.
ER  -


TY  - Preprint
T1  - Data-Driven Impulse Response Regularization via Deep Learning
A1  - Carl Andersson
A1  - Niklas WahlstrÃ¶m
A1  - Thomas B. SchÃ¶n
JO  - ArXiv e-prints
Y1  - 25 January, 2018
UR  - https://arxiv.org/abs/1801.08383
N2  - We consider the problem of impulse response estimation for stable linear single-input single-output systems. It is a well-studied problem where flexible non-parametric models recently offered a leap in performance compared to the classical finite-dimensional model structures. Inspired by this development and the success of deep learning we propose a new flexible data-driven model. Our experiments indicate that the new model is capable of exploiting even more of the hidden patterns that are present in the input-output data as compared to the non-parametric models.
ER  -


TY  - Preprint
T1  - Dual Asymmetric Deep Hashing Learning
A1  - Jinxing Li
A1  - Bob Zhang
A1  - Guangming Lu
A1  - David Zhang
JO  - ArXiv e-prints
Y1  - 25 January, 2018
UR  - https://arxiv.org/abs/1801.08360
N2  - Due to the impressive learning power, deep learning has achieved a remarkable performance in supervised hash function learning. In this paper, we propose a novel asymmetric supervised deep hashing method to preserve the semantic structure among different categories and generate the binary codes simultaneously. Specifically, two asymmetric deep networks are constructed to reveal the similarity between each pair of images according to their semantic labels. The deep hash functions are then learned through two networks by minimizing the gap between the learned features and discrete codes. Furthermore, since the binary codes in the Hamming space also should keep the semantic affinity existing in the original space, another asymmetric pairwise loss is introduced to capture the similarity between the binary codes and real-value features. This asymmetric loss not only improves the retrieval performance, but also contributes to a quick convergence at the training phase. By taking advantage of the two-stream deep structures and two types of asymmetric pairwise functions, an alternating algorithm is designed to optimize the deep features and high-quality binary codes efficiently. Experimental results on three real-world datasets substantiate the effectiveness and superiority of our approach as compared with state-of-the-art.
ER  -


TY  - Preprint
T1  - Phonocardiographic Sensing using Deep Learning for Abnormal Heartbeat Detection
A1  - Siddique Latif
A1  - Muhammad Usman
A1  - Rajib Rana
A1  - Junaid Qadir
JO  - ArXiv e-prints
Y1  - 5 June, 2018
UR  - https://arxiv.org/abs/1801.08322
N2  - Cardiac auscultation involves expert interpretation of abnormalities in heart sounds using stethoscope. Deep learning based cardiac auscultation is of significant interest to the healthcare community as it can help reducing the burden of manual auscultation with automated detection of abnormal heartbeats. However, the problem of automatic cardiac auscultation is complicated due to the requirement of reliability and high accuracy, and due to the presence of background noise in the heartbeat sound. In this work, we propose a Recurrent Neural Networks (RNNs) based automated cardiac auscultation solution. Our choice of RNNs is motivated by the great success of deep learning in medical applications and by the observation that RNNs represent the deep learning configuration most suitable for dealing with sequential or temporal data even in the presence of noise. We explore the use of various RNN models, and demonstrate that these models deliver the abnormal heartbeat classification score with significant improvement. Our proposed approach using RNNs can be potentially be used for real-time abnormal heartbeat detection in the Internet of Medical Things for remote monitoring applications.
ER  -


TY  - Preprint
T1  - Psychlab: A Psychology Laboratory for Deep Reinforcement Learning Agents
A1  - Joel Z. Leibo
A1  - Cyprien de Masson d&#39;Autume
A1  - Daniel Zoran
A1  - David Amos
A1  - Charles Beattie
A1  - Keith Anderson
A1  - Antonio GarcÃ­a CastaÃ±eda
A1  - Manuel Sanchez
A1  - Simon Green
A1  - Audrunas Gruslys
A1  - Shane Legg
A1  - Demis Hassabis
A1  - Matthew M. Botvinick
JO  - ArXiv e-prints
Y1  - 4 February, 2018
UR  - https://arxiv.org/abs/1801.08116
N2  - Psychlab is a simulated psychology laboratory inside the first-person 3D game world of DeepMind Lab (Beattie et al. 2016). Psychlab enables implementations of classical laboratory psychological experiments so that they work with both human and artificial agents. Psychlab has a simple and flexible API that enables users to easily create their own tasks. As examples, we are releasing Psychlab implementations of several classical experimental paradigms including visual search, change detection, random dot motion discrimination, and multiple object tracking. We also contribute a study of the visual psychophysics of a specific state-of-the-art deep reinforcement learning agent: UNREAL (Jaderberg et al. 2016). This study leads to the surprising conclusion that UNREAL learns more quickly about larger target stimuli than it does about smaller stimuli. In turn, this insight motivates a specific improvement in the form of a simple model of foveal vision that turns out to significantly boost UNREAL&#39;s performance, both on Psychlab tasks, and on standard DeepMind Lab tasks. By open-sourcing Psychlab we hope to facilitate a range of future such studies that simultaneously advance deep reinforcement learning and improve its links with cognitive science.
ER  -


TY  - Preprint
T1  - Unsupervised learning from videos using temporal coherency deep networks
A1  - Carolina Redondo-Cabrera
A1  - Roberto J. LÃ³pez-Sastre
JO  - ArXiv e-prints
Y1  - 24 January, 2018
UR  - https://arxiv.org/abs/1801.08100
N2  - In this work we address the challenging problem of unsupervised learning from videos. Existing methods utilize the spatio-temporal continuity in contiguous video frames as regularization for the learning process. Typically, this temporal coherence of close frames is used as a free form of annotation, encouraging the learned representations to exhibit small differences between these frames. But this type of approach fails to capture the dissimilarity between videos with different content, hence learning less discriminative features. We here propose two Siamese architectures for Convolutional Neural Networks, and their corresponding novel loss functions, to learn from unlabeled videos, which jointly exploit the local temporal coherence between contiguous frames, and a global discriminative margin used to separate representations of different videos. An extensive experimental evaluation is presented, where we validate the proposed models on various tasks. First, we show how the learned features can be used to discover actions and scenes in video collections. Second, we show the benefits of such an unsupervised learning from just unlabeled videos, which can be directly used as a prior for the supervised recognition tasks of actions and objects in images, where our results further show that our features can even surpass a traditional and heavily supervised pre-training plus fine-tunning strategy.
ER  -


TY  - Preprint
T1  - Intel nGraph: An Intermediate Representation, Compiler, and Executor for Deep Learning
A1  - Scott Cyphers
A1  - Arjun K. Bansal
A1  - Anahita Bhiwandiwalla
A1  - Jayaram Bobba
A1  - Matthew Brookhart
A1  - Avijit Chakraborty
A1  - Will Constable
A1  - Christian Convey
A1  - Leona Cook
A1  - Omar Kanawi
A1  - Robert Kimball
A1  - Jason Knight
A1  - Nikolay Korovaiko
A1  - Varun Kumar
A1  - Yixing Lao
A1  - Christopher R. Lishka
A1  - Jaikrishnan Menon
A1  - Jennifer Myers
A1  - Sandeep Aswath Narayana
A1  - Adam Procter
A1  - Tristan J. Webb
JO  - ArXiv e-prints
Y1  - 29 January, 2018
UR  - https://arxiv.org/abs/1801.08058
N2  - The Deep Learning (DL) community sees many novel topologies published each year. Achieving high performance on each new topology remains challenging, as each requires some level of manual effort. This issue is compounded by the proliferation of frameworks and hardware platforms. The current approach, which we call &#34;direct optimization&#34;, requires deep changes within each framework to improve the training performance for each hardware backend (CPUs, GPUs, FPGAs, ASICs) and requires $\mathcal{O}(fp)$ effort; where $f$ is the number of frameworks and $p$ is the number of platforms. While optimized kernels for deep-learning primitives are provided via libraries like Intel Math Kernel Library for Deep Neural Networks (MKL-DNN), there are several compiler-inspired ways in which performance can be further optimized. Building on our experience creating neon (a fast deep learning library on GPUs), we developed Intel nGraph, a soon to be open-sourced C++ library to simplify the realization of optimized deep learning performance across frameworks and hardware platforms. Initially-supported frameworks include TensorFlow, MXNet, and Intel neon framework. Initial backends are Intel Architecture CPUs (CPU), the Intel(R) Nervana Neural Network Processor(R) (NNP), and NVIDIA GPUs. Currently supported compiler optimizations include efficient memory management and data layout abstraction. In this paper, we describe our overall architecture and its core components. In the future, we envision extending nGraph API support to a wider range of frameworks, hardware (including FPGAs and ASICs), and compiler optimizations (training versus inference optimizations, multi-node and multi-device scaling via efficient sub-graph partitioning, and HW-specific compounding of operations).
ER  -


TY  - Preprint
T1  - On Scale-out Deep Learning Training for Cloud and HPC
A1  - Srinivas Sridharan
A1  - Karthikeyan Vaidyanathan
A1  - Dhiraj Kalamkar
A1  - Dipankar Das
A1  - Mikhail E. Smorkalov
A1  - Mikhail Shiryaev
A1  - Dheevatsa Mudigere
A1  - Naveen Mellempudi
A1  - Sasikanth Avancha
A1  - Bharat Kaul
A1  - Pradeep Dubey
JO  - ArXiv e-prints
Y1  - 24 January, 2018
UR  - https://arxiv.org/abs/1801.08030
N2  - The exponential growth in use of large deep neural networks has accelerated the need for training these deep neural networks in hours or even minutes. This can only be achieved through scalable and efficient distributed training, since a single node/card cannot satisfy the compute, memory, and I/O requirements of today&#39;s state-of-the-art deep neural networks. However, scaling synchronous Stochastic Gradient Descent (SGD) is still a challenging problem and requires continued research/development. This entails innovations spanning algorithms, frameworks, communication libraries, and system design. In this paper, we describe the philosophy, design, and implementation of Intel Machine Learning Scalability Library (MLSL) and present proof-points demonstrating scaling DL training on 100s to 1000s of nodes across Cloud and HPC systems.
ER  -


TY  - Preprint
T1  - Deep Learning for Sentiment Analysis : A Survey
A1  - Lei Zhang
A1  - Shuai Wang
A1  - Bing Liu
JO  - ArXiv e-prints
Y1  - 30 January, 2018
UR  - https://arxiv.org/abs/1801.07883
N2  - Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state-of-the-art prediction results. Along with the success of deep learning in many other application domains, deep learning is also popularly used in sentiment analysis in recent years. This paper first gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis.
ER  -


TY  - Preprint
T1  - Scalable and accurate deep learning for electronic health records
A1  - Alvin Rajkomar
A1  - Eyal Oren
A1  - Kai Chen
A1  - Andrew M. Dai
A1  - Nissan Hajaj
A1  - Peter J. Liu
A1  - Xiaobing Liu
A1  - Mimi Sun
A1  - Patrik Sundberg
A1  - Hector Yee
A1  - Kun Zhang
A1  - Gavin E. Duggan
A1  - Gerardo Flores
A1  - Michaela Hardt
A1  - Jamie Irvine
A1  - Quoc Le
A1  - Kurt Litsch
A1  - Jake Marcus
A1  - Alexander Mossin
A1  - Justin Tansuwan
A1  - De Wang
A1  - James Wexler
A1  - Jimbo Wilson
A1  - Dana Ludwig
A1  - Samuel L. Volchenboum
JO  - ArXiv e-prints
Y1  - 11 May, 2018
UR  - https://arxiv.org/abs/1801.07860
N2  - Predictive modeling with electronic health record (EHR) data is anticipated to drive personalized medicine and improve healthcare quality. Constructing predictive statistical models typically requires extraction of curated predictor variables from normalized EHR data, a labor-intensive process that discards the vast majority of information in each patient&#39;s record. We propose a representation of patients&#39; entire, raw EHR records based on the Fast Healthcare Interoperability Resources (FHIR) format. We demonstrate that deep learning methods using this representation are capable of accurately predicting multiple medical events from multiple centers without site-specific data harmonization. We validated our approach using de-identified EHR data from two U.S. academic medical centers with 216,221 adult patients hospitalized for at least 24 hours. In the sequential format we propose, this volume of EHR data unrolled into a total of 46,864,534,945 data points, including clinical notes. Deep learning models achieved high accuracy for tasks such as predicting in-hospital mortality (AUROC across sites 0.93-0.94), 30-day unplanned readmission (AUROC 0.75-0.76), prolonged length of stay (AUROC 0.85-0.86), and all of a patient&#39;s final discharge diagnoses (frequency-weighted AUROC 0.90). These models outperformed state-of-the-art traditional predictive models in all cases. We also present a case-study of a neural-network attribution system, which illustrates how clinicians can gain some transparency into the predictions. We believe that this approach can be used to create accurate and scalable predictions for a variety of clinical scenarios, complete with explanations that directly highlight evidence in the patient&#39;s chart.
ER  -


TY  - Preprint
T1  - Deep Learning for Electromyographic Hand Gesture Signal Classification Using Transfer Learning
A1  - Ulysse CÃ´tÃ©-Allard
A1  - Cheikh Latyr Fall
A1  - Alexandre Drouin
A1  - Alexandre Campeau-Lecours
A1  - ClÃ©ment Gosselin
A1  - Kyrre Glette
A1  - FranÃ§ois Laviolette
A1  - Benoit Gosselin
JO  - ArXiv e-prints
Y1  - 12 June, 2018
UR  - https://arxiv.org/abs/1801.07756
N2  - In recent years, the use of deep learning algorithms has become increasingly more prominent for their unparalleled ability to automatically learn discriminant features from large amounts of data. However, within the field of electromyography-based gesture recognition, deep learning algorithms are seldom employed as it requires an unreasonable amount of time for a single person, in a single session, to generate tens of thousands of examples. This work&#39;s hypothesis is that general, informative features can be learned from the large amount of data generated by aggregating the signals of multiple users, thus reducing the recording burden imposed on a single person while enhancing gesture recognition. As such, this paper proposes applying transfer learning on the aggregated data of multiple users, while leveraging the capacity of deep learning algorithms to learn discriminant features from large dataset, without the need for in-depth feature engineering. To this end, two datasets are recorded with the Myo Armband (Thalmic Labs), a low-cost, low-sampling rate (200Hz), 8-channel, consumer-grade, dry electrode sEMG armband. These two datasets are comprised of 19 and 17 able-bodied participants respectively. A third dataset, also recorded with the Myo Armband, was taken from the NinaPro database and is comprised of 10 able-bodied participants. This transfer learning scheme is shown to outperform the current state-of-the-art in gesture recognition. It achieves an average accuracy of 98.31% for 7 hand/wrist gestures over 17 able-bodied participants and 65.57% for 18 hand/wrist gestures over 10 able-bodied participants. Finally, a use-case study employing eight able-bodied participants suggests that real-time feedback reduces the degradation in accuracy normally experienced over time.
ER  -


TY  - Preprint
T1  - Clustering with Deep Learning: Taxonomy and New Methods
A1  - Elie Aljalbout
A1  - Vladimir Golkov
A1  - Yawar Siddiqui
A1  - Maximilian Strobel
A1  - Daniel Cremers
JO  - ArXiv e-prints
Y1  - 13 September, 2018
UR  - https://arxiv.org/abs/1801.07648
N2  - Clustering methods based on deep neural networks have proven promising for clustering real-world data because of their high representational power. In this paper, we propose a systematic taxonomy of clustering methods that utilize deep neural networks. We base our taxonomy on a comprehensive review of recent work and validate the taxonomy in a case study. In this case study, we show that the taxonomy enables researchers and practitioners to systematically create new clustering methods by selectively recombining and replacing distinct aspects of previous methods with the goal of overcoming their individual limitations. The experimental evaluation confirms this and shows that the method created for the case study achieves state-of-the-art clustering quality and surpasses it in some cases.
ER  -


TY  - Preprint
T1  - DeepGestalt - Identifying Rare Genetic Syndromes Using Deep Learning
A1  - Yaron Gurovich
A1  - Yair Hanani
A1  - Omri Bar
A1  - Nicole Fleischer
A1  - Dekel Gelbman
A1  - Lina Basel-Salmon
A1  - Peter Krawitz
A1  - Susanne B Kamphausen
A1  - Martin Zenker
A1  - Lynne M. Bird
A1  - Karen W. Gripp
JO  - ArXiv e-prints
Y1  - 23 January, 2018
UR  - https://arxiv.org/abs/1801.07637
N2  - Facial analysis technologies have recently measured up to the capabilities of expert clinicians in syndrome identification. To date, these technologies could only identify phenotypes of a few diseases, limiting their role in clinical settings where hundreds of diagnoses must be considered.
ER  -


TY  - Preprint
T1  - Secure Mobile Crowdsensing with Deep Learning
A1  - Liang Xiao
A1  - Donghua Jiang
A1  - Dongjin Xu
A1  - Ning An
JO  - ArXiv e-prints
Y1  - 22 January, 2018
UR  - https://arxiv.org/abs/1801.07379
N2  - In order to stimulate secure sensing for Internet of Things (IoT) applications such as healthcare and traffic monitoring, mobile crowdsensing (MCS) systems have to address security threats, such as jamming, spoofing and faked sensing attacks, during both the sensing and the information exchange processes in large-scale dynamic and heterogenous networks. In this article, we investigate secure mobile crowdsensing and present how to use deep learning (DL) methods such as stacked autoencoder (SAE), deep neural network (DNN), and convolutional neural network (CNN) to improve the MCS security approaches including authentication, privacy protection, faked sensing countermeasures, intrusion detection and anti-jamming transmissions in MCS. We discuss the performance gain of these DL-based approaches compared with traditional security schemes and identify the challenges that need to be addressed to implement them in practical MCS systems.
ER  -


TY  - Preprint
T1  - Towards Automated Tuberculosis detection using Deep Learning
A1  - Sonaal Kant
A1  - Muktabh Mayank Srivastava
JO  - ArXiv e-prints
Y1  - 22 January, 2018
UR  - https://arxiv.org/abs/1801.07080
N2  - Tuberculosis(TB) in India is the world&#39;s largest TB epidemic. TB leads to 480,000 deaths every year. Between the years 2006 and 2014, Indian economy lost US$340 Billion due to TB. This combined with the emergence of drug resistant bacteria in India makes the problem worse. The government of India has hence come up with a new strategy which requires a high-sensitivity microscopy based TB diagnosis mechanism. We propose a new Deep Neural Network based drug sensitive TB detection methodology with recall and precision of 83.78% and 67.55% respectively for bacillus detection. This method takes a microscopy image with proper zoom level as input and returns location of suspected TB germs as output. The high accuracy of our method gives it the potential to evolve into a high sensitivity system to diagnose TB when trained at scale.
ER  -


TY  - Preprint
T1  - Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers
A1  - Fred Hohman
A1  - Minsuk Kahng
A1  - Robert Pienta
A1  - Duen Horng Chau
JO  - ArXiv e-prints
Y1  - 14 May, 2018
UR  - https://arxiv.org/abs/1801.06889
N2  - Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W&#39;s and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.
ER  -


TY  - Preprint
T1  - Using Deep Learning for Title-Based Semantic Subject Indexing to Reach Competitive Performance to Full-Text
A1  - Florian Mai
A1  - Lukas Galke
A1  - Ansgar Scherp
JO  - ArXiv e-prints
Y1  - 29 May, 2018
UR  - https://arxiv.org/abs/1801.06717
N2  - For (semi-)automated subject indexing systems in digital libraries, it is often more practical to use metadata such as the title of a publication instead of the full-text or the abstract. Therefore, it is desirable to have good text mining and text classification algorithms that operate well already on the title of a publication. So far, the classification performance on titles is not competitive with the performance on the full-texts if the same number of training samples is used for training. However, it is much easier to obtain title data in large quantities and to use it for training than full-text data. In this paper, we investigate the question how models obtained from training on increasing amounts of title training data compare to models from training on a constant number of full-texts. We evaluate this question on a large-scale dataset from the medical domain (PubMed) and from economics (EconBiz). In these datasets, the titles and annotations of millions of publications are available, and they outnumber the available full-texts by a factor of 20 and 15, respectively. To exploit these large amounts of data to their full potential, we develop three strong deep learning classifiers and evaluate their performance on the two datasets. The results are promising. On the EconBiz dataset, all three classifiers outperform their full-text counterparts by a large margin. The best title-based classifier outperforms the best full-text method by 9.4%. On the PubMed dataset, the best title-based method almost reaches the performance of the best full-text classifier, with a difference of only 2.9%.
ER  -


TY  - Preprint
T1  - A Deep Reinforcement Learning Chatbot (Short Version)
A1  - Iulian V. Serban
A1  - Chinnadhurai Sankar
A1  - Mathieu Germain
A1  - Saizheng Zhang
A1  - Zhouhan Lin
A1  - Sandeep Subramanian
A1  - Taesup Kim
A1  - Michael Pieper
A1  - Sarath Chandar
A1  - Nan Rosemary Ke
A1  - Sai Rajeswar
A1  - Alexandre de Brebisson
A1  - Jose M. R. Sotelo
A1  - Dendi Suhubdy
A1  - Vincent Michalski
A1  - Alexandre Nguyen
A1  - Joelle Pineau
A1  - Yoshua Bengio
JO  - ArXiv e-prints
Y1  - 20 January, 2018
UR  - https://arxiv.org/abs/1801.06700
N2  - We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including neural network and template-based models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than other systems. The results highlight the potential of coupling ensemble systems with deep reinforcement learning as a fruitful path for developing real-world, open-domain conversational agents.
ER  -


TY  - Preprint
T1  - Dimensionality Reduction in Deep Learning for Chest X-Ray Analysis of Lung Cancer
A1  - Yu. Gordienko
A1  - Yu. Kochura
A1  - O. Alienin
A1  - O. Rokovyi
A1  - S. Stirenko
A1  - Peng Gang
A1  - Jiang Hui
A1  - Wei Zeng
JO  - ArXiv e-prints
Y1  - 19 January, 2018
UR  - https://arxiv.org/abs/1801.06495
N2  - Efficiency of some dimensionality reduction techniques, like lung segmentation, bone shadow exclusion, and t-distributed stochastic neighbor embedding (t-SNE) for exclusion of outliers, is estimated for analysis of chest X-ray (CXR) 2D images by deep learning approach to help radiologists identify marks of lung cancer in CXR. Training and validation of the simple convolutional neural network (CNN) was performed on the open JSRT dataset (dataset #01), the JSRT after bone shadow exclusion - BSE-JSRT (dataset #02), JSRT after lung segmentation (dataset #03), BSE-JSRT after lung segmentation (dataset #04), and segmented BSE-JSRT after exclusion of outliers by t-SNE method (dataset #05). The results demonstrate that the pre-processed dataset obtained after lung segmentation, bone shadow exclusion, and filtering out the outliers by t-SNE (dataset #05) demonstrates the highest training rate and best accuracy in comparison to the other pre-processed datasets.
ER  -


TY  - Preprint
T1  - Deep Learning for Detecting Cyberbullying Across Multiple Social Media Platforms
A1  - Sweta Agrawal
A1  - Amit Awekar
JO  - ArXiv e-prints
Y1  - 19 January, 2018
UR  - https://arxiv.org/abs/1801.06482
N2  - Harassment by cyberbullies is a significant phenomenon on the social media. Existing works for cyberbullying detection have at least one of the following three bottlenecks. First, they target only one particular social media platform (SMP). Second, they address just one topic of cyberbullying. Third, they rely on carefully handcrafted features of the data. We show that deep learning based models can overcome all three bottlenecks. Knowledge learned by these models on one dataset can be transferred to other datasets. We performed extensive experiments using three real-world datasets: Formspring (12k posts), Twitter (16k posts), and Wikipedia(100k posts). Our experiments provide several useful insights about cyberbullying detection. To the best of our knowledge, this is the first work that systematically analyzes cyberbullying detection on various topics across multiple SMPs using deep learning based models and transfer learning.
ER  -


TY  - Preprint
T1  - An End-to-End Deep Learning Histochemical Scoring System for Breast Cancer Tissue Microarray
A1  - Jingxin Liu
A1  - Bolei Xu
A1  - Chi Zheng
A1  - Yuanhao Gong
A1  - Jon Garibaldi
A1  - Daniele Soria
A1  - Andew Green
A1  - Ian O. Ellis
A1  - Wenbin Zou
A1  - Guoping Qiu
JO  - ArXiv e-prints
Y1  - 18 January, 2018
UR  - https://arxiv.org/abs/1801.06288
N2  - One of the methods for stratifying different molecular classes of breast cancer is the Nottingham Prognostic Index Plus (NPI+) which uses breast cancer relevant biomarkers to stain tumour tissues prepared on tissue microarray (TMA). To determine the molecular class of the tumour, pathologists will have to manually mark the nuclei activity biomarkers through a microscope and use a semi-quantitative assessment method to assign a histochemical score (H-Score) to each TMA core. Manually marking positively stained nuclei is a time consuming, imprecise and subjective process which will lead to inter-observer and intra-observer discrepancies. In this paper, we present an end-to-end deep learning system which directly predicts the H-Score automatically. Our system imitates the pathologists&#39; decision process and uses one fully convolutional network (FCN) to extract all nuclei region (tumour and non-tumour), a second FCN to extract tumour nuclei region, and a multi-column convolutional neural network which takes the outputs of the first two FCNs and the stain intensity description image as input and acts as the high-level decision making mechanism to directly output the H-Score of the input TMA image. To the best of our knowledge, this is the first end-to-end system that takes a TMA image as input and directly outputs a clinical score. We will present experimental results which demonstrate that the H-Scores predicted by our model have very high and statistically significant correlation with experienced pathologists&#39; scores and that the H-Score discrepancy between our algorithm and the pathologists is on par with the inter-subject discrepancy between the pathologists.
ER  -


TY  - Preprint
T1  - Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning
A1  - Baolin Peng
A1  - Xiujun Li
A1  - Jianfeng Gao
A1  - Jingjing Liu
A1  - Kam-Fai Wong
A1  - Shang-Yu Su
JO  - ArXiv e-prints
Y1  - 23 May, 2018
UR  - https://arxiv.org/abs/1801.06176
N2  - Training a task-completion dialogue agent via reinforcement learning (RL) is costly because it requires many interactions with real users. One common alternative is to use a user simulator. However, a user simulator usually lacks the language complexity of human interlocutors and the biases in its design may tend to degrade the agent. To address these issues, we present Deep Dyna-Q, which to our knowledge is the first deep RL framework that integrates planning for task-completion dialogue policy learning. We incorporate into the dialogue agent a model of the environment, referred to as the world model, to mimic real user response and generate simulated experience. During dialogue policy learning, the world model is constantly updated with real user experience to approach real user behavior, and in turn, the dialogue agent is optimized using both real experience and simulated experience. The effectiveness of our approach is demonstrated on a movie-ticket booking task in both simulated and human-in-the-loop settings.
ER  -


TY  - Preprint
T1  - Deep Learning for Fatigue Estimation on the Basis of Multimodal Human-Machine Interactions
A1  - Yuri Gordienko
A1  - Sergii Stirenko
A1  - Yuriy Kochura
A1  - Oleg Alienin
A1  - Michail Novotarskiy
A1  - Nikita Gordienko
JO  - ArXiv e-prints
Y1  - 30 December, 2017
UR  - https://arxiv.org/abs/1801.06048
N2  - The new method is proposed to monitor the level of current physical load and accumulated fatigue by several objective and subjective characteristics. It was applied to the dataset targeted to estimate the physical load and fatigue by several statistical and machine learning methods. The data from peripheral sensors (accelerometer, GPS, gyroscope, magnetometer) and brain-computing interface (electroencephalography) were collected, integrated, and analyzed by several statistical and machine learning methods (moment analysis, cluster analysis, principal component analysis, etc.). The hypothesis 1 was presented and proved that physical activity can be classified not only by objective parameters, but by subjective parameters also. The hypothesis 2 (experienced physical load and subsequent restoration as fatigue level can be estimated quantitatively and distinctive patterns can be recognized) was presented and some ways to prove it were demonstrated. Several &#34;physical load&#34; and &#34;fatigue&#34; metrics were proposed. The results presented allow to extend application of the machine learning methods for characterization of complex human activity patterns (for example, to estimate their actual physical load and fatigue, and give cautions and advice).
ER  -


TY  - Preprint
T1  - Experience-driven Networking: A Deep Reinforcement Learning based Approach
A1  - Zhiyuan Xu
A1  - Jian Tang
A1  - Jingsong Meng
A1  - Weiyi Zhang
A1  - Yanzhi Wang
A1  - Chi Harold Liu
A1  - Dejun Yang
JO  - ArXiv e-prints
Y1  - 17 January, 2018
UR  - https://arxiv.org/abs/1801.05757
N2  - Modern communication networks have become very complicated and highly dynamic, which makes them hard to model, predict and control. In this paper, we develop a novel experience-driven approach that can learn to well control a communication network from its own experience rather than an accurate mathematical model, just as a human learns a new skill (such as driving, swimming, etc). Specifically, we, for the first time, propose to leverage emerging Deep Reinforcement Learning (DRL) for enabling model-free control in communication networks; and present a novel and highly effective DRL-based control framework, DRL-TE, for a fundamental networking problem: Traffic Engineering (TE). The proposed framework maximizes a widely-used utility function by jointly learning network environment and its dynamics, and making decisions under the guidance of powerful Deep Neural Networks (DNNs). We propose two new techniques, TE-aware exploration and actor-critic-based prioritized experience replay, to optimize the general DRL framework particularly for TE. To validate and evaluate the proposed framework, we implemented it in ns-3, and tested it comprehensively with both representative and randomly generated network topologies. Extensive packet-level simulation results show that 1) compared to several widely-used baseline methods, DRL-TE significantly reduces end-to-end delay and consistently improves the network utility, while offering better or comparable throughput; 2) DRL-TE is robust to network changes; and 3) DRL-TE consistently outperforms a state-ofthe-art DRL method (for continuous control), Deep Deterministic Policy Gradient (DDPG), which, however, does not offer satisfying performance.
ER  -


TY  - Preprint
T1  - The Case for Automatic Database Administration using Deep Reinforcement Learning
A1  - Ankur Sharma
A1  - Felix Martin Schuhknecht
A1  - Jens Dittrich
JO  - ArXiv e-prints
Y1  - 17 January, 2018
UR  - https://arxiv.org/abs/1801.05643
N2  - Like any large software system, a full-fledged DBMS offers an overwhelming amount of configuration knobs. These range from static initialisation parameters like buffer sizes, degree of concurrency, or level of replication to complex runtime decisions like creating a secondary index on a particular column or reorganising the physical layout of the store. To simplify the configuration, industry grade DBMSs are usually shipped with various advisory tools, that provide recommendations for given workloads and machines. However, reality shows that the actual configuration, tuning, and maintenance is usually still done by a human administrator, relying on intuition and experience. Recent work on deep reinforcement learning has shown very promising results in solving problems, that require such a sense of intuition. For instance, it has been applied very successfully in learning how to play complicated games with enormous search spaces. Motivated by these achievements, in this work we explore how deep reinforcement learning can be used to administer a DBMS. First, we will describe how deep reinforcement learning can be used to automatically tune an arbitrary software system like a DBMS by defining a problem environment. Second, we showcase our concept of NoDBA at the concrete example of index selection and evaluate how well it recommends indexes for given workloads.
ER  -


TY  - Preprint
T1  - Cellular-Connected UAVs over 5G: Deep Reinforcement Learning for Interference Management
A1  - Ursula Challita
A1  - Walid Saad
A1  - Christian Bettstetter
JO  - ArXiv e-prints
Y1  - 16 January, 2018
UR  - https://arxiv.org/abs/1801.05500
N2  - In this paper, an interference-aware path planning scheme for a network of cellular-connected unmanned aerial vehicles (UAVs) is proposed. In particular, each UAV aims at achieving a tradeoff between maximizing energy efficiency and minimizing both wireless latency and the interference level caused on the ground network along its path. The problem is cast as a dynamic game among UAVs. To solve this game, a deep reinforcement learning algorithm, based on echo state network (ESN) cells, is proposed. The introduced deep ESN architecture is trained to allow each UAV to map each observation of the network state to an action, with the goal of minimizing a sequence of time-dependent utility functions. Each UAV uses ESN to learn its optimal path, transmission power level, and cell association vector at different locations along its path. The proposed algorithm is shown to reach a subgame perfect Nash equilibrium (SPNE) upon convergence. Moreover, an upper and lower bound for the altitude of the UAVs is derived thus reducing the computational complexity of the proposed algorithm. Simulation results show that the proposed scheme achieves better wireless latency per UAV and rate per ground user (UE) while requiring a number of steps that is comparable to a heuristic baseline that considers moving via the shortest distance towards the corresponding destinations. The results also show that the optimal altitude of the UAVs varies based on the ground network density and the UE data rate requirements and plays a vital role in minimizing the interference level on the ground UEs as well as the wireless transmission delay of the UAV.
ER  -


TY  - Preprint
T1  - Deep learning for determining a near-optimal topological design without any iteration
A1  - Yonggyun Yu
A1  - Taeil Hur
A1  - Jaeho Jung
A1  - In Gwun Jang
JO  - ArXiv e-prints
Y1  - 22 September, 2018
UR  - https://arxiv.org/abs/1801.05463
N2  - In this study, we propose a novel deep learning-based method to predict an optimized structure for a given boundary condition and optimization setting without using any iterative scheme. For this purpose, first, using open-source topology optimization code, datasets of the optimized structures paired with the corresponding information on boundary conditions and optimization settings are generated at low (32 x 32) and high (128 x 128) resolutions. To construct the artificial neural network for the proposed method, a convolutional neural network (CNN)-based encoder and decoder network is trained using the training dataset generated at low resolution. Then, as a two-stage refinement, the conditional generative adversarial network (cGAN) is trained with the optimized structures paired at both low and high resolutions, and is connected to the trained CNN-based encoder and decoder network. The performance evaluation results of the integrated network demonstrate that the proposed method can determine a near-optimal structure in terms of pixel values and compliance with negligible computational time.
ER  -


TY  - Preprint
T1  - Solutions to problems with deep learning
A1  - J Gerard Wolff
JO  - ArXiv e-prints
Y1  - 8 January, 2018
UR  - https://arxiv.org/abs/1801.05457
N2  - Despite the several successes of deep learning systems, there are concerns about their limitations, discussed most recently by Gary Marcus. This paper discusses Marcus&#39;s concerns and some others, together with solutions to several of these problems provided by the &#34;P theory of intelligence&#34; and its realisation in the &#34;SP computer model&#34;. The main advantages of the SP system are: relatively small requirements for data and the ability to learn from a single experience; the ability to model both hierarchical and non-hierarchical structures; strengths in several kinds of reasoning, including `commonsense&#39; reasoning; transparency in the representation of knowledge, and the provision of an audit trail for all processing; the likelihood that the SP system could not be fooled into bizarre or eccentric recognition of stimuli, as deep learning systems can be; the SP system provides a robust solution to the problem of `catastrophic forgetting&#39; in deep learning systems; the SP system provides a theoretically-coherent solution to the problems of correcting over- and under-generalisations in learning, and learning correct structures despite errors in data; unlike most research on deep learning, the SP programme of research draws extensively on research on human learning, perception, and cognition; and the SP programme of research has an overarching theory, supported by evidence, something that is largely missing from research on deep learning. In general, the SP system provides a much firmer foundation than deep learning for the development of artificial general intelligence.
ER  -


TY  - Preprint
T1  - An Automated System for Epilepsy Detection using EEG Brain Signals based on Deep Learning Approach
A1  - Ihsan Ullah
A1  - Muhammad Hussain
A1  - Emad-ul-Haq Qazi
A1  - Hatim Aboalsamh
JO  - ArXiv e-prints
Y1  - 16 January, 2018
UR  - https://arxiv.org/abs/1801.05412
N2  - Epilepsy is a neurological disorder and for its detection, encephalography (EEG) is a commonly used clinical approach. Manual inspection of EEG brain signals is a time-consuming and laborious process, which puts heavy burden on neurologists and affects their performance. Several automatic techniques have been proposed using traditional approaches to assist neurologists in detecting binary epilepsy scenarios e.g. seizure vs. non-seizure or normal vs. ictal. These methods do not perform well when classifying ternary case e.g. ictal vs. normal vs. inter-ictal; the maximum accuracy for this case by the state-of-the-art-methods is 97+-1%. To overcome this problem, we propose a system based on deep learning, which is an ensemble of pyramidal one-dimensional convolutional neural network (P-1D-CNN) models. In a CNN model, the bottleneck is the large number of learnable parameters. P-1D-CNN works on the concept of refinement approach and it results in 60% fewer parameters compared to traditional CNN models. Further to overcome the limitations of small amount of data, we proposed augmentation schemes for learning P-1D-CNN model. In almost all the cases concerning epilepsy detection, the proposed system gives an accuracy of 99.1+-0.9% on the University of Bonn dataset.
ER  -


TY  - Preprint
T1  - Learning Deep Features for One-Class Classification
A1  - Pramuditha Perera
A1  - Vishal M. Patel
JO  - ArXiv e-prints
Y1  - 16 January, 2018
UR  - https://arxiv.org/abs/1801.05365
N2  - We propose a deep learning-based solution for the problem of feature learning in one-class classification. The proposed method operates on top of a Convolutional Neural Network (CNN) of choice and produces descriptive features while maintaining a low intra-class variance in the feature space for the given class. For this purpose two loss functions, compactness loss and descriptiveness loss are proposed along with a parallel CNN architecture. A template matching-based framework is introduced to facilitate the testing process. Extensive experiments on publicly available anomaly detection, novelty detection and mobile active authentication datasets show that the proposed Deep One-Class (DOC) classification method achieves significant improvements over the state-of-the-art.
ER  -


TY  - Preprint
T1  - Evidential Occupancy Grid Map Augmentation using Deep Learning
A1  - Sascha Wirges
A1  - Felix Hartenbach
A1  - Christoph Stiller
JO  - ArXiv e-prints
Y1  - 18 April, 2018
UR  - https://arxiv.org/abs/1801.05297
N2  - A detailed environment representation is a crucial component of automated vehicles. Using single range sensor scans, data is often too sparse and subject to occlusions. Therefore, we present a method to augment occupancy grid maps from single views to be similar to evidential occupancy maps acquired from different views using Deep Learning. To accomplish this, we estimate motion between subsequent range sensor measurements and create an evidential 3D voxel map in an extensive post-processing step. Within this voxel map, we explicitly model uncertainty using evidence theory and create a 2D projection using combination rules. As input for our neural networks, we use a multi-layer grid map consisting of the three features detections, transmissions and intensity, each for ground and non-ground measurements. Finally, we perform a quantitative and qualitative evaluation which shows that different network architectures accurately infer evidential measures in real-time.
ER  -


TY  - Preprint
T1  - Deep Multi-Spectral Registration Using Invariant Descriptor Learning
A1  - Nati Ofir
A1  - Shai Silberstein
A1  - Hila Levi
A1  - Dani Rozenbaum
A1  - Yosi Keller
A1  - Sharon Duvdevani Bar
JO  - ArXiv e-prints
Y1  - 23 May, 2018
UR  - https://arxiv.org/abs/1801.05171
N2  - In this paper, we introduce a novel deep-learning method to align cross-spectral images. Our approach relies on a learned descriptor which is invariant to different spectra. Multi-modal images of the same scene capture different signals and therefore their registration is challenging and it is not solved by classic approaches. To that end, we developed a feature-based approach that solves the visible (VIS) to Near-Infra-Red (NIR) registration problem. Our algorithm detects corners by Harris and matches them by a patch-metric learned on top of CIFAR-10 network descriptor. As our experiments demonstrate we achieve a high-quality alignment of cross-spectral images with a sub-pixel accuracy. Comparing to other existing methods, our approach is more accurate in the task of VIS to NIR registration.
ER  -


TY  - Preprint
T1  - Learning to Navigate: Exploiting Deep Networks to Inform Sample-Based Planning During Vision-Based Navigation
A1  - Justin S. Smith
A1  - Jin-Ha Hwang
A1  - Fu-Jen Chu
A1  - Patricio A. Vela
JO  - ArXiv e-prints
Y1  - 16 January, 2018
UR  - https://arxiv.org/abs/1801.05132
N2  - Recent applications of deep learning to navigation have generated end-to-end navigation solutions whereby visual sensor input is mapped to control signals or to motion primitives. The resulting visual navigation strategies work very well at collision avoidance and have performance that matches traditional reactive navigation algorithms while operating in real-time. It is accepted that these solutions cannot provide the same level of performance as a global planner. However, it is less clear how such end-to-end systems should be integrated into a full navigation pipeline. We evaluate the typical end-to-end solution within a full navigation pipeline in order to expose its weaknesses. Doing so illuminates how to better integrate deep learning methods into the navigation pipeline. In particular, we show that they are an efficient means to provide informed samples for sample-based planners. Controlled simulations with comparison against traditional planners show that the number of samples can be reduced by an order of magnitude while preserving navigation performance. Implementation on a mobile robot matches the simulated performance outcomes.
ER  -


TY  - Preprint
T1  - Deep Metric Learning with BIER: Boosting Independent Embeddings Robustly
A1  - Michael Opitz
A1  - Georg Waltner
A1  - Horst Possegger
A1  - Horst Bischof
JO  - ArXiv e-prints
Y1  - 15 January, 2018
UR  - https://arxiv.org/abs/1801.04815
N2  - Learning similarity functions between image pairs with deep neural networks yields highly correlated activations of embeddings. In this work, we show how to improve the robustness of such embeddings by exploiting the independence within ensembles. To this end, we divide the last embedding layer of a deep network into an embedding ensemble and formulate training this ensemble as an online gradient boosting problem. Each learner receives a reweighted training sample from the previous learners. Further, we propose two loss functions which increase the diversity in our ensemble. These loss functions can be applied either for weight initialization or during training. Together, our contributions leverage large embedding sizes more effectively by significantly reducing correlation of the embedding and consequently increase retrieval accuracy of the embedding. Our method works with any differentiable loss function and does not introduce any additional parameters during test time. We evaluate our metric learning method on image retrieval tasks and show that it improves over state-of-the-art methods on the CUB 200-2011, Cars-196, Stanford Online Products, In-Shop Clothes Retrieval and VehicleID datasets.
ER  -


TY  - Preprint
T1  - Detecting Offensive Language in Tweets Using Deep Learning
A1  - Georgios K. Pitsilis
A1  - Heri Ramampiaro
A1  - Helge Langseth
JO  - ArXiv e-prints
Y1  - 13 January, 2018
UR  - https://arxiv.org/abs/1801.04433
N2  - This paper addresses the important problem of discerning hateful content in social media. We propose a detection scheme that is an ensemble of Recurrent Neural Network (RNN) classifiers, and it incorporates various features associated with user-related information, such as the users&#39; tendency towards racism or sexism. These data are fed as input to the above classifiers along with the word frequency vectors derived from the textual content. Our approach has been evaluated on a publicly available corpus of 16k tweets, and the results demonstrate its effectiveness in comparison to existing state of the art solutions. More specifically, our scheme can successfully distinguish racism and sexism messages from normal text, and achieve higher classification quality than current state-of-the-art algorithms.
ER  -


TY  - Preprint
T1  - Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers
A1  - Ji Gao
A1  - Jack Lanchantin
A1  - Mary Lou Soffa
A1  - Yanjun Qi
JO  - ArXiv e-prints
Y1  - 23 May, 2018
UR  - https://arxiv.org/abs/1801.04354
N2  - Although various techniques have been proposed to generate adversarial samples for white-box attacks on text, little attention has been paid to black-box attacks, which are more realistic scenarios. In this paper, we present a novel algorithm, DeepWordBug, to effectively generate small text perturbations in a black-box setting that forces a deep-learning classifier to misclassify a text input. We employ novel scoring strategies to identify the critical tokens that, if modified, cause the classifier to make an incorrect prediction. Simple character-level transformations are applied to the highest-ranked tokens in order to minimize the edit distance of the perturbation, yet change the original classification. We evaluated DeepWordBug on eight real-world text datasets, including text classification, sentiment analysis, and spam detection. We compare the result of DeepWordBug with two baselines: Random (Black-box) and Gradient (White-box). Our experimental results indicate that DeepWordBug reduces the prediction accuracy of current state-of-the-art deep-learning models, including a decrease of 68\% on average for a Word-LSTM model and 48\% on average for a Char-CNN model.
ER  -


TY  - Preprint
T1  - Towards Arbitrary Noise Augmentation - Deep Learning for Sampling from Arbitrary Probability Distributions
A1  - Felix Horger
A1  - Tobias WÃ¼rfl
A1  - Vincent Christlein
A1  - Andreas Maier
JO  - ArXiv e-prints
Y1  - 10 July, 2018
UR  - https://arxiv.org/abs/1801.04211
N2  - Accurate noise modelling is important for training of deep learning reconstruction algorithms. While noise models are well known for traditional imaging techniques, the noise distribution of a novel sensor may be difficult to determine a priori. Therefore, we propose learning arbitrary noise distributions. To do so, this paper proposes a fully connected neural network model to map samples from a uniform distribution to samples of any explicitly known probability density function. During the training, the Jensen-Shannon divergence between the distribution of the model&#39;s output and the target distribution is minimized. We experimentally demonstrate that our model converges towards the desired state. It provides an alternative to existing sampling methods such as inversion sampling, rejection sampling, Gaussian mixture models and Markov-Chain-Monte-Carlo. Our model has high sampling efficiency and is easily applied to any probability distribution, without the need of further analytical or numerical calculations.
ER  -


TY  - Preprint
T1  - Arhuaco: Deep Learning and Isolation Based Security for Distributed High-Throughput Computing
A1  - A. Gomez Ramirez
A1  - C. Lara
A1  - L. Betev
A1  - D. Bilanovic
A1  - U. Kebschull
JO  - ArXiv e-prints
Y1  - 12 January, 2018
UR  - https://arxiv.org/abs/1801.04179
N2  - Grid computing systems require innovative methods and tools to identify cybersecurity incidents and perform autonomous actions i.e. without administrator intervention. They also require methods to isolate and trace job payload activity in order to protect users and find evidence of malicious behavior. We introduce an integrated approach of security monitoring via Security by Isolation with Linux Containers and Deep Learning methods for the analysis of real time data in Grid jobs running inside virtualized High-Throughput Computing infrastructure in order to detect and prevent intrusions. A dataset for malware detection in Grid computing is described. We show in addition the utilization of generative methods with Recurrent Neural Networks to improve the collected dataset. We present Arhuaco, a prototype implementation of the proposed methods. We empirically study the performance of our technique. The results show that Arhuaco outperforms other methods used in Intrusion Detection Systems for Grid Computing. The study is carried out in the ALICE Collaboration Grid, part of the Worldwide LHC Computing Grid.
ER  -


TY  - Preprint
T1  - Speech Dereverberation Based on Integrated Deep and Ensemble Learning Algorithm
A1  - Wei-Jen Lee
A1  - Syu-Siang Wang
A1  - Fei Chen
A1  - Xugang Lu
A1  - Shao-Yi Chien
A1  - Yu Tsao
JO  - ArXiv e-prints
Y1  - 16 February, 2018
UR  - https://arxiv.org/abs/1801.04052
N2  - Reverberation, which is generally caused by sound reflections from walls, ceilings, and floors, can result in severe performance degradation of acoustic applications. Due to a complicated combination of attenuation and time-delay effects, the reverberation property is difficult to characterize, and it remains a challenging task to effectively retrieve the anechoic speech signals from reverberation ones. In the present study, we proposed a novel integrated deep and ensemble learning algorithm (IDEA) for speech dereverberation. The IDEA consists of offline and online phases. In the offline phase, we train multiple dereverberation models, each aiming to precisely dereverb speech signals in a particular acoustic environment; then a unified fusion function is estimated that aims to integrate the information of multiple dereverberation models. In the online phase, an input utterance is first processed by each of the dereverberation models. The outputs of all models are integrated accordingly to generate the final anechoic signal. We evaluated the IDEA on designed acoustic environments, including both matched and mismatched conditions of the training and testing data. Experimental results confirm that the proposed IDEA outperforms single deep-neural-network-based dereverberation model with the same model architecture and training data.
ER  -


TY  - Preprint
T1  - MXNET-MPI: Embedding MPI parallelism in Parameter Server Task Model for scaling Deep Learning
A1  - Amith R Mamidala
A1  - Georgios Kollias
A1  - Chris Ward
A1  - Fausto Artico
JO  - ArXiv e-prints
Y1  - 11 January, 2018
UR  - https://arxiv.org/abs/1801.03855
N2  - Existing Deep Learning frameworks exclusively use either Parameter Server(PS) approach or MPI parallelism. In this paper, we discuss the drawbacks of such approaches and propose a generic framework supporting both PS and MPI programming paradigms, co-existing at the same time. The key advantage of the new model is to embed the scaling benefits of MPI parallelism into the loosely coupled PS task model. Apart from providing a practical usage model of MPI in cloud, such framework allows for novel communication avoiding algorithms that do parameter averaging in Stochastic Gradient Descent(SGD) approaches. We show how MPI and PS models can synergestically apply algorithms such as Elastic SGD to improve the rate of convergence against existing approaches. These new algorithms directly help scaling SGD clusterwide. Further, we also optimize the critical component of the framework, namely global aggregation or allreduce using a novel concept of tensor collectives. These treat a group of vectors on a node as a single object allowing for the existing single vector algorithms to be directly applicable. We back our claims with sufficient emperical evidence using large scale ImageNet 1K data. Our framework is built upon MXNET but the design is generic and can be adapted to other popular DL infrastructures.
ER  -


TY  - Preprint
T1  - Supervised and Unsupervised Tumor Characterization in the Deep Learning Era
A1  - Sarfaraz Hussein
A1  - Maria M. Chuquicusma
A1  - Pujan Kandel
A1  - Candice W. Bolan
A1  - Michael B. Wallace
A1  - Ulas Bagci
JO  - ArXiv e-prints
Y1  - 29 July, 2018
UR  - https://arxiv.org/abs/1801.03230
N2  - Cancer is among the leading causes of death worldwide. Risk stratification of cancer tumors in radiology images can be improved with computer-aided diagnosis (CAD) tools which can be made faster and more accurate. Tumor characterization through CADs can enable non-invasive cancer staging and prognosis, and foster personalized treatment planning as a part of precision medicine. In this study, we propose both supervised and unsupervised machine learning strategies to improve tumor characterization. Our first approach is based on supervised learning for which we demonstrate significant gains in deep learning algorithms, particularly by utilizing a 3D Convolutional Neural Network along with transfer learning. Motivated by the radiologists&#39; interpretations of the scans, we then show how to incorporate task dependent feature representations into a CAD system via a &#34;graph-regularized sparse Multi-Task Learning (MTL)&#34; framework.
ER  -


TY  - Preprint
T1  - An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos
A1  - B Ravi Kiran
A1  - Dilip Mathew Thomas
A1  - Ranjith Parakkal
JO  - ArXiv e-prints
Y1  - 30 January, 2018
UR  - https://arxiv.org/abs/1801.03149
N2  - Videos represent the primary source of information for surveillance applications and are available in large amounts but in most cases contain little or no annotation for supervised learning. This article reviews the state-of-the-art deep learning based methods for video anomaly detection and categorizes them based on the type of model and criteria of detection. We also perform simple studies to understand the different approaches and provide the criteria of evaluation for spatio-temporal anomaly detection.
ER  -


TY  - Preprint
T1  - Utilising Deep Learning and Genome Wide Association Studies for Epistatic-Driven Preterm Birth Classification in African-American Women
A1  - Paul Fergus
A1  - Casimiro Curbelo Montanez
A1  - Basma Abdulaimma
A1  - Paulo Lisboa
A1  - Carl Chalmers
JO  - ArXiv e-prints
Y1  - 6 January, 2018
UR  - https://arxiv.org/abs/1801.02977
N2  - Genome Wide Association Studies (GWAS) are used to identify statistically significant genetic variants in case-control studies. GWAS typically use a p-value threshold of 5 x 10-8 to identify highly ranked single nucleotide polymorphisms (SNPs). However, evidence has shown that many of these are, in fact, false positives. Using lower p-values it is possible to to investigate the joint epistatic interactions between SNPs and provide better insights into phenotype expression. However, computational complexity is increased exponentially as a function of higher-order combinations. In this paper, we propose a novel framework, based on nonlinear transformations of combinatorically large SNP data, using stacked autoencoders, to identify higher-order SNP interactions. We focus on the challenging problem of classifying preterm births. Evidence suggests that this complex condition has a strong genetic component with unexplained heritability reportedly between 20%-40%. This claim is substantiated using a GWAS data set, obtained from dbGap, which contains predominantly urban low-income African-American women who had normal deliveries (between 37 and 42 weeks of gestation) and preterm deliveries (less than 37 weeks of gestation). Latent representations from original SNP sequences are used to initialize a deep learning classifier before it is fine-tuned for classification tasks (term and preterm births). The complete network models the epistatic effects of major and minor SNP perturbations. All models are evaluated using standard binary classifier performance metrics. The findings show that important information pertaining to SNPs and epistasis can be extracted from 4666 raw SNPs generated using logistic regression (p-value=5 x 10-3) and used to fit a deep learning model and obtain results (Sen=0.9289, Spec=0.9591, Gini=0.9651, Logloss=0.3080, AUC=0.9825, MSE=0.0942) using 500 hidden nodes.
ER  -


TY  - Preprint
T1  - A Predictive Approach Using Deep Feature Learning for Electronic Medical Records: A Comparative Study
A1  - Milad Zafar Nezhad
A1  - Dongxiao Zhu
A1  - Najibesadat Sadati
A1  - Kai Yang
JO  - ArXiv e-prints
Y1  - 6 January, 2018
UR  - https://arxiv.org/abs/1801.02961
N2  - Massive amount of electronic medical records accumulating from patients and populations motivates clinicians and data scientists to collaborate for the advanced analytics to extract knowledge that is essential to address the extensive personalized insights needed for patients, clinicians, providers, scientists, and health policy makers. In this paper, we propose a new predictive approach based on feature representation using deep feature learning and word embedding techniques. Our method uses different deep architectures for feature representation in higher-level abstraction to obtain effective and more robust features from EMRs, and then build prediction models on the top of them. Our approach is particularly useful when the unlabeled data is abundant whereas labeled one is scarce. We investigate the performance of representation learning through a supervised approach. First, we apply our method on a small dataset related to a specific precision medicine problem, which focuses on prediction of left ventricular mass indexed to body surface area (LVMI) as an indicator of heart damage risk in a vulnerable demographic subgroup (African-Americans). Then we use two large datasets from eICU collaborative research database to predict the length of stay in Cardiac-ICU and Neuro-ICU based on high dimensional features. Finally we provide a comparative study and show that our predictive approach leads to better results in comparison with others.
ER  -


TY  - Preprint
T1  - Adversarial Deep Learning for Robust Detection of Binary Encoded Malware
A1  - Abdullah Al-Dujaili
A1  - Alex Huang
A1  - Erik Hemberg
A1  - Una-May O&#39;Reilly
JO  - ArXiv e-prints
Y1  - 25 March, 2018
UR  - https://arxiv.org/abs/1801.02950
N2  - Malware is constantly adapting in order to avoid detection. Model based malware detectors, such as SVM and neural networks, are vulnerable to so-called adversarial examples which are modest changes to detectable malware that allows the resulting malware to evade detection. Continuous-valued methods that are robust to adversarial examples of images have been developed using saddle-point optimization formulations. We are inspired by them to develop similar methods for the discrete, e.g. binary, domain which characterizes the features of malware. A specific extra challenge of malware is that the adversarial examples must be generated in a way that preserves their malicious functionality. We introduce methods capable of generating functionally preserved adversarial malware examples in the binary domain. Using the saddle-point formulation, we incorporate the adversarial examples into the training of models that are robust to them. We evaluate the effectiveness of the methods and others in the literature on a set of Portable Execution~(PE) files. Comparison prompts our introduction of an online measure computed during training to assess general expectation of robustness.
ER  -


TY  - Preprint
T1  - Distributed Deep Reinforcement Learning: Learn how to play Atari games in 21 minutes
A1  - Igor Adamski
A1  - Robert Adamski
A1  - Tomasz Grel
A1  - Adam JÄdrych
A1  - Kamil Kaczmarek
A1  - Henryk Michalewski
JO  - ArXiv e-prints
Y1  - 9 April, 2018
UR  - https://arxiv.org/abs/1801.02852
N2  - We present a study in Distributed Deep Reinforcement Learning (DDRL) focused on scalability of a state-of-the-art Deep Reinforcement Learning algorithm known as Batch Asynchronous Advantage ActorCritic (BA3C). We show that using the Adam optimization algorithm with a batch size of up to 2048 is a viable choice for carrying out large scale machine learning computations. This, combined with careful reexamination of the optimizer&#39;s hyperparameters, using synchronous training on the node level (while keeping the local, single node part of the algorithm asynchronous) and minimizing the memory footprint of the model, allowed us to achieve linear scaling for up to 64 CPU nodes. This corresponds to a training time of 21 minutes on 768 CPU cores, as opposed to 10 hours when using a single node with 24 cores achieved by a baseline single-node implementation.
ER  -


TY  - Preprint
T1  - DeepTraffic: Driving Fast through Dense Traffic with Deep Reinforcement Learning
A1  - Lex Fridman
A1  - Benedikt Jenik
A1  - Jack Terwilliger
JO  - ArXiv e-prints
Y1  - 9 January, 2018
UR  - https://arxiv.org/abs/1801.02805
N2  - We present a micro-traffic simulation (named &#34;DeepTraffic&#34;) where the perception, control, and planning systems for one of the cars are all handled by a single neural network as part of a model-free, off-policy reinforcement learning process. The primary goal of DeepTraffic is to make the hands-on study of deep reinforcement learning accessible to thousands of students, educators, and researchers in order to inspire and fuel the exploration and evaluation of DQN variants and hyperparameter configurations through large-scale, open competition. This paper investigates the crowd-sourced hyperparameter tuning of the policy network that resulted from the first iteration of the DeepTraffic competition where thousands of participants actively searched through the hyperparameter space with the objective of their neural network submission to make it onto the top-10 leaderboard.
ER  -


TY  - Preprint
T1  - Near Maximum Likelihood Decoding with Deep Learning
A1  - Eliya Nachmani
A1  - Yaron Bachar
A1  - Elad Marciano
A1  - David Burshtein
A1  - Yair Be&#39;ery
JO  - ArXiv e-prints
Y1  - 8 January, 2018
UR  - https://arxiv.org/abs/1801.02726
N2  - A novel and efficient neural decoder algorithm is proposed. The proposed decoder is based on the neural Belief Propagation algorithm and the Automorphism Group. By combining neural belief propagation with permutations from the Automorphism Group we achieve near maximum likelihood performance for High Density Parity Check codes. Moreover, the proposed decoder significantly improves the decoding complexity, compared to our earlier work on the topic. We also investigate the training process and show how it can be accelerated. Simulations of the hessian and the condition number show why the learning process is accelerated. We demonstrate the decoding algorithm for various linear block codes of length up to 63 bits.
ER  -


TY  - Preprint
T1  - HeNet: A Deep Learning Approach on Intel$^\circledR$ Processor Trace for Effective Exploit Detection
A1  - Li Chen
A1  - Salmin Sultana
A1  - Ravi Sahita
JO  - ArXiv e-prints
Y1  - 8 January, 2018
UR  - https://arxiv.org/abs/1801.02318
N2  - This paper presents HeNet, a hierarchical ensemble neural network, applied to classify hardware-generated control flow traces for malware detection. Deep learning-based malware detection has so far focused on analyzing executable files and runtime API calls. Static code analysis approaches face challenges due to obfuscated code and adversarial perturbations. Behavioral data collected during execution is more difficult to obfuscate but recent research has shown successful attacks against API call based malware classifiers. We investigate control flow based characterization of a program execution to build robust deep learning malware classifiers. HeNet consists of a low-level behavior model and a top-level ensemble model. The low-level model is a per-application behavior model, trained via transfer learning on a time-series of images generated from control flow trace of an execution. We use Intel$^\circledR$ Processor Trace enabled processor for low overhead execution tracing and design a lightweight image conversion and segmentation of the control flow trace. The top-level ensemble model aggregates the behavior classification of all the trace segments and detects an attack. The use of hardware trace adds portability to our system and the use of deep learning eliminates the manual effort of feature engineering. We evaluate HeNet against real-world exploitations of PDF readers. HeNet achieves 100\% accuracy and 0\% false positive on test set, and higher classification accuracy compared to classical machine learning algorithms.
ER  -


TY  - Preprint
T1  - Deep Fingerprinting: Undermining Website Fingerprinting Defenses with Deep Learning
A1  - Payap Sirinam
A1  - Mohsen Imani
A1  - Marc Juarez
A1  - Matthew Wright
JO  - ArXiv e-prints
Y1  - 19 August, 2018
UR  - https://arxiv.org/abs/1801.02265
N2  - Website fingerprinting enables a local eavesdropper to determine which websites a user is visiting over an encrypted connection. State-of-the-art website fingerprinting attacks have been shown to be effective even against Tor. Recently, lightweight website fingerprinting defenses for Tor have been proposed that substantially degrade existing attacks: WTF-PAD and Walkie-Talkie. In this work, we present Deep Fingerprinting (DF), a new website fingerprinting attack against Tor that leverages a type of deep learning called Convolutional Neural Networks (CNN) with a sophisticated architecture design, and we evaluate this attack against WTF-PAD and Walkie-Talkie. The DF attack attains over 98% accuracy on Tor traffic without defenses, better than all prior attacks, and it is also the only attack that is effective against WTF-PAD with over 90% accuracy. Walkie-Talkie remains effective, holding the attack to just 49.7% accuracy. In the more realistic open-world setting, our attack remains effective, with 0.99 precision and 0.94 recall on undefended traffic. Against traffic defended with WTF-PAD in this setting, the attack still can get 0.96 precision and 0.68 recall. These findings highlight the need for effective defenses that protect against this new attack and that could be deployed in Tor.
ER  -


TY  - Preprint
T1  - Theory of Deep Learning IIb: Optimization Properties of SGD
A1  - Chiyuan Zhang
A1  - Qianli Liao
A1  - Alexander Rakhlin
A1  - Brando Miranda
A1  - Noah Golowich
A1  - Tomaso Poggio
JO  - ArXiv e-prints
Y1  - 7 January, 2018
UR  - https://arxiv.org/abs/1801.02254
N2  - In Theory IIb we characterize with a mix of theory and experiments the optimization of deep convolutional networks by Stochastic Gradient Descent. The main new result in this paper is theoretical and experimental evidence for the following conjecture about SGD: SGD concentrates in probability -- like the classical Langevin equation -- on large volume, &#34;flat&#34; minima, selecting flat minimizers which are with very high probability also global minimizers
ER  -


TY  - Preprint
T1  - Detection and segmentation of the Left Ventricle in Cardiac MRI using Deep Learning
A1  - Alexandre Attia
A1  - Sharone Dayan
JO  - ArXiv e-prints
Y1  - 7 January, 2018
UR  - https://arxiv.org/abs/1801.02171
N2  - Manual segmentation of the Left Ventricle (LV) is a tedious and meticulous task that can vary depending on the patient, the Magnetic Resonance Images (MRI) cuts and the experts. Still today, we consider manual delineation done by experts as being the ground truth for cardiac diagnosticians. Thus, we are reviewing the paper - written by Avendi and al. - who presents a combined approach with Convolutional Neural Networks, Stacked Auto-Encoders and Deformable Models, to try and automate the segmentation while performing more accurately. Furthermore, we have implemented parts of the paper (around three quarts) and experimented both the original method and slightly modified versions when changing the architecture and the parameters.
ER  -


TY  - Preprint
T1  - Handover Control in Wireless Systems via Asynchronous Multi-User Deep Reinforcement Learning
A1  - Zhi Wang
A1  - Lihua Li
A1  - Yue Xu
A1  - Hui Tian
A1  - Shuguang Cui
JO  - ArXiv e-prints
Y1  - 8 May, 2018
UR  - https://arxiv.org/abs/1801.02077
N2  - In this paper, we propose a two-layer framework to learn the optimal handover (HO) controllers in possibly large-scale wireless systems supporting mobile Internet-of-Things (IoT) users or traditional cellular users, where the user mobility patterns could be heterogeneous. In particular, our proposed framework first partitions the user equipments (UEs) with different mobility patterns into clusters, where the mobility patterns are similar in the same cluster. Then, within each cluster, an asynchronous multi-user deep reinforcement learning scheme is developed to control the HO processes across the UEs in each cluster, in the goal of lowering the HO rate while ensuring certain system throughput. In this scheme, we use a deep neural network (DNN) as an HO controller learned by each UE via reinforcement learning in a collaborative fashion. Moreover, we use supervised learning in initializing the DNN controller before the execution of reinforcement learning to exploit what we already know with traditional HO schemes and to mitigate the negative effects of random exploration at the initial stage. Furthermore, we show that the adopted global-parameter-based asynchronous framework enables us to train faster with more UEs, which could nicely address the scalability issue to support large systems. Finally, simulation results demonstrate that the proposed framework can achieve better performance than the state-of-art on-line schemes, in terms of HO rates.
ER  -


TY  - Preprint
T1  - Faster Deep Q-learning using Neural Episodic Control
A1  - Daichi Nishio
A1  - Satoshi Yamane
JO  - ArXiv e-prints
Y1  - 3 June, 2018
UR  - https://arxiv.org/abs/1801.01968
N2  - The research on deep reinforcement learning which estimates Q-value by deep learning has been attracted the interest of researchers recently. In deep reinforcement learning, it is important to efficiently learn the experiences that an agent has collected by exploring environment. We propose NEC2DQN that improves learning speed of a poor sample efficiency algorithm such as DQN by using good one such as NEC at the beginning of learning. We show it is able to learn faster than Double DQN or N-step DQN in the experiments of Pong.
ER  -


TY  - Preprint
T1  - Learning Implicit Brain MRI Manifolds with Deep Learning
A1  - Camilo Bermudez
A1  - Andrew J. Plassard
A1  - Larry T. Davis
A1  - Allen T. Newton
A1  - Susan M Resnick
A1  - Bennett A. Landman
JO  - ArXiv e-prints
Y1  - 5 January, 2018
UR  - https://arxiv.org/abs/1801.01847
N2  - An important task in image processing and neuroimaging is to extract quantitative information from the acquired images in order to make observations about the presence of disease or markers of development in populations. Having a lowdimensional manifold of an image allows for easier statistical comparisons between groups and the synthesis of group representatives. Previous studies have sought to identify the best mapping of brain MRI to a low-dimensional manifold, but have been limited by assumptions of explicit similarity measures. In this work, we use deep learning techniques to investigate implicit manifolds of normal brains and generate new, high-quality images. We explore implicit manifolds by addressing the problems of image synthesis and image denoising as important tools in manifold learning. First, we propose the unsupervised synthesis of T1-weighted brain MRI using a Generative Adversarial Network (GAN) by learning from 528 examples of 2D axial slices of brain MRI. Synthesized images were first shown to be unique by performing a crosscorrelation with the training set. Real and synthesized images were then assessed in a blinded manner by two imaging experts providing an image quality score of 1-5. The quality score of the synthetic image showed substantial overlap with that of the real images. Moreover, we use an autoencoder with skip connections for image denoising, showing that the proposed method results in higher PSNR than FSL SUSAN after denoising. This work shows the power of artificial networks to synthesize realistic imaging data, which can be used to improve image processing techniques and provide a quantitative framework to structural changes in the brain.
ER  -


TY  - Preprint
T1  - VulDeePecker: A Deep Learning-Based System for Vulnerability Detection
A1  - Zhen Li
A1  - Deqing Zou
A1  - Shouhuai Xu
A1  - Xinyu Ou
A1  - Hai Jin
A1  - Sujuan Wang
A1  - Zhijun Deng
A1  - Yuyi Zhong
JO  - ArXiv e-prints
Y1  - 5 January, 2018
UR  - https://arxiv.org/abs/1801.01681
N2  - The automatic detection of software vulnerabilities is an important research problem. However, existing solutions to this problem rely on human experts to define features and often miss many vulnerabilities (i.e., incurring high false negative rate). In this paper, we initiate the study of using deep learning-based vulnerability detection to relieve human experts from the tedious and subjective task of manually defining features. Since deep learning is motivated to deal with problems that are very different from the problem of vulnerability detection, we need some guiding principles for applying deep learning to vulnerability detection. In particular, we need to find representations of software programs that are suitable for deep learning. For this purpose, we propose using code gadgets to represent programs and then transform them into vectors, where a code gadget is a number of (not necessarily consecutive) lines of code that are semantically related to each other. This leads to the design and implementation of a deep learning-based vulnerability detection system, called Vulnerability Deep Pecker (VulDeePecker). In order to evaluate VulDeePecker, we present the first vulnerability dataset for deep learning approaches. Experimental results show that VulDeePecker can achieve much fewer false negatives (with reasonable false positives) than other approaches. We further apply VulDeePecker to 3 software products (namely Xen, Seamonkey, and Libav) and detect 4 vulnerabilities, which are not reported in the National Vulnerability Database but were &#34;silently&#34; patched by the vendors when releasing later versions of these products; in contrast, these vulnerabilities are almost entirely missed by the other vulnerability detection systems we experimented with.
ER  -


TY  - Preprint
T1  - Deep learning for word-level handwritten Indic script identification
A1  - Soumya Ukil
A1  - Swarnendu Ghosh
A1  - Sk Md Obaidullah
A1  - K. C. Santosh
A1  - Kaushik Roy
A1  - Nibaran Das
JO  - ArXiv e-prints
Y1  - 4 January, 2018
UR  - https://arxiv.org/abs/1801.01627
N2  - We propose a novel method that uses convolutional neural networks (CNNs) for feature extraction. Not just limited to conventional spatial domain representation, we use multilevel 2D discrete Haar wavelet transform, where image representations are scaled to a variety of different sizes. These are then used to train different CNNs to select features. To be precise, we use 10 different CNNs that select a set of 10240 features, i.e. 1024/CNN. With this, 11 different handwritten scripts are identified, where 1K words per script are used. In our test, we have achieved the maximum script identification rate of 94.73% using multi-layer perceptron (MLP). Our results outperform the state-of-the-art techniques.
ER  -


TY  - Preprint
T1  - Learning $3$D-FilterMap for Deep Convolutional Neural Networks
A1  - Yingzhen Yang
A1  - Jianchao Yang
A1  - Ning Xu
A1  - Wei Han
JO  - ArXiv e-prints
Y1  - 4 January, 2018
UR  - https://arxiv.org/abs/1801.01609
N2  - We present a novel and compact architecture for deep Convolutional Neural Networks (CNNs) in this paper, termed $3$D-FilterMap Convolutional Neural Networks ($3$D-FM-CNNs). The convolution layer of $3$D-FM-CNN learns a compact representation of the filters, named $3$D-FilterMap, instead of a set of independent filters in the conventional convolution layer. The filters are extracted from the $3$D-FilterMap as overlapping $3$D submatrics with weight sharing among nearby filters, and these filters are convolved with the input to generate the output of the convolution layer for $3$D-FM-CNN. Due to the weight sharing scheme, the parameter size of the $3$D-FilterMap is much smaller than that of the filters to be learned in the conventional convolution layer when $3$D-FilterMap generates the same number of filters. Our work is fundamentally different from the network compression literature that reduces the size of a learned large network in the sense that a small network is directly learned from scratch. Experimental results demonstrate that $3$D-FM-CNN enjoys a small parameter space by learning compact $3$D-FilterMaps, while achieving performance compared to that of the baseline CNNs which learn the same number of filters as that generated by the corresponding $3$D-FilterMap.
ER  -


TY  - Preprint
T1  - Combination of Hyperband and Bayesian Optimization for Hyperparameter Optimization in Deep Learning
A1  - Jiazhuo Wang
A1  - Jason Xu
A1  - Xuejun Wang
JO  - ArXiv e-prints
Y1  - 4 January, 2018
UR  - https://arxiv.org/abs/1801.01596
N2  - Deep learning has achieved impressive results on many problems. However, it requires high degree of expertise or a lot of experience to tune well the hyperparameters, and such manual tuning process is likely to be biased. Moreover, it is not practical to try out as many different hyperparameter configurations in deep learning as in other machine learning scenarios, because evaluating each single hyperparameter configuration in deep learning would mean training a deep neural network, which usually takes quite long time. Hyperband algorithm achieves state-of-the-art performance on various hyperparameter optimization problems in the field of deep learning. However, Hyperband algorithm does not utilize history information of previous explored hyperparameter configurations, thus the solution found is suboptimal. We propose to combine Hyperband algorithm with Bayesian optimization (which does not ignore history when sampling next trial configuration). Experimental results show that our combination approach is superior to other hyperparameter optimization approaches including Hyperband algorithm.
ER  -


TY  - Preprint
T1  - A deep learning approach for detecting traffic accidents from social media data
A1  - Zhenhua Zhang
A1  - Qing Heb
A1  - Jing Gao
A1  - Ming Ni
JO  - ArXiv e-prints
Y1  - 4 January, 2018
UR  - https://arxiv.org/abs/1801.01528
N2  - This paper employs deep learning in detecting the traffic accident from social media data. First, we thoroughly investigate the 1-year over 3 million tweet contents in two metropolitan areas: Northern Virginia and New York City. Our results show that paired tokens can capture the association rules inherent in the accident-related tweets and further increase the accuracy of the traffic accident detection. Second, two deep learning methods: Deep Belief Network (DBN) and Long Short-Term Memory (LSTM) are investigated and implemented on the extracted token. Results show that DBN can obtain an overall accuracy of 85% with about 44 individual token features and 17 paired token features. The classification results from DBN outperform those of Support Vector Machines (SVMs) and supervised Latent Dirichlet allocation (sLDA). Finally, to validate this study, we compare the accident-related tweets with both the traffic accident log on freeways and traffic data on local roads from 15,000 loop detectors. It is found that nearly 66% of the accident-related tweets can be located by the accident log and more than 80% of them can be tied to nearby abnormal traffic data. Several important issues of using Twitter to detect traffic accidents have been brought up by the comparison including the location and time bias, as well as the characteristics of influential users and hashtags.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning based Optimal Control of Hot Water Systems
A1  - Hussain Kazmi
A1  - Fahad Mehmood
A1  - Stefan Lodeweyckx
A1  - Johan Driesen
JO  - ArXiv e-prints
Y1  - 4 January, 2018
UR  - https://arxiv.org/abs/1801.01467
N2  - Energy consumption for hot water production is a major draw in high efficiency buildings. Optimizing this has typically been approached from a thermodynamics perspective, decoupled from occupant influence. Furthermore, optimization usually presupposes existence of a detailed dynamics model for the hot water system. These assumptions lead to suboptimal energy efficiency in the real world. In this paper, we present a novel reinforcement learning based methodology which optimizes hot water production. The proposed methodology is completely generalizable, and does not require an offline step or human domain knowledge to build a model for the hot water vessel or the heating element. Occupant preferences too are learnt on the fly. The proposed system is applied to a set of 32 houses in the Netherlands where it reduces energy consumption for hot water production by roughly 20% with no loss of occupant comfort. Extrapolating, this translates to absolute savings of roughly 200 kWh for a single household on an annual basis. This performance can be replicated to any domestic hot water system and optimization objective, given that the fairly minimal requirements on sensor data are met. With millions of hot water systems operational worldwide, the proposed framework has the potential to reduce energy consumption in existing and new systems on a multi Gigawatt-hour scale in the years to come.
ER  -


TY  - Preprint
T1  - Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning
A1  - Charles Schaff
A1  - David Yunis
A1  - Ayan Chakrabarti
A1  - Matthew R. Walter
JO  - ArXiv e-prints
Y1  - 14 September, 2018
UR  - https://arxiv.org/abs/1801.01432
N2  - The physical design of a robot and the policy that controls its motion are inherently coupled, and should be determined according to the task and environment. In an increasing number of applications, data-driven and learning-based approaches, such as deep reinforcement learning, have proven effective at designing control policies. For most tasks, the only way to evaluate a physical design with respect to such control policies is empirical--i.e., by picking a design and training a control policy for it. Since training these policies is time-consuming, it is computationally infeasible to train separate policies for all possible designs as a means to identify the best one. In this work, we address this limitation by introducing a method that performs simultaneous joint optimization of the physical design and control network. Our approach maintains a distribution over designs and uses reinforcement learning to optimize a control policy to maximize expected reward over the design distribution. We give the controller access to design parameters to allow it to tailor its policy to each design in the distribution. Throughout training, we shift the distribution towards higher-performing designs, eventually converging to a design and control policy that are jointly optimal. We evaluate our approach in the context of legged locomotion, and demonstrate that it discovers novel designs and walking gaits, outperforming baselines in both performance and efficiency.
ER  -


TY  - Preprint
T1  - What have we learned from deep representations for action recognition?
A1  - Christoph Feichtenhofer
A1  - Axel Pinz
A1  - Richard P. Wildes
A1  - Andrew Zisserman
JO  - ArXiv e-prints
Y1  - 4 January, 2018
UR  - https://arxiv.org/abs/1801.01415
N2  - As the success of deep models has led to their deployment in all areas of computer vision, it is increasingly important to understand how these representations work and what they are capturing. In this paper, we shed light on deep spatiotemporal representations by visualizing what two-stream models have learned in order to recognize actions in video. We show that local detectors for appearance and motion objects arise to form distributed representations for recognizing human actions. Key observations include the following. First, cross-stream fusion enables the learning of true spatiotemporal features rather than simply separate appearance and motion features. Second, the networks can learn local representations that are highly class specific, but also generic representations that can serve a range of classes. Third, throughout the hierarchy of the network, features become more abstract and show increasing invariance to aspects of the data that are unimportant to desired distinctions (e.g. motion patterns across various speeds). Fourth, visualizations can be used not only to shed light on learned representations, but also to reveal idiosyncracies of training data and to explain failure cases of the system.
ER  -


TY  - Preprint
T1  - Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor
A1  - Tuomas Haarnoja
A1  - Aurick Zhou
A1  - Pieter Abbeel
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 8 August, 2018
UR  - https://arxiv.org/abs/1801.01290
N2  - Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.
ER  -


TY  - Preprint
T1  - DeepTriage: Exploring the Effectiveness of Deep Learning for Bug Triaging
A1  - Senthil Mani
A1  - Anush Sankaran
A1  - Rahul Aralikatte
JO  - ArXiv e-prints
Y1  - 4 January, 2018
UR  - https://arxiv.org/abs/1801.01275
N2  - For a given software bug report, identifying an appropriate developer who could potentially fix the bug is the primary task of a bug triaging process. A bug title (summary) and a detailed description is present in most of the bug tracking systems. Automatic bug triaging algorithm can be formulated as a classification problem, with the bug title and description as the input, mapping it to one of the available developers (classes). The major challenge is that the bug description usually contains a combination of free unstructured text, code snippets, and stack trace making the input data noisy. The existing bag-of-words (BOW) feature models do not consider the syntactical and sequential word information available in the unstructured text. We propose a novel bug report representation algorithm using an attention based deep bidirectional recurrent neural network (DBRNN-A) model that learns a syntactic and semantic feature from long word sequences in an unsupervised manner. Instead of BOW features, the DBRNN-A based bug representation is then used for training the classifier. Using an attention mechanism enables the model to learn the context representation over a long word sequence, as in a bug report. To provide a large amount of data to learn the feature learning model, the unfixed bug reports (~70% bugs in an open source bug tracking system) are leveraged, which were completely ignored in the previous studies. Another contribution is to make this research reproducible by making the source code available and creating a public benchmark dataset of bug reports from three open source bug tracking system: Google Chromium (383,104 bug reports), Mozilla Core (314,388 bug reports), and Mozilla Firefox (162,307 bug reports). Experimentally we compare our approach with BOW model and machine learning approaches and observe that DBRNN-A provides a higher rank-10 average accuracy.
ER  -


TY  - Preprint
T1  - Deep Learning Reconstruction for 9-View Dual Energy CT Baggage Scanner
A1  - Yoseob Han
A1  - Jingu Kang
A1  - Jong Chul Ye
JO  - ArXiv e-prints
Y1  - 4 January, 2018
UR  - https://arxiv.org/abs/1801.01258
N2  - For homeland and transportation security applications, 2D X-ray explosive detection system (EDS) have been widely used, but they have limitations in recognizing 3D shape of the hidden objects. Among various types of 3D computed tomography (CT) systems to address this issue, this paper is interested in a stationary CT using fixed X-ray sources and detectors. However, due to the limited number of projection views, analytic reconstruction algorithms produce severe streaking artifacts. Inspired by recent success of deep learning approach for sparse view CT reconstruction, here we propose a novel image and sinogram domain deep learning architecture for 3D reconstruction from very sparse view measurement. The algorithm has been tested with the real data from a prototype 9-view dual energy stationary CT EDS carry-on baggage scanner developed by GEMSS Medical Systems, Korea, which confirms the superior reconstruction performance over the existing approaches.
ER  -


TY  - Preprint
T1  - ScreenerNet: Learning Self-Paced Curriculum for Deep Neural Networks
A1  - Tae-Hoon Kim
A1  - Jonghyun Choi
JO  - ArXiv e-prints
Y1  - 6 June, 2018
UR  - https://arxiv.org/abs/1801.00904
N2  - We propose to learn a curriculum or a syllabus for supervised learning and deep reinforcement learning with deep neural networks by an attachable deep neural network, called ScreenerNet. Specifically, we learn a weight for each sample by jointly training the ScreenerNet and the main network in an end-to-end self-paced fashion. The ScreenerNet neither has sampling bias nor requires to remember the past learning history. We show the networks augmented with the ScreenerNet achieve early convergence with better accuracy than the state-of-the-art curricular learning methods in extensive experiments using three popular vision datasets such as MNIST, CIFAR10 and Pascal VOC2012, and a Cart-pole task using Deep Q-learning. Moreover, the ScreenerNet can extend other curriculum learning methods such as Prioritized Experience Replay (PER) for further accuracy improvement.
ER  -


TY  - Preprint
T1  - Deep Learning for Identifying Potential Conceptual Shifts for Co-creative Drawing
A1  - Pegah Karimi
A1  - Nicholas Davis
A1  - Kazjon Grace
A1  - Mary Lou Maher
JO  - ArXiv e-prints
Y1  - 2 January, 2018
UR  - https://arxiv.org/abs/1801.00723
N2  - We present a system for identifying conceptual shifts between visual categories, which will form the basis for a co-creative drawing system to help users draw more creative sketches. The system recognizes human sketches and matches them to structurally similar sketches from categories to which they do not belong. This would allow a co-creative drawing system to produce an ambiguous sketch that blends features from both categories.
ER  -


TY  - Preprint
T1  - High Dimensional Spaces, Deep Learning and Adversarial Examples
A1  - Simant Dube
JO  - ArXiv e-prints
Y1  - 15 April, 2018
UR  - https://arxiv.org/abs/1801.00634
N2  - In this paper, we analyze deep learning from a mathematical point of view and derive several novel results. The results are based on intriguing mathematical properties of high dimensional spaces. We first look at perturbation based adversarial examples and show how they can be understood using topological and geometrical arguments in high dimensions. We point out mistake in an argument presented in prior published literature, and we present a more rigorous, general and correct mathematical result to explain adversarial examples in terms of topology of image manifolds. Second, we look at optimization landscapes of deep neural networks and examine the number of saddle points relative to that of local minima. Third, we show how multiresolution nature of images explains perturbation based adversarial examples in form of a stronger result. Our results state that expectation of $L_2$-norm of adversarial perturbations is $O\left(\frac{1}{\sqrt{n}}\right)$ and therefore shrinks to 0 as image resolution $n$ becomes arbitrarily large. Finally, by incorporating the parts-whole manifold learning hypothesis for natural images, we investigate the working of deep neural networks and root causes of adversarial examples and discuss how future improvements can be made and how adversarial examples can be eliminated.
ER  -


TY  - Preprint
T1  - Deep Learning: A Critical Appraisal
A1  - Gary Marcus
JO  - ArXiv e-prints
Y1  - 2 January, 2018
UR  - https://arxiv.org/abs/1801.00631
N2  - Although deep learning has historical roots going back decades, neither the term &#34;deep learning&#34; nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton&#39;s now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.
ER  -


TY  - Preprint
T1  - Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey
A1  - Naveed Akhtar
A1  - Ajmal Mian
JO  - ArXiv e-prints
Y1  - 26 February, 2018
UR  - https://arxiv.org/abs/1801.00553
N2  - Deep learning is at the heart of the current rise of machine learning and artificial intelligence. In the field of Computer Vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has lead to a large influx of contributions in this direction. This article presents the first comprehensive survey on adversarial attacks on deep learning in Computer Vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, we draw on the literature to provide a broader outlook of the research direction.
ER  -


TY  - Preprint
T1  - Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction
A1  - Dan Xu
A1  - Wanli Ouyang
A1  - Xavier Alameda-Pineda
A1  - Elisa Ricci
A1  - Xiaogang Wang
A1  - Nicu Sebe
JO  - ArXiv e-prints
Y1  - 1 January, 2018
UR  - https://arxiv.org/abs/1801.00524
N2  - Recent works have shown that exploiting multi-scale representations deeply learned via convolutional neural networks (CNN) is of tremendous importance for accurate contour detection. This paper presents a novel approach for predicting contours which advances the state of the art in two fundamental aspects, i.e. multi-scale feature generation and fusion. Different from previous works directly consider- ing multi-scale feature maps obtained from the inner layers of a primary CNN architecture, we introduce a hierarchical deep model which produces more rich and complementary representations. Furthermore, to refine and robustly fuse the representations learned at different scales, the novel Attention-Gated Conditional Random Fields (AG-CRFs) are proposed. The experiments ran on two publicly available datasets (BSDS500 and NYUDv2) demonstrate the effectiveness of the latent AG-CRF model and of the overall hierarchical framework.
ER  -


TY  - Preprint
T1  - Accelerating Deep Learning with Memcomputing
A1  - Haik Manukian
A1  - Fabio L. Traversa
A1  - Massimiliano Di Ventra
JO  - ArXiv e-prints
Y1  - 23 January, 2018
UR  - https://arxiv.org/abs/1801.00512
N2  - Restricted Boltzmann machines (RBMs) and their extensions, called &#39;deep-belief networks&#39;, are powerful neural networks that have found applications in the fields of machine learning and big data. The standard way to training these models resorts to an iterative unsupervised procedure based on Gibbs sampling, called &#39;contrastive divergence&#39; (CD), and additional supervised tuning via back-propagation. However, this procedure has been shown not to follow any gradient and can lead to suboptimal solutions. In this paper, we show an efficient alternative to CD by means of simulations of digital memcomputing machines (DMMs). We test our approach on pattern recognition using a modified version of the MNIST data set. DMMs sample effectively the vast phase space given by the model distribution of the RBM, and provide a very good approximation close to the optimum. This efficient search significantly reduces the number of pretraining iterations necessary to achieve a given level of accuracy, as well as a total performance gain over CD. In fact, the acceleration of pretraining achieved by simulating DMMs is comparable to, in number of iterations, the recently reported hardware application of the quantum annealing method on the same network and data set. Notably, however, DMMs perform far better than the reported quantum annealing results in terms of quality of the training. We also compare our method to advances in supervised training, like batch-normalization and rectifiers, that work to reduce the advantage of pretraining. We find that the memcomputing method still maintains a quality advantage ($&gt;1\%$ in accuracy, and a $20\%$ reduction in error rate) over these approaches. Furthermore, our method is agnostic about the connectivity of the network. Therefore, it can be extended to train full Boltzmann machines, and even deep networks at once.
ER  -


TY  - Preprint
T1  - Towards Building an Intelligent Anti-Malware System: A Deep Learning Approach using Support Vector Machine (SVM) for Malware Classification
A1  - Abien Fred Agarap
A1  - Francis John Hill Pepito
JO  - ArXiv e-prints
Y1  - 31 December, 2017
UR  - https://arxiv.org/abs/1801.00318
N2  - Effective and efficient mitigation of malware is a long-time endeavor in the information security community. The development of an anti-malware system that can counteract an unknown malware is a prolific activity that may benefit several sectors. We envision an intelligent anti-malware system that utilizes the power of deep learning (DL) models. Using such models would enable the detection of newly-released malware through mathematical generalization. That is, finding the relationship between a given malware $x$ and its corresponding malware family $y$, $f: x \mapsto y$. To accomplish this feat, we used the Malimg dataset (Nataraj et al., 2011) which consists of malware images that were processed from malware binaries, and then we trained the following DL models 1 to classify each malware family: CNN-SVM (Tang, 2013), GRU-SVM (Agarap, 2017), and MLP-SVM. Empirical evidence has shown that the GRU-SVM stands out among the DL models with a predictive accuracy of ~84.92%. This stands to reason for the mentioned model had the relatively most sophisticated architecture design among the presented models. The exploration of an even more optimal DL-SVM model is the next stage towards the engineering of an intelligent anti-malware system.
ER  -


TY  - Preprint
T1  - Early detection of Crossfire attacks using deep learning
A1  - Saurabh Misra
A1  - Mengxuan Tan
A1  - Mostafa Rezazad
A1  - Matthias R. Brust
A1  - Ngai-Man Cheung
JO  - ArXiv e-prints
Y1  - 19 April, 2018
UR  - https://arxiv.org/abs/1801.00235
N2  - Crossfire attack is a recently proposed threat designed to disconnect whole geographical areas, such as cities or states, from the Internet. Orchestrated in multiple phases, the attack uses a massively distributed botnet to generate low-rate benign traffic aiming to congest selected network links, so-called target links. The adoption of benign traffic, while simultaneously targeting multiple network links, makes the detection of the Crossfire attack a serious challenge. In this paper, we propose a framework for early detection of Crossfire attack, i.e., detection in the warm-up period of the attack. We propose to monitor traffic at the potential decoy servers and discuss the advantages comparing with other monitoring approaches. Since the low-rate attack traffic is very difficult to distinguish from the background traffic, we investigate several deep learning methods to mine the spatiotemporal features for attack detection. We investigate Autoencoder, Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) Network to detect the Crossfire attack during its warm-up period. We report encouraging experiment results.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for List-wise Recommendations
A1  - Xiangyu Zhao
A1  - Liang Zhang
A1  - Zhuoye Ding
A1  - Dawei Yin
A1  - Yihong Zhao
A1  - Jiliang Tang
JO  - ArXiv e-prints
Y1  - 5 January, 2018
UR  - https://arxiv.org/abs/1801.00209
N2  - Recommender systems play a crucial role in mitigating the problem of information overload by suggesting users&#39; personalized items or services. The vast majority of traditional recommender systems consider the recommendation procedure as a static process and make recommendations following a fixed strategy. In this paper, we propose a novel recommender system with the capability of continuously improving its strategies during the interactions with users. We model the sequential interactions between users and a recommender system as a Markov Decision Process (MDP) and leverage Reinforcement Learning (RL) to automatically learn the optimal strategies via recommending trial-and-error items and receiving reinforcements of these items from users&#39; feedbacks. In particular, we introduce an online user-agent interacting environment simulator, which can pre-train and evaluate model parameters offline before applying the model online. Moreover, we validate the importance of list-wise recommendations during the interactions between users and agent, and develop a novel approach to incorporate them into the proposed framework LIRD for list-wide recommendations. The experimental results based on a real-world e-commerce dataset demonstrate the effectiveness of the proposed framework.
ER  -


TY  - Preprint
T1  - Theory of Deep Learning III: explaining the non-overfitting puzzle
A1  - Tomaso Poggio
A1  - Kenji Kawaguchi
A1  - Qianli Liao
A1  - Brando Miranda
A1  - Lorenzo Rosasco
A1  - Xavier Boix
A1  - Jack Hidary
A1  - Hrushikesh Mhaskar
JO  - ArXiv e-prints
Y1  - 16 January, 2018
UR  - https://arxiv.org/abs/1801.00173
N2  - A main puzzle of deep networks revolves around the absence of overfitting despite large overparametrization and despite the large capacity demonstrated by zero training error on randomly labeled data. In this note, we show that the dynamics associated to gradient descent minimization of nonlinear networks is topologically equivalent, near the asymptotically stable minima of the empirical error, to linear gradient system in a quadratic potential with a degenerate (for square loss) or almost degenerate (for logistic or crossentropy loss) Hessian. The proposition depends on the qualitative theory of dynamical systems and is supported by numerical results. Our main propositions extend to deep nonlinear networks two properties of gradient descent for linear networks, that have been recently established (1) to be key to their generalization properties: 1. Gradient descent enforces a form of implicit regularization controlled by the number of iterations, and asymptotically converges to the minimum norm solution for appropriate initial conditions of gradient descent. This implies that there is usually an optimum early stopping that avoids overfitting of the loss. This property, valid for the square loss and many other loss functions, is relevant especially for regression. 2. For classification, the asymptotic convergence to the minimum norm solution implies convergence to the maximum margin solution which guarantees good classification error for &#34;low noise&#34; datasets. This property holds for loss functions such as the logistic and cross-entropy loss independently of the initial conditions. The robustness to overparametrization has suggestive implications for the robustness of the architecture of deep convolutional networks with respect to the curse of dimensionality.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward
A1  - Kaiyang Zhou
A1  - Yu Qiao
A1  - Tao Xiang
JO  - ArXiv e-prints
Y1  - 13 February, 2018
UR  - https://arxiv.org/abs/1801.00054
N2  - Video summarization aims to facilitate large-scale video browsing by producing short, concise summaries that are diverse and representative of original videos. In this paper, we formulate video summarization as a sequential decision-making process and develop a deep summarization network (DSN) to summarize videos. DSN predicts for each video frame a probability, which indicates how likely a frame is selected, and then takes actions based on the probability distributions to select frames, forming video summaries. To train our DSN, we propose an end-to-end, reinforcement learning-based framework, where we design a novel reward function that jointly accounts for diversity and representativeness of generated summaries and does not rely on labels or user interactions at all. During training, the reward function judges how diverse and representative the generated summaries are, while DSN strives for earning higher rewards by learning to produce more diverse and more representative summaries. Since labels are not required, our method can be fully unsupervised. Extensive experiments on two benchmark datasets show that our unsupervised method not only outperforms other state-of-the-art unsupervised methods, but also is comparable to or even superior than most of published supervised approaches.
ER  -


TY  - Preprint
T1  - A Deep Belief Network Based Machine Learning System for Risky Host Detection
A1  - Wangyan Feng
A1  - Shuning Wu
A1  - Xiaodan Li
A1  - Kevin Kunkle
JO  - ArXiv e-prints
Y1  - 29 December, 2017
UR  - https://arxiv.org/abs/1801.00025
N2  - To assure cyber security of an enterprise, typically SIEM (Security Information and Event Management) system is in place to normalize security event from different preventive technologies and flag alerts. Analysts in the security operation center (SOC) investigate the alerts to decide if it is truly malicious or not. However, generally the number of alerts is overwhelming with majority of them being false positive and exceeding the SOC&#39;s capacity to handle all alerts. There is a great need to reduce the false positive rate as much as possible. While most previous research focused on network intrusion detection, we focus on risk detection and propose an intelligent Deep Belief Network machine learning system. The system leverages alert information, various security logs and analysts&#39; investigation results in a real enterprise environment to flag hosts that have high likelihood of being compromised. Text mining and graph based method are used to generate targets and create features for machine learning. In the experiment, Deep Belief Network is compared with other machine learning algorithms, including multi-layer neural network, random forest, support vector machine and logistic regression. Results on real enterprise data indicate that the deep belief network machine learning system performs better than other algorithms for our problem and is six times more effective than current rule-based system. We also implement the whole system from data collection, label creation, feature engineering to host score generation in a real enterprise production environment.
ER  -


TY  - Preprint
T1  - Deep Learning Interior Tomography for Region-of-Interest Reconstruction
A1  - Yoseob Han
A1  - Jawook Gu
A1  - Jong Chul Ye
JO  - ArXiv e-prints
Y1  - 3 January, 2018
UR  - https://arxiv.org/abs/1712.10248
N2  - Interior tomography for the region-of-interest (ROI) imaging has advantages of using a small detector and reducing X-ray radiation dose. However, standard analytic reconstruction suffers from severe cupping artifacts due to existence of null space in the truncated Radon transform. Existing penalized reconstruction methods may address this problem but they require extensive computations due to the iterative reconstruction. Inspired by the recent deep learning approaches to low-dose and sparse view CT, here we propose a deep learning architecture that removes null space signals from the FBP reconstruction. Experimental results have shown that the proposed method provides near-perfect reconstruction with about 7-10 dB improvement in PSNR over existing methods in spite of significantly reduced run-time complexity.
ER  -


TY  - Preprint
T1  - Learning Deep Similarity Models with Focus Ranking for Fabric Image Retrieval
A1  - Daiguo Deng
A1  - Ruomei Wang
A1  - Hefeng Wu
A1  - Huayong He
A1  - Qi Li
A1  - Xiaonan Luo
JO  - ArXiv e-prints
Y1  - 29 December, 2017
UR  - https://arxiv.org/abs/1712.10211
N2  - Fabric image retrieval is beneficial to many applications including clothing searching, online shopping and cloth modeling. Learning pairwise image similarity is of great importance to an image retrieval task. With the resurgence of Convolutional Neural Networks (CNNs), recent works have achieved significant progresses via deep representation learning with metric embedding, which drives similar examples close to each other in a feature space, and dissimilar ones apart from each other. In this paper, we propose a novel embedding method termed focus ranking that can be easily unified into a CNN for jointly learning image representations and metrics in the context of fine-grained fabric image retrieval. Focus ranking aims to rank similar examples higher than all dissimilar ones by penalizing ranking disorders via the minimization of the overall cost attributed to similar samples being ranked below dissimilar ones. At the training stage, training samples are organized into focus ranking units for efficient optimization. We build a large-scale fabric image retrieval dataset (FIRD) with about 25,000 images of 4,300 fabrics, and test the proposed model on the FIRD dataset. Experimental results show the superiority of the proposed model over existing metric embedding models.
ER  -


TY  - Preprint
T1  - Learning Deep and Compact Models for Gesture Recognition
A1  - Koustav Mullick
A1  - Anoop M. Namboodiri
JO  - ArXiv e-prints
Y1  - 29 December, 2017
UR  - https://arxiv.org/abs/1712.10136
N2  - We look at the problem of developing a compact and accurate model for gesture recognition from videos in a deep-learning framework. Towards this we propose a joint 3DCNN-LSTM model that is end-to-end trainable and is shown to be better suited to capture the dynamic information in actions. The solution achieves close to state-of-the-art accuracy on the ChaLearn dataset, with only half the model size. We also explore ways to derive a much more compact representation in a knowledge distillation framework followed by model compression. The final model is less than $1~MB$ in size, which is less than one hundredth of our initial model, with a drop of $7\%$ in accuracy, and is suitable for real-time gesture recognition on mobile devices.
ER  -


TY  - Preprint
T1  - Active Robotic Mapping through Deep Reinforcement Learning
A1  - Shane Barratt
JO  - ArXiv e-prints
Y1  - 28 December, 2017
UR  - https://arxiv.org/abs/1712.10069
N2  - We propose an approach to learning agents for active robotic mapping, where the goal is to map the environment as quickly as possible. The agent learns to map efficiently in simulated environments by receiving rewards corresponding to how fast it constructs an accurate map. In contrast to prior work, this approach learns an exploration policy based on a user-specified prior over environment configurations and sensor model, allowing it to specialize to the specifications. We evaluate the approach through a simulated Disaster Mapping scenario and find that it achieves performance slightly better than a near-optimal myopic exploration scheme, suggesting that it could be useful in more complicated problem scenarios.
ER  -


TY  - Preprint
T1  - Automatic Analysis of EEGs Using Big Data and Hybrid Deep Learning Architectures
A1  - Meysam Golmohammadi
A1  - Amir Hossein Harati Nejad Torbati
A1  - Silvia Lopez de Diego
A1  - Iyad Obeid
A1  - Joseph Picone
JO  - ArXiv e-prints
Y1  - 28 December, 2017
UR  - https://arxiv.org/abs/1712.09771
N2  - Objective: A clinical decision support tool that automatically interprets EEGs can reduce time to diagnosis and enhance real-time applications such as ICU monitoring. Clinicians have indicated that a sensitivity of 95% with a specificity below 5% was the minimum requirement for clinical acceptance. We propose a highperformance classification system based on principles of big data and machine learning. Methods: A hybrid machine learning system that uses hidden Markov models (HMM) for sequential decoding and deep learning networks for postprocessing is proposed. These algorithms were trained and evaluated using the TUH EEG Corpus, which is the world&#39;s largest publicly available database of clinical EEG data. Results: Our approach delivers a sensitivity above 90% while maintaining a specificity below 5%. This system detects three events of clinical interest: (1) spike and/or sharp waves, (2) periodic lateralized epileptiform discharges, (3) generalized periodic epileptiform discharges. It also detects three events used to model background noise: (1) artifacts, (2) eye movement (3) background. Conclusions: A hybrid HMM/deep learning system can deliver a low false alarm rate on EEG event detection, making automated analysis a viable option for clinicians. Significance: The TUH EEG Corpus enables application of highly data consumptive machine learning algorithms to EEG analysis. Performance is approaching clinical acceptance for real-time applications.
ER  -


TY  - Preprint
T1  - Multiple Instance Deep Learning for Weakly Supervised Small-Footprint Audio Event Detection
A1  - Shao-Yen Tseng
A1  - Juncheng Li
A1  - Yun Wang
A1  - Joseph Szurley
A1  - Florian Metze
A1  - Samarjit Das
JO  - ArXiv e-prints
Y1  - 26 March, 2018
UR  - https://arxiv.org/abs/1712.09673
N2  - State-of-the-art audio event detection (AED) systems rely on supervised learning using strongly labeled data. However, this dependence severely limits scalability to large-scale datasets where fine resolution annotations are too expensive to obtain. In this paper, we propose a small-footprint multiple instance learning (MIL) framework for multi-class AED using weakly annotated labels. The proposed MIL framework uses audio embeddings extracted from a pre-trained convolutional neural network as input features. We show that by using audio embeddings the MIL framework can be implemented using a simple DNN with performance comparable to recurrent neural networks.
ER  -


TY  - Preprint
T1  - Whatever Does Not Kill Deep Reinforcement Learning, Makes It Stronger
A1  - Vahid Behzadan
A1  - Arslan Munir
JO  - ArXiv e-prints
Y1  - 23 December, 2017
UR  - https://arxiv.org/abs/1712.09344
N2  - Recent developments have established the vulnerability of deep Reinforcement Learning (RL) to policy manipulation attacks via adversarial perturbations. In this paper, we investigate the robustness and resilience of deep RL to training-time and test-time attacks. Through experimental results, we demonstrate that under noncontiguous training-time attacks, Deep Q-Network (DQN) agents can recover and adapt to the adversarial conditions by reactively adjusting the policy. Our results also show that policies learned under adversarial perturbations are more robust to test-time attacks. Furthermore, we compare the performance of $Îµ$-greedy and parameter-space noise exploration methods in terms of robustness and resilience against adversarial perturbations.
ER  -


TY  - Preprint
T1  - Deep Meta Learning for Real-Time Visual Tracking based on Target-Specific Feature Space
A1  - Janghoon Choi
A1  - Junseok Kwon
A1  - Kyoung Mu Lee
JO  - ArXiv e-prints
Y1  - 25 December, 2017
UR  - https://arxiv.org/abs/1712.09153
N2  - In this paper, we propose a novel on-line visual tracking framework based on Siamese matching network and meta-learner network which runs at real-time speed. Conventional deep convolutional feature based discriminative visual tracking algorithms require continuous re-training of classifiers or correlation filters for solving complex optimization tasks to adapt to the new appearance of a target object. To remove this process, our proposed algorithm incorporates and utilizes a meta-learner network to provide the matching network with new appearance information of the target object by adding the target-aware feature space. The parameters for the target-specific feature space are provided instantly from a single forward-pass of the meta-learner network. By eliminating the necessity of continuously solving the complex optimization tasks in the course of tracking, experimental results demonstrate that our algorithm performs at a real-time speed of $62$ fps while maintaining a competitive performance among other state-of-the-art tracking algorithms.
ER  -


TY  - Preprint
T1  - Android Malware Detection using Deep Learning on API Method Sequences
A1  - ElMouatez Billah Karbab
A1  - Mourad Debbabi
A1  - Abdelouahid Derhab
A1  - Djedjiga Mouheb
JO  - ArXiv e-prints
Y1  - 24 December, 2017
UR  - https://arxiv.org/abs/1712.08996
N2  - Android OS experiences a blazing popularity since the last few years. This predominant platform has established itself not only in the mobile world but also in the Internet of Things (IoT) devices. This popularity, however, comes at the expense of security, as it has become a tempting target of malicious apps. Hence, there is an increasing need for sophisticated, automatic, and portable malware detection solutions. In this paper, we propose MalDozer, an automatic Android malware detection and family attribution framework that relies on sequences classification using deep learning techniques. Starting from the raw sequence of the app&#39;s API method calls, MalDozer automatically extracts and learns the malicious and the benign patterns from the actual samples to detect Android malware. MalDozer can serve as a ubiquitous malware detection system that is not only deployed on servers, but also on mobile and even IoT devices. We evaluate MalDozer on multiple Android malware datasets ranging from 1K to 33K malware apps, and 38K benign apps. The results show that MalDozer can correctly detect malware and attribute them to their actual families with an F1-Score of 96%-99% and a false positive rate of 0.06%-2%, under all tested datasets and settings.
ER  -


TY  - Preprint
T1  - Deep Learning for Massive MIMO CSI Feedback
A1  - Chao-Kai Wen
A1  - Wan-Ting Shih
A1  - Shi Jin
JO  - ArXiv e-prints
Y1  - 23 April, 2018
UR  - https://arxiv.org/abs/1712.08919
N2  - In frequency division duplex mode, the downlink channel state information (CSI) should be sent to the base station through feedback links so that the potential gains of a massive multiple-input multiple-output can be exhibited. However, such a transmission is hindered by excessive feedback overhead. In this letter, we use deep learning technology to develop CsiNet, a novel CSI sensing and recovery {mechanism} that learns to effectively use channel structure from training samples. CsiNet learns a transformation from CSI to a near-optimal number of representations (or codewords) and an inverse transformation from codewords to CSI. We perform experiments to demonstrate that CsiNet can recover CSI with significantly improved reconstruction quality compared with existing compressive sensing (CS)-based methods. Even at excessively low compression regions where CS-based methods cannot work, CsiNet retains effective beamforming gain.
ER  -


TY  - Preprint
T1  - Dropout Feature Ranking for Deep Learning Models
A1  - Chun-Hao Chang
A1  - Ladislav Rampasek
A1  - Anna Goldenberg
JO  - ArXiv e-prints
Y1  - 9 March, 2018
UR  - https://arxiv.org/abs/1712.08645
N2  - Deep neural networks (DNNs) achieve state-of-the-art results in a variety of domains. Unfortunately, DNNs are notorious for their non-interpretability, and thus limit their applicability in hypothesis-driven domains such as biology and healthcare. Moreover, in the resource-constraint setting, it is critical to design tests relying on fewer more informative features leading to high accuracy performance within reasonable budget. We aim to close this gap by proposing a new general feature ranking method for deep learning. We show that our simple yet effective method performs on par or compares favorably to eight strawman, classical and deep-learning feature ranking methods in two simulations and five very different datasets on tasks ranging from classification to regression, in both static and time series scenarios. We also illustrate the use of our method on a drug response dataset and show that it identifies genes relevant to the drug-response.
ER  -


TY  - Preprint
T1  - Learning in the Machine: the Symmetries of the Deep Learning Channel
A1  - Pierre Baldi
A1  - Peter Sadowski
A1  - Zhiqin Lu
JO  - ArXiv e-prints
Y1  - 22 December, 2017
UR  - https://arxiv.org/abs/1712.08608
N2  - In a physical neural system, learning rules must be local both in space and time. In order for learning to occur, non-local information must be communicated to the deep synapses through a communication channel, the deep learning channel. We identify several possible architectures for this learning channel (Bidirectional, Conjoined, Twin, Distinct) and six symmetry challenges: 1) symmetry of architectures; 2) symmetry of weights; 3) symmetry of neurons; 4) symmetry of derivatives; 5) symmetry of processing; and 6) symmetry of learning rules. Random backpropagation (RBP) addresses the second and third symmetry, and some of its variations, such as skipped RBP (SRBP) address the first and the fourth symmetry. Here we address the last two desirable symmetries showing through simulations that they can be achieved and that the learning channel is particularly robust to symmetry variations. Specifically, random backpropagation and its variations can be performed with the same non-linear neurons used in the main input-output forward channel, and the connections in the learning channel can be adapted using the same algorithm used in the forward channel, removing the need for any specialized hardware in the learning channel. Finally, we provide mathematical results in simple cases showing that the learning equations in the forward and backward channels converge to fixed points, for almost any initial conditions. In symmetric architectures, if the weights in both channels are small at initialization, adaptation in both channels leads to weights that are essentially symmetric during and after learning. Biological connections are discussed.
ER  -


TY  - Preprint
T1  - Differential geometry and stochastic dynamics with deep learning numerics
A1  - Line KÃ¼hnel
A1  - Alexis Arnaudon
A1  - Stefan Sommer
JO  - ArXiv e-prints
Y1  - 22 December, 2017
UR  - https://arxiv.org/abs/1712.08364
N2  - In this paper, we demonstrate how deterministic and stochastic dynamics on manifolds, as well as differential geometric constructions can be implemented concisely and efficiently using modern computational frameworks that mix symbolic expressions with efficient numerical computations. In particular, we use the symbolic expression and automatic differentiation features of the python library Theano, originally developed for high-performance computations in deep learning. We show how various aspects of differential geometry and Lie group theory, connections, metrics, curvature, left/right invariance, geodesics and parallel transport can be formulated with Theano using the automatic computation of derivatives of any order. We will also show how symbolic stochastic integrators and concepts from non-linear statistics can be formulated and optimized with only a few lines of code. We will then give explicit examples on low-dimensional classical manifolds for visualization and demonstrate how this approach allows both a concise implementation and efficient scaling to high dimensional problems.
ER  -


TY  - Preprint
T1  - Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning
A1  - Saurabh Kumar
A1  - Pararth Shah
A1  - Dilek Hakkani-Tur
A1  - Larry Heck
JO  - ArXiv e-prints
Y1  - 21 December, 2017
UR  - https://arxiv.org/abs/1712.08266
N2  - We present a framework combining hierarchical and multi-agent deep reinforcement learning approaches to solve coordination problems among a multitude of agents using a semi-decentralized model. The framework extends the multi-agent learning setup by introducing a meta-controller that guides the communication between agent pairs, enabling agents to focus on communicating with only one other agent at any step. This hierarchical decomposition of the task allows for efficient exploration to learn policies that identify globally optimal solutions even as the number of collaborating agents increases. We show promising initial experimental results on a simulated distributed scheduling problem.
ER  -


TY  - Preprint
T1  - A Deep Reinforcement Learning-Based Framework for Content Caching
A1  - Chen Zhong
A1  - M. Cenk Gursoy
A1  - Senem Velipasalar
JO  - ArXiv e-prints
Y1  - 21 December, 2017
UR  - https://arxiv.org/abs/1712.08132
N2  - Content caching at the edge nodes is a promising technique to reduce the data traffic in next-generation wireless networks. Inspired by the success of Deep Reinforcement Learning (DRL) in solving complicated control problems, this work presents a DRL-based framework with Wolpertinger architecture for content caching at the base station. The proposed framework is aimed at maximizing the long-term cache hit rate, and it requires no knowledge of the content popularity distribution. To evaluate the proposed framework, we compare the performance with other caching algorithms, including Least Recently Used (LRU), Least Frequently Used (LFU), and First-In First-Out (FIFO) caching strategies. Meanwhile, since the Wolpertinger architecture can effectively limit the action space size, we also compare the performance with Deep Q-Network to identify the impact of dropping a portion of the actions. Our results show that the proposed framework can achieve improved short-term cache hit rate and improved and stable long-term cache hit rate in comparison with LRU, LFU, and FIFO schemes. Additionally, the performance is shown to be competitive in comparison to Deep Q-learning, while the proposed framework can provide significant savings in runtime.
ER  -


TY  - Preprint
T1  - A Deep Learning Interpretable Classifier for Diabetic Retinopathy Disease Grading
A1  - Jordi de la Torre
A1  - Aida Valls
A1  - Domenec Puig
JO  - ArXiv e-prints
Y1  - 21 December, 2017
UR  - https://arxiv.org/abs/1712.08107
N2  - Deep neural network models have been proven to be very successful in image classification tasks, also for medical diagnosis, but their main concern is its lack of interpretability. They use to work as intuition machines with high statistical confidence but unable to give interpretable explanations about the reported results. The vast amount of parameters of these models make difficult to infer a rationale interpretation from them. In this paper we present a diabetic retinopathy interpretable classifier able to classify retine images into the different levels of disease severity and of explaining its results by assigning a score for every point in the hidden and input space, evaluating its contribution to the final classification in a linear way. The generated visual maps can be interpreted by an expert in order to compare its own knowledge with the interpretation given by the model.
ER  -


TY  - Preprint
T1  - Multiview Deep Learning for Predicting Twitter Users&#39; Location
A1  - Tien Huu Do
A1  - Duc Minh Nguyen
A1  - Evaggelia Tsiligianni
A1  - Bruno Cornelis
A1  - Nikos Deligiannis
JO  - ArXiv e-prints
Y1  - 21 December, 2017
UR  - https://arxiv.org/abs/1712.08091
N2  - The problem of predicting the location of users on large social networks like Twitter has emerged from real-life applications such as social unrest detection and online marketing. Twitter user geolocation is a difficult and active research topic with a vast literature. Most of the proposed methods follow either a content-based or a network-based approach. The former exploits user-generated content while the latter utilizes the connection or interaction between Twitter users. In this paper, we introduce a novel method combining the strength of both approaches. Concretely, we propose a multi-entry neural network architecture named MENET leveraging the advances in deep learning and multiview learning. The generalizability of MENET enables the integration of multiple data representations. In the context of Twitter user geolocation, we realize MENET with textual, network, and metadata features. Considering the natural distribution of Twitter users across the concerned geographical area, we subdivide the surface of the earth into multi-scale cells and train MENET with the labels of the cells. We show that our method outperforms the state of the art by a large margin on three benchmark datasets.
ER  -


TY  - Preprint
T1  - Simulating Patho-realistic Ultrasound Images using Deep Generative Networks with Adversarial Learning
A1  - Francis Tom
A1  - Debdoot Sheet
JO  - ArXiv e-prints
Y1  - 8 January, 2018
UR  - https://arxiv.org/abs/1712.07881
N2  - Ultrasound imaging makes use of backscattering of waves during their interaction with scatterers present in biological tissues. Simulation of synthetic ultrasound images is a challenging problem on account of inability to accurately model various factors of which some include intra-/inter scanline interference, transducer to surface coupling, artifacts on transducer elements, inhomogeneous shadowing and nonlinear attenuation. Current approaches typically solve wave space equations making them computationally expensive and slow to operate. We propose a generative adversarial network (GAN) inspired approach for fast simulation of patho-realistic ultrasound images. We apply the framework to intravascular ultrasound (IVUS) simulation. A stage 0 simulation performed using pseudo B-mode ultrasound image simulator yields speckle mapping of a digitally defined phantom. The stage I GAN subsequently refines them to preserve tissue specific speckle intensities. The stage II GAN further refines them to generate high resolution images with patho-realistic speckle profiles. We evaluate patho-realism of simulated images with a visual Turing test indicating an equivocal confusion in discriminating simulated from real. We also quantify the shift in tissue specific intensity distributions of the real and simulated images to prove their similarity.
ER  -


TY  - Preprint
T1  - Wolf in Sheep&#39;s Clothing - The Downscaling Attack Against Deep Learning Applications
A1  - Qixue Xiao
A1  - Kang Li
A1  - Deyue Zhang
A1  - Yier Jin
JO  - ArXiv e-prints
Y1  - 21 December, 2017
UR  - https://arxiv.org/abs/1712.07805
N2  - This paper considers security risks buried in the data processing pipeline in common deep learning applications. Deep learning models usually assume a fixed scale for their training and input data. To allow deep learning applications to handle a wide range of input data, popular frameworks, such as Caffe, TensorFlow, and Torch, all provide data scaling functions to resize input to the dimensions used by deep learning models. Image scaling algorithms are intended to preserve the visual features of an image after scaling. However, common image scaling algorithms are not designed to handle human crafted images. Attackers can make the scaling outputs look dramatically different from the corresponding input images.
ER  -


TY  - Preprint
T1  - Towards a Deep Improviser: a prototype deep learning post-tonal free music generator
A1  - Roger T. Dean
A1  - Jamie Forth
JO  - ArXiv e-prints
Y1  - 21 December, 2017
UR  - https://arxiv.org/abs/1712.07799
N2  - Two modest-sized symbolic corpora of post-tonal and post-metric keyboard music have been constructed, one algorithmic, the other improvised. Deep learning models of each have been trained and largely optimised. Our purpose is to obtain a model with sufficient generalisation capacity that in response to a small quantity of separate fresh input seed material, it can generate outputs that are distinctive, rather than recreative of the learned corpora or the seed material. This objective has been first assessed statistically, and as judged by k-sample Anderson-Darling and Cramer tests, has been achieved. Music has been generated using the approach, and informal judgements place it roughly on a par with algorithmic and composed music in related forms. Future work will aim to enhance the model such that it can be evaluated in relation to expression, meaning and utility in real-time performance.
ER  -


TY  - Preprint
T1  - Deep learning for predicting refractive error from retinal fundus images
A1  - Avinash V. Varadarajan
A1  - Ryan Poplin
A1  - Katy Blumer
A1  - Christof Angermueller
A1  - Joe Ledsam
A1  - Reena Chopra
A1  - Pearse A. Keane
A1  - Greg S. Corrado
A1  - Lily Peng
A1  - Dale R. Webster
JO  - ArXiv e-prints
Y1  - 21 December, 2017
UR  - https://arxiv.org/abs/1712.07798
N2  - Refractive error, one of the leading cause of visual impairment, can be corrected by simple interventions like prescribing eyeglasses. We trained a deep learning algorithm to predict refractive error from the fundus photographs from participants in the UK Biobank cohort, which were 45 degree field of view images and the AREDS clinical trial, which contained 30 degree field of view images. Our model use the &#34;attention&#34; method to identify features that are correlated with refractive error. Mean absolute error (MAE) of the algorithm&#39;s prediction compared to the refractive error obtained in the AREDS and UK Biobank. The resulting algorithm had a MAE of 0.56 diopters (95% CI: 0.55-0.56) for estimating spherical equivalent on the UK Biobank dataset and 0.91 diopters (95% CI: 0.89-0.92) for the AREDS dataset. The baseline expected MAE (obtained by simply predicting the mean of this population) was 1.81 diopters (95% CI: 1.79-1.84) for UK Biobank and 1.63 (95% CI: 1.60-1.67) for AREDS. Attention maps suggested that the foveal region was one of the most important areas used by the algorithm to make this prediction, though other regions also contribute to the prediction. The ability to estimate refractive error with high accuracy from retinal fundus photos has not been previously known and demonstrates that deep learning can be applied to make novel predictions from medical images. Given that several groups have recently shown that it is feasible to obtain retinal fundus photos using mobile phones and inexpensive attachments, this work may be particularly relevant in regions of the world where autorefractors may not be readily available.
ER  -


TY  - Preprint
T1  - The Character Thinks Ahead: creative writing with deep learning nets and its stylistic assessment
A1  - Roger T. Dean
A1  - Hazel Smith
JO  - ArXiv e-prints
Y1  - 21 December, 2017
UR  - https://arxiv.org/abs/1712.07794
N2  - We discuss how to control outputs from deep learning models of text corpora so as to create contemporary poetic works. We assess whether these controls are successful in the immediate sense of creating stylo- metric distinctiveness. The specific context is our piece The Character Thinks Ahead (2016/17); the potential applications are broad.
ER  -


TY  - Preprint
T1  - Deep Learning with Lung Segmentation and Bone Shadow Exclusion Techniques for Chest X-Ray Analysis of Lung Cancer
A1  - Yu. Gordienko
A1  - Peng Gang
A1  - Jiang Hui
A1  - Wei Zeng
A1  - Yu. Kochura
A1  - O. Alienin
A1  - O. Rokovyi
A1  - S. Stirenko
JO  - ArXiv e-prints
Y1  - 20 December, 2017
UR  - https://arxiv.org/abs/1712.07632
N2  - The recent progress of computing, machine learning, and especially deep learning, for image recognition brings a meaningful effect for automatic detection of various diseases from chest X-ray images (CXRs). Here efficiency of lung segmentation and bone shadow exclusion techniques is demonstrated for analysis of 2D CXRs by deep learning approach to help radiologists identify suspicious lesions and nodules in lung cancer patients. Training and validation was performed on the original JSRT dataset (dataset #01), BSE-JSRT dataset, i.e. the same JSRT dataset, but without clavicle and rib shadows (dataset #02), original JSRT dataset after segmentation (dataset #03), and BSE-JSRT dataset after segmentation (dataset #04). The results demonstrate the high efficiency and usefulness of the considered pre-processing techniques in the simplified configuration even. The pre-processed dataset without bones (dataset #02) demonstrates the much better accuracy and loss results in comparison to the other pre-processed datasets after lung segmentation (datasets #02 and #03).
ER  -


TY  - Preprint
T1  - Use of Deep Learning in Modern Recommendation System: A Summary of Recent Works
A1  - Ayush Singhal
A1  - Pradeep Sinha
A1  - Rakesh Pant
JO  - ArXiv e-prints
Y1  - 20 December, 2017
UR  - https://arxiv.org/abs/1712.07525
N2  - With the exponential increase in the amount of digital information over the internet, online shops, online music, video and image libraries, search engines and recommendation system have become the most convenient ways to find relevant information within a short time. In the recent times, deep learning&#39;s advances have gained significant attention in the field of speech recognition, image processing and natural language processing. Meanwhile, several recent studies have shown the utility of deep learning in the area of recommendation systems and information retrieval as well. In this short review, we cover the recent advances made in the field of recommendation using various variants of deep learning technology. We organize the review in three parts: Collaborative system, Content based system and Hybrid system. The review also discusses the contribution of deep learning integrated recommendation systems into several application domains. The review concludes by discussion of the impact of deep learning in recommendation system in various domain and whether deep learning has shown any significant improvement over the conventional systems for recommendation. Finally, we also provide future directions of research which are possible based on the current state of use of deep learning in recommendation systems.
ER  -


TY  - Preprint
T1  - Learning a Wavelet-like Auto-Encoder to Accelerate Deep Neural Networks
A1  - Tianshui Chen
A1  - Liang Lin
A1  - Wangmeng Zuo
A1  - Xiaonan Luo
A1  - Lei Zhang
JO  - ArXiv e-prints
Y1  - 20 December, 2017
UR  - https://arxiv.org/abs/1712.07493
N2  - Accelerating deep neural networks (DNNs) has been attracting increasing attention as it can benefit a wide range of applications, e.g., enabling mobile systems with limited computing resources to own powerful visual recognition ability. A practical strategy to this goal usually relies on a two-stage process: operating on the trained DNNs (e.g., approximating the convolutional filters with tensor decomposition) and fine-tuning the amended network, leading to difficulty in balancing the trade-off between acceleration and maintaining recognition performance. In this work, aiming at a general and comprehensive way for neural network acceleration, we develop a Wavelet-like Auto-Encoder (WAE) that decomposes the original input image into two low-resolution channels (sub-images) and incorporate the WAE into the classification neural networks for joint training. The two decomposed channels, in particular, are encoded to carry the low-frequency information (e.g., image profiles) and high-frequency (e.g., image details or noises), respectively, and enable reconstructing the original input image through the decoding process. Then, we feed the low-frequency channel into a standard classification network such as VGG or ResNet and employ a very lightweight network to fuse with the high-frequency channel to obtain the classification result. Compared to existing DNN acceleration solutions, our framework has the following advantages: i) it is tolerant to any existing convolutional neural networks for classification without amending their structures; ii) the WAE provides an interpretable way to preserve the main components of the input image for classification.
ER  -


TY  - Preprint
T1  - Intelligent Power Control for Spectrum Sharing in Cognitive Radios: A Deep Reinforcement Learning Approach
A1  - Xingjian Li
A1  - Jun Fang
A1  - Wen Cheng
A1  - Huiping Duan
A1  - Zhi Chen
A1  - Hongbin Li
JO  - ArXiv e-prints
Y1  - 8 April, 2018
UR  - https://arxiv.org/abs/1712.07365
N2  - We consider the problem of spectrum sharing in a cognitive radio system consisting of a primary user and a secondary user. The primary user and the secondary user work in a non-cooperative manner. Specifically, the primary user is assumed to update its transmit power based on a pre-defined power control policy. The secondary user does not have any knowledge about the primary user&#39;s transmit power, or its power control strategy. The objective of this paper is to develop a learning-based power control method for the secondary user in order to share the common spectrum with the primary user. To assist the secondary user, a set of sensor nodes are spatially deployed to collect the received signal strength information at different locations in the wireless environment. We develop a deep reinforcement learning-based method, which the secondary user can use to intelligently adjust its transmit power such that after a few rounds of interaction with the primary user, both users can transmit their own data successfully with required qualities of service. Our experimental results show that the secondary user can interact with the primary user efficiently to reach a goal state (defined as a state in which both users can successfully transmit their data) from any initial states within a few number of steps.
ER  -


TY  - Preprint
T1  - Revisiting the Master-Slave Architecture in Multi-Agent Deep Reinforcement Learning
A1  - Xiangyu Kong
A1  - Bo Xin
A1  - Fangchen Liu
A1  - Yizhou Wang
JO  - ArXiv e-prints
Y1  - 19 December, 2017
UR  - https://arxiv.org/abs/1712.07305
N2  - Many tasks in artificial intelligence require the collaboration of multiple agents. We exam deep reinforcement learning for multi-agent domains. Recent research efforts often take the form of two seemingly conflicting perspectives, the decentralized perspective, where each agent is supposed to have its own controller; and the centralized perspective, where one assumes there is a larger model controlling all agents. In this regard, we revisit the idea of the master-slave architecture by incorporating both perspectives within one framework. Such a hierarchical structure naturally leverages advantages from one another. The idea of combining both perspectives is intuitive and can be well motivated from many real world systems, however, out of a variety of possible realizations, we highlights three key ingredients, i.e. composed action representation, learnable communication and independent reasoning. With network designs to facilitate these explicitly, our proposal consistently outperforms latest competing methods both in synthetic experiments and when applied to challenging StarCraft micromanagement tasks.
ER  -


TY  - Preprint
T1  - Adversarial Examples: Attacks and Defenses for Deep Learning
A1  - Xiaoyong Yuan
A1  - Pan He
A1  - Qile Zhu
A1  - Xiaolin Li
JO  - ArXiv e-prints
Y1  - 6 July, 2018
UR  - https://arxiv.org/abs/1712.07107
N2  - With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks have been recently found vulnerable to well-designed input samples, called adversarial examples. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for deep neural networks, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples and explore the challenges and the potential solutions.
ER  -


TY  - Preprint
T1  - DeepNorm-A Deep Learning Approach to Text Normalization
A1  - Maryam Zare
A1  - Shaurya Rohatgi
JO  - ArXiv e-prints
Y1  - 17 December, 2017
UR  - https://arxiv.org/abs/1712.06994
N2  - This paper presents an simple yet sophisticated approach to the challenge by Sproat and Jaitly (2016)- given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. Text normalization for a token seems very straightforward without it&#39;s context. But given the context of the used token and then normalizing becomes tricky for some classes. We present a novel approach in which the prediction of our classification algorithm is used by our sequence to sequence model to predict the normalized text of the input token. Our approach takes very less time to learn and perform well unlike what has been reported by Google (5 days on their GPU cluster). We have achieved an accuracy of 97.62 which is impressive given the resources we use. Our approach is using the best of both worlds, gradient boosting - state of the art in most classification tasks and sequence to sequence learning - state of the art in machine translation. We present our experiments and report results with various parameter settings.
ER  -


TY  - Preprint
T1  - Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning
A1  - Felipe Petroski Such
A1  - Vashisht Madhavan
A1  - Edoardo Conti
A1  - Joel Lehman
A1  - Kenneth O. Stanley
A1  - Jeff Clune
JO  - ArXiv e-prints
Y1  - 20 April, 2018
UR  - https://arxiv.org/abs/1712.06567
N2  - Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of neuroevolution techniques that improve performance. We demonstrate the latter by showing that combining DNNs with novelty search, which encourages exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g.\ DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN (it can train Atari in ${\raise.17ex\hbox{$\scriptstyle\sim$}}$4 hours on one desktop or ${\raise.17ex\hbox{$\scriptstyle\sim$}}$1 hour distributed on 720 cores), and enables a state-of-the-art, up to 10,000-fold compact encoding technique.
ER  -


TY  - Preprint
T1  - Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents
A1  - Edoardo Conti
A1  - Vashisht Madhavan
A1  - Felipe Petroski Such
A1  - Joel Lehman
A1  - Kenneth O. Stanley
A1  - Jeff Clune
JO  - ArXiv e-prints
Y1  - 12 June, 2018
UR  - https://arxiv.org/abs/1712.06560
N2  - Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.
ER  -


TY  - Preprint
T1  - Multi-modal Face Pose Estimation with Multi-task Manifold Deep Learning
A1  - Chaoqun Hong
A1  - Jun Yu
JO  - ArXiv e-prints
Y1  - 18 December, 2017
UR  - https://arxiv.org/abs/1712.06467
N2  - Human face pose estimation aims at estimating the gazing direction or head postures with 2D images. It gives some very important information such as communicative gestures, saliency detection and so on, which attracts plenty of attention recently. However, it is challenging because of complex background, various orientations and face appearance visibility. Therefore, a descriptive representation of face images and mapping it to poses are critical. In this paper, we make use of multi-modal data and propose a novel face pose estimation method that uses a novel deep learning framework named Multi-task Manifold Deep Learning $M^2DL$. It is based on feature extraction with improved deep neural networks and multi-modal mapping relationship with multi-task learning. In the proposed deep learning based framework, Manifold Regularized Convolutional Layers (MRCL) improve traditional convolutional layers by learning the relationship among outputs of neurons. Besides, in the proposed mapping relationship learning method, different modals of face representations are naturally combined to learn the mapping function from face images to poses. In this way, the computed mapping model with multiple tasks is improved. Experimental results on three challenging benchmark datasets DPOSE, HPID and BKHPD demonstrate the outstanding performance of $M^2DL$.
ER  -


TY  - Preprint
T1  - Towards a Deep Reinforcement Learning Approach for Tower Line Wars
A1  - Per-Arne Andersen
A1  - Morten Goodwin
A1  - Ole-Christoffer Granmo
JO  - ArXiv e-prints
Y1  - 17 December, 2017
UR  - https://arxiv.org/abs/1712.06180
N2  - There have been numerous breakthroughs with reinforcement learning in the recent years, perhaps most notably on Deep Reinforcement Learning successfully playing and winning relatively advanced computer games. There is undoubtedly an anticipation that Deep Reinforcement Learning will play a major role when the first AI masters the complicated game plays needed to beat a professional Real-Time Strategy game player. For this to be possible, there needs to be a game environment that targets and fosters AI research, and specifically Deep Reinforcement Learning. Some game environments already exist, however, these are either overly simplistic such as Atari 2600 or complex such as Starcraft II from Blizzard Entertainment. We propose a game environment in between Atari 2600 and Starcraft II, particularly targeting Deep Reinforcement Learning algorithm research. The environment is a variant of Tower Line Wars from Warcraft III, Blizzard Entertainment. Further, as a proof of concept that the environment can harbor Deep Reinforcement algorithms, we propose and apply a Deep Q-Reinforcement architecture. The architecture simplifies the state space so that it is applicable to Q-learning, and in turn improves performance compared to current state-of-the-art methods. Our experiments show that the proposed architecture can learn to play the environment well, and score 33% better than standard Deep Q-learning which in turn proves the usefulness of the game environment.
ER  -


TY  - Preprint
T1  - Railway Track Specific Traffic Signal Selection Using Deep Learning
A1  - S Ritika
A1  - Shruti Mittal
A1  - Dattaraj Rao
JO  - ArXiv e-prints
Y1  - 17 December, 2017
UR  - https://arxiv.org/abs/1712.06107
N2  - With the railway transportation Industry moving actively towards automation, accurate location and inventory of wayside track assets like traffic signals, crossings, switches, mileposts, etc. is of extreme importance. With the new Positive Train Control (PTC) regulation coming into effect, many railway safety rules will be tied directly to location of assets like mileposts and signals. Newer speed regulations will be enforced based on location of the Train with respect to a wayside asset. Hence it is essential for the railroads to have an accurate database of the types and locations of these assets. This paper talks about a real-world use-case of detecting railway signals from a camera mounted on a moving locomotive and tracking their locations. The camera is engineered to withstand the environment factors on a moving train and provide a consistent steady image at around 30 frames per second. Using advanced image analysis and deep learning techniques, signals are detected in these camera images and a database of their locations is created. Railway signals differ a lot from road signals in terms of shapes and rules for placement with respect to track. Due to space constraint and traffic densities in urban areas signals are not placed on the same side of the track and multiple lines can run in parallel. Hence there is need to associate signal detected with the track on which the train runs. We present a method to associate the signals to the specific track they belong to using a video feed from the front facing camera mounted on the lead locomotive. A pipeline of track detection, region of interest selection, signal detection has been implemented which gives an overall accuracy of 94.7% on a route covering 150km with 247 signals.
ER  -


TY  - Preprint
T1  - Efficient B-mode Ultrasound Image Reconstruction from Sub-sampled RF Data using Deep Learning
A1  - Yeo Hun Yoon
A1  - Shujaat Khan
A1  - Jaeyoung Huh
A1  - Jong Chul Ye
JO  - ArXiv e-prints
Y1  - 7 August, 2018
UR  - https://arxiv.org/abs/1712.06096
N2  - In portable, three dimensional, and ultra-fast ultrasound imaging systems, there is an increasing demand for the reconstruction of high quality images from a limited number of radio-frequency (RF) measurements due to receiver (Rx) or transmit (Xmit) event sub-sampling. However, due to the presence of side lobe artifacts from RF sub-sampling, the standard beamformer often produces blurry images with less contrast, which are unsuitable for diagnostic purposes. Existing compressed sensing approaches often require either hardware changes or computationally expensive algorithms, but their quality improvements are limited. To address this problem, here we propose a novel deep learning approach that directly interpolates the missing RF data by utilizing redundancy in the Rx-Xmit plane. Our extensive experimental results using sub-sampled RF data from a multi-line acquisition B-mode system confirm that the proposed method can effectively reduce the data rate without sacrificing image quality.
ER  -


TY  - Preprint
T1  - &#34;Zero-Shot&#34; Super-Resolution using Deep Internal Learning
A1  - Assaf Shocher
A1  - Nadav Cohen
A1  - Michal Irani
JO  - ArXiv e-prints
Y1  - 17 December, 2017
UR  - https://arxiv.org/abs/1712.06087
N2  - Deep Learning has led to a dramatic leap in Super-Resolution (SR) performance in the past few years. However, being supervised, these SR methods are restricted to specific training data, where the acquisition of the low-resolution (LR) images from their high-resolution (HR) counterparts is predetermined (e.g., bicubic downscaling), without any distracting artifacts (e.g., sensor noise, image compression, non-ideal PSF, etc). Real LR images, however, rarely obey these restrictions, resulting in poor SR results by SotA (State of the Art) methods. In this paper we introduce &#34;Zero-Shot&#34; SR, which exploits the power of Deep Learning, but does not rely on prior training. We exploit the internal recurrence of information inside a single image, and train a small image-specific CNN at test time, on examples extracted solely from the input image itself. As such, it can adapt itself to different settings per image. This allows to perform SR of real old photos, noisy images, biological data, and other images where the acquisition process is unknown or non-ideal. On such images, our method outperforms SotA CNN-based SR methods, as well as previous unsupervised SR methods. To the best of our knowledge, this is the first unsupervised CNN-based SR method.
ER  -


TY  - Preprint
T1  - Deep Learning for Distant Speech Recognition
A1  - Mirco Ravanelli
JO  - ArXiv e-prints
Y1  - 17 December, 2017
UR  - https://arxiv.org/abs/1712.06086
N2  - Deep learning is an emerging technology that is considered one of the most promising directions for reaching higher levels of artificial intelligence. Among the other achievements, building computers that understand speech represents a crucial leap towards intelligent machines. Despite the great efforts of the past decades, however, a natural and robust human-machine speech interaction still appears to be out of reach, especially when users interact with a distant microphone in noisy and reverberant environments. The latter disturbances severely hamper the intelligibility of a speech signal, making Distant Speech Recognition (DSR) one of the major open challenges in the field.
ER  -


TY  - Preprint
T1  - Using Deep learning methods for generation of a personalized list of shuffled songs
A1  - Rushin Gindra
A1  - Srushti Kotak
A1  - Asmita Natekar
A1  - Grishma Sharma
JO  - ArXiv e-prints
Y1  - 17 December, 2017
UR  - https://arxiv.org/abs/1712.06076
N2  - The shuffle mode, where songs are played in a randomized order that is decided upon for all tracks at once, is widely found and known to exist in music player systems. There are only few music enthusiasts who use this mode since it either is too random to suit their mood or it keeps on repeating the same list every time. In this paper, we propose to build a convolutional deep belief network(CDBN) that is trained to perform genre recognition based on audio features retrieved from the records of the Million Song Dataset. The learned parameters shall be used to initialize a multi-layer perceptron which takes extracted features of user&#39;s playlist as input alongside the metadata to classify to various categories. These categories will be shuffled retrospectively based on the metadata to autonomously provide with a list that is efficacious in playing songs that are desired by humans in normal conditions.
ER  -


TY  - Preprint
T1  - Learning a Virtual Codec Based on Deep Convolutional Neural Network to Compress Image
A1  - Lijun Zhao
A1  - Huihui Bai
A1  - Anhong Wang
A1  - Yao Zhao
JO  - ArXiv e-prints
Y1  - 16 January, 2018
UR  - https://arxiv.org/abs/1712.05969
N2  - Although deep convolutional neural network has been proved to efficiently eliminate coding artifacts caused by the coarse quantization of traditional codec, it&#39;s difficult to train any neural network in front of the encoder for gradient&#39;s back-propagation. In this paper, we propose an end-to-end image compression framework based on convolutional neural network to resolve the problem of non-differentiability of the quantization function in the standard codec. First, the feature description neural network is used to get a valid description in the low-dimension space with respect to the ground-truth image so that the amount of image data is greatly reduced for storage or transmission. After image&#39;s valid description, standard image codec such as JPEG is leveraged to further compress image, which leads to image&#39;s great distortion and compression artifacts, especially blocking artifacts, detail missing, blurring, and ringing artifacts. Then, we use a post-processing neural network to remove these artifacts. Due to the challenge of directly learning a non-linear function for a standard codec based on convolutional neural network, we propose to learn a virtual codec neural network to approximate the projection from the valid description image to the post-processed compressed image, so that the gradient could be efficiently back-propagated from the post-processing neural network to the feature description neural network during training. Meanwhile, an advanced learning algorithm is proposed to train our deep neural networks for compression. Obviously, the priority of the proposed method is compatible with standard existing codecs and our learning strategy can be easily extended into these codecs based on convolutional neural network. Experimental results have demonstrated the advances of the proposed method as compared to several state-of-the-art approaches, especially at very low bit-rate.
ER  -


TY  - Preprint
T1  - Cyberattack Detection in Mobile Cloud Computing: A Deep Learning Approach
A1  - Khoi Khac Nguyen
A1  - Dinh Thai Hoang
A1  - Dusit Niyato
A1  - Ping Wang
A1  - Diep Nguyen
A1  - Eryk Dutkiewicz
JO  - ArXiv e-prints
Y1  - 16 December, 2017
UR  - https://arxiv.org/abs/1712.05914
N2  - With the rapid growth of mobile applications and cloud computing, mobile cloud computing has attracted great interest from both academia and industry. However, mobile cloud applications are facing security issues such as data integrity, users&#39; confidentiality, and service availability. A preventive approach to such problems is to detect and isolate cyber threats before they can cause serious impacts to the mobile cloud computing system. In this paper, we propose a novel framework that leverages a deep learning approach to detect cyberattacks in mobile cloud environment. Through experimental results, we show that our proposed framework not only recognizes diverse cyberattacks, but also achieves a high accuracy (up to 97.11%) in detecting the attacks. Furthermore, we present the comparisons with current machine learning-based approaches to demonstrate the effectiveness of our proposed solution.
ER  -


TY  - Preprint
T1  - Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning
A1  - Xinyun Chen
A1  - Chang Liu
A1  - Bo Li
A1  - Kimberly Lu
A1  - Dawn Song
JO  - ArXiv e-prints
Y1  - 14 December, 2017
UR  - https://arxiv.org/abs/1712.05526
N2  - Deep learning models have achieved high performance on many tasks, and thus have been applied to many security-critical scenarios. For example, deep learning-based face recognition systems have been used to authenticate users to access many security-sensitive applications like payment apps. Such usages of deep learning systems provide the adversaries with sufficient incentives to perform attacks against these systems for their adversarial purposes. In this work, we consider a new type of attacks, called backdoor attacks, where the attacker&#39;s goal is to create a backdoor into a learning-based authentication system, so that he can easily circumvent the system by leveraging the backdoor. Specifically, the adversary aims at creating backdoor instances, so that the victim learning system will be misled to classify the backdoor instances as a target label specified by the adversary. In particular, we study backdoor poisoning attacks, which achieve backdoor attacks using poisoning strategies. Different from all existing work, our studied poisoning strategies can apply under a very weak threat model: (1) the adversary has no knowledge of the model and the training set used by the victim system; (2) the attacker is allowed to inject only a small amount of poisoning samples; (3) the backdoor key is hard to notice even by human beings to achieve stealthiness. We conduct evaluation to demonstrate that a backdoor adversary can inject only around 50 poisoning samples, while achieving an attack success rate of above 90%. We are also the first work to show that a data poisoning attack can create physically implementable backdoors without touching the training process. Our work demonstrates that backdoor poisoning attacks pose real threats to a learning system, and thus highlights the importance of further investigation and proposing defense strategies against them.
ER  -


TY  - Preprint
T1  - DLR : Toward a deep learned rhythmic representation for music content analysis
A1  - Yeonwoo Jeong
A1  - Keunwoo Choi
A1  - Hosan Jeong
JO  - ArXiv e-prints
Y1  - 14 December, 2017
UR  - https://arxiv.org/abs/1712.05119
N2  - In the use of deep neural networks, it is crucial to provide appropriate input representations for the network to learn from. In this paper, we propose an approach to learn a representation that focus on rhythmic representation which is named as DLR (Deep Learning Rhythmic representation). The proposed approach aims to learn DLR from the raw audio signal and use it for other music informatics tasks. A 1-dimensional convolutional network is utilised in the learning of DLR. In the experiment, we present the results from the source task and the target task as well as visualisations of DLRs. The results reveals that DLR provides compact rhythmic information which can be used on multi-tagging task.
ER  -


TY  - Preprint
T1  - Learning to Navigate by Growing Deep Networks
A1  - Thushan Ganegedara
A1  - Lionel Ott
A1  - Fabio Ramos
JO  - ArXiv e-prints
Y1  - 13 December, 2017
UR  - https://arxiv.org/abs/1712.05084
N2  - Adaptability is central to autonomy. Intuitively, for high-dimensional learning problems such as navigating based on vision, internal models with higher complexity allow to accurately encode the information available. However, most learning methods rely on models with a fixed structure and complexity. In this paper, we present a self-supervised framework for robots to learn to navigate, without any prior knowledge of the environment, by incrementally building the structure of a deep network as new data becomes available. Our framework captures images from a monocular camera and self labels the images to continuously train and predict actions from a computationally efficient adaptive deep architecture based on Autoencoders (AE), in a self-supervised fashion. The deep architecture, named Reinforced Adaptive Denoising Autoencoders (RA-DAE), uses reinforcement learning to dynamically change the network structure by adding or removing neurons. Experiments were conducted in simulation and real-world indoor and outdoor environments to assess the potential of self-supervised navigation. RA-DAE demonstrates better performance than equivalent non-adaptive deep learning alternatives and can continue to expand its knowledge, trading-off past and present information.
ER  -


TY  - Preprint
T1  - MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels
A1  - Lu Jiang
A1  - Zhengyuan Zhou
A1  - Thomas Leung
A1  - Li-Jia Li
A1  - Li Fei-Fei
JO  - ArXiv e-prints
Y1  - 13 August, 2018
UR  - https://arxiv.org/abs/1712.05055
N2  - Recent deep networks are capable of memorizing the entire data even when the labels are completely random. To overcome the overfitting on corrupted labels, we propose a novel technique of learning another neural network, called MentorNet, to supervise the training of the base deep networks, namely, StudentNet. During training, MentorNet provides a curriculum (sample weighting scheme) for StudentNet to focus on the sample the label of which is probably correct. Unlike the existing curriculum that is usually predefined by human experts, MentorNet learns a data-driven curriculum dynamically with StudentNet. Experimental results demonstrate that our approach can significantly improve the generalization performance of deep networks trained on corrupted training data. Notably, to the best of our knowledge, we achieve the best-published result on WebVision, a large benchmark containing 2.2 million images of real-world noisy labels. The code are at https://github.com/google/mentornet
ER  -


TY  - Preprint
T1  - Evolving Unsupervised Deep Neural Networks for Learning Meaningful Representations
A1  - Yanan Sun
A1  - Gary G. Yen
A1  - Zhang Yi
JO  - ArXiv e-prints
Y1  - 23 February, 2018
UR  - https://arxiv.org/abs/1712.05043
N2  - Deep Learning (DL) aims at learning the \emph{meaningful representations}. A meaningful representation refers to the one that gives rise to significant performance improvement of associated Machine Learning (ML) tasks by replacing the raw data as the input. However, optimal architecture design and model parameter estimation in DL algorithms are widely considered to be intractable. Evolutionary algorithms are much preferable for complex and non-convex problems due to its inherent characteristics of gradient-free and insensitivity to local optimum. In this paper, we propose a computationally economical algorithm for evolving \emph{unsupervised deep neural networks} to efficiently learn \emph{meaningful representations}, which is very suitable in the current Big Data era where sufficient labeled data for training is often expensive to acquire. In the proposed algorithm, finding an appropriate architecture and the initialized parameter values for a ML task at hand is modeled by one computational efficient gene encoding approach, which is employed to effectively model the task with a large number of parameters. In addition, a local search strategy is incorporated to facilitate the exploitation search for further improving the performance. Furthermore, a small proportion labeled data is utilized during evolution search to guarantee the learnt representations to be meaningful. The performance of the proposed algorithm has been thoroughly investigated over classification tasks. Specifically, error classification rate on MNIST with $1.15\%$ is reached by the proposed algorithm consistently, which is a very promising result against state-of-the-art unsupervised DL algorithms.
ER  -


TY  - Preprint
T1  - FFT-Based Deep Learning Deployment in Embedded Systems
A1  - Sheng Lin
A1  - Ning Liu
A1  - Mahdi Nazemi
A1  - Hongjia Li
A1  - Caiwen Ding
A1  - Yanzhi Wang
A1  - Massoud Pedram
JO  - ArXiv e-prints
Y1  - 13 December, 2017
UR  - https://arxiv.org/abs/1712.04910
N2  - Deep learning has delivered its powerfulness in many application domains, especially in image and speech recognition. As the backbone of deep learning, deep neural networks (DNNs) consist of multiple layers of various types with hundreds to thousands of neurons. Embedded platforms are now becoming essential for deep learning deployment due to their portability, versatility, and energy efficiency. The large model size of DNNs, while providing excellent accuracy, also burdens the embedded platforms with intensive computation and storage. Researchers have investigated on reducing DNN model size with negligible accuracy loss. This work proposes a Fast Fourier Transform (FFT)-based DNN training and inference model suitable for embedded platforms with reduced asymptotic complexity of both computation and storage, making our approach distinguished from existing approaches. We develop the training and inference algorithms based on FFT as the computing kernel and deploy the FFT-based inference model on embedded platforms achieving extraordinary processing speed.
ER  -


TY  - Preprint
T1  - Mathematics of Deep Learning
A1  - Rene Vidal
A1  - Joan Bruna
A1  - Raja Giryes
A1  - Stefano Soatto
JO  - ArXiv e-prints
Y1  - 13 December, 2017
UR  - https://arxiv.org/abs/1712.04741
N2  - Recently there has been a dramatic increase in the performance of recognition systems due to the introduction of deep architectures for representation learning and classification. However, the mathematical reasons for this success remain elusive. This tutorial will review recent work that aims to provide a mathematical justification for several properties of deep networks, such as global optimality, geometric stability, and invariance of the learned representations.
ER  -


TY  - Preprint
T1  - The Effectiveness of Data Augmentation in Image Classification using Deep Learning
A1  - Luis Perez
A1  - Jason Wang
JO  - ArXiv e-prints
Y1  - 13 December, 2017
UR  - https://arxiv.org/abs/1712.04621
N2  - In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classification. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and flipping input images. We artificially constrain our access to data to a small subset of the ImageNet dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with GANs to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classifier, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.
ER  -


TY  - Preprint
T1  - Multi-focus Attention Network for Efficient Deep Reinforcement Learning
A1  - Jinyoung Choi
A1  - Beom-Jin Lee
A1  - Byoung-Tak Zhang
JO  - ArXiv e-prints
Y1  - 12 December, 2017
UR  - https://arxiv.org/abs/1712.04603
N2  - Deep reinforcement learning (DRL) has shown incredible performance in learning various tasks to the human level. However, unlike human perception, current DRL models connect the entire low-level sensory input to the state-action values rather than exploiting the relationship between and among entities that constitute the sensory input. Because of this difference, DRL needs vast amount of experience samples to learn. In this paper, we propose a Multi-focus Attention Network (MANet) which mimics human ability to spatially abstract the low-level sensory input into multiple entities and attend to them simultaneously. The proposed method first divides the low-level input into several segments which we refer to as partial states. After this segmentation, parallel attention layers attend to the partial states relevant to solving the task. Our model estimates state-action values using these attended partial states. In our experiments, MANet attains highest scores with significantly less experience samples. Additionally, the model shows higher performance compared to the Deep Q-network and the single attention model as benchmarks. Furthermore, we extend our model to attentive communication model for performing multi-agent cooperative tasks. In multi-agent cooperative task experiments, our model shows 20% faster learning than existing state-of-the-art model.
ER  -


TY  - Preprint
T1  - Over the Air Deep Learning Based Radio Signal Classification
A1  - Timothy J. O&#39;Shea
A1  - Tamoghna Roy
A1  - T. Charles Clancy
JO  - ArXiv e-prints
Y1  - 12 December, 2017
UR  - https://arxiv.org/abs/1712.04578
N2  - We conduct an in depth study on the performance of deep learning based radio signal classification for radio communications signals. We consider a rigorous baseline method using higher order moments and strong boosted gradient tree classification and compare performance between the two approaches across a range of configurations and channel impairments. We consider the effects of carrier frequency offset, symbol rate, and multi-path fading in simulation and conduct over-the-air measurement of radio classification performance in the lab using software radios and compare performance and training strategies for both. Finally we conclude with a discussion of remaining problems, and design considerations for using such techniques.
ER  -


TY  - Preprint
T1  - auDeep: Unsupervised Learning of Representations from Audio with Deep Recurrent Neural Networks
A1  - Michael Freitag
A1  - Shahin Amiriparian
A1  - Sergey Pugachevskiy
A1  - Nicholas Cummins
A1  - BjÃ¶rn Schuller
JO  - ArXiv e-prints
Y1  - 22 December, 2017
UR  - https://arxiv.org/abs/1712.04382
N2  - auDeep is a Python toolkit for deep unsupervised representation learning from acoustic data. It is based on a recurrent sequence to sequence autoencoder approach which can learn representations of time series data by taking into account their temporal dynamics. We provide an extensive command line interface in addition to a Python API for users and developers, both of which are comprehensively documented and publicly available at https://github.com/auDeep/auDeep. Experimental results indicate that auDeep features are competitive with state-of-the art audio classification.
ER  -


TY  - Preprint
T1  - Music Generation by Deep Learning - Challenges and Directions
A1  - Jean-Pierre Briot
A1  - FranÃ§ois Pachet
JO  - ArXiv e-prints
Y1  - 30 September, 2018
UR  - https://arxiv.org/abs/1712.04371
N2  - In addition to traditional tasks such as prediction, classification and translation, deep learning is receiving growing attention as an approach for music generation, as witnessed by recent research groups such as Magenta at Google and CTRL (Creator Technology Research Lab) at Spotify. The motivation is in using the capacity of deep learning architectures and training techniques to automatically learn musical styles from arbitrary musical corpora and then to generate samples from the estimated distribution. However, a direct application of deep learning to generate content rapidly reaches limits as the generated content tends to mimic the training set without exhibiting true creativity. Moreover, deep learning architectures do not offer direct ways for controlling generation (e.g., imposing some tonality or other arbitrary constraints). Furthermore, deep learning architectures alone are autistic automata which generate music autonomously without human user interaction, far from the objective of interactively assisting musicians to compose and refine music. Issues such as: control, structure, creativity and interactivity are the focus of our analysis. In this paper, we select some limitations of a direct application of deep learning to music generation, analyze why the issues are not fulfilled and how to address them by possible approaches. Various examples of recent systems are cited as examples of promising directions.
ER  -


TY  - Preprint
T1  - Simulated Autonomous Driving on Realistic Road Networks using Deep Reinforcement Learning
A1  - Patrick Klose
A1  - Rudolf Mester
JO  - ArXiv e-prints
Y1  - 3 April, 2018
UR  - https://arxiv.org/abs/1712.04363
N2  - Using Deep Reinforcement Learning (DRL) can be a promising approach to handle various tasks in the field of (simulated) autonomous driving. However, recent publications mainly consider learning in unusual driving environments. This paper presents Driving School for Autonomous Agents (DSA^2), a software for validating DRL algorithms in more usual driving environments based on artificial and realistic road networks. We also present the results of applying DSA^2 for handling the task of driving on a straight road while regulating the velocity of one vehicle according to different speed limits.
ER  -


TY  - Preprint
T1  - Predicting Yelp Star Reviews Based on Network Structure with Deep Learning
A1  - Luis Perez
JO  - ArXiv e-prints
Y1  - 11 December, 2017
UR  - https://arxiv.org/abs/1712.04350
N2  - In this paper, we tackle the real-world problem of predicting Yelp star-review rating based on business features (such as images, descriptions), user features (average previous ratings), and, of particular interest, network properties (which businesses has a user rated before). We compare multiple models on different sets of features -- from simple linear regression on network features only to deep learning models on network and item features.
ER  -


TY  - Preprint
T1  - Deep Learning for IoT Big Data and Streaming Analytics: A Survey
A1  - Mehdi Mohammadi
A1  - Ala Al-Fuqaha
A1  - Sameh Sorour
A1  - Mohsen Guizani
JO  - ArXiv e-prints
Y1  - 4 June, 2018
UR  - https://arxiv.org/abs/1712.04301
N2  - In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices will result in big or fast/real-time data streams. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process that makes IoT a worthy paradigm for businesses and a quality-of-life improving technology. In this paper, we provide a thorough overview on using a class of advanced machine learning techniques, namely Deep Learning (DL), to facilitate the analytics and learning in the IoT domain. We start by articulating IoT data characteristics and identifying two major treatments for IoT data from a machine learning perspective, namely IoT big data analytics and IoT streaming data analytics. We also discuss why DL is a promising approach to achieve the desired analytics in these types of data and applications. The potential of using emerging DL techniques for IoT data analytics are then discussed, and its promises and challenges are introduced. We present a comprehensive background on different DL architectures and algorithms. We also analyze and summarize major reported research attempts that leveraged DL in the IoT domain. The smart IoT devices that have incorporated DL in their intelligence background are also discussed. DL implementation approaches on the fog and cloud centers in support of IoT applications are also surveyed. Finally, we shed light on some challenges and potential directions for future research. At the end of each section, we highlight the lessons learned based on our experiments and review of the recent literature.
ER  -


TY  - Preprint
T1  - Deep learning enhanced mobile-phone microscopy
A1  - Yair Rivenson
A1  - Hatice Ceylan Koydemir
A1  - Hongda Wang
A1  - Zhensong Wei
A1  - Zhengshuang Ren
A1  - Harun Gunaydin
A1  - Yibo Zhang
A1  - Zoltan Gorocs
A1  - Kyle Liang
A1  - Derek Tseng
A1  - Aydogan Ozcan
JO  - ArXiv e-prints
Y1  - 12 December, 2017
UR  - https://arxiv.org/abs/1712.04139
N2  - Mobile-phones have facilitated the creation of field-portable, cost-effective imaging and sensing technologies that approach laboratory-grade instrument performance. However, the optical imaging interfaces of mobile-phones are not designed for microscopy and produce spatial and spectral distortions in imaging microscopic specimens. Here, we report on the use of deep learning to correct such distortions introduced by mobile-phone-based microscopes, facilitating the production of high-resolution, denoised and colour-corrected images, matching the performance of benchtop microscopes with high-end objective lenses, also extending their limited depth-of-field. After training a convolutional neural network, we successfully imaged various samples, including blood smears, histopathology tissue sections, and parasites, where the recorded images were highly compressed to ease storage and transmission for telemedicine applications. This method is applicable to other low-cost, aberrated imaging systems, and could offer alternatives for costly and bulky microscopes, while also providing a framework for standardization of optical images for clinical and biomedical applications.
ER  -


TY  - Preprint
T1  - Deep Learning for Reliable Mobile Edge Analytics in Intelligent Transportation Systems
A1  - Aidin Ferdowsi
A1  - Ursula Challita
A1  - Walid Saad
JO  - ArXiv e-prints
Y1  - 12 December, 2017
UR  - https://arxiv.org/abs/1712.04135
N2  - Intelligent transportation systems (ITSs) will be a major component of tomorrow&#39;s smart cities. However, realizing the true potential of ITSs requires ultra-low latency and reliable data analytics solutions that can combine, in real-time, a heterogeneous mix of data stemming from the ITS network and its environment. Such data analytics capabilities cannot be provided by conventional cloud-centric data processing techniques whose communication and computing latency can be high. Instead, edge-centric solutions that are tailored to the unique ITS environment must be developed. In this paper, an edge analytics architecture for ITSs is introduced in which data is processed at the vehicle or roadside smart sensor level in order to overcome the ITS latency and reliability challenges. With a higher capability of passengers&#39; mobile devices and intra-vehicle processors, such a distributed edge computing architecture can leverage deep learning techniques for reliable mobile sensing in ITSs. In this context, the ITS mobile edge analytics challenges pertaining to heterogeneous data, autonomous control, vehicular platoon control, and cyber-physical security are investigated. Then, different deep learning solutions for such challenges are proposed. The proposed deep learning solutions will enable ITS edge analytics by endowing the ITS devices with powerful computer vision and signal processing functions. Preliminary results show that the proposed edge analytics architecture, coupled with the power of deep learning algorithms, can provide a reliable, secure, and truly smart transportation environment.
ER  -


TY  - Preprint
T1  - 200x Low-dose PET Reconstruction using Deep Learning
A1  - Junshen Xu
A1  - Enhao Gong
A1  - John Pauly
A1  - Greg Zaharchuk
JO  - ArXiv e-prints
Y1  - 11 December, 2017
UR  - https://arxiv.org/abs/1712.04119
N2  - Positron emission tomography (PET) is widely used in various clinical applications, including cancer diagnosis, heart disease and neuro disorders. The use of radioactive tracer in PET imaging raises concerns due to the risk of radiation exposure. To minimize this potential risk in PET imaging, efforts have been made to reduce the amount of radio-tracer usage. However, lowing dose results in low Signal-to-Noise-Ratio (SNR) and loss of information, both of which will heavily affect clinical diagnosis. Besides, the ill-conditioning of low-dose PET image reconstruction makes it a difficult problem for iterative reconstruction algorithms. Previous methods proposed are typically complicated and slow, yet still cannot yield satisfactory results at significantly low dose. Here, we propose a deep learning method to resolve this issue with an encoder-decoder residual deep network with concatenate skip connections. Experiments shows the proposed method can reconstruct low-dose PET image to a standard-dose quality with only two-hundredth dose. Different cost functions for training model are explored. Multi-slice input strategy is introduced to provide the network with more structural information and make it more robust to noise. Evaluation on ultra-low-dose clinical data shows that the proposed method can achieve better result than the state-of-the-art methods and reconstruct images with comparable quality using only 0.5% of the original regular dose.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning Boosted by External Knowledge
A1  - Nicolas Bougie
A1  - Ryutaro Ichise
JO  - ArXiv e-prints
Y1  - 11 December, 2017
UR  - https://arxiv.org/abs/1712.04101
N2  - Recent improvements in deep reinforcement learning have allowed to solve problems in many 2D domains such as Atari games. However, in complex 3D environments, numerous learning episodes are required which may be too time consuming or even impossible especially in real-world scenarios. We present a new architecture to combine external knowledge and deep reinforcement learning using only visual input. A key concept of our system is augmenting image input by adding environment feature information and combining two sources of decision. We evaluate the performances of our method in a 3D partially-observable environment from the Microsoft Malmo platform. Experimental evaluation exhibits higher performance and faster learning compared to a single reinforcement learning model.
ER  -


TY  - Preprint
T1  - End-to-end Learning from Spectrum Data: A Deep Learning approach for Wireless Signal Identification in Spectrum Monitoring applications
A1  - Merima Kulin
A1  - Tarik Kazaz
A1  - Ingrid Moerman
A1  - Eli de Poorter
JO  - ArXiv e-prints
Y1  - 11 December, 2017
UR  - https://arxiv.org/abs/1712.03987
N2  - This paper presents end-to-end learning from spectrum data - an umbrella term for new sophisticated wireless signal identification approaches in spectrum monitoring applications based on deep neural networks. End-to-end learning allows to (i) automatically learn features directly from simple wireless signal representations, without requiring design of hand-crafted expert features like higher order cyclic moments, and (ii) train wireless signal classifiers in one end-to-end step which eliminates the need for complex multi-stage machine learning processing pipelines. The purpose of this article is to present the conceptual framework of end-to-end learning for spectrum monitoring and systematically introduce a generic methodology to easily design and implement wireless signal classifiers. Furthermore, we investigate the importance of the choice of wireless data representation to various spectrum monitoring tasks. In particular, two case studies are elaborated (i) modulation recognition and (ii) wireless technology interference detection. For each case study three convolutional neural networks are evaluated for the following wireless signal representations: temporal IQ data, the amplitude/phase representation and the frequency domain representation. From our analysis we prove that the wireless data representation impacts the accuracy depending on the specifics and similarities of the wireless signals that need to be differentiated, with different data representations resulting in accuracy variations of up to 29%. Experimental results show that using the amplitude/phase representation for recognizing modulation formats can lead to performance improvements up to 2% and 12% for medium to high SNR compared to IQ and frequency domain data, respectively. For the task of detecting interference, frequency domain representation outperformed amplitude/phase and IQ data representation up to 20%.
ER  -


TY  - Preprint
T1  - StrassenNets: Deep Learning with a Multiplication Budget
A1  - Michael Tschannen
A1  - Aran Khanna
A1  - Anima Anandkumar
JO  - ArXiv e-prints
Y1  - 8 June, 2018
UR  - https://arxiv.org/abs/1712.03942
N2  - A large fraction of the arithmetic operations required to evaluate deep neural networks (DNNs) consists of matrix multiplications, in both convolution and fully connected layers. We perform end-to-end learning of low-cost approximations of matrix multiplications in DNN layers by casting matrix multiplications as 2-layer sum-product networks (SPNs) (arithmetic circuits) and learning their (ternary) edge weights from data. The SPNs disentangle multiplication and addition operations and enable us to impose a budget on the number of multiplication operations. Combining our method with knowledge distillation and applying it to image classification DNNs (trained on ImageNet) and language modeling DNNs (using LSTMs), we obtain a first-of-a-kind reduction in number of multiplications (over 99.5%) while maintaining the predictive performance of the full-precision models. Finally, we demonstrate that the proposed framework is able to rediscover Strassen&#39;s matrix multiplication algorithm, learning to multiply $2 \times 2$ matrices using only 7 multiplications instead of 8.
ER  -


TY  - Preprint
T1  - NestedNet: Learning Nested Sparse Structures in Deep Neural Networks
A1  - Eunwoo Kim
A1  - Chanho Ahn
A1  - Songhwai Oh
JO  - ArXiv e-prints
Y1  - 27 March, 2018
UR  - https://arxiv.org/abs/1712.03781
N2  - Recently, there have been increasing demands to construct compact deep architectures to remove unnecessary redundancy and to improve the inference speed. While many recent works focus on reducing the redundancy by eliminating unneeded weight parameters, it is not possible to apply a single deep architecture for multiple devices with different resources. When a new device or circumstantial condition requires a new deep architecture, it is necessary to construct and train a new network from scratch. In this work, we propose a novel deep learning framework, called a nested sparse network, which exploits an n-in-1-type nested structure in a neural network. A nested sparse network consists of multiple levels of networks with a different sparsity ratio associated with each level, and higher level networks share parameters with lower level networks to enable stable nested learning. The proposed framework realizes a resource-aware versatile architecture as the same network can meet diverse resource requirements. Moreover, the proposed nested network can learn different forms of knowledge in its internal networks at different levels, enabling multiple tasks using a single network, such as coarse-to-fine hierarchical classification. In order to train the proposed nested sparse network, we propose efficient weight connection learning and channel and layer scheduling strategies. We evaluate our network in multiple tasks, including adaptive deep compression, knowledge distillation, and learning class hierarchy, and demonstrate that nested sparse networks perform competitively, but more efficiently, compared to existing methods.
ER  -


TY  - Preprint
T1  - Robust Deep Reinforcement Learning with Adversarial Attacks
A1  - Anay Pattanaik
A1  - Zhenyi Tang
A1  - Shuijing Liu
A1  - Gautham Bommannan
A1  - Girish Chowdhary
JO  - ArXiv e-prints
Y1  - 10 December, 2017
UR  - https://arxiv.org/abs/1712.03632
N2  - This paper proposes adversarial attacks for Reinforcement Learning (RL) and then improves the robustness of Deep Reinforcement Learning algorithms (DRL) to parameter uncertainties with the help of these attacks. We show that even a naively engineered attack successfully degrades the performance of DRL algorithm. We further improve the attack using gradient information of an engineered loss function which leads to further degradation in performance. These attacks are then leveraged during training to improve the robustness of RL within robust control framework. We show that this adversarial training of DRL algorithms like Deep Double Q learning and Deep Deterministic Policy Gradients leads to significant increase in robustness to parameter variations for RL benchmarks such as Cart-pole, Mountain Car, Hopper and Half Cheetah environment.
ER  -


TY  - Preprint
T1  - Gradient Normalization &amp; Depth Based Decay For Deep Learning
A1  - Robert Kwiatkowski
A1  - Oscar Chang
JO  - ArXiv e-prints
Y1  - 28 February, 2018
UR  - https://arxiv.org/abs/1712.03607
N2  - In this paper we introduce a novel method of gradient normalization and decay with respect to depth. Our method leverages the simple concept of normalizing all gradients in a deep neural network, and then decaying said gradients with respect to their depth in the network. Our proposed normalization and decay techniques can be used in conjunction with most current state of the art optimizers and are a very simple addition to any network. This method, although simple, showed improvements in convergence time on state of the art networks such as DenseNet and ResNet on image classification tasks, as well as on an LSTM for natural language processing tasks.
ER  -


TY  - Preprint
T1  - Visual aesthetic analysis using deep neural network: model and techniques to increase accuracy without transfer learning
A1  - Muktabh Mayank Srivastava
A1  - Sonaal Kant
JO  - ArXiv e-prints
Y1  - 31 January, 2018
UR  - https://arxiv.org/abs/1712.03382
N2  - We train a deep Convolutional Neural Network (CNN) from scratch for visual aesthetic analysis in images and discuss techniques we adopt to improve the accuracy. We avoid the prevalent best transfer learning approaches of using pretrained weights to perform the task and train a model from scratch to get accuracy of 78.7% on AVA2 Dataset close to the best models available (85.6%). We further show that accuracy increases to 81.48% on increasing the training set by incremental 10 percentile of entire AVA dataset showing our algorithm gets better with more data.
ER  -


TY  - Preprint
T1  - Music Transcription by Deep Learning with Data and &#34;Artificial Semantic&#34; Augmentation
A1  - Vladyslav Sarnatskyi
A1  - Vadym Ovcharenko
A1  - Mariia Tkachenko
A1  - Sergii Stirenko
A1  - Yuri Gordienko
A1  - Anis Rojbi
JO  - ArXiv e-prints
Y1  - 8 December, 2017
UR  - https://arxiv.org/abs/1712.03228
N2  - In this progress paper the previous results of the single note recognition by deep learning are presented. The several ways for data augmentation and &#34;artificial semantic&#34; augmentation are proposed to enhance efficiency of deep learning approaches for monophonic and polyphonic note recognition by increase of dimensions of training data, their lossless and lossy transformations.
ER  -


TY  - Preprint
T1  - Class Rectification Hard Mining for Imbalanced Deep Learning
A1  - Qi Dong
A1  - Shaogang Gong
A1  - Xiatian Zhu
JO  - ArXiv e-prints
Y1  - 8 December, 2017
UR  - https://arxiv.org/abs/1712.03162
N2  - Recognising detailed facial or clothing attributes in images of people is a challenging task for computer vision, especially when the training data are both in very large scale and extremely imbalanced among different attribute classes. To address this problem, we formulate a novel scheme for batch incremental hard sample mining of minority attribute classes from imbalanced large scale training data. We develop an end-to-end deep learning framework capable of avoiding the dominant effect of majority classes by discovering sparsely sampled boundaries of minority classes. This is made possible by introducing a Class Rectification Loss (CRL) regularising algorithm. We demonstrate the advantages and scalability of CRL over existing state-of-the-art attribute recognition and imbalanced data learning models on two large scale imbalanced benchmark datasets, the CelebA facial attribute dataset and the X-Domain clothing attribute dataset.
ER  -


TY  - Preprint
T1  - Combining Deep Universal Features, Semantic Attributes, and Hierarchical Classification for Zero-Shot Learning
A1  - Jared Markowitz
A1  - Aurora C. Schmidt
A1  - Philippe M. Burlina
A1  - I-Jeng Wang
JO  - ArXiv e-prints
Y1  - 8 December, 2017
UR  - https://arxiv.org/abs/1712.03151
N2  - We address zero-shot (ZS) learning, building upon prior work in hierarchical classification by combining it with approaches based on semantic attribute estimation. For both non-novel and novel image classes we compare multiple formulations of the problem, starting with deep universal features in each case. We investigate the effect of using different posterior probabilities as inputs to the hierarchical classifier, comparing the performances of posteriors derived from distances to SVM classifier boundaries with those of posteriors based on semantic attribute estimation. Using a dataset consisting of 150 object classes from the ImageNet ILSVRC2012 data set, we find that the hierarchical classification method that maximizes expected reward for non-novel classes differs from the method that maximizes expected reward for novel classes. We also show that using input posteriors based on semantic attributes improves the expected reward for novel classes.
ER  -


TY  - Preprint
T1  - Enabling Cooperative Inference of Deep Learning on Wearables and Smartphones
A1  - Mengwei Xu
A1  - Feng Qian
A1  - Saumay Pushp
JO  - ArXiv e-prints
Y1  - 1 December, 2017
UR  - https://arxiv.org/abs/1712.03073
N2  - Deep Learning (DL) algorithm is the state-of-the-art algorithm of many computer science fields and applied on many intelligent mobile applications. In this paper, we propose a system called CoINF, a practical, adaptive, and flexible deep learning framework that enables cooperative inference between wearable devices (e.g., smartwatches and smart glasses) and handhelds. Our framework accelerates the processing and saves the energy consumption of generic deep learning models inference on wearables via judiciously offloading the workloads to paired handhelds at fine granularity in considering of the system environment, the application requirements, and user preference. Deployed as a user-space library, CoINF offers developer-friendly APIs that are as simple as those in traditional DL libraries such as TensorFlow, with all complicated offloading details hidden. We have implemented a prototype of CoINF on Android OS, and used real deep learning models to evaluate its performance on commercial off-the-shelf smartphone and smartwatches. The experimental results show that our framework can achieve substantial execution speedup and energy saving compared to wearable-only and handheld-only strategies.
ER  -


TY  - Preprint
T1  - Compact Hash Code Learning with Binary Deep Neural Network
A1  - Thanh-Toan Do
A1  - Dang-Khoa Le Tan
A1  - Tuan Hoang
A1  - Ngai-Man Cheung
JO  - ArXiv e-prints
Y1  - 6 February, 2018
UR  - https://arxiv.org/abs/1712.02956
N2  - In this work, we firstly propose deep network models and learning algorithms for learning binary hash codes given image representations under both unsupervised and supervised manners. Then, by leveraging the powerful capacity of convolutional neural networks, we propose an end-to-end architecture which jointly learns to extract visual features and produce binary hash codes. Our novel network designs constrain one hidden layer to directly output the binary codes. This addresses a challenging issue in some previous works: optimizing nonsmooth objective functions due to binarization. Additionally, we incorporate independence and balance properties in the direct and strict forms into the learning schemes. Furthermore, we also include similarity preserving property in our objective functions. Our resulting optimizations involving these binary, independence, and balance constraints are difficult to solve. We propose to attack them with alternating optimization and careful relaxation. Experimental results on the benchmark datasets show that our proposed methods compare favorably with the state of the art.
ER  -


TY  - Preprint
T1  - Representations of Sound in Deep Learning of Audio Features from Music
A1  - Sergey Shuvaev
A1  - Hamza Giaffar
A1  - Alexei A. Koulakov
JO  - ArXiv e-prints
Y1  - 7 December, 2017
UR  - https://arxiv.org/abs/1712.02898
N2  - The work of a single musician, group or composer can vary widely in terms of musical style. Indeed, different stylistic elements, from performance medium and rhythm to harmony and texture, are typically exploited and developed across an artist&#39;s lifetime. Yet, there is often a discernable character to the work of, for instance, individual composers at the perceptual level - an experienced listener can often pick up on subtle clues in the music to identify the composer or performer. Here we suggest that a convolutional network may learn these subtle clues or features given an appropriate representation of the music. In this paper, we apply a deep convolutional neural network to a large audio dataset and empirically evaluate its performance on audio classification tasks. Our trained network demonstrates accurate performance on such classification tasks when presented with 5 s examples of music obtained by simple transformations of the raw audio waveform. A particularly interesting example is the spectral representation of music obtained by application of a logarithmically spaced filter bank, mirroring the early stages of auditory signal transduction in mammals. The most successful representation of music to facilitate discrimination was obtained via a random matrix transform (RMT). Networks based on logarithmic filter banks and RMT were able to correctly guess the one composer out of 31 possibilities in 68 and 84 percent of cases respectively.
ER  -


TY  - Preprint
T1  - MoDL: Model Based Deep Learning Architecture for Inverse Problems
A1  - Hemant Kumar Aggarwal
A1  - Merry P. Mani
A1  - Mathews Jacob
JO  - ArXiv e-prints
Y1  - 10 August, 2018
UR  - https://arxiv.org/abs/1712.02862
N2  - We introduce a model-based image reconstruction framework with a convolution neural network (CNN) based regularization prior. The proposed formulation provides a systematic approach for deriving deep architectures for inverse problems with the arbitrary structure. Since the forward model is explicitly accounted for, a smaller network with fewer parameters is sufficient to capture the image information compared to black-box deep learning approaches, thus reducing the demand for training data and training time. Since we rely on end-to-end training, the CNN weights are customized to the forward model, thus offering improved performance over approaches that rely on pre-trained denoisers. The main difference of the framework from existing end-to-end training strategies is the sharing of the network weights across iterations and channels. Our experiments show that the decoupling of the number of iterations from the network complexity offered by this approach provides benefits including lower demand for training data, reduced risk of overfitting, and implementations with significantly reduced memory footprint. We propose to enforce data-consistency by using numerical optimization blocks such as conjugate gradients algorithm within the network; this approach offers faster convergence per iteration, compared to methods that rely on proximal gradients steps to enforce data consistency. Our experiments show that the faster convergence translates to improved performance, especially when the available GPU memory restricts the number of iterations.
ER  -


TY  - Preprint
T1  - Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing
A1  - Syed Shakib Sarwar
A1  - Aayush Ankit
A1  - Kaushik Roy
JO  - ArXiv e-prints
Y1  - 17 September, 2018
UR  - https://arxiv.org/abs/1712.02719
N2  - Deep convolutional neural network (DCNN) based supervised learning is a widely practiced approach for large-scale image classification. However, retraining these large networks to accommodate new, previously unseen data demands high computational time and energy requirements. Also, previously seen training samples may not be available at the time of retraining. We propose an efficient training methodology and incrementally growing a DCNN to allow new classes to be learned while sharing part of the base network. Our proposed methodology is inspired by transfer learning techniques, although it does not forget previously learned classes. An updated network for learning new set of classes is formed using previously learned convolutional layers (shared from initial part of base network) with addition of few newly added convolutional kernels included in the later layers of the network. We evaluated the proposed scheme on several recognition applications. The classification accuracy achieved by our approach is comparable to the regular incremental learning approach (where networks are updated with new training samples only, without any network sharing).
ER  -


TY  - Preprint
T1  - Deep Primal-Dual Reinforcement Learning: Accelerating Actor-Critic using Bellman Duality
A1  - Woon Sang Cho
A1  - Mengdi Wang
JO  - ArXiv e-prints
Y1  - 6 December, 2017
UR  - https://arxiv.org/abs/1712.02467
N2  - We develop a parameterized Primal-Dual $Ï$ Learning method based on deep neural networks for Markov decision process with large state space and off-policy reinforcement learning. In contrast to the popular Q-learning and actor-critic methods that are based on successive approximations to the nonlinear Bellman equation, our method makes primal-dual updates to the policy and value functions utilizing the fundamental linear Bellman duality. Naive parametrization of the primal-dual $Ï$ learning method using deep neural networks would encounter two major challenges: (1) each update requires computing a probability distribution over the state space and is intractable; (2) the iterates are unstable since the parameterized Lagrangian function is no longer linear. We address these challenges by proposing a relaxed Lagrangian formulation with a regularization penalty using the advantage function. We show that the dual policy update step in our method is equivalent to the policy gradient update in the actor-critic method in some special case, while the value updates differ substantially. The main advantage of the primal-dual $Ï$ learning method lies in that the value and policy updates are closely coupled together using the Bellman duality and therefore more informative. Experiments on a simple cart-pole problem show that the algorithm significantly outperforms the one-step temporal-difference actor-critic method, which is the most relevant benchmark method to compare with. We believe that the primal-dual updates to the value and policy functions would expedite the learning process. The proposed methods might open a door to more efficient algorithms and sharper theoretical analysis.
ER  -


TY  - Preprint
T1  - Listening to Chaotic Whispers: A Deep Learning Framework for News-oriented Stock Trend Prediction
A1  - Ziniu Hu
A1  - Weiqing Liu
A1  - Jiang Bian
A1  - Xuanzhe Liu
A1  - Tie-Yan Liu
JO  - ArXiv e-prints
Y1  - 16 March, 2018
UR  - https://arxiv.org/abs/1712.02136
N2  - Stock trend prediction plays a critical role in seeking maximized profit from stock investment. However, precise trend prediction is very difficult since the highly volatile and non-stationary nature of stock market. Exploding information on Internet together with advancing development of natural language processing and text mining techniques have enable investors to unveil market trends and volatility from online content. Unfortunately, the quality, trustworthiness and comprehensiveness of online content related to stock market varies drastically, and a large portion consists of the low-quality news, comments, or even rumors. To address this challenge, we imitate the learning process of human beings facing such chaotic online news, driven by three principles: sequential content dependency, diverse influence, and effective and efficient learning. In this paper, to capture the first two principles, we designed a Hybrid Attention Networks to predict the stock trend based on the sequence of recent related news. Moreover, we apply the self-paced learning mechanism to imitate the third principle. Extensive experiments on real-world stock market data demonstrate the effectiveness of our approach.
ER  -


TY  - Preprint
T1  - OLÃ: Orthogonal Low-rank Embedding, A Plug and Play Geometric Loss for Deep Learning
A1  - JosÃ© Lezama
A1  - Qiang Qiu
A1  - Pablo MusÃ©
A1  - Guillermo Sapiro
JO  - ArXiv e-prints
Y1  - 5 December, 2017
UR  - https://arxiv.org/abs/1712.01727
N2  - Deep neural networks trained using a softmax layer at the top and the cross-entropy loss are ubiquitous tools for image classification. Yet, this does not naturally enforce intra-class similarity nor inter-class margin of the learned deep representations. To simultaneously achieve these two goals, different solutions have been proposed in the literature, such as the pairwise or triplet losses. However, such solutions carry the extra task of selecting pairs or triplets, and the extra computational burden of computing and learning for many combinations of them. In this paper, we propose a plug-and-play loss term for deep networks that explicitly reduces intra-class variance and enforces inter-class margin simultaneously, in a simple and elegant geometric manner. For each class, the deep features are collapsed into a learned linear subspace, or union of them, and inter-class subspaces are pushed to be as orthogonal as possible. Our proposed Orthogonal Low-rank Embedding (OLÃ) does not require carefully crafting pairs or triplets of samples for training, and works standalone as a classification loss, being the first reported deep metric learning framework of its kind. Because of the improved margin between features of different classes, the resulting deep networks generalize better, are more discriminative, and more robust. We demonstrate improved classification performance in general object recognition, plugging the proposed loss term into existing off-the-shelf architectures. In particular, we show the advantage of the proposed loss in the small data/model scenario, and we significantly advance the state-of-the-art on the Stanford STL-10 benchmark.
ER  -


TY  - Preprint
T1  - Autonomous development and learning in artificial intelligence and robotics: Scaling up deep learning to human--like learning
A1  - Pierre-Yves Oudeyer
JO  - ArXiv e-prints
Y1  - 5 December, 2017
UR  - https://arxiv.org/abs/1712.01626
N2  - Autonomous lifelong development and learning is a fundamental capability of humans, differentiating them from current deep learning systems. However, other branches of artificial intelligence have designed crucial ingredients towards autonomous learning: curiosity and intrinsic motivation, social learning and natural interaction with peers, and embodiment. These mechanisms guide exploration and autonomous choice of goals, and integrating them with deep learning opens stimulating perspectives. Deep learning (DL) approaches made great advances in artificial intelligence, but are still far away from human learning. As argued convincingly by Lake et al., differences include human capabilities to learn causal models of the world from very little data, leveraging compositional representations and priors like intuitive physics and psychology. However, there are other fundamental differences between current DL systems and human learning, as well as technical ingredients to fill this gap, that are either superficially, or not adequately, discussed by Lake et al. These fundamental mechanisms relate to autonomous development and learning. They are bound to play a central role in artificial intelligence in the future. Current DL systems require engineers to manually specify a task-specific objective function for every new task, and learn through off-line processing of large training databases. On the contrary, humans learn autonomously open-ended repertoires of skills, deciding for themselves which goals to pursue or value, and which skills to explore, driven by intrinsic motivation/curiosity and social learning through natural interaction with peers. Such learning processes are incremental, online, and progressive. Human child development involves a progressive increase of complexity in a curriculum of learning where skills are explored, acquired, and built on each other, through particular ordering and timing. Finally, human learning happens in the physical world, and through bodily and physical experimentation, under severe constraints on energy, time, and computational resources. In the two last decades, the field of Developmental and Cognitive Robotics (Cangelosi and Schlesinger, 2015, Asada et al., 2009), in strong interaction with developmental psychology and neuroscience, has achieved significant advances in computational
ER  -


TY  - Preprint
T1  - Deep Learning for automatic sale receipt understanding
A1  - RizlÃ¨ne Raoui-Outach
A1  - CÃ©cile Million-Rousseau
A1  - Alexandre Benoit
A1  - Patrick Lambert
JO  - ArXiv e-prints
Y1  - 5 December, 2017
UR  - https://arxiv.org/abs/1712.01606
N2  - As a general rule, data analytics are now mandatory for companies. Scanned document analysis brings additional challenges introduced by paper damages and scanning quality.In an industrial context, this work focuses on the automatic understanding of sale receipts which enable access to essential and accurate consumption statistics. Given an image acquired with a smart-phone, the proposed work mainly focuses on the first steps of the full tool chain which aims at providing essential information such as the store brand, purchased products and related prices with the highest possible confidence. To get this high confidence level, even if scanning is not perfectly controlled, we propose a double check processing tool-chain using Deep Convolutional Neural Networks (DCNNs) on one hand and more classical image and text processings on another hand.The originality of this work relates in this double check processing and in the joint use of DCNNs for different applications and text analysis.
ER  -


TY  - Preprint
T1  - Deep learning for semantic segmentation of remote sensing images with rich spectral content
A1  - A Hamida
A1  - A. BenoÃ®t
A1  - P. Lambert
A1  - L Klein
A1  - C Amar
A1  - N. Audebert
A1  - S. LefÃ¨vre
JO  - ArXiv e-prints
Y1  - 5 December, 2017
UR  - https://arxiv.org/abs/1712.01600
N2  - With the rapid development of Remote Sensing acquisition techniques, there is a need to scale and improve processing tools to cope with the observed increase of both data volume and richness. Among popular techniques in remote sensing, Deep Learning gains increasing interest but depends on the quality of the training data. Therefore, this paper presents recent Deep Learning approaches for fine or coarse land cover semantic segmentation estimation. Various 2D architectures are tested and a new 3D model is introduced in order to jointly process the spatial and spectral dimensions of the data. Such a set of networks enables the comparison of the different spectral fusion schemes. Besides, we also assess the use of a &#34; noisy ground truth &#34; (i.e. outdated and low spatial resolution labels) for training and testing the networks.
ER  -


TY  - Preprint
T1  - Beyond Grand Theft Auto V for Training, Testing and Enhancing Deep Learning in Self Driving Cars
A1  - Mark Martinez
A1  - Chawin Sitawarin
A1  - Kevin Finch
A1  - Lennart Meincke
A1  - Alex Yablonski
A1  - Alain Kornhauser
JO  - ArXiv e-prints
Y1  - 4 December, 2017
UR  - https://arxiv.org/abs/1712.01397
N2  - As an initial assessment, over 480,000 labeled virtual images of normal highway driving were readily generated in Grand Theft Auto V&#39;s virtual environment. Using these images, a CNN was trained to detect following distance to cars/objects ahead, lane markings, and driving angle (angular heading relative to lane centerline): all variables necessary for basic autonomous driving. Encouraging results were obtained when tested on over 50,000 labeled virtual images from substantially different GTA-V driving environments. This initial assessment begins to define both the range and scope of the labeled images needed for training as well as the range and scope of labeled images needed for testing the definition of boundaries and limitations of trained networks. It is the efficacy and flexibility of a &#34;GTA-V&#34;-like virtual environment that is expected to provide an efficient well-defined foundation for the training and testing of Convolutional Neural Networks for safe driving. Additionally, described is the Princeton Virtual Environment (PVE) for the training, testing and enhancement of safe driving AI, which is being developed using the video-game engine Unity. PVE is being developed to recreate rare but critical corner cases that can be used in re-training and enhancing machine learning models and understanding the limitations of current self driving models. The Florida Tesla crash is being used as an initial reference.
ER  -


TY  - Preprint
T1  - Iterative Deep Learning for Network Topology Extraction
A1  - Carles Ventura
A1  - Jordi Pont-Tuset
A1  - Sergi Caelles
A1  - Kevis-Kokitsi Maninis
A1  - Luc Van Gool
JO  - ArXiv e-prints
Y1  - 4 December, 2017
UR  - https://arxiv.org/abs/1712.01217
N2  - This paper tackles the task of estimating the topology of filamentary networks such as retinal vessels and road networks. Building on top of a global model that performs a dense semantical classification of the pixels of the image, we design a Convolutional Neural Network (CNN) that predicts the local connectivity between the central pixel of an input patch and its border points. By iterating this local connectivity we sweep the whole image and infer the global topology of the filamentary network, inspired by a human delineating a complex network with the tip of their finger.
ER  -


TY  - Preprint
T1  - Deep Learning Can Reverse Photon Migration for Diffuse Optical Tomography
A1  - Jaejun Yoo
A1  - Sohail Sabir
A1  - Duchang Heo
A1  - Kee Hyun Kim
A1  - Abdul Wahab
A1  - Yoonseok Choi
A1  - Seul-I Lee
A1  - Eun Young Chae
A1  - Hak Hee Kim
A1  - Young Min Bae
A1  - Young-wook Choi
A1  - Seungryong Cho
A1  - Jong Chul Ye
JO  - ArXiv e-prints
Y1  - 4 December, 2017
UR  - https://arxiv.org/abs/1712.00912
N2  - Can artificial intelligence (AI) learn complicated non-linear physics? Here we propose a novel deep learning approach that learns non-linear photon scattering physics and obtains accurate 3D distribution of optical anomalies. In contrast to the traditional black-box deep learning approaches to inverse problems, our deep network learns to invert the Lippmann-Schwinger integral equation which describes the essential physics of photon migration of diffuse near-infrared (NIR) photons in turbid media. As an example for clinical relevance, we applied the method to our prototype diffuse optical tomography (DOT). We show that our deep neural network, trained with only simulation data, can accurately recover the location of anomalies within biomimetic phantoms and live animals without the use of an exogenous contrast agent.
ER  -


TY  - Preprint
T1  - A Deep Learning Approach to Drone Monitoring
A1  - Yueru Chen
A1  - Pranav Aggarwal
A1  - Jongmoo Choi
A1  - C. -C. Jay Kuo
JO  - ArXiv e-prints
Y1  - 3 December, 2017
UR  - https://arxiv.org/abs/1712.00863
N2  - A drone monitoring system that integrates deep-learning-based detection and tracking modules is proposed in this work. The biggest challenge in adopting deep learning methods for drone detection is the limited amount of training drone images. To address this issue, we develop a model-based drone augmentation technique that automatically generates drone images with a bounding box label on drone&#39;s location. To track a small flying drone, we utilize the residual information between consecutive image frames. Finally, we present an integrated detection and tracking system that outperforms the performance of each individual module containing detection or tracking only. The experiments show that, even being trained on synthetic data, the proposed system performs well on real world drone images with complex background. The USC drone detection and tracking dataset with user labeled bounding boxes is available to the public.
ER  -


TY  - Preprint
T1  - Fruit recognition from images using deep learning
A1  - Horea MureÅan
A1  - Mihai Oltean
JO  - ArXiv e-prints
Y1  - 14 September, 2018
UR  - https://arxiv.org/abs/1712.00580
N2  - In this paper we introduce a new, high-quality, dataset of images containing fruits. We also present the results of some numerical experiment for training a neural network to detect fruits. We discuss the reason why we chose to use fruits in this project by proposing a few applications that could use this kind of neural network.
ER  -


TY  - Preprint
T1  - Anesthesiologist-level forecasting of hypoxemia with only SpO2 data using deep learning
A1  - Gabriel Erion
A1  - Hugh Chen
A1  - Scott M. Lundberg
A1  - Su-In Lee
JO  - ArXiv e-prints
Y1  - 2 December, 2017
UR  - https://arxiv.org/abs/1712.00563
N2  - We use a deep learning model trained only on a patient&#39;s blood oxygenation data (measurable with an inexpensive fingertip sensor) to predict impending hypoxemia (low blood oxygen) more accurately than trained anesthesiologists with access to all the data recorded in a modern operating room. We also provide a simple way to visualize the reason why a patient&#39;s risk is low or high by assigning weight to the patient&#39;s past blood oxygen values. This work has the potential to provide cutting-edge clinical decision support in low-resource settings, where rates of surgical complication and death are substantially greater than in high-resource areas.
ER  -


TY  - Preprint
T1  - Deep Learning Scaling is Predictable, Empirically
A1  - Joel Hestness
A1  - Sharan Narang
A1  - Newsha Ardalani
A1  - Gregory Diamos
A1  - Heewoo Jun
A1  - Hassan Kianinejad
A1  - Md. Mostofa Ali Patwary
A1  - Yang Yang
A1  - Yanqi Zhou
JO  - ArXiv e-prints
Y1  - 1 December, 2017
UR  - https://arxiv.org/abs/1712.00409
N2  - Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art.
ER  -


TY  - Preprint
T1  - Deep Learning with Permutation-invariant Operator for Multi-instance Histopathology Classification
A1  - Jakub M. Tomczak
A1  - Maximilian Ilse
A1  - Max Welling
JO  - ArXiv e-prints
Y1  - 5 December, 2017
UR  - https://arxiv.org/abs/1712.00310
N2  - The computer-aided analysis of medical scans is a longstanding goal in the medical imaging field. Currently, deep learning has became a dominant methodology for supporting pathologists and radiologist. Deep learning algorithms have been successfully applied to digital pathology and radiology, nevertheless, there are still practical issues that prevent these tools to be widely used in practice. The main obstacles are low number of available cases and large size of images (a.k.a. the small n, large p problem in machine learning), and a very limited access to annotation at a pixel level that can lead to severe overfitting and large computational requirements. We propose to handle these issues by introducing a framework that processes a medical image as a collection of small patches using a single, shared neural network. The final diagnosis is provided by combining scores of individual patches using a permutation-invariant operator (combination). In machine learning community such approach is called a multi-instance learning (MIL).
ER  -


TY  - Preprint
T1  - Learning Deep Representations for Word Spotting Under Weak Supervision
A1  - Neha Gurjar
A1  - Sebastian Sudholt
A1  - Gernot A. Fink
JO  - ArXiv e-prints
Y1  - 26 January, 2018
UR  - https://arxiv.org/abs/1712.00250
N2  - Convolutional Neural Networks have made their mark in various fields of computer vision in recent years. They have achieved state-of-the-art performance in the field of document analysis as well. However, CNNs require a large amount of annotated training data and, hence, great manual effort. In our approach, we introduce a method to drastically reduce the manual annotation effort while retaining the high performance of a CNN for word spotting in handwritten documents. The model is learned with weak supervision using a combination of synthetically generated training data and a small subset of the training partition of the handwritten data set. We show that the network achieves results highly competitive to the state-of-the-art in word spotting with shorter training times and a fraction of the annotation effort.
ER  -


TY  - Preprint
T1  - Deep Learning for Metagenomic Data: using 2D Embeddings and Convolutional Neural Networks
A1  - Thanh Hai Nguyen
A1  - Yann Chevaleyre
A1  - Edi Prifti
A1  - Nataliya Sokolovska
A1  - Jean-Daniel Zucker
JO  - ArXiv e-prints
Y1  - 1 December, 2017
UR  - https://arxiv.org/abs/1712.00244
N2  - Deep learning (DL) techniques have had unprecedented success when applied to images, waveforms, and texts to cite a few. In general, when the sample size (N) is much greater than the number of features (d), DL outperforms previous machine learning (ML) techniques, often through the use of convolution neural networks (CNNs). However, in many bioinformatics ML tasks, we encounter the opposite situation where d is greater than N. In these situations, applying DL techniques (such as feed-forward networks) would lead to severe overfitting. Thus, sparse ML techniques (such as LASSO e.g.) usually yield the best results on these tasks. In this paper, we show how to apply CNNs on data which do not have originally an image structure (in particular on metagenomic data). Our first contribution is to show how to map metagenomic data in a meaningful way to 1D or 2D images. Based on this representation, we then apply a CNN, with the aim of predicting various diseases. The proposed approach is applied on six different datasets including in total over 1000 samples from various diseases. This approach could be a promising one for prediction tasks in the bioinformatics field.
ER  -


TY  - Preprint
T1  - Deep-Reinforcement Learning Multiple Access for Heterogeneous Wireless Networks
A1  - Yiding Yu
A1  - Taotao Wang
A1  - Soung Chang Liew
JO  - ArXiv e-prints
Y1  - 16 July, 2018
UR  - https://arxiv.org/abs/1712.00162
N2  - This paper investigates the use of deep reinforcement learning (DRL) in a MAC protocol for heterogeneous wireless networking referred to as Deep-reinforcement Learning Multiple Access (DLMA). The thrust of this work is partially inspired by the vision of DARPA SC2, a 3-year competition whereby competitors are to come up with a clean-slate design that &#34;best share spectrum with any network(s), in any environment, without prior knowledge, leveraging on machine-learning technique&#34;. Specifically, this paper considers the problem of sharing time slots among a multiple of time-slotted networks that adopt different MAC protocols. One of the MAC protocols is DLMA. The other two are TDMA and ALOHA. The nodes operating DLMA do not know that the other two MAC protocols are TDMA and ALOHA. Yet, by a series of observations of the environment, its own actions, and the resulting rewards, a DLMA node can learn an optimal MAC strategy to coexist harmoniously with the TDMA and ALOHA nodes according to a specified objective (e.g., the objective could be the sum throughput of all networks, or a general alpha-fairness objective).
ER  -


TY  - Preprint
T1  - Comparing Deep Reinforcement Learning and Evolutionary Methods in Continuous Control
A1  - Shangtong Zhang
A1  - Osmar R. Zaiane
JO  - ArXiv e-prints
Y1  - 7 March, 2018
UR  - https://arxiv.org/abs/1712.00006
N2  - Reinforcement Learning and the Evolutionary Strategy are two major approaches in addressing complicated control problems. Both are strong contenders and have their own devotee communities. Both groups have been very active in developing new advances in their own domain and devising, in recent years, leading-edge techniques to address complex continuous control tasks. Here, in the context of Deep Reinforcement Learning, we formulate a parallelized version of the Proximal Policy Optimization method and a Deep Deterministic Policy Gradient method. Moreover, we conduct a thorough comparison between the state-of-the-art techniques in both camps fro continuous control; evolutionary methods and Deep Reinforcement Learning methods. The results show there is no consistent winner.
ER  -


TY  - Preprint
T1  - A Deep Learning Framework for Short-term Power Load Forecasting
A1  - Tinghui Ouyang
A1  - Yusen He
A1  - Huajin Li
A1  - Zhiyu Sun
A1  - Stephen Baek
JO  - ArXiv e-prints
Y1  - 1 December, 2017
UR  - https://arxiv.org/abs/1711.11519
N2  - The scheduling and operation of power system becomes prominently complex and uncertain, especially with the penetration of distributed power. Load forecasting matters to the effective operation of power system. This paper proposes a novel deep learning framework to forecast the short-term grid load. First, the load data is processed by Box-Cox transformation, and two parameters (electricity price and temperature) are investigated. Then, to quantify the tail-dependence of power load on the two parameters, parametric Copula models are fitted and the threshold of peak load are computed. Next, a deep belief network is built to forecast the hourly load of the power grid. One year grid load data collected from an urbanized area in Texas, United States is utilized in the case studies. Short-term load forecasting are examined in four seasons independently. Day-ahead and week-ahead load forecasting experiments are conducted in each season using the proposed framework. The proposed framework is compared with classical neural networks, support vector regression machine, extreme learning machine, and classical deep belief networks. The load forecasting performances are assessed by mean absolute percentage error, root mean square error, and hit rate. Computational results confirm the effectiveness of the proposed data-driven deep learning framework. The prediction accuracies of both day-ahead forecasting and week-ahead forecasting demonstrate that the proposed framework outperforms the tested algorithms.
ER  -


TY  - Preprint
T1  - A Semantic Loss Function for Deep Learning with Symbolic Knowledge
A1  - Jingyi Xu
A1  - Zilu Zhang
A1  - Tal Friedman
A1  - Yitao Liang
A1  - Guy Van den Broeck
JO  - ArXiv e-prints
Y1  - 7 June, 2018
UR  - https://arxiv.org/abs/1711.11157
N2  - This paper develops a novel methodology for using symbolic knowledge in deep learning. From first principles, we derive a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that it effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. Moreover, it significantly increases the ability of the neural network to predict structured objects, such as rankings and paths. These discrete concepts are tremendously difficult to learn, and benefit from a tight integration of deep learning and symbolic reasoning methods.
ER  -


TY  - Preprint
T1  - Deep Learning for identifying radiogenomic associations in breast cancer
A1  - Zhe Zhu
A1  - Ehab Albadawy
A1  - Ashirbani Saha
A1  - Jun Zhang
A1  - Michael R. Harowicz
A1  - Maciej A. Mazurowski
JO  - ArXiv e-prints
Y1  - 29 November, 2017
UR  - https://arxiv.org/abs/1711.11097
N2  - Purpose: To determine whether deep learning models can distinguish between breast cancer molecular subtypes based on dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Materials and methods: In this institutional review board-approved single-center study, we analyzed DCE-MR images of 270 patients at our institution. Lesions of interest were identified by radiologists. The task was to automatically determine whether the tumor is of the Luminal A subtype or of another subtype based on the MR image patches representing the tumor. Three different deep learning approaches were used to classify the tumor according to their molecular subtypes: learning from scratch where only tumor patches were used for training, transfer learning where networks pre-trained on natural images were fine-tuned using tumor patches, and off-the-shelf deep features where the features extracted by neural networks trained on natural images were used for classification with a support vector machine. Network architectures utilized in our experiments were GoogleNet, VGG, and CIFAR. We used 10-fold crossvalidation method for validation and area under the receiver operating characteristic (AUC) as the measure of performance. Results: The best AUC performance for distinguishing molecular subtypes was 0.65 (95% CI:[0.57,0.71]) and was achieved by the off-the-shelf deep features approach. The highest AUC performance for training from scratch was 0.58 (95% CI:[0.51,0.64]) and the best AUC performance for transfer learning was 0.60 (95% CI:[0.52,0.65]) respectively. For the off-the-shelf approach, the features extracted from the fully connected layer performed the best. Conclusion: Deep learning may play a role in discovering radiogenomic associations in breast cancer.
ER  -


TY  - Preprint
T1  - Detection-aided liver lesion segmentation using deep learning
A1  - Miriam Bellver
A1  - Kevis-Kokitsi Maninis
A1  - Jordi Pont-Tuset
A1  - Xavier Giro-i-Nieto
A1  - Jordi Torres
A1  - Luc Van Gool
JO  - ArXiv e-prints
Y1  - 29 November, 2017
UR  - https://arxiv.org/abs/1711.11069
N2  - A fully automatic technique for segmenting the liver and localizing its unhealthy tissues is a convenient tool in order to diagnose hepatic diseases and assess the response to the according treatments. In this work we propose a method to segment the liver and its lesions from Computed Tomography (CT) scans using Convolutional Neural Networks (CNNs), that have proven good results in a variety of computer vision tasks, including medical imaging. The network that segments the lesions consists of a cascaded architecture, which first focuses on the region of the liver in order to segment the lesions on it. Moreover, we train a detector to localize the lesions, and mask the results of the segmentation network with the positive detections. The segmentation architecture is based on DRIU, a Fully Convolutional Network (FCN) with side outputs that work on feature maps of different resolutions, to finally benefit from the multi-scale information learned by different stages of the network. The main contribution of this work is the use of a detector to localize the lesions, which we show to be beneficial to remove false positives triggered by the segmentation network. Source code and models are available at https://imatge-upc.github.io/liverseg-2017-nipsws/ .
ER  -


TY  - Preprint
T1  - Security Risks in Deep Learning Implementations
A1  - Qixue Xiao
A1  - Kang Li
A1  - Deyue Zhang
A1  - Weilin Xu
JO  - ArXiv e-prints
Y1  - 29 November, 2017
UR  - https://arxiv.org/abs/1711.11008
N2  - Advance in deep learning algorithms overshadows their security risk in software implementations. This paper discloses a set of vulnerabilities in popular deep learning frameworks including Caffe, TensorFlow, and Torch. Contrast to the small code size of deep learning models, these deep learning frameworks are complex and contain heavy dependencies on numerous open source packages. This paper considers the risks caused by these vulnerabilities by studying their impact on common deep learning applications such as voice recognition and image classifications. By exploiting these framework implementations, attackers can launch denial-of-service attacks that crash or hang a deep learning application, or control-flow hijacking attacks that cause either system compromise or recognition evasions. The goal of this paper is to draw attention on the software implementations and call for the community effort to improve the security of deep learning frameworks.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for De-Novo Drug Design
A1  - Mariya Popova
A1  - Olexandr Isayev
A1  - Alexander Tropsha
JO  - ArXiv e-prints
Y1  - 31 May, 2018
UR  - https://arxiv.org/abs/1711.10907
N2  - We propose a novel computational strategy for de novo design of molecules with desired properties termed ReLeaSE (Reinforcement Learning for Structural Evolution). Based on deep and reinforcement learning approaches, ReLeaSE integrates two deep neural networks - generative and predictive - that are trained separately but employed jointly to generate novel targeted chemical libraries. ReLeaSE employs simple representation of molecules by their SMILES strings only. Generative models are trained with stack-augmented memory network to produce chemically feasible SMILES strings, and predictive models are derived to forecast the desired properties of the de novo generated compounds. In the first phase of the method, generative and predictive models are trained separately with a supervised learning algorithm. In the second phase, both models are trained jointly with the reinforcement learning approach to bias the generation of new chemical structures towards those with the desired physical and/or biological properties. In the proof-of-concept study, we have employed the ReLeaSE method to design chemical libraries with a bias toward structural complexity or biased toward compounds with either maximal, minimal, or specific range of physical properties such as melting point or hydrophobicity, as well as to develop novel putative inhibitors of JAK2. The approach proposed herein can find a general use for generating targeted chemical libraries of novel compounds optimized for either a single desired property or multiple properties.
ER  -


TY  - Preprint
T1  - Automating Vehicles by Deep Reinforcement Learning using Task Separation with Hill Climbing
A1  - Mogens Graf Plessen
JO  - ArXiv e-prints
Y1  - 2 August, 2018
UR  - https://arxiv.org/abs/1711.10785
N2  - Within the context of autonomous driving a model-based reinforcement learning algorithm is proposed for the design of neural network-parameterized controllers. Classical model-based control methods, which include sampling- and lattice-based algorithms and model predictive control, suffer from the trade-off between model complexity and computational burden required for the online solution of expensive optimization or search problems at every short sampling time. To circumvent this trade-off, a 2-step procedure is motivated: first learning of a controller during offline training based on an arbitrarily complicated mathematical system model, before online fast feedforward evaluation of the trained controller. The contribution of this paper is the proposition of a simple gradient-free and model-based algorithm for deep reinforcement learning using task separation with hill climbing (TSHC). In particular, (i) simultaneous training on separate deterministic tasks with the purpose of encoding many motion primitives in a neural network, and (ii) the employment of maximally sparse rewards in combination with virtual velocity constraints (VVCs) in setpoint proximity are advocated.
ER  -


TY  - Preprint
T1  - End-to-End Optimization of Task-Oriented Dialogue Model with Deep Reinforcement Learning
A1  - Bing Liu
A1  - Gokhan Tur
A1  - Dilek Hakkani-Tur
A1  - Pararth Shah
A1  - Larry Heck
JO  - ArXiv e-prints
Y1  - 30 November, 2017
UR  - https://arxiv.org/abs/1711.10712
N2  - In this paper, we present a neural network based task-oriented dialogue system that can be optimized end-to-end with deep reinforcement learning (RL). The system is able to track dialogue state, interface with knowledge bases, and incorporate query results into agent&#39;s responses to successfully complete task-oriented dialogues. Dialogue policy learning is conducted with a hybrid supervised and deep RL methods. We first train the dialogue agent in a supervised manner by learning directly from task-oriented dialogue corpora, and further optimize it with deep RL during its interaction with users. In the experiments on two different dialogue task domains, our model demonstrates robust performance in tracking dialogue state and producing reasonable system responses. We show that deep RL based optimization leads to significant improvement on task success rate and reduction in dialogue length comparing to supervised training model. We further show benefits of training task-oriented dialogue model end-to-end comparing to component-wise optimization with experiment results on dialogue simulations and human evaluations.
ER  -


TY  - Preprint
T1  - Deep-Person: Learning Discriminative Deep Features for Person Re-Identification
A1  - Xiang Bai
A1  - Mingkun Yang
A1  - Tengteng Huang
A1  - Zhiyong Dou
A1  - Rui Yu
A1  - Yongchao Xu
JO  - ArXiv e-prints
Y1  - 23 July, 2018
UR  - https://arxiv.org/abs/1711.10658
N2  - Recently, many methods of person re-identification (Re-ID) rely on part-based feature representation to learn a discriminative pedestrian descriptor. However, the spatial context between these parts is ignored for the independent extractor to each separate part. In this paper, we propose to apply Long Short-Term Memory (LSTM) in an end-to-end way to model the pedestrian, seen as a sequence of body parts from head to foot. Integrating the contextual information strengthens the discriminative ability of local representation. We also leverage the complementary information between local and global feature. Furthermore, we integrate both identification task and ranking task in one network, where a discriminative embedding and a similarity measurement are learned concurrently. This results in a novel three-branch framework named Deep-Person, which learns highly discriminative features for person Re-ID. Experimental results demonstrate that Deep-Person outperforms the state-of-the-art methods by a large margin on three challenging datasets including Market-1501, CUHK03, and DukeMTMC-reID. Specifically, combining with a re-ranking approach, we achieve a 90.84% mAP on Market-1501 under single query setting.
ER  -


TY  - Preprint
T1  - Deep learning analysis of breast MRIs for prediction of occult invasive disease in ductal carcinoma in situ
A1  - Zhe Zhu
A1  - Michael Harowicz
A1  - Jun Zhang
A1  - Ashirbani Saha
A1  - Lars J. Grimm
A1  - E. Shelley Hwang
A1  - Maciej A. Mazurowski
JO  - ArXiv e-prints
Y1  - 28 November, 2017
UR  - https://arxiv.org/abs/1711.10577
N2  - Purpose: To determine whether deep learning-based algorithms applied to breast MR images can aid in the prediction of occult invasive disease following the di- agnosis of ductal carcinoma in situ (DCIS) by core needle biopsy. Material and Methods: In this institutional review board-approved study, we analyzed dynamic contrast-enhanced fat-saturated T1-weighted MRI sequences of 131 patients at our institution with a core needle biopsy-confirmed diagnosis of DCIS. The patients had no preoperative therapy before breast MRI and no prior history of breast cancer. We explored two different deep learning approaches to predict whether there was a hidden (occult) invasive component in the analyzed tumors that was ultimately detected at surgical excision. In the first approach, we adopted the transfer learning strategy, in which a network pre-trained on a large dataset of natural images is fine-tuned with our DCIS images. Specifically, we used the GoogleNet model pre-trained on the ImageNet dataset. In the second approach, we used a pre-trained network to extract deep features, and a support vector machine (SVM) that utilizes these features to predict the upstaging of the DCIS. We used 10-fold cross validation and the area under the ROC curve (AUC) to estimate the performance of the predictive models. Results: The best classification performance was obtained using the deep features approach with GoogleNet model pre-trained on ImageNet as the feature extractor and a polynomial kernel SVM used as the classifier (AUC = 0.70, 95% CI: 0.58- 0.79). For the transfer learning based approach, the highest AUC obtained was 0.53 (95% CI: 0.41-0.62). Conclusion: Convolutional neural networks could potentially be used to identify occult invasive disease in patients diagnosed with DCIS at the initial core needle biopsy.
ER  -


TY  - Preprint
T1  - Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations
A1  - Maziar Raissi
A1  - Paris Perdikaris
A1  - George Em Karniadakis
JO  - ArXiv e-prints
Y1  - 28 November, 2017
UR  - https://arxiv.org/abs/1711.10566
N2  - We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this second part of our two-part treatise, we focus on the problem of data-driven discovery of partial differential equations. Depending on whether the available data is scattered in space-time or arranged in fixed temporal snapshots, we introduce two main classes of algorithms, namely continuous time and discrete time models. The effectiveness of our approach is demonstrated using a wide range of benchmark problems in mathematical physics, including conservation laws, incompressible fluid flow, and the propagation of nonlinear shallow-water waves.
ER  -


TY  - Preprint
T1  - Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations
A1  - Maziar Raissi
A1  - Paris Perdikaris
A1  - George Em Karniadakis
JO  - ArXiv e-prints
Y1  - 28 November, 2017
UR  - https://arxiv.org/abs/1711.10561
N2  - We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information. In this first part, we demonstrate how these networks can be used to infer solutions to partial differential equations, and obtain physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters.
ER  -


TY  - Preprint
T1  - Deep Lesion Graphs in the Wild: Relationship Learning and Organization of Significant Radiology Image Findings in a Diverse Large-scale Lesion Database
A1  - Ke Yan
A1  - Xiaosong Wang
A1  - Le Lu
A1  - Ling Zhang
A1  - Adam Harrison
A1  - Mohammadhad Bagheri
A1  - Ronald Summers
JO  - ArXiv e-prints
Y1  - 28 July, 2018
UR  - https://arxiv.org/abs/1711.10535
N2  - Radiologists in their daily work routinely find and annotate significant abnormalities on a large number of radiology images. Such abnormalities, or lesions, have collected over years and stored in hospitals&#39; picture archiving and communication systems. However, they are basically unsorted and lack semantic annotations like type and location. In this paper, we aim to organize and explore them by learning a deep feature representation for each lesion. A large-scale and comprehensive dataset, DeepLesion, is introduced for this task. DeepLesion contains bounding boxes and size measurements of over 32K lesions. To model their similarity relationship, we leverage multiple supervision information including types, self-supervised location coordinates and sizes. They require little manual annotation effort but describe useful attributes of the lesions. Then, a triplet network is utilized to learn lesion embeddings with a sequential sampling strategy to depict their hierarchical similarity structure. Experiments show promising qualitative and quantitative results on lesion retrieval, clustering, and classification. The learned embeddings can be further employed to build a lesion graph for various clinically useful applications. We propose algorithms for intra-patient lesion matching and missing annotation mining. Experimental results validate their effectiveness.
ER  -


TY  - Preprint
T1  - Learning from Longitudinal Face Demonstration - Where Tractable Deep Modeling Meets Inverse Reinforcement Learning
A1  - Chi Nhan Duong
A1  - Kha Gia Quach
A1  - Khoa Luu
A1  - T. Hoang Ngan Le
A1  - Marios Savvides
A1  - Tien D. Bui
JO  - ArXiv e-prints
Y1  - 4 September, 2018
UR  - https://arxiv.org/abs/1711.10520
N2  - This paper presents a novel Subject-dependent Deep Aging Path (SDAP), which inherits the merits of both Generative Probabilistic Modeling and Inverse Reinforcement Learning to model the facial structures and the longitudinal face aging process of a given subject. The proposed SDAP is optimized using tractable log-likelihood objective functions with Convolutional Neural Networks (CNNs) based deep feature extraction. Instead of applying a fixed aging development path for all input faces and subjects, SDAP is able to provide the most appropriate aging development path for individual subject that optimizes the reward aging formulation. Unlike previous methods that can take only one image as the input, SDAP further allows multiple images as inputs, i.e. all information of a subject at either the same or different ages, to produce the optimal aging path for the given subject. Finally, SDAP allows efficiently synthesizing in-the-wild aging faces. The proposed model is experimented in both tasks of face aging synthesis and cross-age face verification. The experimental results consistently show SDAP achieves the state-of-the-art performance on numerous face aging databases, i.e. FG-NET, MORPH, AginG Faces in the Wild (AGFW), and Cross-Age Celebrity Dataset (CACD). Furthermore, we also evaluate the performance of SDAP on large-scale Megaface challenge to demonstrate the advantages of the proposed solution.
ER  -


TY  - Preprint
T1  - Parameters Optimization of Deep Learning Models using Particle Swarm Optimization
A1  - Basheer Qolomany
A1  - Majdi Maabreh
A1  - Ala Al-Fuqaha
A1  - Ajay Gupta
A1  - Driss Benhaddou
JO  - ArXiv e-prints
Y1  - 28 November, 2017
UR  - https://arxiv.org/abs/1711.10354
N2  - Deep learning has been successfully applied in several fields such as machine translation, manufacturing, and pattern recognition. However, successful application of deep learning depends upon appropriately setting its parameters to achieve high quality results. The number of hidden layers and the number of neurons in each layer of a deep machine learning network are two key parameters, which have main influence on the performance of the algorithm. Manual parameter setting and grid search approaches somewhat ease the users tasks in setting these important parameters. Nonetheless, these two techniques can be very time consuming. In this paper, we show that the Particle swarm optimization (PSO) technique holds great potential to optimize parameter settings and thus saves valuable computational resources during the tuning process of deep learning models. Specifically, we use a dataset collected from a Wi-Fi campus network to train deep learning models to predict the number of occupants and their locations. Our preliminary experiments indicate that PSO provides an efficient approach for tuning the optimal number of hidden layers and the number of neurons in each layer of the deep learning algorithm when compared to the grid search method. Our experiments illustrate that the exploration process of the landscape of configurations to find the optimal parameters is decreased by 77%-85%. In fact, the PSO yields even better accuracy results.
ER  -


TY  - Preprint
T1  - Providing theoretical learning guarantees to Deep Learning Networks
A1  - Rodrigo Fernandes de Mello
A1  - Martha Dais Ferreira
A1  - Moacir Antonelli Ponti
JO  - ArXiv e-prints
Y1  - 28 November, 2017
UR  - https://arxiv.org/abs/1711.10292
N2  - Deep Learning (DL) is one of the most common subjects when Machine Learning and Data Science approaches are considered. There are clearly two movements related to DL: the first aggregates researchers in quest to outperform other algorithms from literature, trying to win contests by considering often small decreases in the empirical risk; and the second investigates overfitting evidences, questioning the learning capabilities of DL classifiers. Motivated by such opposed points of view, this paper employs the Statistical Learning Theory (SLT) to study the convergence of Deep Neural Networks, with particular interest in Convolutional Neural Networks. In order to draw theoretical conclusions, we propose an approach to estimate the Shattering coefficient of those classification algorithms, providing a lower bound for the complexity of their space of admissible functions, a.k.a. algorithm bias. Based on such estimator, we generalize the complexity of network biases, and, next, we study AlexNet and VGG16 architectures in the point of view of their Shattering coefficients, and number of training examples required to provide theoretical learning guarantees. From our theoretical formulation, we show the conditions which Deep Neural Networks learn as well as point out another issue: DL benchmarks may be strictly driven by empirical risks, disregarding the complexity of algorithms biases.
ER  -


TY  - Preprint
T1  - Learning from Between-class Examples for Deep Sound Recognition
A1  - Yuji Tokozume
A1  - Yoshitaka Ushiku
A1  - Tatsuya Harada
JO  - ArXiv e-prints
Y1  - 28 February, 2018
UR  - https://arxiv.org/abs/1711.10282
N2  - Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher&#39;s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.
ER  -


TY  - Preprint
T1  - Homomorphic Parameter Compression for Distributed Deep Learning Training
A1  - Jaehee Jang
A1  - Byungook Na
A1  - Sungroh Yoon
JO  - ArXiv e-prints
Y1  - 27 November, 2017
UR  - https://arxiv.org/abs/1711.10123
N2  - Distributed training of deep neural networks has received significant research interest, and its major approaches include implementations on multiple GPUs and clusters. Parallelization can dramatically improve the efficiency of training deep and complicated models with large-scale data. A fundamental barrier against the speedup of DNN training, however, is the trade-off between computation and communication time. In other words, increasing the number of worker nodes decreases the time consumed in computation while simultaneously increasing communication overhead under constrained network bandwidth, especially in commodity hardware environments. To alleviate this trade-off, we suggest the idea of homomorphic parameter compression, which compresses parameters with the least expense and trains the DNN with the compressed representation. Although the specific method is yet to be discovered, we demonstrate that there is a high probability that the homomorphism can reduce the communication overhead, thanks to little compression and decompression times. We also provide theoretical speedup of homomorphic compression.
ER  -


TY  - Preprint
T1  - DeepChess: End-to-End Deep Neural Network for Automatic Learning in Chess
A1  - Eli David
A1  - Nathan S. Netanyahu
A1  - Lior Wolf
JO  - ArXiv e-prints
Y1  - 27 November, 2017
UR  - https://arxiv.org/abs/1711.09667
N2  - We present an end-to-end learning method for chess, relying on deep neural networks. Without any a priori knowledge, in particular without any knowledge regarding the rules of chess, a deep neural network is trained using a combination of unsupervised pretraining and supervised training. The unsupervised training extracts high level features from a given position, and the supervised training learns to compare two chess positions and select the more favorable one. The training relies entirely on datasets of several million chess games, and no further domain specific knowledge is incorporated.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Sepsis Treatment
A1  - Aniruddh Raghu
A1  - Matthieu Komorowski
A1  - Imran Ahmed
A1  - Leo Celi
A1  - Peter Szolovits
A1  - Marzyeh Ghassemi
JO  - ArXiv e-prints
Y1  - 27 November, 2017
UR  - https://arxiv.org/abs/1711.09602
N2  - Sepsis is a leading cause of mortality in intensive care units and costs hospitals billions annually. Treating a septic patient is highly challenging, because individual patients respond very differently to medical interventions and there is no universally agreed-upon treatment for sepsis. In this work, we propose an approach to deduce treatment policies for septic patients by using continuous state-space models and deep reinforcement learning. Our model learns clinically interpretable treatment policies, similar in important aspects to the treatment policies of physicians. The learned policies could be used to aid intensive care clinicians in medical decision making and improve the likelihood of patient survival.
ER  -


TY  - Preprint
T1  - A Big Data Analysis Framework Using Apache Spark and Deep Learning
A1  - Anand Gupta
A1  - Hardeo Thakur
A1  - Ritvik Shrivastava
A1  - Pulkit Kumar
A1  - Sreyashi Nag
JO  - ArXiv e-prints
Y1  - 25 November, 2017
UR  - https://arxiv.org/abs/1711.09279
N2  - With the spreading prevalence of Big Data, many advances have recently been made in this field. Frameworks such as Apache Hadoop and Apache Spark have gained a lot of traction over the past decades and have become massively popular, especially in industries. It is becoming increasingly evident that effective big data analysis is key to solving artificial intelligence problems. Thus, a multi-algorithm library was implemented in the Spark framework, called MLlib. While this library supports multiple machine learning algorithms, there is still scope to use the Spark setup efficiently for highly time-intensive and computationally expensive procedures like deep learning. In this paper, we propose a novel framework that combines the distributive computational abilities of Apache Spark and the advanced machine learning architecture of a deep multi-layer perceptron (MLP), using the popular concept of Cascade Learning. We conduct empirical analysis of our framework on two real world datasets. The results are encouraging and corroborate our proposed framework, in turn proving that it is an improvement over traditional big data analysis methods that use either Spark or Deep learning as individual elements.
ER  -


TY  - Preprint
T1  - Malaria Likelihood Prediction By Effectively Surveying Households Using Deep Reinforcement Learning
A1  - Pranav Rajpurkar
A1  - Vinaya Polamreddi
A1  - Anusha Balakrishnan
JO  - ArXiv e-prints
Y1  - 25 November, 2017
UR  - https://arxiv.org/abs/1711.09223
N2  - We build a deep reinforcement learning (RL) agent that can predict the likelihood of an individual testing positive for malaria by asking questions about their household. The RL agent learns to determine which survey question to ask next and when to stop to make a prediction about their likelihood of malaria based on their responses hitherto. The agent incurs a small penalty for each question asked, and a large reward/penalty for making the correct/wrong prediction; it thus has to learn to balance the length of the survey with the accuracy of its final predictions. Our RL agent is a Deep Q-network that learns a policy directly from the responses to the questions, with an action defined for each possible survey question and for each possible prediction class. We focus on Kenya, where malaria is a massive health burden, and train the RL agent on a dataset of 6481 households from the Kenya Malaria Indicator Survey 2015. To investigate the importance of having survey questions be adaptive to responses, we compare our RL agent to a supervised learning (SL) baseline that fixes its set of survey questions a priori. We evaluate on prediction accuracy and on the number of survey questions asked on a holdout set and find that the RL agent is able to predict with 80% accuracy, using only 2.5 questions on average. In addition, the RL agent learns to survey adaptively to responses and is able to match the SL baseline in prediction accuracy while significantly reducing survey length.
ER  -


TY  - Preprint
T1  - Micro-Doppler Based Human-Robot Classification Using Ensemble and Deep Learning Approaches
A1  - Sherif Abdulatif
A1  - Qian Wei
A1  - Fady Aziz
A1  - Bernhard Kleiner
A1  - Urs Schneider
JO  - ArXiv e-prints
Y1  - 26 February, 2018
UR  - https://arxiv.org/abs/1711.09177
N2  - Radar sensors can be used for analyzing the induced frequency shifts due to micro-motions in both range and velocity dimensions identified as micro-Doppler ($\boldsymbolÎ¼$-D) and micro-Range ($\boldsymbolÎ¼$-R), respectively. Different moving targets will have unique $\boldsymbolÎ¼$-D and $\boldsymbolÎ¼$-R signatures that can be used for target classification. Such classification can be used in numerous fields, such as gait recognition, safety and surveillance. In this paper, a 25 GHz FMCW Single-Input Single-Output (SISO) radar is used in industrial safety for real-time human-robot identification. Due to the real-time constraint, joint Range-Doppler (R-D) maps are directly analyzed for our classification problem. Furthermore, a comparison between the conventional classical learning approaches with handcrafted extracted features, ensemble classifiers and deep learning approaches is presented. For ensemble classifiers, restructured range and velocity profiles are passed directly to ensemble trees, such as gradient boosting and random forest without feature extraction. Finally, a Deep Convolutional Neural Network (DCNN) is used and raw R-D images are directly fed into the constructed network. DCNN shows a superior performance of 99\% accuracy in identifying humans from robots on a single R-D map.
ER  -


TY  - Preprint
T1  - An Exploration of Word Embedding Initialization in Deep-Learning Tasks
A1  - Tom Kocmi
A1  - OndÅej Bojar
JO  - ArXiv e-prints
Y1  - 24 November, 2017
UR  - https://arxiv.org/abs/1711.09160
N2  - Word embeddings are the interface between the world of discrete units of text processing and the continuous, differentiable world of neural networks. In this work, we examine various random and pretrained initialization methods for embeddings used in deep networks and their effect on the performance on four NLP tasks with both recurrent and convolutional architectures. We confirm that pretrained embeddings are a little better than random initialization, especially considering the speed of learning. On the other hand, we do not see any significant difference between various methods of random initialization, as long as the variance is kept reasonably low. High-variance initialization prevents the network to use the space of embeddings and forces it to use other free parameters to accomplish the task. We support this hypothesis by observing the performance in learning lexical relations and by the fact that the network can learn to perform reasonably in its task even with fixed random embeddings.
ER  -


TY  - Preprint
T1  - Deep Cross-Modal Correlation Learning for Audio and Lyrics in Music Retrieval
A1  - Yi Yu
A1  - Suhua Tang
A1  - Francisco Raposo
A1  - Lei Chen
JO  - ArXiv e-prints
Y1  - 28 November, 2017
UR  - https://arxiv.org/abs/1711.08976
N2  - Little research focuses on cross-modal correlation learning where temporal structures of different data modalities such as audio and lyrics are taken into account. Stemming from the characteristic of temporal structures of music in nature, we are motivated to learn the deep sequential correlation between audio and lyrics. In this work, we propose a deep cross-modal correlation learning architecture involving two-branch deep neural networks for audio modality and text modality (lyrics). Different modality data are converted to the same canonical space where inter modal canonical correlation analysis is utilized as an objective function to calculate the similarity of temporal structures. This is the first study on understanding the correlation between language and music audio through deep architectures for learning the paired temporal correlation of audio and lyrics. Pre-trained Doc2vec model followed by fully-connected layers (fully-connected deep neural network) is used to represent lyrics. Two significant contributions are made in the audio branch, as follows: i) pre-trained CNN followed by fully-connected layers is investigated for representing music audio. ii) We further suggest an end-to-end architecture that simultaneously trains convolutional layers and fully-connected layers to better learn temporal structures of music audio. Particularly, our end-to-end deep architecture contains two properties: simultaneously implementing feature learning and cross-modal correlation learning, and learning joint representation by considering temporal structures. Experimental results, using audio to retrieve lyrics or using lyrics to retrieve audio, verify the effectiveness of the proposed deep correlation learning architectures in cross-modal music retrieval.
ER  -


TY  - Preprint
T1  - Action Branching Architectures for Deep Reinforcement Learning
A1  - Arash Tavakoli
A1  - Fabio Pardo
A1  - Petar Kormushev
JO  - ArXiv e-prints
Y1  - 24 November, 2017
UR  - https://arxiv.org/abs/1711.08946
N2  - Discrete-action algorithms have been central to numerous recent successes of deep reinforcement learning. However, applying these algorithms to high-dimensional action tasks requires tackling the combinatorial increase of the number of possible actions with the number of action dimensions. This problem is further exacerbated for continuous-action tasks that require fine control of actions via discretization. In this paper, we propose a novel neural architecture featuring a shared decision module followed by several network branches, one for each action dimension. This approach achieves a linear increase of the number of network outputs with the number of degrees of freedom by allowing a level of independence for each individual action dimension. To illustrate the approach, we present a novel agent, called Branching Dueling Q-Network (BDQ), as a branching variant of the Dueling Double Deep Q-Network (Dueling DDQN). We evaluate the performance of our agent on a set of challenging continuous control tasks. The empirical results show that the proposed agent scales gracefully to environments with increasing action dimensionality and indicate the significance of the shared decision module in coordination of the distributed action branches. Furthermore, we show that the proposed agent performs competitively against a state-of-the-art continuous control algorithm, Deep Deterministic Policy Gradient (DDPG).
ER  -


TY  - Preprint
T1  - SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels
A1  - Matthias Fey
A1  - Jan Eric Lenssen
A1  - Frank Weichert
A1  - Heinrich MÃ¼ller
JO  - ArXiv e-prints
Y1  - 23 May, 2018
UR  - https://arxiv.org/abs/1711.08920
N2  - We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured and geometric input, e.g., graphs or meshes. Our main contribution is a novel convolution operator based on B-splines, that makes the computation time independent from the kernel size due to the local support property of the B-spline basis functions. As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights. In contrast to related approaches that filter in the spectral domain, the proposed method aggregates features purely in the spatial domain. In addition, SplineCNN allows entire end-to-end training of deep architectures, using only the geometric structure as input, instead of handcrafted feature descriptors. For validation, we apply our method on tasks from the fields of image graph classification, shape correspondence and graph node classification, and show that it outperforms or pars state-of-the-art approaches while being significantly faster and having favorable properties like domain-independence.
ER  -


TY  - Preprint
T1  - Deep learning analysis of the myocardium in coronary CT angiography for identification of patients with functionally significant coronary artery stenosis
A1  - Majd Zreik
A1  - Nikolas Lessmann
A1  - Robbert W. van Hamersvelt
A1  - Jelmer M. Wolterink
A1  - Michiel Voskuil
A1  - Max A. Viergever
A1  - Tim Leiner
A1  - Ivana IÅ¡gum
JO  - ArXiv e-prints
Y1  - 6 December, 2017
UR  - https://arxiv.org/abs/1711.08917
N2  - In patients with coronary artery stenoses of intermediate severity, the functional significance needs to be determined. Fractional flow reserve (FFR) measurement, performed during invasive coronary angiography (ICA), is most often used in clinical practice. To reduce the number of ICA procedures, we present a method for automatic identification of patients with functionally significant coronary artery stenoses, employing deep learning analysis of the left ventricle (LV) myocardium in rest coronary CT angiography (CCTA). The study includes consecutively acquired CCTA scans of 166 patients with FFR measurements. To identify patients with a functionally significant coronary artery stenosis, analysis is performed in several stages. First, the LV myocardium is segmented using a multiscale convolutional neural network (CNN). To characterize the segmented LV myocardium, it is subsequently encoded using unsupervised convolutional autoencoder (CAE). Thereafter, patients are classified according to the presence of functionally significant stenosis using an SVM classifier based on the extracted and clustered encodings. Quantitative evaluation of LV myocardium segmentation in 20 images resulted in an average Dice coefficient of 0.91 and an average mean absolute distance between the segmented and reference LV boundaries of 0.7 mm. Classification of patients was evaluated in the remaining 126 CCTA scans in 50 10-fold cross-validation experiments and resulted in an area under the receiver operating characteristic curve of 0.74 +- 0.02. At sensitivity levels 0.60, 0.70 and 0.80, the corresponding specificity was 0.77, 0.71 and 0.59, respectively. The results demonstrate that automatic analysis of the LV myocardium in a single CCTA scan acquired at rest, without assessment of the anatomy of the coronary arteries, can be used to identify patients with functionally significant coronary artery stenosis.
ER  -


TY  - Preprint
T1  - Critical Learning Periods in Deep Neural Networks
A1  - Alessandro Achille
A1  - Matteo Rovere
A1  - Stefano Soatto
JO  - ArXiv e-prints
Y1  - 15 May, 2018
UR  - https://arxiv.org/abs/1711.08856
N2  - Critical periods are phases in the early development of humans and animals during which experience can irreversibly affect the architecture of neuronal networks. In this work, we study the effects of visual stimulus deficits on the training of artificial neural networks (ANNs). Introducing well-characterized visual deficits, such as cataract-like blurring, in the early training phase of a standard deep neural network causes a permanent performance loss that closely mimics critical period behavior in humans and animal models. Deficits that do not affect low-level image statistics, such as vertical flipping of the images, have no lasting effect on the ANNs&#39; performance and can be rapidly overcome with further training. In addition, the deeper the ANN is, the more pronounced the critical period. To better understand this phenomenon, we use Fisher Information as a measure of the strength of the network&#39;s connections during the training. Our information-theoretic analysis suggests that the first few epochs are critical for the creation of strong connections across different layers, optimal for processing the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial rapid learning phase of ANN training, under-scrutinized compared to its asymptotic behavior, plays a key role in defining the final performance of networks. Our results also show how critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.
ER  -


TY  - Preprint
T1  - Deep Learning for Real-Time Crime Forecasting and its Ternarization
A1  - Bao Wang
A1  - Penghang Yin
A1  - Andrea L. Bertozzi
A1  - P. Jeffrey Brantingham
A1  - Stanley J. Osher
A1  - Jack Xin
JO  - ArXiv e-prints
Y1  - 23 November, 2017
UR  - https://arxiv.org/abs/1711.08833
N2  - Real-time crime forecasting is important. However, accurate prediction of when and where the next crime will happen is difficult. No known physical model provides a reasonable approximation to such a complex system. Historical crime data are sparse in both space and time and the signal of interests is weak. In this work, we first present a proper representation of crime data. We then adapt the spatial temporal residual network on the well represented data to predict the distribution of crime in Los Angeles at the scale of hours in neighborhood-sized parcels. These experiments as well as comparisons with several existing approaches to prediction demonstrate the superiority of the proposed model in terms of accuracy. Finally, we present a ternarization technique to address the resource consumption issue for its deployment in real world. This work is an extension of our short conference proceeding paper [Wang et al, Arxiv 1707.03340].
ER  -


TY  - Preprint
T1  - Learning Deep Representations of Medical Images using Siamese CNNs with Application to Content-Based Image Retrieval
A1  - Yu-An Chung
A1  - Wei-Hung Weng
JO  - ArXiv e-prints
Y1  - 27 December, 2017
UR  - https://arxiv.org/abs/1711.08490
N2  - Deep neural networks have been investigated in learning latent representations of medical images, yet most of the studies limit their approach in a single supervised convolutional neural network (CNN), which usually rely heavily on a large scale annotated dataset for training. To learn image representations with less supervision involved, we propose a deep Siamese CNN (SCNN) architecture that can be trained with only binary image pair information. We evaluated the learned image representations on a task of content-based medical image retrieval using a publicly available multiclass diabetic retinopathy fundus image dataset. The experimental results show that our proposed deep SCNN is comparable to the state-of-the-art single supervised CNN, and requires much less supervision for training.
ER  -


TY  - Preprint
T1  - RGB-D-based Human Motion Recognition with Deep Learning: A Survey
A1  - Pichao Wang
A1  - Wanqing Li
A1  - Philip Ogunbona
A1  - Jun Wan
A1  - Sergio Escalera
JO  - ArXiv e-prints
Y1  - 24 April, 2018
UR  - https://arxiv.org/abs/1711.08362
N2  - Human motion recognition is one of the most important branches of human-centered research activities. In recent years, motion recognition based on RGB-D data has attracted much attention. Along with the development in artificial intelligence, deep learning techniques have gained remarkable success in computer vision. In particular, convolutional neural networks (CNN) have achieved great success for image-based tasks, and recurrent neural networks (RNN) are renowned for sequence-based problems. Specifically, deep learning methods based on the CNN and RNN architectures have been adopted for motion recognition using RGB-D data. In this paper, a detailed overview of recent advances in RGB-D-based motion recognition is presented. The reviewed methods are broadly categorized into four groups, depending on the modality adopted for recognition: RGB-based, depth-based, skeleton-based and RGB+D-based. As a survey focused on the application of deep learning to RGB-D-based motion recognition, we explicitly discuss the advantages and limitations of existing techniques. Particularly, we highlighted the methods of encoding spatial-temporal-structural information inherent in video sequence, and discuss potential directions for future research.
ER  -


TY  - Preprint
T1  - DeepSign: Deep Learning for Automatic Malware Signature Generation and Classification
A1  - Eli David
A1  - Nathan S. Netanyahu
JO  - ArXiv e-prints
Y1  - 23 November, 2017
UR  - https://arxiv.org/abs/1711.08336
N2  - This paper presents a novel deep learning based method for automatic malware signature generation and classification. The method uses a deep belief network (DBN), implemented with a deep stack of denoising autoencoders, generating an invariant compact representation of the malware behavior. While conventional signature and token based methods for malware detection do not detect a majority of new variants for existing malware, the results presented in this paper show that signatures generated by the DBN allow for an accurate classification of new malware variants. Using a dataset containing hundreds of variants for several major malware families, our method achieves 98.6% classification accuracy using the signatures generated by the DBN. The presented method is completely agnostic to the type of malware behavior that is logged (e.g., API calls and their parameters, registry entries, websites and ports accessed, etc.), and can use any raw input from a sandbox to successfully train the deep neural network which is used to generate malware signatures.
ER  -


TY  - Preprint
T1  - Finding Algebraic Structure of Care in Time: A Deep Learning Approach
A1  - Phuoc Nguyen
A1  - Truyen Tran
A1  - Svetha Venkatesh
JO  - ArXiv e-prints
Y1  - 20 November, 2017
UR  - https://arxiv.org/abs/1711.07980
N2  - Understanding the latent processes from Electronic Medical Records could be a game changer in modern healthcare. However, the processes are complex due to the interaction between at least three dynamic components: the illness, the care and the recording practice. Existing methods are inadequate in capturing the dynamic structure of care. We propose an end-to-end model that reads medical record and predicts future risk. The model adopts the algebraic view in that discrete medical objects are embedded into continuous vectors lying in the same space. The bag of disease and comorbidities recorded at each hospital visit are modeled as function of sets. The same holds for the bag of treatments. The interaction between diseases and treatments at a visit is modeled as the residual of the diseases minus the treatments. Finally, the health trajectory, which is a sequence of visits, is modeled using a recurrent neural network. We report preliminary results on chronic diseases - diabetes and mental health - for predicting unplanned readmission.
ER  -


TY  - Preprint
T1  - Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge
A1  - Emmanuel de Bezenac
A1  - Arthur Pajot
A1  - Patrick Gallinari
JO  - ArXiv e-prints
Y1  - 9 January, 2018
UR  - https://arxiv.org/abs/1711.07970
N2  - We consider the use of Deep Learning methods for modeling complex phenomena like those occurring in natural physical processes. With the large amount of data gathered on these phenomena the data intensive paradigm could begin to challenge more traditional approaches elaborated over the years in fields like maths or physics. However, despite considerable successes in a variety of application domains, the machine learning field is not yet ready to handle the level of complexity required by such problems. Using an example application, namely Sea Surface Temperature Prediction, we show how general background knowledge gained from physics could be used as a guideline for designing efficient Deep Learning models. In order to motivate the approach and to assess its generality we demonstrate a formal link between the solution of a class of differential equations underlying a large family of physical phenomena and the proposed model. Experiments and comparison with series of baselines including a state of the art numerical approach is then provided.
ER  -


TY  - Preprint
T1  - Hidden Tree Markov Networks: Deep and Wide Learning for Structured Data
A1  - Davide Bacciu
JO  - ArXiv e-prints
Y1  - 21 November, 2017
UR  - https://arxiv.org/abs/1711.07784
N2  - The paper introduces the Hidden Tree Markov Network (HTN), a neuro-probabilistic hybrid fusing the representation power of generative models for trees with the incremental and discriminative learning capabilities of neural networks. We put forward a modular architecture in which multiple generative models of limited complexity are trained to learn structural feature detectors whose outputs are then combined and integrated by neural layers at a later stage. In this respect, the model is both deep, thanks to the unfolding of the generative models on the input structures, as well as wide, given the potentially large number of generative modules that can be trained in parallel. Experimental results show that the proposed approach can outperform state-of-the-art syntactic kernels as well as generative kernels built on the same probabilistic model as the HTN.
ER  -


TY  - Preprint
T1  - Understanding Deep Learning Generalization by Maximum Entropy
A1  - Guanhua Zheng
A1  - Jitao Sang
A1  - Changsheng Xu
JO  - ArXiv e-prints
Y1  - 21 November, 2017
UR  - https://arxiv.org/abs/1711.07758
N2  - Deep learning achieves remarkable generalization capability with overwhelming number of model parameters. Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored. This paper attempts to provide an alternative understanding from the perspective of maximum entropy. We first derive two feature conditions that softmax regression strictly apply maximum entropy principle. DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle. The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development.
ER  -


TY  - Preprint
T1  - A deep learning-based method for relative location prediction in CT scan images
A1  - Jiajia Guo
A1  - Hongwei Du
A1  - Bensheng Qiu
A1  - Xiao Liang
JO  - ArXiv e-prints
Y1  - 20 November, 2017
UR  - https://arxiv.org/abs/1711.07624
N2  - Relative location prediction in computed tomography (CT) scan images is a challenging problem. In this paper, a regression model based on one-dimensional convolutional neural networks is proposed to determine the relative location of a CT scan image both robustly and precisely. A public dataset is employed to validate the performance of the study&#39;s proposed method using a 5-fold cross validation. Experimental results demonstrate an excellent performance of the proposed model when compared with the state-of-the-art techniques, achieving a median absolute error of 1.04 cm and mean absolute error of 1.69 cm.
ER  -


TY  - Preprint
T1  - Teaching a Machine to Read Maps with Deep Reinforcement Learning
A1  - Gino Brunner
A1  - Oliver Richter
A1  - Yuyi Wang
A1  - Roger Wattenhofer
JO  - ArXiv e-prints
Y1  - 20 November, 2017
UR  - https://arxiv.org/abs/1711.07479
N2  - The ability to use a 2D map to navigate a complex 3D environment is quite remarkable, and even difficult for many humans. Localization and navigation is also an important problem in domains such as robotics, and has recently become a focus of the deep reinforcement learning community. In this paper we teach a reinforcement learning agent to read a map in order to find the shortest way out of a random maze it has never seen before. Our system combines several state-of-the-art methods such as A3C and incorporates novel elements such as a recurrent localization cell. Our agent learns to localize itself based on 3D first person images and an approximate orientation angle. The agent generalizes well to bigger mazes, showing that it learned useful localization and navigation capabilities.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Multi-Resource Multi-Machine Job Scheduling
A1  - Weijia Chen
A1  - Yuedong Xu
A1  - Xiaofeng Wu
JO  - ArXiv e-prints
Y1  - 20 November, 2017
UR  - https://arxiv.org/abs/1711.07440
N2  - Minimizing job scheduling time is a fundamental issue in data center networks that has been extensively studied in recent years. The incoming jobs require different CPU and memory units, and span different number of time slots. The traditional solution is to design efficient heuristic algorithms with performance guarantee under certain assumptions. In this paper, we improve a recently proposed job scheduling algorithm using deep reinforcement learning and extend it to multiple server clusters. Our study reveals that deep reinforcement learning method has the potential to outperform traditional resource allocation algorithms in a variety of complicated environments.
ER  -


TY  - Preprint
T1  - Memory Based Online Learning of Deep Representations from Video Streams
A1  - Federico Pernici
A1  - Federico Bartoli
A1  - Matteo Bruni
A1  - Alberto Del Bimbo
JO  - ArXiv e-prints
Y1  - 17 November, 2017
UR  - https://arxiv.org/abs/1711.07368
N2  - We present a novel online unsupervised method for face identity learning from video streams. The method exploits deep face descriptors together with a memory based learning mechanism that takes advantage of the temporal coherence of visual data. Specifically, we introduce a discriminative feature matching solution based on Reverse Nearest Neighbour and a feature forgetting strategy that detect redundant features and discard them appropriately while time progresses. It is shown that the proposed learning procedure is asymptotically stable and can be effectively used in relevant applications like multiple face identification and tracking from unconstrained video streams. Experimental results show that the proposed method achieves comparable results in the task of multiple face tracking and better performance in face identification with offline approaches exploiting future information. Code will be publicly available.
ER  -


TY  - Preprint
T1  - Classification with Costly Features using Deep Reinforcement Learning
A1  - JaromÃ­r Janisch
A1  - TomÃ¡Å¡ PevnÃ½
A1  - Viliam LisÃ½
JO  - ArXiv e-prints
Y1  - 20 November, 2017
UR  - https://arxiv.org/abs/1711.07364
N2  - We study a classification problem where each feature can be acquired for a cost and the goal is to optimize the trade-off between classification precision and the total feature cost. We frame the problem as a sequential decision-making problem, where we classify one sample in each episode. At each step, an agent can use values of acquired features to decide whether to purchase another one or whether to classify the sample. We use vanilla Double Deep Q-learning, a standard reinforcement learning technique, to find a classification policy. We show that this generic approach outperforms Adapt-Gbrt, currently the best-performing algorithm developed specifically for classification with costly features.
ER  -


TY  - Preprint
T1  - Detection of Tooth caries in Bitewing Radiographs using Deep Learning
A1  - Muktabh Mayank Srivastava
A1  - Pratyush Kumar
A1  - Lalit Pradhan
A1  - Srikrishna Varadarajan
JO  - ArXiv e-prints
Y1  - 23 November, 2017
UR  - https://arxiv.org/abs/1711.07312
N2  - We develop a Computer Aided Diagnosis (CAD) system, which enhances the performance of dentists in detecting wide range of dental caries. The CAD System achieves this by acting as a second opinion for the dentists with way higher sensitivity on the task of detecting cavities than the dentists themselves. We develop annotated dataset of more than 3000 bitewing radiographs and utilize it for developing a system for automated diagnosis of dental caries. Our system consists of a deep fully convolutional neural network (FCNN) consisting 100+ layers, which is trained to mark caries on bitewing radiographs. We have compared the performance of our proposed system with three certified dentists for marking dental caries. We exceed the average performance of the dentists in both recall (sensitivity) and F1-Score (agreement with truth) by a very large margin. Working example of our system is shown in Figure 1.
ER  -


TY  - Preprint
T1  - MIT Autonomous Vehicle Technology Study: Large-Scale Deep Learning Based Analysis of Driver Behavior and Interaction with Automation
A1  - Lex Fridman
A1  - Daniel E. Brown
A1  - Michael Glazer
A1  - William Angell
A1  - Spencer Dodd
A1  - Benedikt Jenik
A1  - Jack Terwilliger
A1  - Julia Kindelsberger
A1  - Li Ding
A1  - Sean Seaman
A1  - Hillary Abraham
A1  - Alea Mehler
A1  - Andrew Sipperley
A1  - Anthony Pettinato
A1  - Bobbie Seppelt
A1  - Linda Angell
A1  - Bruce Mehler
A1  - Bryan Reimer
JO  - ArXiv e-prints
Y1  - 30 September, 2018
UR  - https://arxiv.org/abs/1711.06976
N2  - For the foreseeble future, human beings will likely remain an integral part of the driving task, monitoring the AI system as it performs anywhere from just over 0% to just under 100% of the driving. The governing objectives of the MIT Autonomous Vehicle Technology (MIT-AVT) study are to (1) undertake large-scale real-world driving data collection that includes high-definition video to fuel the development of deep learning based internal and external perception systems, (2) gain a holistic understanding of how human beings interact with vehicle automation technology by integrating video data with vehicle state data, driver characteristics, mental models, and self-reported experiences with technology, and (3) identify how technology and other factors related to automation adoption and use can be improved in ways that save lives. In pursuing these objectives, we have instrumented 21 Tesla Model S and Model X vehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac CT6 vehicles for both long-term (over a year per driver) and medium term (one month per driver) naturalistic driving data collection. Furthermore, we are continually developing new methods for analysis of the massive-scale dataset collected from the instrumented vehicle fleet. The recorded data streams include IMU, GPS, CAN messages, and high-definition video streams of the driver face, the driver cabin, the forward roadway, and the instrument cluster (on select vehicles). The study is on-going and growing. To date, we have 99 participants, 11,846 days of participation, 405,807 miles, and 5.5 billion video frames. This paper presents the design of the study, the data collection hardware, the processing of the data, and the computer vision algorithms currently being used to extract actionable knowledge from the data.
ER  -


TY  - Preprint
T1  - DLTK: State of the Art Reference Implementations for Deep Learning on Medical Images
A1  - Nick Pawlowski
A1  - Sofia Ira Ktena
A1  - Matthew C. H. Lee
A1  - Bernhard Kainz
A1  - Daniel Rueckert
A1  - Ben Glocker
A1  - Martin Rajchl
JO  - ArXiv e-prints
Y1  - 18 November, 2017
UR  - https://arxiv.org/abs/1711.06853
N2  - We present DLTK, a toolkit providing baseline implementations for efficient experimentation with deep learning methods on biomedical images. It builds on top of TensorFlow and its high modularity and easy-to-use examples allow for a low-threshold access to state-of-the-art implementations for typical medical imaging problems. A comparison of DLTK&#39;s reference implementations of popular network architectures for image segmentation demonstrates new top performance on the publicly available challenge data &#34;Multi-Atlas Labeling Beyond the Cranial Vault&#34;. The average test Dice similarity coefficient of $81.5$ exceeds the previously best performing CNN ($75.7$) and the accuracy of the challenge winning method ($79.0$).
ER  -


TY  - Preprint
T1  - MorphNet: Fast &amp; Simple Resource-Constrained Structure Learning of Deep Networks
A1  - Ariel Gordon
A1  - Elad Eban
A1  - Ofir Nachum
A1  - Bo Chen
A1  - Hao Wu
A1  - Tien-Ju Yang
A1  - Edward Choi
JO  - ArXiv e-prints
Y1  - 17 April, 2018
UR  - https://arxiv.org/abs/1711.06798
N2  - We present MorphNet, an approach to automate the design of neural network structures. MorphNet iteratively shrinks and expands a network, shrinking via a resource-weighted sparsifying regularizer on activations and expanding via a uniform multiplicative factor on all layers. In contrast to previous approaches, our method is scalable to large networks, adaptable to specific resource constraints (e.g. the number of floating-point operations per inference), and capable of increasing the network&#39;s performance. When applied to standard network architectures on a wide variety of datasets, our approach discovers novel structures in each domain, obtaining higher performance while respecting the resource constraint.
ER  -


TY  - Preprint
T1  - Deep supervised learning using local errors
A1  - Hesham Mostafa
A1  - Vishwajith Ramesh
A1  - Gert Cauwenberghs
JO  - ArXiv e-prints
Y1  - 17 November, 2017
UR  - https://arxiv.org/abs/1711.06756
N2  - Error backpropagation is a highly effective mechanism for learning high-quality hierarchical features in deep networks. Updating the features or weights in one layer, however, requires waiting for the propagation of error signals from higher layers. Learning using delayed and non-local errors makes it hard to reconcile backpropagation with the learning mechanisms observed in biological neural networks as it requires the neurons to maintain a memory of the input long enough until the higher-layer errors arrive. In this paper, we propose an alternative learning mechanism where errors are generated locally in each layer using fixed, random auxiliary classifiers. Lower layers could thus be trained independently of higher layers and training could either proceed layer by layer, or simultaneously in all layers using local error information. We address biological plausibility concerns such as weight symmetry requirements and show that the proposed learning mechanism based on fixed, broad, and random tuning of each neuron to the classification categories outperforms the biologically-motivated feedback alignment learning technique on the MNIST, CIFAR10, and SVHN datasets, approaching the performance of standard backpropagation. Our approach highlights a potential biological mechanism for the supervised, or task-dependent, learning of feature hierarchies. In addition, we show that it is well suited for learning deep networks in custom hardware where it can drastically reduce memory traffic and data communication overheads.
ER  -


TY  - Preprint
T1  - Learning to Play Othello with Deep Neural Networks
A1  - PaweÅ Liskowski
A1  - Wojciech JaÅkowski
A1  - Krzysztof Krawiec
JO  - ArXiv e-prints
Y1  - 17 November, 2017
UR  - https://arxiv.org/abs/1711.06583
N2  - Achieving superhuman playing level by AlphaGo corroborated the capabilities of convolutional neural architectures (CNNs) for capturing complex spatial patterns. This result was to a great extent due to several analogies between Go board states and 2D images CNNs have been designed for, in particular translational invariance and a relatively large board. In this paper, we verify whether CNN-based move predictors prove effective for Othello, a game with significantly different characteristics, including a much smaller board size and complete lack of translational invariance. We compare several CNN architectures and board encodings, augment them with state-of-the-art extensions, train on an extensive database of experts&#39; moves, and examine them with respect to move prediction accuracy and playing strength. The empirical evaluation confirms high capabilities of neural move predictors and suggests a strong correlation between prediction accuracy and playing strength. The best CNNs not only surpass all other 1-ply Othello players proposed to date but defeat (2-ply) Edax, the best open-source Othello player.
ER  -


TY  - Preprint
T1  - Learning a Robust Representation via a Deep Network on Symmetric Positive Definite Manifolds
A1  - Zhi Gao
A1  - Yuwei Wu
A1  - Xingyuan Bu
A1  - Yunde Jia
JO  - ArXiv e-prints
Y1  - 20 November, 2017
UR  - https://arxiv.org/abs/1711.06540
N2  - Recent studies have shown that aggregating convolutional features of a pre-trained Convolutional Neural Network (CNN) can obtain impressive performance for a variety of visual tasks. The symmetric Positive Definite (SPD) matrix becomes a powerful tool due to its remarkable ability to learn an appropriate statistic representation to characterize the underlying structure of visual features. In this paper, we propose to aggregate deep convolutional features into an SPD matrix representation through the SPD generation and the SPD transformation under an end-to-end deep network. To this end, several new layers are introduced in our network, including a nonlinear kernel aggregation layer, an SPD matrix transformation layer, and a vectorization layer. The nonlinear kernel aggregation layer is employed to aggregate the convolutional features into a real SPD matrix directly. The SPD matrix transformation layer is designed to construct a more compact and discriminative SPD representation. The vectorization and normalization operations are performed in the vectorization layer for reducing the redundancy and accelerating the convergence. The SPD matrix in our network can be considered as a mid-level representation bridging convolutional features and high-level semantic features. To demonstrate the effectiveness of our method, we conduct extensive experiments on visual classification. Experiment results show that our method notably outperforms state-of-the-art methods.
ER  -


TY  - Preprint
T1  - Training Simplification and Model Simplification for Deep Learning: A Minimal Effort Back Propagation Method
A1  - Xu Sun
A1  - Xuancheng Ren
A1  - Shuming Ma
A1  - Bingzhen Wei
A1  - Wei Li
A1  - Houfeng Wang
JO  - ArXiv e-prints
Y1  - 17 November, 2017
UR  - https://arxiv.org/abs/1711.06528
N2  - We propose a simple yet effective technique to simplify the training and the resulting model of neural networks. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-$k$ elements (in terms of magnitude) are kept. As a result, only $k$ rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction in the computational cost. Based on the sparsified gradients, we further simplify the model by eliminating the rows or columns that are seldom updated, which will reduce the computational cost both in the training and decoding, and potentially accelerate decoding in real-world applications. Surprisingly, experimental results demonstrate that most of time we only need to update fewer than 5% of the weights at each back propagation pass. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given. The model simplification results show that we could adaptively simplify the model which could often be reduced by around 9x, without any loss on accuracy or even with improved accuracy.
ER  -


TY  - Preprint
T1  - Wikipedia for Smart Machines and Double Deep Machine Learning
A1  - Moshe BenBassat
JO  - ArXiv e-prints
Y1  - 22 May, 2018
UR  - https://arxiv.org/abs/1711.06517
N2  - Very important breakthroughs in data centric deep learning algorithms led to impressive performance in transactional point applications of Artificial Intelligence (AI) such as Face Recognition, or EKG classification. With all due appreciation, however, knowledge blind data only machine learning algorithms have severe limitations for non-transactional AI applications, such as medical diagnosis beyond the EKG results. Such applications require deeper and broader knowledge in their problem solving capabilities, e.g. integrating anatomy and physiology knowledge with EKG results and other patient findings. Following a review and illustrations of such limitations for several real life AI applications, we point at ways to overcome them. The proposed Wikipedia for Smart Machines initiative aims at building repositories of software structures that represent humanity science &amp; technology knowledge in various parts of life; knowledge that we all learn in schools, universities and during our professional life. Target readers for these repositories are smart machines; not human. AI software developers will have these Reusable Knowledge structures readily available, hence, the proposed name ReKopedia. Big Data is by now a mature technology, it is time to focus on Big Knowledge. Some will be derived from data, some will be obtained from mankind gigantic repository of knowledge. Wikipedia for smart machines along with the new Double Deep Learning approach offer a paradigm for integrating datacentric deep learning algorithms with algorithms that leverage deep knowledge, e.g. evidential reasoning and causality reasoning. For illustration, a project is described to produce ReKopedia knowledge modules for medical diagnosis of about 1,000 disorders. Data is important, but knowledge deep, basic, and commonsense is equally important.
ER  -


TY  - Preprint
T1  - Vision Based Railway Track Monitoring using Deep Learning
A1  - Shruti Mittal
A1  - Dattaraj Rao
JO  - ArXiv e-prints
Y1  - 17 November, 2017
UR  - https://arxiv.org/abs/1711.06423
N2  - Computer vision based methods have been explored in the past for detection of railway track defects, but full automation has always been a challenge because both traditional image processing methods and deep learning classifiers trained from scratch fail to generalize that well to infinite novel scenarios seen in the real world, given limited amount of labeled data. Advancements have been made recently to make machine learning models utilize knowledge from a different but related domain. In this paper, we show that even though similar domain data is not available, transfer learning provides the model understanding of other real world objects and enables training production scale deep learning classifiers for uncontrolled real world data. Our models efficiently detect both track defects like sunkinks, loose ballast and railway assets like switches and signals. Models were validated with hours of track videos recorded in different continents resulting in different weather conditions, different ambience and surroundings. A track health index concept has also been proposed to monitor complete rail network.
ER  -


TY  - Preprint
T1  - Improving Palliative Care with Deep Learning
A1  - Anand Avati
A1  - Kenneth Jung
A1  - Stephanie Harman
A1  - Lance Downing
A1  - Andrew Ng
A1  - Nigam H. Shah
JO  - ArXiv e-prints
Y1  - 16 November, 2017
UR  - https://arxiv.org/abs/1711.06402
N2  - Improving the quality of end-of-life care for hospitalized patients is a priority for healthcare organizations. Studies have shown that physicians tend to over-estimate prognoses, which in combination with treatment inertia results in a mismatch between patients wishes and actual care at the end of life. We describe a method to address this problem using Deep Learning and Electronic Health Record (EHR) data, which is currently being piloted, with Institutional Review Board approval, at an academic medical center. The EHR data of admitted patients are automatically evaluated by an algorithm, which brings patients who are likely to benefit from palliative care services to the attention of the Palliative Care team. The algorithm is a Deep Neural Network trained on the EHR data from previous years, to predict all-cause 3-12 month mortality of patients as a proxy for patients that could benefit from palliative care. Our predictions enable the Palliative Care team to take a proactive approach in reaching out to such patients, rather than relying on referrals from treating physicians, or conduct time consuming chart reviews of all patients. We also present a novel interpretation technique which we use to provide explanations of the model&#39;s predictions.
ER  -


TY  - Preprint
T1  - Towards Deep Learning Models for Psychological State Prediction using Smartphone Data: Challenges and Opportunities
A1  - Gatis Mikelsons
A1  - Matthew Smith
A1  - Abhinav Mehrotra
A1  - Mirco Musolesi
JO  - ArXiv e-prints
Y1  - 16 November, 2017
UR  - https://arxiv.org/abs/1711.06350
N2  - There is an increasing interest in exploiting mobile sensing technologies and machine learning techniques for mental health monitoring and intervention. Researchers have effectively used contextual information, such as mobility, communication and mobile phone usage patterns for quantifying individuals&#39; mood and wellbeing. In this paper, we investigate the effectiveness of neural network models for predicting users&#39; level of stress by using the location information collected by smartphones. We characterize the mobility patterns of individuals using the GPS metrics presented in the literature and employ these metrics as input to the network. We evaluate our approach on the open-source StudentLife dataset. Moreover, we discuss the challenges and trade-offs involved in building machine learning models for digital mental health and highlight potential future work in this direction.
ER  -


TY  - Preprint
T1  - Performance Modeling and Evaluation of Distributed Deep Learning Frameworks on GPUs
A1  - Shaohuai Shi
A1  - Qiang Wang
A1  - Xiaowen Chu
JO  - ArXiv e-prints
Y1  - 20 August, 2018
UR  - https://arxiv.org/abs/1711.05979
N2  - Deep learning frameworks have been widely deployed on GPU servers for deep learning applications in both academia and industry. In training deep neural networks (DNNs), there are many standard processes or algorithms, such as convolution and stochastic gradient descent (SGD), but the running performance of different frameworks might be different even running the same deep model on the same GPU hardware. In this study, we evaluate the running performance of four state-of-the-art distributed deep learning frameworks (i.e., Caffe-MPI, CNTK, MXNet, and TensorFlow) over single-GPU, multi-GPU, and multi-node environments. We first build performance models of standard processes in training DNNs with SGD, and then we benchmark the running performance of these frameworks with three popular convolutional neural networks (i.e., AlexNet, GoogleNet and ResNet-50), after that, we analyze what factors that result in the performance gap among these four frameworks. Through both analytical and experimental analysis, we identify bottlenecks and overheads which could be further optimized. The main contribution is that the proposed performance models and the analysis provide further optimization directions in both algorithmic design and system configuration.
ER  -


TY  - Preprint
T1  - Less-forgetful Learning for Domain Expansion in Deep Neural Networks
A1  - Heechul Jung
A1  - Jeongwoo Ju
A1  - Minju Jung
A1  - Junmo Kim
JO  - ArXiv e-prints
Y1  - 16 November, 2017
UR  - https://arxiv.org/abs/1711.05959
N2  - Expanding the domain that deep neural network has already learned without accessing old domain data is a challenging task because deep neural networks forget previously learned information when learning new data from a new domain. In this paper, we propose a less-forgetful learning method for the domain expansion scenario. While existing domain adaptation techniques solely focused on adapting to new domains, the proposed technique focuses on working well with both old and new domains without needing to know whether the input is from the old or new domain. First, we present two naive approaches which will be problematic, then we provide a new method using two proposed properties for less-forgetful learning. Finally, we prove the effectiveness of our method through experiments on image classification tasks. All datasets used in the paper, will be released on our website for someone&#39;s follow-up study.
ER  -


TY  - Preprint
T1  - Real-Time Document Image Classification using Deep CNN and Extreme Learning Machines
A1  - Andreas KÃ¶lsch
A1  - Muhammad Zeshan Afzal
A1  - Markus Ebbecke
A1  - Marcus Liwicki
JO  - ArXiv e-prints
Y1  - 3 November, 2017
UR  - https://arxiv.org/abs/1711.05862
N2  - This paper presents an approach for real-time training and testing for document image classification. In production environments, it is crucial to perform accurate and (time-)efficient training. Existing deep learning approaches for classifying documents do not meet these requirements, as they require much time for training and fine-tuning the deep architectures. Motivated from Computer Vision, we propose a two-stage approach. The first stage trains a deep network that works as feature extractor and in the second stage, Extreme Learning Machines (ELMs) are used for classification. The proposed approach outperforms all previously reported structural and deep learning based methods with a final accuracy of 83.24% on Tobacco-3482 dataset, leading to a relative error reduction of 25% when compared to a previous Convolutional Neural Network (CNN) based approach (DeepDocClassifier). More importantly, the training time of the ELM is only 1.176 seconds and the overall prediction time for 2,482 images is 3.066 seconds. As such, this novel approach makes deep learning-based document classification suitable for large-scale real-time applications.
ER  -


TY  - Preprint
T1  - Zero-Shot Learning via Class-Conditioned Deep Generative Models
A1  - Wenlin Wang
A1  - Yunchen Pu
A1  - Vinay Kumar Verma
A1  - Kai Fan
A1  - Yizhe Zhang
A1  - Changyou Chen
A1  - Piyush Rai
A1  - Lawrence Carin
JO  - ArXiv e-prints
Y1  - 19 November, 2017
UR  - https://arxiv.org/abs/1711.05820
N2  - We present a deep generative model for learning to predict classes not seen at training time. Unlike most existing methods for this problem, that represent each class as a point (via a semantic embedding), we represent each seen/unseen class using a class-specific latent-space distribution, conditioned on class attributes. We use these latent-space distributions as a prior for a supervised variational autoencoder (VAE), which also facilitates learning highly discriminative feature representations for the inputs. The entire framework is learned end-to-end using only the seen-class training data. The model infers corresponding attributes of a test image by maximizing the VAE lower bound; the inferred attributes may be linked to labels not seen when training. We further extend our model to a (1) semi-supervised/transductive setting by leveraging unlabeled unseen-class data via an unsupervised learning module, and (2) few-shot learning where we also have a small number of labeled inputs from the unseen classes. We compare our model with several state-of-the-art methods through a comprehensive set of experiments on a variety of benchmark data sets.
ER  -


TY  - Preprint
T1  - BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems
A1  - Zachary Lipton
A1  - Xiujun Li
A1  - Jianfeng Gao
A1  - Lihong Li
A1  - Faisal Ahmed
A1  - Li Deng
JO  - ArXiv e-prints
Y1  - 19 November, 2017
UR  - https://arxiv.org/abs/1711.05715
N2  - We present a new algorithm that significantly improves the efficiency of exploration for deep Q-learning agents in dialogue systems. Our agents explore via Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop neural network. Our algorithm learns much faster than common exploration strategies such as Îµ-greedy, Boltzmann, bootstrapping, and intrinsic-reward-based ones. Additionally, we show that spiking the replay buffer with experiences from just a few successful episodes can make Q-learning feasible when it might otherwise fail.
ER  -


TY  - Preprint
T1  - Deep Epitome for Unravelling Generalized Hamming Network: A Fuzzy Logic Interpretation of Deep Learning
A1  - Lixin Fan
JO  - ArXiv e-prints
Y1  - 14 November, 2017
UR  - https://arxiv.org/abs/1711.05397
N2  - This paper gives a rigorous analysis of trained Generalized Hamming Networks(GHN) proposed by Fan (2017) and discloses an interesting finding about GHNs, i.e., stacked convolution layers in a GHN is equivalent to a single yet wide convolution layer. The revealed equivalence, on the theoretical side, can be regarded as a constructive manifestation of the universal approximation theorem Cybenko(1989); Hornik (1991). In practice, it has profound and multi-fold implications. For network visualization, the constructed deep epitomes at each layer provide a visualization of network internal representation that does not rely on the input data. Moreover, deep epitomes allows the direct extraction of features in just one step, without resorting to regularized optimizations used in existing visualization tools.
ER  -


TY  - Preprint
T1  - A Deep Learning Approach for Expert Identification in Question Answering Communities
A1  - Chen Zheng
A1  - Shuangfei Zhai
A1  - Zhongfei Zhang
JO  - ArXiv e-prints
Y1  - 14 November, 2017
UR  - https://arxiv.org/abs/1711.05350
N2  - In this paper, we describe an effective convolutional neural network framework for identifying the expert in question answering community. This approach uses the convolutional neural network and combines user feature representations with question feature representations to compute scores that the user who gets the highest score is the expert on this question. Unlike prior work, this method does not measure expert based on measure answer content quality to identify the expert but only require question sentence and user embedding feature to identify the expert. Remarkably, Our model can be applied to different languages and different domains. The proposed framework is trained on two datasets, The first dataset is Stack Overflow and the second one is Zhihu. The Top-1 accuracy results of our experiments show that our framework outperforms the best baseline framework for expert identification.
ER  -


TY  - Preprint
T1  - CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning
A1  - Pranav Rajpurkar
A1  - Jeremy Irvin
A1  - Kaylie Zhu
A1  - Brandon Yang
A1  - Hershel Mehta
A1  - Tony Duan
A1  - Daisy Ding
A1  - Aarti Bagul
A1  - Curtis Langlotz
A1  - Katie Shpanskaya
A1  - Matthew P. Lungren
A1  - Andrew Y. Ng
JO  - ArXiv e-prints
Y1  - 25 December, 2017
UR  - https://arxiv.org/abs/1711.05225
N2  - We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.
ER  -


TY  - Preprint
T1  - Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice
A1  - Jeffrey Pennington
A1  - Samuel S. Schoenholz
A1  - Surya Ganguli
JO  - ArXiv e-prints
Y1  - 13 November, 2017
UR  - https://arxiv.org/abs/1711.04735
N2  - It is well known that the initialization of weights in deep neural networks can have a dramatic impact on learning speed. For example, ensuring the mean squared singular value of a network&#39;s input-output Jacobian is $O(1)$ is essential for avoiding the exponential vanishing or explosion of gradients. The stronger condition that all singular values of the Jacobian concentrate near $1$ is a property known as dynamical isometry. For deep linear networks, dynamical isometry can be achieved through orthogonal weight initialization and has been shown to dramatically speed up learning; however, it has remained unclear how to extend these results to the nonlinear setting. We address this question by employing powerful tools from free probability theory to compute analytically the entire singular value distribution of a deep network&#39;s input-output Jacobian. We explore the dependence of the singular value distribution on the depth of the network, the weight initialization, and the choice of nonlinearity. Intriguingly, we find that ReLU networks are incapable of dynamical isometry. On the other hand, sigmoidal networks can achieve isometry, but only with orthogonal weight initialization. Moreover, we demonstrate empirically that deep nonlinear networks achieving dynamical isometry learn orders of magnitude faster than networks that do not. Indeed, we show that properly-initialized deep sigmoidal networks consistently outperform deep ReLU networks. Overall, our analysis reveals that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning.
ER  -


TY  - Preprint
T1  - Conditional Random Field and Deep Feature Learning for Hyperspectral Image Segmentation
A1  - Fahim Irfan Alam
A1  - Jun Zhou
A1  - Alan Wee-Chung Liew
A1  - Xiuping Jia
A1  - Jocelyn Chanussot
A1  - Yongsheng Gao
JO  - ArXiv e-prints
Y1  - 27 December, 2017
UR  - https://arxiv.org/abs/1711.04483
N2  - Image segmentation is considered to be one of the critical tasks in hyperspectral remote sensing image processing. Recently, convolutional neural network (CNN) has established itself as a powerful model in segmentation and classification by demonstrating excellent performances. The use of a graphical model such as a conditional random field (CRF) contributes further in capturing contextual information and thus improving the segmentation performance. In this paper, we propose a method to segment hyperspectral images by considering both spectral and spatial information via a combined framework consisting of CNN and CRF. We use multiple spectral cubes to learn deep features using CNN, and then formulate deep CRF with CNN-based unary and pairwise potential functions to effectively extract the semantic correlations between patches consisting of three-dimensional data cubes. Effective piecewise training is applied in order to avoid the computationally expensive iterative CRF inference. Furthermore, we introduce a deep deconvolution network that improves the segmentation masks. We also introduce a new dataset and experimented our proposed method on it along with several widely adopted benchmark datasets to evaluate the effectiveness of our method. By comparing our results with those from several state-of-the-art models, we show the promising potential of our method.
ER  -


TY  - Preprint
T1  - All-Transfer Learning for Deep Neural Networks and its Application to Sepsis Classification
A1  - Yoshihide Sawada
A1  - Yoshikuni Sato
A1  - Toru Nakada
A1  - Kei Ujimoto
A1  - Nobuhiro Hayashi
JO  - ArXiv e-prints
Y1  - 13 November, 2017
UR  - https://arxiv.org/abs/1711.04450
N2  - In this article, we propose a transfer learning method for deep neural networks (DNNs). Deep learning has been widely used in many applications. However, applying deep learning is problematic when a large amount of training data are not available. One of the conventional methods for solving this problem is transfer learning for DNNs. In the field of image recognition, state-of-the-art transfer learning methods for DNNs re-use parameters trained on source domain data except for the output layer. However, this method may result in poor classification performance when the amount of target domain data is significantly small. To address this problem, we propose a method called All-Transfer Deep Learning, which enables the transfer of all parameters of a DNN. With this method, we can compute the relationship between the source and target labels by the source domain knowledge. We applied our method to actual two-dimensional electrophoresis image~(2-DE image) classification for determining if an individual suffers from sepsis; the first attempt to apply a classification approach to 2-DE images for proteomics, which has attracted considerable attention as an extension beyond genomics. The results suggest that our proposed method outperforms conventional transfer learning methods for DNNs.
ER  -


TY  - Preprint
T1  - Towards Automated ICD Coding Using Deep Learning
A1  - Haoran Shi
A1  - Pengtao Xie
A1  - Zhiting Hu
A1  - Ming Zhang
A1  - Eric P. Xing
JO  - ArXiv e-prints
Y1  - 30 November, 2017
UR  - https://arxiv.org/abs/1711.04075
N2  - International Classification of Diseases(ICD) is an authoritative health care classification system of different diseases and conditions for clinical and management purposes. Considering the complicated and dedicated process to assign correct codes to each patient admission based on overall diagnosis, we propose a hierarchical deep learning model with attention mechanism which can automatically assign ICD diagnostic codes given written diagnosis. We utilize character-aware neural language models to generate hidden representations of written diagnosis descriptions and ICD codes, and design an attention mechanism to address the mismatch between the numbers of descriptions and corresponding codes. Our experimental results show the strong potential of automated ICD coding from diagnosis descriptions. Our best model achieves 0.53 and 0.90 of F1 score and area under curve of receiver operating characteristic respectively. The result outperforms those achieved using character-unaware encoding method or without attention mechanism. It indicates that our proposed deep learning model can code automatically in a reasonable way and provide a framework for computer-auxiliary ICD coding.
ER  -


TY  - Preprint
T1  - Applications of Deep Learning and Reinforcement Learning to Biological Data
A1  - Mufti Mahmud
A1  - M. Shamim Kaiser
A1  - Amir Hussain
A1  - Stefano Vassanelli
JO  - ArXiv e-prints
Y1  - 7 January, 2018
UR  - https://arxiv.org/abs/1711.03985
N2  - Rapid advances of hardware-based technologies during the past decades have opened up new possibilities for Life scientists to gather multimodal data in various application domains (e.g., Omics, Bioimaging, Medical Imaging, and [Brain/Body]-Machine Interfaces), thus generating novel opportunities for development of dedicated data intensive machine learning techniques. Overall, recent research in Deep learning (DL), Reinforcement learning (RL), and their combination (Deep RL) promise to revolutionize Artificial Intelligence. The growth in computational power accompanied by faster and increased data storage and declining computing costs have already allowed scientists in various fields to apply these techniques on datasets that were previously intractable for their size and complexity. This review article provides a comprehensive survey on the application of DL, RL, and Deep RL techniques in mining Biological data. In addition, we compare performances of DL techniques when applied to different datasets across various application domains. Finally, we outline open issues in this challenging research area and discuss future development perspectives.
ER  -


TY  - Preprint
T1  - Towards the Use of Deep Reinforcement Learning with Global Policy For Query-based Extractive Summarisation
A1  - Diego Molla
JO  - ArXiv e-prints
Y1  - 14 November, 2017
UR  - https://arxiv.org/abs/1711.03859
N2  - Supervised approaches for text summarisation suffer from the problem of mismatch between the target labels/scores of individual sentences and the evaluation score of the final summary. Reinforcement learning can solve this problem by providing a learning mechanism that uses the score of the final summary as a guide to determine the decisions made at the time of selection of each sentence. In this paper we present a proof-of-concept approach that applies a policy-gradient algorithm to learn a stochastic policy using an undiscounted reward. The method has been applied to a policy consisting of a simple neural network and simple features. The resulting deep reinforcement learning system is able to learn a global policy and obtain encouraging results.
ER  -


TY  - Preprint
T1  - Online Deep Learning: Learning Deep Neural Networks on the Fly
A1  - Doyen Sahoo
A1  - Quang Pham
A1  - Jing Lu
A1  - Steven C. H. Hoi
JO  - ArXiv e-prints
Y1  - 10 November, 2017
UR  - https://arxiv.org/abs/1711.03705
N2  - Deep Neural Networks (DNNs) are typically trained by backpropagation in a batch learning setting, which requires the entire training data to be made available prior to the learning task. This is not scalable for many real-world scenarios where new data arrives sequentially in a stream form. We aim to address an open challenge of &#34;Online Deep Learning&#34; (ODL) for learning DNNs on the fly in an online setting. Unlike traditional online learning that often optimizes some convex objective function with respect to a shallow model (e.g., a linear/kernel-based hypothesis), ODL is significantly more challenging since the optimization of the DNN objective function is non-convex, and regular backpropagation does not work well in practice, especially for online learning settings. In this paper, we present a new online deep learning framework that attempts to tackle the challenges by learning DNN models of adaptive depth from a sequence of training data in an online learning setting. In particular, we propose a novel Hedge Backpropagation (HBP) method for online updating the parameters of DNN effectively, and validate the efficacy of our method on large-scale data sets, including both stationary and concept drifting scenarios.
ER  -


TY  - Preprint
T1  - p-FP: Extraction, Classification, and Prediction of Website Fingerprints with Deep Learning
A1  - Se Eun Oh
A1  - Saikrishna Sunkam
A1  - Nicholas Hopper
JO  - ArXiv e-prints
Y1  - 2 April, 2018
UR  - https://arxiv.org/abs/1711.03656
N2  - Recent advances in learning Deep Neural Network (DNN) architectures have received a great deal of attention due to their ability to outperform state-of-the-art classifiers across a wide range of applications, with little or no feature engineering. In this paper, we broadly study the applicability of deep learning to website fingerprinting. We show that unsupervised DNNs can be used to extract low-dimensional feature vectors that improve the performance of state-of-the-art website fingerprinting attacks. When used as classifiers, we show that they can match or exceed performance of existing attacks across a range of application scenarios, including fingerprinting Tor website traces, fingerprinting search engine queries over Tor, defeating fingerprinting defenses, and fingerprinting TLS-encrypted websites. Finally, we show that DNNs can be used to predict the fingerprintability of a website based on its contents, achieving 99% accuracy on a data set of 4500 website downloads.
ER  -


TY  - Preprint
T1  - What Really is Deep Learning Doing?
A1  - Chuyu Xiong
JO  - ArXiv e-prints
Y1  - 6 November, 2017
UR  - https://arxiv.org/abs/1711.03577
N2  - Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more. Yet, what deep learning is really doing is still an open question. There are a lot of works in this direction. For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation. In order to address this very crucial question, here we see deep learning from perspective of mechanical learning and learning machine (see [1], [2]). From this particular angle, we can see deep learning much better and answer with confidence: What deep learning is really doing? why it works well, how it works, and how much data is necessary for learning. We also will discuss advantages and disadvantages of deep learning at the end of this work.
ER  -


TY  - Preprint
T1  - DLPaper2Code: Auto-generation of Code from Deep Learning Research Papers
A1  - Akshay Sethi
A1  - Anush Sankaran
A1  - Naveen Panwar
A1  - Shreya Khare
A1  - Senthil Mani
JO  - ArXiv e-prints
Y1  - 9 November, 2017
UR  - https://arxiv.org/abs/1711.03543
N2  - With an abundance of research papers in deep learning, reproducibility or adoption of the existing works becomes a challenge. This is due to the lack of open source implementations provided by the authors. Further, re-implementing research papers in a different library is a daunting task. To address these challenges, we propose a novel extensible approach, DLPaper2Code, to extract and understand deep learning design flow diagrams and tables available in a research paper and convert them to an abstract computational graph. The extracted computational graph is then converted into execution ready source code in both Keras and Caffe, in real-time. An arXiv-like website is created where the automatically generated designs is made publicly available for 5,000 research papers. The generated designs could be rated and edited using an intuitive drag-and-drop UI framework in a crowdsourced manner. To evaluate our approach, we create a simulated dataset with over 216,000 valid design visualizations using a manually defined grammar. Experiments on the simulated dataset show that the proposed framework provide more than $93\%$ accuracy in flow diagram content extraction.
ER  -


TY  - Preprint
T1  - Performance Evaluation of Deep Learning Tools in Docker Containers
A1  - Pengfei Xu
A1  - Shaohuai Shi
A1  - Xiaowen Chu
JO  - ArXiv e-prints
Y1  - 9 November, 2017
UR  - https://arxiv.org/abs/1711.03386
N2  - With the success of deep learning techniques in a broad range of application domains, many deep learning software frameworks have been developed and are being updated frequently to adapt to new hardware features and software libraries, which bring a big challenge for end users and system administrators. To address this problem, container techniques are widely used to simplify the deployment and management of deep learning software. However, it remains unknown whether container techniques bring any performance penalty to deep learning applications. The purpose of this work is to systematically evaluate the impact of docker container on the performance of deep learning applications. We first benchmark the performance of system components (IO, CPU and GPU) in a docker container and the host system and compare the results to see if there&#39;s any difference. According to our results, we find that computational intensive jobs, either running on CPU or GPU, have small overhead indicating docker containers can be applied to deep learning programs. Then we evaluate the performance of some popular deep learning tools deployed in a docker container and the host system. It turns out that the docker container will not cause noticeable drawbacks while running those deep learning tools. So encapsulating deep learning tool in a container is a feasible solution.
ER  -


TY  - Preprint
T1  - Deep Hyperspherical Learning
A1  - Weiyang Liu
A1  - Yan-Ming Zhang
A1  - Xingguo Li
A1  - Zhiding Yu
A1  - Bo Dai
A1  - Tuo Zhao
A1  - Le Song
JO  - ArXiv e-prints
Y1  - 30 January, 2018
UR  - https://arxiv.org/abs/1711.03189
N2  - Convolution as inner product has been the founding basis of convolutional neural networks (CNNs) and the key to end-to-end visual representation learning. Benefiting from deeper architectures, recent CNNs have demonstrated increasingly strong representation abilities. Despite such improvement, the increased depth and larger parameter space have also led to challenges in properly training a network. In light of such challenges, we propose hyperspherical convolution (SphereConv), a novel learning framework that gives angular representations on hyperspheres. We introduce SphereNet, deep hyperspherical convolution networks that are distinct from conventional inner product based convolutional networks. In particular, SphereNet adopts SphereConv as its basic convolution operator and is supervised by generalized angular softmax loss - a natural loss formulation under SphereConv. We show that SphereNet can effectively encode discriminative representation and alleviate training difficulty, leading to easier optimization, faster convergence and comparable (even better) classification accuracy over convolutional counterparts. We also provide some theoretical insights for the advantages of learning on hyperspheres. In addition, we introduce the learnable SphereConv, i.e., a natural improvement over prefixed SphereConv, and SphereNorm, i.e., hyperspherical learning as a normalization method. Experiments have verified our conclusions.
ER  -


TY  - Preprint
T1  - Multi-stage Suture Detection for Robot Assisted Anastomosis based on Deep Learning
A1  - Yang Hu
A1  - Yun Gu
A1  - Jie Yang
A1  - Guang-Zhong Yang
JO  - ArXiv e-prints
Y1  - 8 November, 2017
UR  - https://arxiv.org/abs/1711.03179
N2  - In robotic surgery, task automation and learning from demonstration combined with human supervision is an emerging trend for many new surgical robot platforms. One such task is automated anastomosis, which requires bimanual needle handling and suture detection. Due to the complexity of the surgical environment and varying patient anatomies, reliable suture detection is difficult, which is further complicated by occlusion and thread topologies. In this paper, we propose a multi-stage framework for suture thread detection based on deep learning. Fully convolutional neural networks are used to obtain the initial detection and the overlapping status of suture thread, which are later fused with the original image to learn a gradient road map of the thread. Based on the gradient road map, multiple segments of the thread are extracted and linked to form the whole thread using a curvilinear structure detector. Experiments on two different types of sutures demonstrate the accuracy of the proposed framework.
ER  -


TY  - Preprint
T1  - Learning Deep Mean Field Games for Modeling Large Population Behavior
A1  - Jiachen Yang
A1  - Xiaojing Ye
A1  - Rakshit Trivedi
A1  - Huan Xu
A1  - Hongyuan Zha
JO  - ArXiv e-prints
Y1  - 22 April, 2018
UR  - https://arxiv.org/abs/1711.03156
N2  - We consider the problem of representing collective behavior of large populations and predicting the evolution of a population distribution over a discrete state space. A discrete time mean field game (MFG) is motivated as an interpretable model founded on game theory for understanding the aggregate effect of individual actions and predicting the temporal evolution of population distributions. We achieve a synthesis of MFG and Markov decision processes (MDP) by showing that a special MFG is reducible to an MDP. This enables us to broaden the scope of mean field game theory and infer MFG models of large real-world systems via deep inverse reinforcement learning. Our method learns both the reward function and forward dynamics of an MFG from real data, and we report the first empirical test of a mean field game model of a real-world social media population.
ER  -


TY  - Preprint
T1  - DLVM: A modern compiler infrastructure for deep learning systems
A1  - Richard Wei
A1  - Lane Schwartz
A1  - Vikram Adve
JO  - ArXiv e-prints
Y1  - 2 February, 2018
UR  - https://arxiv.org/abs/1711.03016
N2  - Deep learning software demands reliability and performance. However, many of the existing deep learning frameworks are software libraries that act as an unsafe DSL in Python and a computation graph interpreter. We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain-specific optimizations and a code generator targeting GPU via LLVM. Designed as a modern compiler infrastructure inspired by LLVM, DLVM is more modular and more generic than existing deep learning compiler frameworks, and supports tensor DSLs with high expressivity. With our prototypical staged DSL embedded in Swift, we argue that the DLVM system enables a form of modular, safe and performant frameworks for deep learning.
ER  -


TY  - Preprint
T1  - Optimal Auction For Edge Computing Resource Management in Mobile Blockchain Networks: A Deep Learning Approach
A1  - Nguyen Cong Luong
A1  - Zehui Xiong
A1  - Ping Wang
A1  - Dusit Niyato
JO  - ArXiv e-prints
Y1  - 16 November, 2017
UR  - https://arxiv.org/abs/1711.02844
N2  - Blockchain has recently been applied in many applications such as bitcoin, smart grid, and Internet of Things (IoT) as a public ledger of transactions. However, the use of blockchain in mobile environments is still limited because the mining process consumes too much computing and energy resources on mobile devices. Edge computing offered by the Edge Computing Service Provider can be adopted as a viable solution for offloading the mining tasks from the mobile devices, i.e., miners, in the mobile blockchain environment. However, a mechanism needs to be designed for edge resource allocation to maximize the revenue for the Edge Computing Service Provider and to ensure incentive compatibility and individual rationality is still open. In this paper, we develop an optimal auction based on deep learning for the edge resource allocation. Specifically, we construct a multi-layer neural network architecture based on an analytical solution of the optimal auction. The neural networks first perform monotone transformations of the miners&#39; bids. Then, they calculate allocation and conditional payment rules for the miners. We use valuations of the miners as the data training to adjust parameters of the neural networks so as to optimize the loss function which is the expected, negated revenue of the Edge Computing Service Provider. We show the experimental results to confirm the benefits of using the deep learning for deriving the optimal auction for mobile blockchain with high revenue
ER  -


TY  - Preprint
T1  - Traffic Prediction Based on Random Connectivity in Deep Learning with Long Short-Term Memory
A1  - Yuxiu Hua
A1  - Zhifeng Zhao
A1  - Rongpeng Li
A1  - Xianfu Chen
A1  - Zhiming Liu
A1  - Honggang Zhang
JO  - ArXiv e-prints
Y1  - 3 April, 2018
UR  - https://arxiv.org/abs/1711.02833
N2  - Traffic prediction plays an important role in evaluating the performance of telecommunication networks and attracts intense research interests. A significant number of algorithms and models have been put forward to analyse traffic data and make prediction. In the recent big data era, deep learning has been exploited to mine the profound information hidden in the data. In particular, Long Short-Term Memory (LSTM), one kind of Recurrent Neural Network (RNN) schemes, has attracted a lot of attentions due to its capability of processing the long-range dependency embedded in the sequential traffic data. However, LSTM has considerable computational cost, which can not be tolerated in tasks with stringent latency requirement. In this paper, we propose a deep learning model based on LSTM, called Random Connectivity LSTM (RCLSTM). Compared to the conventional LSTM, RCLSTM makes a notable breakthrough in the formation of neural network, which is that the neurons are connected in a stochastic manner rather than full connected. So, the RCLSTM, with certain intrinsic sparsity, have many neural connections absent (distinguished from the full connectivity) and which leads to the reduction of the parameters to be trained and the computational cost. We apply the RCLSTM to predict traffic and validate that the RCLSTM with even 35% neural connectivity still shows a satisfactory performance. When we gradually add training samples, the performance of RCLSTM becomes increasingly closer to the baseline LSTM. Moreover, for the input traffic sequences of enough length, the RCLSTM exhibits even superior prediction accuracy than the baseline LSTM.
ER  -


TY  - Preprint
T1  - Sequential Keystroke Behavioral Biometrics for Mobile User Identification via Multi-view Deep Learning
A1  - Lichao Sun
A1  - Yuqi Wang
A1  - Bokai Cao
A1  - Philip S. Yu
A1  - Witawas Srisa-an
A1  - Alex D Leow
JO  - ArXiv e-prints
Y1  - 14 November, 2017
UR  - https://arxiv.org/abs/1711.02703
N2  - With the rapid growth in smartphone usage, more organizations begin to focus on providing better services for mobile users. User identification can help these organizations to identify their customers and then cater services that have been customized for them. Currently, the use of cookies is the most common form to identify users. However, cookies are not easily transportable (e.g., when a user uses a different login account, cookies do not follow the user). This limitation motivates the need to use behavior biometric for user identification. In this paper, we propose DEEPSERVICE, a new technique that can identify mobile users based on user&#39;s keystroke information captured by a special keyboard or web browser. Our evaluation results indicate that DEEPSERVICE is highly accurate in identifying mobile users (over 93% accuracy). The technique is also efficient and only takes less than 1 ms to perform identification.
ER  -


TY  - Preprint
T1  - Deep Learning and Model Predictive Control for Self-Tuning Mode-Locked Lasers
A1  - Thomas Baumeister
A1  - Steven L. Brunton
A1  - J. Nathan Kutz
JO  - ArXiv e-prints
Y1  - 2 November, 2017
UR  - https://arxiv.org/abs/1711.02702
N2  - Self-tuning optical systems are of growing importance in technological applications such as mode-locked fiber lasers. Such self-tuning paradigms require {\em intelligent} algorithms capable of inferring approximate models of the underlying physics and discovering appropriate control laws in order to maintain robust performance for a given objective. In this work, we demonstrate the first integration of a {\em deep learning} (DL) architecture with {\em model predictive control} (MPC) in order to self-tune a mode-locked fiber laser. Not only can our DL-MPC algorithmic architecture approximate the unknown fiber birefringence, it also builds a dynamical model of the laser and appropriate control law for maintaining robust, high-energy pulses despite a stochastically drifting birefringence. We demonstrate the effectiveness of this method on a fiber laser which is mode-locked by nonlinear polarization rotation. The method advocated can be broadly applied to a variety of optical systems that require robust controllers.
ER  -


TY  - Preprint
T1  - Learning Robust Bed Making using Deep Imitation Learning with DART
A1  - Michael Laskey
A1  - Chris Powers
A1  - Ruta Joshi
A1  - Arshan Poursohi
A1  - Ken Goldberg
JO  - ArXiv e-prints
Y1  - 7 November, 2017
UR  - https://arxiv.org/abs/1711.02525
N2  - Bed-making is a universal home task that can be challenging for senior citizens due to reaching motions. Automating bed-making has multiple technical challenges such as perception in an unstructured environments, deformable object manipulation, obstacle avoidance and sequential decision making. We explore how DART, an LfD algorithm for learning robust policies, can be applied to automating bed making without fiducial markers with a Toyota Human Support Robot (HSR). By gathering human demonstrations for grasping the sheet and failure detection, we can learn deep neural network policies that leverage pre-trained YOLO features to automate the task. Experiments with a 1/2 scale twin bed and distractors placed on the bed, suggest policies learned on 50 demonstrations with DART achieve 96% sheet coverage, which is over 200% better than a corner detector baseline using contour detection.
ER  -


TY  - Preprint
T1  - Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?
A1  - Maithra Raghu
A1  - Alex Irpan
A1  - Jacob Andreas
A1  - Robert Kleinberg
A1  - Quoc V. Le
A1  - Jon Kleinberg
JO  - ArXiv e-prints
Y1  - 28 June, 2018
UR  - https://arxiv.org/abs/1711.02301
N2  - Deep reinforcement learning has achieved many recent successes, but our understanding of its strengths and limitations is hampered by the lack of rich environments in which we can fully characterize optimal behavior, and correspondingly diagnose individual actions against such a characterization. Here we consider a family of combinatorial games, arising from work of Erdos, Selfridge, and Spencer, and we propose their use as environments for evaluating and comparing different approaches to reinforcement learning. These games have a number of appealing features: they are challenging for current learning approaches, but they form (i) a low-dimensional, simply parametrized environment where (ii) there is a linear closed form solution for optimal behavior from any state, and (iii) the difficulty of the game can be tuned by changing environment parameters in an interpretable way. We use these Erdos-Selfridge-Spencer games not only to compare different algorithms, but test for generalization, make comparisons to supervised learning, analyse multiagent play, and even develop a self play algorithm. Code can be found at: https://github.com/rubai5/ESS_Game
ER  -


TY  - Preprint
T1  - Distributed Representation for Traditional Chinese Medicine Herb via Deep Learning Models
A1  - Wei Li
A1  - Zheng Yang
JO  - ArXiv e-prints
Y1  - 5 November, 2017
UR  - https://arxiv.org/abs/1711.01701
N2  - Traditional Chinese Medicine (TCM) has accumulated a big amount of precious resource in the long history of development. TCM prescriptions that consist of TCM herbs are an important form of TCM treatment, which are similar to natural language documents, but in a weakly ordered fashion. Directly adapting language modeling style methods to learn the embeddings of the herbs can be problematic as the herbs are not strictly in order, the herbs in the front of the prescription can be connected to the very last ones. In this paper, we propose to represent TCM herbs with distributed representations via Prescription Level Language Modeling (PLLM). In one of our experiments, the correlation between our calculated similarity between medicines and the judgment of professionals achieves a Spearman score of 55.35 indicating a strong correlation, which surpasses human beginners (TCM related field bachelor student) by a big margin (over 10%).
ER  -


TY  - Preprint
T1  - Composing Meta-Policies for Autonomous Driving Using Hierarchical Deep Reinforcement Learning
A1  - Richard Liaw
A1  - Sanjay Krishnan
A1  - Animesh Garg
A1  - Daniel Crankshaw
A1  - Joseph E. Gonzalez
A1  - Ken Goldberg
JO  - ArXiv e-prints
Y1  - 4 November, 2017
UR  - https://arxiv.org/abs/1711.01503
N2  - Rather than learning new control policies for each new task, it is possible, when tasks share some structure, to compose a &#34;meta-policy&#34; from previously learned policies. This paper reports results from experiments using Deep Reinforcement Learning on a continuous-state, discrete-action autonomous driving simulator. We explore how Deep Neural Networks can represent meta-policies that switch among a set of previously learned policies, specifically in settings where the dynamics of a new scenario are composed of a mixture of previously learned dynamics and where the state observation is possibly corrupted by sensing noise. We also report the results of experiments varying dynamics mixes, distractor policies, magnitudes/distributions of sensing noise, and obstacles. In a fully observed experiment, the meta-policy learning algorithm achieves 2.6x the reward achieved by the next best policy composition technique with 80% less exploration. In a partially observed experiment, the meta-policy learning algorithm converges after 50 iterations while a direct application of RL fails to converge even after 200 iterations.
ER  -


TY  - Preprint
T1  - The Case for Meta-Cognitive Machine Learning: On Model Entropy and Concept Formation in Deep Learning
A1  - Johan Loeckx
JO  - ArXiv e-prints
Y1  - 4 November, 2017
UR  - https://arxiv.org/abs/1711.01431
N2  - Machine learning is usually defined in behaviourist terms, where external validation is the primary mechanism of learning. In this paper, I argue for a more holistic interpretation in which finding more probable, efficient and abstract representations is as central to learning as performance. In other words, machine learning should be extended with strategies to reason over its own learning process, leading to so-called meta-cognitive machine learning. As such, the de facto definition of machine learning should be reformulated in these intrinsically multi-objective terms, taking into account not only the task performance but also internal learning objectives. To this end, we suggest a &#34;model entropy function&#34; to be defined that quantifies the efficiency of the internal learning processes. It is conjured that the minimization of this model entropy leads to concept formation. Besides philosophical aspects, some initial illustrations are included to support the claims.
ER  -


TY  - Preprint
T1  - Deep Stacking Networks for Low-Resource Chinese Word Segmentation with Transfer Learning
A1  - Jingjing Xu
A1  - Xu Sun
A1  - Sujian Li
A1  - Xiaoyan Cai
A1  - Bingzhen Wei
JO  - ArXiv e-prints
Y1  - 4 November, 2017
UR  - https://arxiv.org/abs/1711.01427
N2  - In recent years, neural networks have proven to be effective in Chinese word segmentation. However, this promising performance relies on large-scale training data. Neural networks with conventional architectures cannot achieve the desired results in low-resource datasets due to the lack of labelled training data. In this paper, we propose a deep stacking framework to improve the performance on word segmentation tasks with insufficient data by integrating datasets from diverse domains. Our framework consists of two parts, domain-based models and deep stacking networks. The domain-based models are used to learn knowledge from different datasets. The deep stacking networks are designed to integrate domain-based models. To reduce model conflicts, we innovatively add communication paths among models and design various structures of deep stacking networks, including Gaussian-based Stacking Networks, Concatenate-based Stacking Networks, Sequence-based Stacking Networks and Tree-based Stacking Networks. We conduct experiments on six low-resource datasets from various domains. Our proposed framework shows significant performance improvements on all datasets compared with several strong baselines.
ER  -


TY  - Preprint
T1  - Predicting Discharge Medications at Admission Time Based on Deep Learning
A1  - Yuan Yang
A1  - Pengtao Xie
A1  - Xin Gao
A1  - Carol Cheng
A1  - Christy Li
A1  - Hongbao Zhang
A1  - Eric Xing
JO  - ArXiv e-prints
Y1  - 5 December, 2017
UR  - https://arxiv.org/abs/1711.01386
N2  - Predicting discharge medications right after a patient being admitted is an important clinical decision, which provides physicians with guidance on what type of medication regimen to plan for and what possible changes on initial medication may occur during an inpatient stay. It also facilitates medication reconciliation process with easy detection of medication discrepancy at discharge time to improve patient safety. However, since the information available upon admission is limited and patients&#39; condition may evolve during an inpatient stay, these predictions could be a difficult decision for physicians to make. In this work, we investigate how to leverage deep learning technologies to assist physicians in predicting discharge medications based on information documented in the admission note. We build a convolutional neural network which takes an admission note as input and predicts the medications placed on the patient at discharge time. Our method is able to distill semantic patterns from unstructured and noisy texts, and is capable of capturing the pharmacological correlations among medications. We evaluate our method on 25K patient visits and compare with 4 strong baselines. Our methods demonstrate a 20% increase in macro-averaged F1 score than the best baseline.
ER  -


TY  - Preprint
T1  - Deep Learning-Based Dynamic Watermarking for Secure Signal Authentication in the Internet of Things
A1  - Aidin Ferdowsi
A1  - Walid Saad
JO  - ArXiv e-prints
Y1  - 3 November, 2017
UR  - https://arxiv.org/abs/1711.01306
N2  - Securing the Internet of Things (IoT) is a necessary milestone toward expediting the deployment of its applications and services. In particular, the functionality of the IoT devices is extremely dependent on the reliability of their message transmission. Cyber attacks such as data injection, eavesdropping, and man-in-the-middle threats can lead to security challenges. Securing IoT devices against such attacks requires accounting for their stringent computational power and need for low-latency operations. In this paper, a novel deep learning method is proposed for dynamic watermarking of IoT signals to detect cyber attacks. The proposed learning framework, based on a long short-term memory (LSTM) structure, enables the IoT devices to extract a set of stochastic features from their generated signal and dynamically watermark these features into the signal. This method enables the IoT&#39;s cloud center, which collects signals from the IoT devices, to effectively authenticate the reliability of the signals. Furthermore, the proposed method prevents complicated attack scenarios such as eavesdropping in which the cyber attacker collects the data from the IoT devices and aims to break the watermarking algorithm. Simulation results show that, with an attack detection delay of under 1 second the messages can be transmitted from IoT devices with an almost 100% reliability.
ER  -


TY  - Preprint
T1  - Compressing Word Embeddings via Deep Compositional Code Learning
A1  - Raphael Shu
A1  - Hideki Nakayama
JO  - ArXiv e-prints
Y1  - 17 November, 2017
UR  - https://arxiv.org/abs/1711.01068
N2  - Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.
ER  -


TY  - Preprint
T1  - In-Bed Pose Estimation: Deep Learning with Shallow Dataset
A1  - Shuangjun Liu
A1  - Yu Yin
A1  - Sarah Ostadabbas
JO  - ArXiv e-prints
Y1  - 7 July, 2018
UR  - https://arxiv.org/abs/1711.01005
N2  - Although human pose estimation for various computer vision (CV) applications has been studied extensively in the last few decades, yet in-bed pose estimation using camera-based vision methods has been ignored by the CV community because it is assumed to be identical to the general purpose pose estimation methods. However, in-bed pose estimation has its own specialized aspects and comes with specific challenges including the notable differences in lighting conditions throughout a day and also having different pose distribution from the common human surveillance viewpoint. In this paper, we demonstrate that these challenges significantly lessen the effectiveness of existing general purpose pose estimation models. In order to address the lighting variation challenge, infrared selective (IRS) image acquisition technique is proposed to provide uniform quality data under various lighting conditions. In addition, to deal with unconventional pose perspective, a 2-end histogram of oriented gradient (HOG) rectification method is presented. In this work, we explored the idea of employing a pre-trained convolutional neural network (CNN) model trained on large public datasets of general human poses and fine-tuning the model using our own shallow in-bed IRS dataset. We developed an IRS imaging system and collected IRS image data from several realistic life-size mannequins in a simulated hospital room environment. A pre-trained CNN called convolutional pose machine (CPM) was repurposed for in-bed pose estimation by fine-tuning its specific intermediate layers. Using the HOG rectification method, the pose estimation performance of CPM significantly improved by 26.4% in PCK0.1 criteria compared to the model without such rectification.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Resource Allocation in V2V Communications
A1  - Hao Ye
A1  - Geoffrey Ye Li
JO  - ArXiv e-prints
Y1  - 6 November, 2017
UR  - https://arxiv.org/abs/1711.00968
N2  - In this article, we develop a decentralized resource allocation mechanism for vehicle-to-vehicle (V2V) communication systems based on deep reinforcement learning. Each V2V link is considered as an agent, making its own decisions to find optimal sub-band and power level for transmission. Since the proposed method is decentralized, the global information is not required for each agent to make its decisions, hence the transmission overhead is small. From the simulation results, each agent can learn how to satisfy the V2V constraints while minimizing the interference to vehicle-to-infrastructure (V2I) communications.
ER  -


TY  - Preprint
T1  - Deep Active Learning over the Long Tail
A1  - Yonatan Geifman
A1  - Ran El-Yaniv
JO  - ArXiv e-prints
Y1  - 2 November, 2017
UR  - https://arxiv.org/abs/1711.00941
N2  - This paper is concerned with pool-based active learning for deep neural networks. Motivated by coreset dataset compression ideas, we present a novel active learning algorithm that queries consecutive points from the pool using farthest-first traversals in the space of neural activation over a representation layer. We show consistent and overwhelming improvement in sample complexity over passive learning (random sampling) for three datasets: MNIST, CIFAR-10, and CIFAR-100. In addition, our algorithm outperforms the traditional uncertainty sampling technique (obtained using softmax activations), and we identify cases where uncertainty sampling is only slightly better than random sampling.
ER  -


TY  - Preprint
T1  - Deep Air Learning: Interpolation, Prediction, and Feature Analysis of Fine-grained Air Quality
A1  - Zhongang Qi
A1  - Tianchun Wang
A1  - Guojie Song
A1  - Weisong Hu
A1  - Xi Li
A1  -  Zhongfei
A1  -  Zhang
JO  - ArXiv e-prints
Y1  - 10 April, 2018
UR  - https://arxiv.org/abs/1711.00939
N2  - The interpolation, prediction, and feature analysis of fine-gained air quality are three important topics in the area of urban air computing. The solutions to these topics can provide extremely useful information to support air pollution control, and consequently generate great societal and technical impacts. Most of the existing work solves the three problems separately by different models. In this paper, we propose a general and effective approach to solve the three problems in one model called the Deep Air Learning (DAL). The main idea of DAL lies in embedding feature selection and semi-supervised learning in different layers of the deep learning network. The proposed approach utilizes the information pertaining to the unlabeled spatio-temporal data to improve the performance of the interpolation and the prediction, and performs feature selection and association analysis to reveal the main relevant features to the variation of the air quality. We evaluate our approach with extensive experiments based on real data sources obtained in Beijing, China. Experiments show that DAL is superior to the peer models from the recent literature when solving the topics of interpolation, prediction, and feature analysis of fine-gained air quality.
ER  -


TY  - Preprint
T1  - Using Deep Learning to Examine the Association between the Built Environment and Neighborhood Adult Obesity Prevalence
A1  - Adyasha Maharana
A1  - Elaine O. Nsoesie
JO  - ArXiv e-prints
Y1  - 2 November, 2017
UR  - https://arxiv.org/abs/1711.00885
N2  - More than one-third of the adult population in the United States is obese. Obesity has been linked to factors such as, genetics, diet, physical activity and the environment. However, evidence indicating associations between the built environment and obesity has varied across studies and geographical contexts. Here, we used deep learning and approximately 150,000 high resolution satellite images to extract features of the built environment. We then developed linear regression models to consistently quantify the association between the extracted features and obesity prevalence at the census tract level for six cities in the United States. The extracted features of the built environment explained 72% to 90% of the variation in obesity prevalence across cities. Outof-sample predictions were considerably high with correlations greater than 80% between predicted and true obesity prevalence across all census tracts. This study supports a strong association between the built environment and obesity prevalence. Additionally, it also illustrates that features of the built environment extracted from satellite images can be useful for studying health indicators, such as obesity. Understanding the association between specific features of the built environment and obesity prevalence can lead to structural changes that could encourage physical activity and decreases in obesity prevalence.
ER  -


TY  - Preprint
T1  - Deep Learning from Noisy Image Labels with Quality Embedding
A1  - Jiangchao Yao
A1  - Jiajie Wang
A1  - Ivor Tsang
A1  - Ya Zhang
A1  - Jun Sun
A1  - Chengqi Zhang
A1  - Rui Zhang
JO  - ArXiv e-prints
Y1  - 1 November, 2017
UR  - https://arxiv.org/abs/1711.00583
N2  - There is an emerging trend to leverage noisy image datasets in many visual recognition tasks. However, the label noise among the datasets severely degenerates the \mbox{performance of deep} learning approaches. Recently, one mainstream is to introduce the latent label to handle label noise, which has shown promising improvement in the network designs. Nevertheless, the mismatch between latent labels and noisy labels still affects the predictions in such methods. To address this issue, we propose a quality embedding model, which explicitly introduces a quality variable to represent the trustworthiness of noisy labels. Our key idea is to identify the mismatch between the latent and noisy labels by embedding the quality variables into different subspaces, which effectively minimizes the noise effect. At the same time, the high-quality labels is still able to be applied for training. To instantiate the model, we further propose a Contrastive-Additive Noise network (CAN), which consists of two important layers: (1) the contrastive layer estimates the quality variable in the embedding space to reduce noise effect; and (2) the additive layer aggregates the prior predictions and noisy labels as the posterior to train the classifier. Moreover, to tackle the optimization difficulty, we deduce an SGD algorithm with the reparameterization tricks, which makes our method scalable to big data. We conduct the experimental evaluation of the proposed method over a range of noisy image datasets. Comprehensive results have demonstrated CAN outperforms the state-of-the-art deep learning approaches.
ER  -


TY  - Preprint
T1  - Active Clothing Material Perception using Tactile Sensing and Deep Learning
A1  - Wenzhen Yuan
A1  - Yuchen Mo
A1  - Shaoxiong Wang
A1  - Edward Adelson
JO  - ArXiv e-prints
Y1  - 25 February, 2018
UR  - https://arxiv.org/abs/1711.00574
N2  - Humans represent and discriminate the objects in the same category using their properties, and an intelligent robot should be able to do the same. In this paper, we build a robot system that can autonomously perceive the object properties through touch. We work on the common object category of clothing. The robot moves under the guidance of an external Kinect sensor, and squeezes the clothes with a GelSight tactile sensor, then it recognizes the 11 properties of the clothing according to the tactile data. Those properties include the physical properties, like thickness, fuzziness, softness and durability, and semantic properties, like wearing season and preferred washing methods. We collect a dataset of 153 varied pieces of clothes, and conduct 6616 robot exploring iterations on them. To extract the useful information from the high-dimensional sensory output, we applied Convolutional Neural Networks (CNN) on the tactile data for recognizing the clothing properties, and on the Kinect depth images for selecting exploration locations. Experiments show that using the trained neural networks, the robot can autonomously explore the unknown clothes and learn their properties. This work proposes a new framework for active tactile perception system with vision-touch system, and has potential to enable robots to help humans with varied clothing related housework.
ER  -


TY  - Preprint
T1  - Full-info Training for Deep Speaker Feature Learning
A1  - Lantian Li
A1  - Zhiyuan Tang
A1  - Dong Wang
A1  - Thomas Fang Zheng
JO  - ArXiv e-prints
Y1  - 27 February, 2018
UR  - https://arxiv.org/abs/1711.00366
N2  - In recent studies, it has shown that speaker patterns can be learned from very short speech segments (e.g., 0.3 seconds) by a carefully designed convolutional &amp; time-delay deep neural network (CT-DNN) model. By enforcing the model to discriminate the speakers in the training data, frame-level speaker features can be derived from the last hidden layer. In spite of its good performance, a potential problem of the present model is that it involves a parametric classifier, i.e., the last affine layer, which may consume some discriminative knowledge, thus leading to `information leak&#39; for the feature learning. This paper presents a full-info training approach that discards the parametric classifier and enforces all the discriminative knowledge learned by the feature net. Our experiments on the Fisher database demonstrate that this new training scheme can produce more coherent features, leading to consistent and notable performance improvement on the speaker verification task.
ER  -


TY  - Preprint
T1  - Paraphrase Generation with Deep Reinforcement Learning
A1  - Zichao Li
A1  - Xin Jiang
A1  - Lifeng Shang
A1  - Hang Li
JO  - ArXiv e-prints
Y1  - 23 August, 2018
UR  - https://arxiv.org/abs/1711.00279
N2  - Automatic generation of paraphrases from a given sentence is an important yet challenging task in natural language processing (NLP), and plays a key role in a number of applications such as question answering, search, and dialogue. In this paper, we present a deep reinforcement learning approach to paraphrase generation. Specifically, we propose a new framework for the task, which consists of a \textit{generator} and an \textit{evaluator}, both of which are learned from data. The generator, built as a sequence-to-sequence learning model, can produce paraphrases given a sentence. The evaluator, constructed as a deep matching model, can judge whether two sentences are paraphrases of each other. The generator is first trained by deep learning and then further fine-tuned by reinforcement learning in which the reward is given by the evaluator. For the learning of the evaluator, we propose two methods based on supervised learning and inverse reinforcement learning respectively, depending on the type of available training data. Empirical study shows that the learned evaluator can guide the generator to produce more accurate paraphrases. Experimental results demonstrate the proposed models (the generators) outperform the state-of-the-art methods in paraphrase generation in both automatic evaluation and human evaluation.
ER  -


TY  - Preprint
T1  - Acquiring Target Stacking Skills by Goal-Parameterized Deep Reinforcement Learning
A1  - Wenbin Li
A1  - Jeannette Bohg
A1  - Mario Fritz
JO  - ArXiv e-prints
Y1  - 22 November, 2017
UR  - https://arxiv.org/abs/1711.00267
N2  - Understanding physical phenomena is a key component of human intelligence and enables physical interaction with previously unseen environments. In this paper, we study how an artificial agent can autonomously acquire this intuition through interaction with the environment. We created a synthetic block stacking environment with physics simulation in which the agent can learn a policy end-to-end through trial and error. Thereby, we bypass to explicitly model physical knowledge within the policy. We are specifically interested in tasks that require the agent to reach a given goal state that may be different for every new trial. To this end, we propose a deep reinforcement learning framework that learns policies which are parametrized by a goal. We validated the model on a toy example navigating in a grid world with different target positions and in a block stacking task with different target structures of the final tower. In contrast to prior work, our policies show better generalization across different goals.
ER  -


TY  - Preprint
T1  - Learning deep features for source color laser printer identification based on cascaded learning
A1  - Do-Guk Kim
A1  - Jong-Uk Hou
A1  - Heung-Kyu Lee
JO  - ArXiv e-prints
Y1  - 1 November, 2017
UR  - https://arxiv.org/abs/1711.00207
N2  - Color laser printers have fast printing speed and high resolution, and forgeries using color laser printers can cause significant harm to society. A source printer identification technique can be employed as a countermeasure to those forgeries. This paper presents a color laser printer identification method based on cascaded learning of deep neural networks. The refiner network is trained by adversarial training to refine the synthetic dataset for halftone color decomposition. The halftone color decomposing ConvNet is trained with the refined dataset, and the trained knowledge is transferred to the printer identifying ConvNet to enhance the accuracy. The robustness about rotation and scaling is considered in training process, which is not considered in existing methods. Experiments are performed on eight color laser printers, and the performance is compared with several existing methods. The experimental results clearly show that the proposed method outperforms existing source color laser printer identification methods.
ER  -


TY  - Preprint
T1  - A multitask deep learning model for real-time deployment in embedded systems
A1  - Miquel MartÃ­
A1  - Atsuto Maki
JO  - ArXiv e-prints
Y1  - 31 October, 2017
UR  - https://arxiv.org/abs/1711.00146
N2  - We propose an approach to Multitask Learning (MTL) to make deep learning models faster and lighter for applications in which multiple tasks need to be solved simultaneously, which is particularly useful in embedded, real-time systems. We develop a multitask model for both Object Detection and Semantic Segmentation and analyze the challenges that appear during its training. Our multitask network is 1.6x faster, lighter and uses less memory than deploying the single-task models in parallel. We conclude that MTL has the potential to give superior performance in exchange of a more complex training process that introduces challenges not present in single-task models.
ER  -


TY  - Preprint
T1  - Multi-Task Learning by Deep Collaboration and Application in Facial Landmark Detection
A1  - Ludovic Trottier
A1  - Philippe GiguÃ¨re
A1  - Brahim Chaib-draa
JO  - ArXiv e-prints
Y1  - 15 March, 2018
UR  - https://arxiv.org/abs/1711.00111
N2  - Convolutional neural networks (CNNs) have become the most successful approach in many vision-related domains. However, they are limited to domains where data is abundant. Recent works have looked at multi-task learning (MTL) to mitigate data scarcity by leveraging domain-specific information from related tasks. In this paper, we present a novel soft-parameter sharing mechanism for CNNs in a MTL setting, which we refer to as Deep Collaboration. We propose taking into account the notion that task relevance depends on depth by using lateral transformation blocs with skip connections. This allows extracting task-specific features at various depth without sacrificing features relevant to all tasks. We show that CNNs connected with our Deep Collaboration obtain better accuracy on facial landmark detection with related tasks. We finally verify that our approach effectively allows knowledge sharing by showing depth-specific influence of tasks that we know are related.
ER  -


TY  - Preprint
T1  - Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering
A1  - Elliot Meyerson
A1  - Risto Miikkulainen
JO  - ArXiv e-prints
Y1  - 12 February, 2018
UR  - https://arxiv.org/abs/1711.00108
N2  - Existing deep multitask learning (MTL) approaches align layers shared between tasks in a parallel ordering. Such an organization significantly constricts the types of shared structure that can be learned. The necessity of parallel ordering for deep MTL is first tested by comparing it with permuted ordering of shared layers. The results indicate that a flexible ordering can enable more effective sharing, thus motivating the development of a soft ordering approach, which learns how shared layers are applied in different ways for different tasks. Deep MTL with soft ordering outperforms parallel ordering methods across a series of domains. These results suggest that the power of deep MTL comes from learning highly general building blocks that can be assembled to meet the demands of each task.
ER  -


TY  - Preprint
T1  - Separation of Water and Fat Magnetic Resonance Imaging Signals Using Deep Learning with Convolutional Neural Networks
A1  - James W Goldfarb
JO  - ArXiv e-prints
Y1  - 27 October, 2017
UR  - https://arxiv.org/abs/1711.00107
N2  - Purpose: A new method for magnetic resonance (MR) imaging water-fat separation using a convolutional neural network (ConvNet) and deep learning (DL) is presented. Feasibility of the method with complex and magnitude images is demonstrated with a series of patient studies and accuracy of predicted quantitative values is analyzed.
ER  -


TY  - Preprint
T1  - Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm
A1  - Chelsea Finn
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 14 February, 2018
UR  - https://arxiv.org/abs/1710.11622
N2  - Learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. A popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs. Alternatively, a more recent approach to meta-learning aims to acquire deep representations that can be effectively fine-tuned, via standard gradient descent, to new tasks. In this paper, we consider the meta-learning problem from the perspective of universality, formalizing the notion of learning algorithm approximation and comparing the expressive power of the aforementioned recurrent models to the more recent approaches that embed gradient descent into the meta-learner. In particular, we seek to answer the following question: does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning algorithm? We find that this is indeed true, and further find, in our experiments, that gradient-based meta-learning consistently leads to learning strategies that generalize more widely compared to those represented by recurrent models.
ER  -


TY  - Preprint
T1  - Deep Learning as a Mixed Convex-Combinatorial Optimization Problem
A1  - Abram L. Friesen
A1  - Pedro Domingos
JO  - ArXiv e-prints
Y1  - 16 April, 2018
UR  - https://arxiv.org/abs/1710.11573
N2  - As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning. However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn networks of them in a principled way. We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such. The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve. Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches. Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case. Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator.
ER  -


TY  - Preprint
T1  - Regret Minimization for Partially Observable Deep Reinforcement Learning
A1  - Peter H. Jin
A1  - Sergey Levine
A1  - Kurt Keutzer
JO  - ArXiv e-prints
Y1  - 31 October, 2017
UR  - https://arxiv.org/abs/1710.11424
N2  - Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state. We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (MalmÃ¶) first-person navigation benchmarks.
ER  -


TY  - Preprint
T1  - TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning
A1  - Gregory Farquhar
A1  - Tim RocktÃ¤schel
A1  - Maximilian Igl
A1  - Shimon Whiteson
JO  - ArXiv e-prints
Y1  - 8 March, 2018
UR  - https://arxiv.org/abs/1710.11417
N2  - Combining deep model-free reinforcement learning with on-line planning is a promising approach to building on the successes of deep RL. On-line planning with look-ahead trees has proven successful in environments where transition models are known a priori. However, in complex environments where transition models need to be learned from data, the deficiencies of learned models have limited their utility for planning. To address these challenges, we propose TreeQN, a differentiable, recursive, tree-structured model that serves as a drop-in replacement for any value function network in deep RL with discrete actions. TreeQN dynamically constructs a tree by recursively applying a transition model in a learned abstract state space and then aggregating predicted rewards and state-values using a tree backup to estimate Q-values. We also propose ATreeC, an actor-critic variant that augments TreeQN with a softmax layer to form a stochastic policy network. Both approaches are trained end-to-end, such that the learned model is optimised for its actual use in the tree. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a box-pushing task, as well as n-step DQN and value prediction networks (Oh et al. 2017) on multiple Atari games. Furthermore, we present ablation studies that demonstrate the effect of different auxiliary losses on learning transition models.
ER  -


TY  - Preprint
T1  - ChainerMN: Scalable Distributed Deep Learning Framework
A1  - Takuya Akiba
A1  - Keisuke Fukuda
A1  - Shuji Suzuki
JO  - ArXiv e-prints
Y1  - 31 October, 2017
UR  - https://arxiv.org/abs/1710.11351
N2  - One of the keys for deep learning to have made a breakthrough in various fields was to utilize high computing powers centering around GPUs. Enabling the use of further computing abilities by distributed processing is essential not only to make the deep learning bigger and faster but also to tackle unsolved challenges. We present the design, implementation, and evaluation of ChainerMN, the distributed deep learning framework we have developed. We demonstrate that ChainerMN can scale the learning process of the ResNet-50 model to the ImageNet dataset up to 128 GPUs with the parallel efficiency of 90%.
ER  -


TY  - Preprint
T1  - Resolution and Relevance Trade-offs in Deep Learning
A1  - Juyong Song
A1  - Matteo Marsili
A1  - Junghyo Jo
JO  - ArXiv e-prints
Y1  - 19 March, 2018
UR  - https://arxiv.org/abs/1710.11324
N2  - Deep learning has been successfully applied to various tasks, but its underlying mechanism remains unclear. Neural networks associate similar inputs in the visible layer to the same state of hidden variables in deep layers. The fraction of inputs that are associated to the same state is a natural measure of similarity and is simply related to the cost in bits required to represent these inputs. The degeneracy of states with the same information cost provides instead a natural measure of noise and is simply related the entropy of the frequency of states, that we call relevance. Representations with minimal noise, at a given level of similarity (resolution), are those that maximise the relevance. A signature of such efficient representations is that frequency distributions follow power laws. We show, in extensive numerical experiments, that deep neural networks extract a hierarchy of efficient representations from data, because they i) achieve low levels of noise (i.e. high relevance) and ii) exhibit power law distributions. We also find that the layer that is most efficient to reliably generate patterns of training data is the one for which relevance and resolution are traded at the same price, which implies that frequency distribution follows Zipf&#39;s law.
ER  -


TY  - Preprint
T1  - Deep Learning and Conditional Random Fields-based Depth Estimation and Topographical Reconstruction from Conventional Endoscopy
A1  - Faisal Mahmood
A1  - Nicholas J. Durr
JO  - ArXiv e-prints
Y1  - 27 November, 2017
UR  - https://arxiv.org/abs/1710.11216
N2  - Colorectal cancer is the fourth leading cause of cancer deaths worldwide and the second leading cause in the United States. The risk of colorectal cancer can be mitigated by the identification and removal of premalignant lesions through optical colonoscopy. Unfortunately, conventional colonoscopy misses more than 20% of the polyps that should be removed, due in part to poor contrast of lesion topography. Imaging tissue topography during a colonoscopy is difficult because of the size constraints of the endoscope and the deforming mucosa. Most existing methods make geometric assumptions or incorporate a priori information, which limits accuracy and sensitivity. In this paper, we present a method that avoids these restrictions, using a joint deep convolutional neural network-conditional random field (CNN-CRF) framework. Estimated depth is used to reconstruct the topography of the surface of the colon from a single image. We train the unary and pairwise potential functions of a CRF in a CNN on synthetic data, generated by developing an endoscope camera model and rendering over 100,000 images of an anatomically-realistic colon. We validate our approach with real endoscopy images from a porcine colon, transferred to a synthetic-like domain, with ground truth from registered computed tomography measurements. The CNN-CRF approach estimates depths with a relative error of 0.152 for synthetic endoscopy images and 0.242 for real endoscopy images. We show that the estimated depth maps can be used for reconstructing the topography of the mucosa from conventional colonoscopy images. This approach can easily be integrated into existing endoscopy systems and provides a foundation for improving computer-aided detection algorithms for detection, segmentation and classification of lesions.
ER  -


TY  - Preprint
T1  - How deep learning works --The geometry of deep learning
A1  - Xiao Dong
A1  - Jiasong Wu
A1  - Ling Zhou
JO  - ArXiv e-prints
Y1  - 30 October, 2017
UR  - https://arxiv.org/abs/1710.10784
N2  - Why and how that deep learning works well on different tasks remains a mystery from a theoretical perspective. In this paper we draw a geometric picture of the deep learning system by finding its analogies with two existing geometric structures, the geometry of quantum computations and the geometry of the diffeomorphic template matching. In this framework, we give the geometric structures of different deep learning systems including convolutional neural networks, residual networks, recursive neural networks, recurrent neural networks and the equilibrium prapagation framework. We can also analysis the relationship between the geometrical structures and their performance of different networks in an algorithmic level so that the geometric framework may guide the design of the structures and algorithms of deep learning systems.
ER  -


TY  - Preprint
T1  - Predicting Head Movement in Panoramic Video: A Deep Reinforcement Learning Approach
A1  - Yuhang Song
A1  - Mai Xu
A1  - Minglang Qiao
A1  - Jianyi Wang
A1  - Liangyu Huo
A1  - Zulin Wang
JO  - ArXiv e-prints
Y1  - 20 September, 2018
UR  - https://arxiv.org/abs/1710.10755
N2  - Panoramic video provides immersive and interactive experience by enabling humans to control the field of view (FoV) through head movement (HM). Thus, HM plays a key role in modeling human attention on panoramic video. This paper establishes a database collecting subjects&#39; HM in panoramic video sequences. From this database, we find that the HM data are highly consistent across subjects. Furthermore, we find that deep reinforcement learning (DRL) can be applied to predict HM positions, via maximizing the reward of imitating human HM scanpaths through the agent&#39;s actions. Based on our findings, we propose a DRL-based HM prediction (DHP) approach with offline and online versions, called offline-DHP and online-DHP. In offline-DHP, multiple DRL workflows are run to determine potential HM positions at each panoramic frame. Then, a heat map of the potential HM positions, named the HM map, is generated as the output of offline-DHP. In online-DHP, the next HM position of one subject is estimated given the currently observed HM position, which is achieved by developing a DRL algorithm upon the learned offline-DHP model. Finally, the experiments validate that our approach is effective in both offline and online prediction of HM positions for panoramic video, and that the learned offline-DHP model can improve the performance of online-DHP.
ER  -


TY  - Preprint
T1  - On Pre-Trained Image Features and Synthetic Images for Deep Learning
A1  - Stefan Hinterstoisser
A1  - Vincent Lepetit
A1  - Paul Wohlhart
A1  - Kurt Konolige
JO  - ArXiv e-prints
Y1  - 16 November, 2017
UR  - https://arxiv.org/abs/1710.10710
N2  - Deep Learning methods usually require huge amounts of training data to perform at their full potential, and often require expensive manual labeling. Using synthetic images is therefore very attractive to train object detectors, as the labeling comes for free, and several approaches have been proposed to combine synthetic and real images for training.
ER  -


TY  - Preprint
T1  - Regularization for Deep Learning: A Taxonomy
A1  - Jan KukaÄka
A1  - Vladimir Golkov
A1  - Daniel Cremers
JO  - ArXiv e-prints
Y1  - 29 October, 2017
UR  - https://arxiv.org/abs/1710.10686
N2  - Regularization is one of the crucial ingredients of deep learning, yet the term regularization has various definitions, and regularization methods are often studied separately from each other. In our work we present a systematic, unifying taxonomy to categorize existing methods. We distinguish methods that affect data, network architectures, error terms, regularization terms, and optimization procedures. We do not provide all details about the listed methods; instead, we present an overview of how the methods can be sorted into meaningful categories and sub-categories. This helps revealing links and fundamental similarities between them. Finally, we include practical recommendations both for users and for developers of new regularization methods.
ER  -


TY  - Preprint
T1  - Automatic Knee Osteoarthritis Diagnosis from Plain Radiographs: A Deep Learning-Based Approach
A1  - Aleksei Tiulpin
A1  - JÃ©rÃ´me Thevenot
A1  - Esa Rahtu
A1  - Petri Lehenkari
A1  - Simo Saarakkala
JO  - ArXiv e-prints
Y1  - 29 October, 2017
UR  - https://arxiv.org/abs/1710.10589
N2  - Knee osteoarthritis (OA) is the most common musculoskeletal disorder. OA diagnosis is currently conducted by assessing symptoms and evaluating plain radiographs, but this process suffers from subjectivity. In this study, we present a new transparent computer-aided diagnosis method based on the Deep Siamese Convolutional Neural Network to automatically score knee OA severity according to the Kellgren-Lawrence grading scale. We trained our method using the data solely from the Multicenter Osteoarthritis Study and validated it on randomly selected 3,000 subjects (5,960 knees) from Osteoarthritis Initiative dataset. Our method yielded a quadratic Kappa coefficient of 0.83 and average multiclass accuracy of 66.71\% compared to the annotations given by a committee of clinical experts. Here, we also report a radiological OA diagnosis area under the ROC curve of 0.93. We also present attention maps -- given as a class probability distribution -- highlighting the radiological features affecting the network decision. This information makes the decision process transparent for the practitioner, which builds better trust toward automatic methods. We believe that our model is useful for clinical decision making and for OA research; therefore, we openly release our training codes and the data set created in this study.
ER  -


TY  - Preprint
T1  - A Bayesian Data Augmentation Approach for Learning Deep Models
A1  - Toan Tran
A1  - Trung Pham
A1  - Gustavo Carneiro
A1  - Lyle Palmer
A1  - Ian Reid
JO  - ArXiv e-prints
Y1  - 29 October, 2017
UR  - https://arxiv.org/abs/1710.10564
N2  - Data augmentation is an essential part of the training process applied to deep learning models. The motivation is that a robust training process for deep learning models depends on large annotated datasets, which are expensive to be acquired, stored and processed. Therefore a reasonable alternative is to be able to automatically generate new annotated training samples using a process known as data augmentation. The dominant data augmentation approach in the field assumes that new training samples can be obtained via random geometric or appearance transformations applied to annotated training samples, but this is a strong assumption because it is unclear if this is a reliable generative model for producing new training samples. In this paper, we provide a novel Bayesian formulation to data augmentation, where new annotated training points are treated as missing variables and generated based on the distribution learned from the training set. For learning, we introduce a theoretically sound algorithm --- generalised Monte Carlo expectation maximisation, and demonstrate one possible implementation via an extension of the Generative Adversarial Network (GAN). Classification results on MNIST, CIFAR-10 and CIFAR-100 show the better performance of our proposed method compared to the current dominant data augmentation approach mentioned above --- the results also show that our approach produces better classification results than similar GAN models.
ER  -


TY  - Preprint
T1  - Topic Based Sentiment Analysis Using Deep Learning
A1  - Sharath T. S.
A1  - Shubhangi Tandon
JO  - ArXiv e-prints
Y1  - 28 October, 2017
UR  - https://arxiv.org/abs/1710.10498
N2  - In this paper , we tackle Sentiment Analysis conditioned on a Topic in Twitter data using Deep Learning . We propose a 2-tier approach : In the first phase we create our own Word Embeddings and see that they do perform better than state-of-the-art embeddings when used with standard classifiers. We then perform inference on these embeddings to learn more about a word with respect to all the topics being considered, and also the top n-influencing words for each topic. In the second phase we use these embeddings to predict the sentiment of the tweet with respect to a given topic, and all other topics under discussion.
ER  -


TY  - Preprint
T1  - Label Embedding Network: Learning Label Representation for Soft Training of Deep Networks
A1  - Xu Sun
A1  - Bingzhen Wei
A1  - Xuancheng Ren
A1  - Shuming Ma
JO  - ArXiv e-prints
Y1  - 28 October, 2017
UR  - https://arxiv.org/abs/1710.10393
N2  - We propose a method, called Label Embedding Network, which can learn label representation (label embedding) during the training process of deep networks. With the proposed method, the label embedding is adaptively and automatically learned through back propagation. The original one-hot represented loss function is converted into a new loss function with soft distributions, such that the originally unrelated labels have continuous interactions with each other during the training process. As a result, the trained model can achieve substantially higher accuracy and with faster convergence speed. Experimental results based on competitive tasks demonstrate the effectiveness of the proposed method, and the learned label embedding is reasonable and interpretable. The proposed method achieves comparable or even better results than the state-of-the-art systems. The source code is available at \url{https://github.com/lancopku/LabelEmb}.
ER  -


TY  - Preprint
T1  - Deep Generative Dual Memory Network for Continual Learning
A1  - Nitin Kamra
A1  - Umang Gupta
A1  - Yan Liu
JO  - ArXiv e-prints
Y1  - 25 May, 2018
UR  - https://arxiv.org/abs/1710.10368
N2  - Despite advances in deep learning, neural networks can only learn multiple tasks when trained on them jointly. When tasks arrive sequentially, they lose performance on previously learnt tasks. This phenomenon called catastrophic forgetting is a fundamental challenge to overcome before neural networks can learn continually from incoming data. In this work, we derive inspiration from human memory to develop an architecture capable of learning continuously from sequentially incoming tasks, while averting catastrophic forgetting. Specifically, our contributions are: (i) a dual memory architecture emulating the complementary learning systems (hippocampus and the neocortex) in the human brain, (ii) memory consolidation via generative replay of past experiences, (iii) demonstrating advantages of generative replay and dual memories via experiments, and (iv) improved performance retention on challenging tasks even for low capacity models. Our architecture displays many characteristics of the mammalian memory and provides insights on the connection between sleep and learning.
ER  -


TY  - Preprint
T1  - Diff-DAC: Distributed Actor-Critic for Average Multitask Deep Reinforcement Learning
A1  - Sergio Valcarcel Macua
A1  - Aleksi Tukiainen
A1  - Daniel GarcÃ­a-OcaÃ±a HernÃ¡ndez
A1  - David Baldazo
A1  - Enrique Munoz de Cote
A1  - Santiago Zazo
JO  - ArXiv e-prints
Y1  - 22 April, 2018
UR  - https://arxiv.org/abs/1710.10363
N2  - We propose a fully distributed actor-critic algorithm approximated by deep neural networks, named \textit{Diff-DAC}, with application to single-task and to average multitask reinforcement learning (MRL). Each agent has access to data from its local task only, but it aims to learn a policy that performs well on average for the whole set of tasks. During the learning process, agents communicate their value-policy parameters to their neighbors, diffusing the information across the network, so that they converge to a common policy, with no need for a central node. The method is scalable, since the computational and communication costs per agent grow with its number of neighbors. We derive Diff-DAC&#39;s from duality theory and provide novel insights into the standard actor-critic framework, showing that it is actually an instance of the dual ascent method that approximates the solution of a linear program. Experiments suggest that Diff-DAC can outperform the single previous distributed MRL approach (i.e., Dist-MTLPS) and even the centralized architecture.
ER  -


TY  - Preprint
T1  - Deep Residual Learning for Small-Footprint Keyword Spotting
A1  - Raphael Tang
A1  - Jimmy Lin
JO  - ArXiv e-prints
Y1  - 21 September, 2018
UR  - https://arxiv.org/abs/1710.10361
N2  - We explore the application of deep residual learning and dilated convolutions to the keyword spotting task, using the recently-released Google Speech Commands Dataset as our benchmark. Our best residual network (ResNet) implementation significantly outperforms Google&#39;s previous convolutional neural networks in terms of accuracy. By varying model depth and width, we can achieve compact models that also outperform previous small-footprint variants. To our knowledge, we are the first to examine these approaches for keyword spotting, and our results establish an open-source state-of-the-art reference to support the development of future speech-based interfaces.
ER  -


TY  - Preprint
T1  - Generalization Tower Network: A Novel Deep Neural Network Architecture for Multi-Task Learning
A1  - Yuhang Song
A1  - Main Xu
A1  - Songyang Zhang
A1  - Liangyu Huo
JO  - ArXiv e-prints
Y1  - 1 January, 2018
UR  - https://arxiv.org/abs/1710.10036
N2  - Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by incorporating deep neural networks in learning representations from the input to RL. However, the conventional deep neural network architecture is limited in learning representations for multi-task RL (MT-RL), as multiple tasks can refer to different kinds of representations. In this paper, we thus propose a novel deep neural network architecture, namely generalization tower network (GTN), which can achieve MT-RL within a single learned model. Specifically, the architecture of GTN is composed of both horizontal and vertical streams. In our GTN architecture, horizontal streams are used to learn representation shared in similar tasks. In contrast, the vertical streams are introduced to be more suitable for handling diverse tasks, which encodes hierarchical shared knowledge of these tasks. The effectiveness of the introduced vertical stream is validated by experimental results. Experimental results further verify that our GTN architecture is able to advance the state-of-the-art MT-RL, via being tested on 51 Atari games.
ER  -


TY  - Preprint
T1  - Deep Learning for Accelerated Ultrasound Imaging
A1  - Yeo Hun Yoon
A1  - Jong Chul Ye
JO  - ArXiv e-prints
Y1  - 27 October, 2017
UR  - https://arxiv.org/abs/1710.10006
N2  - In portable, 3-D, or ultra-fast ultrasound (US) imaging systems, there is an increasing demand to reconstruct high quality images from limited number of data. However, the existing solutions require either hardware changes or computationally expansive algorithms. To overcome these limitations, here we propose a novel deep learning approach that interpolates the missing RF data by utilizing the sparsity of the RF data in the Fourier domain. Extensive experimental results from sub-sampled RF data from a real US system confirmed that the proposed method can effectively reduce the data rate without sacrificing the image quality.
ER  -


TY  - Preprint
T1  - Improving Deep Learning by Inverse Square Root Linear Units (ISRLUs)
A1  - Brad Carlile
A1  - Guy Delamarter
A1  - Paul Kinney
A1  - Akiko Marti
A1  - Brian Whitney
JO  - ArXiv e-prints
Y1  - 9 November, 2017
UR  - https://arxiv.org/abs/1710.09967
N2  - We introduce the &#34;inverse square root linear unit&#34; (ISRLU) to speed up learning in deep neural networks. ISRLU has better performance than ELU but has many of the same benefits. ISRLU and ELU have similar curves and characteristics. Both have negative values, allowing them to push mean unit activation closer to zero, and bring the normal gradient closer to the unit natural gradient, ensuring a noise-robust deactivation state, lessening the over fitting risk. The significant performance advantage of ISRLU on traditional CPUs also carry over to more efficient HW implementations on HW/SW codesign for CNNs/RNNs. In experiments with TensorFlow, ISRLU leads to faster learning and better generalization than ReLU on CNNs. This work also suggests a computationally efficient variant called the &#34;inverse square root unit&#34; (ISRU) which can be used for RNNs. Many RNNs use either long short-term memory (LSTM) and gated recurrent units (GRU) which are implemented with tanh and sigmoid activation functions. ISRU has less com- putational complexity but still has a similar curve to tanh and sigmoid.
ER  -


TY  - Preprint
T1  - A Deep Learning Approach to the Citywide Traffic Accident Risk Prediction
A1  - Honglei Ren
A1  - You Song
A1  - Jingwen Wang
A1  - Yucheng Hu
A1  - Jinzhi Lei
JO  - ArXiv e-prints
Y1  - 15 April, 2018
UR  - https://arxiv.org/abs/1710.09543
N2  - With the rapid development of urbanization, the boom of vehicle numbers has resulted in serious traffic accidents, which led to casualties and huge economic losses. The ability to predict the risk of traffic accident is important in the prevention of the occurrence of accidents and to reduce the damages caused by accidents in a proactive way. However, traffic accident risk prediction with high spatiotemporal resolution is difficult, mainly due to the complex traffic environment, human behavior, and lack of real-time traffic-related data. In this study, we collected big traffic accident data. By analyzing the spatial and temporal patterns of traffic accident frequency, we presented the spatiotemporal correlation of traffic accidents. Based on the patterns we found in analysis, we proposed a high accurate deep learning model based on recurrent neural network toward the prediction of traffic accident risk. The predictive accident risk can be potential applied to the traffic accident warning system. The proposed method can be integrated into an intelligent traffic control system toward a more reasonable traffic prediction and command organization.
ER  -


TY  - Preprint
T1  - Maximum Principle Based Algorithms for Deep Learning
A1  - Qianxiao Li
A1  - Long Chen
A1  - Cheng Tai
A1  - Weinan E
JO  - ArXiv e-prints
Y1  - 2 June, 2018
UR  - https://arxiv.org/abs/1710.09513
N2  - The continuous dynamical system approach to deep learning is explored in order to devise alternative frameworks for training algorithms. Training is recast as a control problem and this allows us to formulate necessary optimality conditions in continuous time using the Pontryagin&#39;s maximum principle (PMP). A modification of the method of successive approximations is then used to solve the PMP, giving rise to an alternative training algorithm for deep learning. This approach has the advantage that rigorous error estimates and convergence results can be established. We also show that it may avoid some pitfalls of gradient-based methods, such as slow convergence on flat landscapes near saddle points. Furthermore, we demonstrate that it obtains favorable initial convergence rate per-iteration, provided Hamiltonian maximization can be efficiently carried out - a step which is still in need of improvement. Overall, the approach opens up new avenues to attack problems associated with deep learning, such as trapping in slow manifolds and inapplicability of gradient-based methods for discrete trainable variables.
ER  -


TY  - Preprint
T1  - Real-Time Automatic Fetal Brain Extraction in Fetal MRI by Deep Learning
A1  - Seyed Sadegh Mohseni Salehi
A1  - Seyed Raein Hashemi
A1  - Clemente Velasco-Annis
A1  - Abdelhakim Ouaalam
A1  - Judy A. Estroff
A1  - Deniz Erdogmus
A1  - Simon K. Warfield
A1  - Ali Gholipour
JO  - ArXiv e-prints
Y1  - 25 October, 2017
UR  - https://arxiv.org/abs/1710.09338
N2  - Brain segmentation is a fundamental first step in neuroimage analysis. In the case of fetal MRI, it is particularly challenging and important due to the arbitrary orientation of the fetus, organs that surround the fetal head, and intermittent fetal motion. Several promising methods have been proposed but are limited in their performance in challenging cases and in real-time segmentation. We aimed to develop a fully automatic segmentation method that independently segments sections of the fetal brain in 2D fetal MRI slices in real-time. To this end, we developed and evaluated a deep fully convolutional neural network based on 2D U-net and autocontext, and compared it to two alternative fast methods based on 1) a voxelwise fully convolutional network and 2) a method based on SIFT features, random forest and conditional random field. We trained the networks with manual brain masks on 250 stacks of training images, and tested on 17 stacks of normal fetal brain images as well as 18 stacks of extremely challenging cases based on extreme motion, noise, and severely abnormal brain shape. Experimental results show that our U-net approach outperformed the other methods and achieved average Dice metrics of 96.52% and 78.83% in the normal and challenging test sets, respectively. With an unprecedented performance and a test run time of about 1 second, our network can be used to segment the fetal brain in real-time while fetal MRI slices are being acquired. This can enable real-time motion tracking, motion detection, and 3D reconstruction of fetal brain MRI.
ER  -


TY  - Preprint
T1  - Deep Transfer Learning for Error Decoding from Non-Invasive EEG
A1  - Martin VÃ¶lker
A1  - Robin T. Schirrmeister
A1  - Lukas D. J. Fiederer
A1  - Wolfram Burgard
A1  - Tonio Ball
JO  - ArXiv e-prints
Y1  - 10 January, 2018
UR  - https://arxiv.org/abs/1710.09139
N2  - We recorded high-density EEG in a flanker task experiment (31 subjects) and an online BCI control paradigm (4 subjects). On these datasets, we evaluated the use of transfer learning for error decoding with deep convolutional neural networks (deep ConvNets). In comparison with a regularized linear discriminant analysis (rLDA) classifier, ConvNets were significantly better in both intra- and inter-subject decoding, achieving an average accuracy of 84.1 % within subject and 81.7 % on unknown subjects (flanker task). Neither method was, however, able to generalize reliably between paradigms. Visualization of features the ConvNets learned from the data showed plausible patterns of brain activity, revealing both similarities and differences between the different kinds of errors. Our findings indicate that deep learning techniques are useful to infer information about the correctness of action in BCI applications, particularly for the transfer of pre-trained classifiers to new recording sessions or subjects.
ER  -


TY  - Preprint
T1  - Pre-Processing-Free Gear Fault Diagnosis Using Small Datasets with Deep Convolutional Neural Network-Based Transfer Learning
A1  - Pei Cao
A1  - Shengli Zhang
A1  - Jiong Tang
JO  - ArXiv e-prints
Y1  - 24 October, 2017
UR  - https://arxiv.org/abs/1710.08904
N2  - Early fault diagnosis in complex mechanical systems such as gearbox has always been a great challenge, even with the recent development in deep neural networks. The performance of a classic fault diagnosis system predominantly depends on the features extracted and the classifier subsequently applied. Although a large number of attempts have been made regarding feature extraction techniques, the methods require great human involvements are heavily depend on domain expertise and may thus be non-representative and biased from application to application. On the other hand, while the deep neural networks based approaches feature adaptive feature extractions and inherent classifications, they usually require a substantial set of training data and thus hinder their usage for engineering applications with limited training data such as gearbox fault diagnosis. This paper develops a deep convolutional neural network-based transfer learning approach that not only entertains pre-processing free adaptive feature extractions, but also requires only a small set of training data. The proposed approach performs gear fault diagnosis using pre-processing free raw accelerometer data and experiments with various sizes of training data were conducted. The superiority of the proposed approach is revealed by comparing the performance with other methods such as locally trained convolution neural network and angle-frequency analysis based support vector machine. The achieved accuracy indicates that the proposed approach is not only viable and robust, but also has the potential to be readily applicable to other fault diagnosis practices.
ER  -


TY  - Preprint
T1  - Benchmark of Deep Learning Models on Large Healthcare MIMIC Datasets
A1  - Sanjay Purushotham
A1  - Chuizheng Meng
A1  - Zhengping Che
A1  - Yan Liu
JO  - ArXiv e-prints
Y1  - 23 October, 2017
UR  - https://arxiv.org/abs/1710.08531
N2  - Deep learning models (aka Deep Neural Networks) have revolutionized many fields including computer vision, natural language processing, speech recognition, and is being increasingly used in clinical healthcare applications. However, few works exist which have benchmarked the performance of the deep learning models with respect to the state-of-the-art machine learning models and prognostic scoring systems on publicly available healthcare datasets. In this paper, we present the benchmarking results for several clinical prediction tasks such as mortality prediction, length of stay prediction, and ICD-9 code group prediction using Deep Learning models, ensemble of machine learning models (Super Learner algorithm), SAPS II and SOFA scores. We used the Medical Information Mart for Intensive Care III (MIMIC-III) (v1.4) publicly available dataset, which includes all patients admitted to an ICU at the Beth Israel Deaconess Medical Center from 2001 to 2012, for the benchmarking tasks. Our results show that deep learning models consistently outperform all the other approaches especially when the `raw&#39; clinical time series data is used as input features to the models.
ER  -


TY  - Preprint
T1  - Serving deep learning models in a serverless platform
A1  - Vatche Ishakian
A1  - Vinod Muthusamy
A1  - Aleksander Slominski
JO  - ArXiv e-prints
Y1  - 9 February, 2018
UR  - https://arxiv.org/abs/1710.08460
N2  - Serverless computing has emerged as a compelling paradigm for the development and deployment of a wide range of event based cloud applications. At the same time, cloud providers and enterprise companies are heavily adopting machine learning and Artificial Intelligence to either differentiate themselves, or provide their customers with value added services. In this work we evaluate the suitability of a serverless computing environment for the inferencing of large neural network models. Our experimental evaluations are executed on the AWS Lambda environment using the MxNet deep learning framework. Our experimental results show that while the inferencing latency can be within an acceptable range, longer delays due to cold starts can skew the latency distribution and hence risk violating more stringent SLAs.
ER  -


TY  - Preprint
T1  - Computational ghost imaging using deep learning
A1  - Tomoyoshi Shimobaba
A1  - Yutaka Endo
A1  - Takashi Nishitsuji
A1  - Takayuki Takahashi
A1  - Yuki Nagahama
A1  - Satoki Hasegawa
A1  - Marie Sano
A1  - Ryuji Hirayama
A1  - Takashi Kakue
A1  - Atsushi Shiraki
A1  - Tomoyoshi Ito
JO  - ArXiv e-prints
Y1  - 18 October, 2017
UR  - https://arxiv.org/abs/1710.08343
N2  - Computational ghost imaging (CGI) is a single-pixel imaging technique that exploits the correlation between known random patterns and the measured intensity of light transmitted (or reflected) by an object. Although CGI can obtain two- or three- dimensional images with a single or a few bucket detectors, the quality of the reconstructed images is reduced by noise due to the reconstruction of images from random patterns. In this study, we improve the quality of CGI images using deep learning. A deep neural network is used to automatically learn the features of noise-contaminated CGI images. After training, the network is able to predict low-noise images from new noise-contaminated CGI images.
ER  -


TY  - Preprint
T1  - Content Based Document Recommender using Deep Learning
A1  - Nishant Nikhil
A1  - Muktabh Mayank Srivastava
JO  - ArXiv e-prints
Y1  - 23 October, 2017
UR  - https://arxiv.org/abs/1710.08321
N2  - With the recent advancements in information technology there has been a huge surge in amount of data available. But information retrieval technology has not been able to keep up with this pace of information generation resulting in over spending of time for retrieving relevant information. Even though systems exist for assisting users to search a database along with filtering and recommending relevant information, but recommendation system which uses content of documents for recommendation still have a long way to mature. Here we present a Deep Learning based supervised approach to recommend similar documents based on the similarity of content. We combine the C-DSSM model with Word2Vec distributed representations of words to create a novel model to classify a document pair as relevant/irrelavant by assigning a score to it. Using our model retrieval of documents can be done in O(1) time and the memory complexity is O(n), where n is number of documents.
ER  -


TY  - Preprint
T1  - An efficient deep learning hashing neural network for mobile visual search
A1  - Heng Qi
A1  - Wu Liu
A1  - Liang Liu
JO  - ArXiv e-prints
Y1  - 21 October, 2017
UR  - https://arxiv.org/abs/1710.07750
N2  - Mobile visual search applications are emerging that enable users to sense their surroundings with smart phones. However, because of the particular challenges of mobile visual search, achieving a high recognition bitrate has becomes a consistent target of previous related works. In this paper, we propose a few-parameter, low-latency, and high-accuracy deep hashing approach for constructing binary hash codes for mobile visual search. First, we exploit the architecture of the MobileNet model, which significantly decreases the latency of deep feature extraction by reducing the number of model parameters while maintaining accuracy. Second, we add a hash-like layer into MobileNet to train the model on labeled mobile visual data. Evaluations show that the proposed system can exceed state-of-the-art accuracy performance in terms of the MAP. More importantly, the memory consumption is much less than that of other deep learning models. The proposed method requires only $13$ MB of memory for the neural network and achieves a MAP of $97.80\%$ on the mobile location recognition dataset used for testing.
ER  -


TY  - Preprint
T1  - Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning
A1  - Wei Ping
A1  - Kainan Peng
A1  - Andrew Gibiansky
A1  - Sercan O. Arik
A1  - Ajay Kannan
A1  - Sharan Narang
A1  - Jonathan Raiman
A1  - John Miller
JO  - ArXiv e-prints
Y1  - 22 February, 2018
UR  - https://arxiv.org/abs/1710.07654
N2  - We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on one single-GPU server.
ER  -


TY  - Preprint
T1  - Deep Learning Based NLOS Identification with Commodity WLAN Devices
A1  - Jeong-Sik Choi
A1  - Woong-Hee Lee
A1  - Jae-Hyun Lee
A1  - Jong-Ho Lee
A1  - Seong-Cheol Kim
JO  - ArXiv e-prints
Y1  - 8 December, 2017
UR  - https://arxiv.org/abs/1710.07450
N2  - Identifying line-of-sight (LOS) and non-LOS (NLOS) channel conditions can improve the performance of many wireless applications, such as signal strength-based localization algorithms. For this purpose, channel state information (CSI) obtained by commodity IEEE 802.11n devices can be used, because it contains information about channel impulse response (CIR). However, because of the limited sampling rate of the devices, a high-resolution CIR is not available, and it is difficult to detect the existence of an LOS path from a single CSI measurement, but it can be inferred from the variation pattern of CSI over time. To this end, we propose a recurrent neural network (RNN) model, which takes a series of CSI to identify the corresponding channel condition. We collect numerous measurement data under an indoor office environment, train the proposed RNN model, and compare the performance with those of existing schemes that use handcrafted features. The proposed method efficiently learns a non-linear relationship between input and output, and thus, yields high accuracy even for data obtained in a very short period.
ER  -


TY  - Preprint
T1  - Unified Backpropagation for Multi-Objective Deep Learning
A1  - Arash Shahriari
JO  - ArXiv e-prints
Y1  - 20 October, 2017
UR  - https://arxiv.org/abs/1710.07438
N2  - A common practice in most of deep convolutional neural architectures is to employ fully-connected layers followed by Softmax activation to minimize cross-entropy loss for the sake of classification. Recent studies show that substitution or addition of the Softmax objective to the cost functions of support vector machines or linear discriminant analysis is highly beneficial to improve the classification performance in hybrid neural networks. We propose a novel paradigm to link the optimization of several hybrid objectives through unified backpropagation. This highly alleviates the burden of extensive boosting for independent objective functions or complex formulation of multiobjective gradients. Hybrid loss functions are linked by basic probability assignment from evidence theory. We conduct our experiments for a variety of scenarios and standard datasets to evaluate the advantage of our proposed unification approach to deliver consistent improvements into the classification performance of deep convolutional neural networks.
ER  -


TY  - Preprint
T1  - Distributed Deep Transfer Learning by Basic Probability Assignment
A1  - Arash Shahriari
JO  - ArXiv e-prints
Y1  - 20 October, 2017
UR  - https://arxiv.org/abs/1710.07437
N2  - Transfer learning is a popular practice in deep neural networks, but fine-tuning of large number of parameters is a hard task due to the complex wiring of neurons between splitting layers and imbalance distributions of data in pretrained and transferred domains. The reconstruction of the original wiring for the target domain is a heavy burden due to the size of interconnections across neurons. We propose a distributed scheme that tunes the convolutional filters individually while backpropagates them jointly by means of basic probability assignment. Some of the most recent advances in evidence theory show that in a vast variety of the imbalanced regimes, optimizing of some proper objective functions derived from contingency matrices prevents biases towards high-prior class distributions. Therefore, the original filters get gradually transferred based on individual contributions to overall performance of the target domain. This largely reduces the expected complexity of transfer learning whilst highly improves precision. Our experiments on standard benchmarks and scenarios confirm the consistent improvement of our distributed deep transfer learning strategy.
ER  -


TY  - Preprint
T1  - Deep Self-taught Learning for Remote Sensing Image Classification
A1  - Anika Bettge
A1  - Ribana Roscher
A1  - Susanne Wenzel
JO  - ArXiv e-prints
Y1  - 19 December, 2017
UR  - https://arxiv.org/abs/1710.07096
N2  - This paper addresses the land cover classification task for remote sensing images by deep self-taught learning. Our self-taught learning approach learns suitable feature representations of the input data using sparse representation and undercomplete dictionary learning. We propose a deep learning framework which extracts representations in multiple layers and use the output of the deepest layer as input to a classification algorithm. We evaluate our approach using a multispectral Landsat 5 TM image of a study area in the North of Novo Progresso (South America) and the Zurich Summer Data Set provided by the University of Zurich. Experiments indicate that features learned by a deep self-taught learning framework can be used for classification and improve the results compared to classification results using the original feature representation.
ER  -


TY  - Preprint
T1  - Feature versus Raw Sequence: Deep Learning Comparative Study on Predicting Pre-miRNA
A1  - Jaya Thomas
A1  - Sonia Thomas
A1  - Lee Sael
JO  - ArXiv e-prints
Y1  - 17 October, 2017
UR  - https://arxiv.org/abs/1710.06798
N2  - Should we input known genome sequence features or input sequence itself in deep learning framework? As deep learning more popular in various applications, researchers often come to question whether to generate features or use raw sequences for deep learning. To answer this question, we study the prediction accuracy of precursor miRNA prediction of feature-based deep belief network and sequence-based convolution neural network. Tested on a variant of six-layer convolution neural net and three-layer deep belief network, we find the raw sequence input based convolution neural network model performs similar or slightly better than feature based deep belief networks with best accuracy values of 0.995 and 0.990, respectively. Both the models outperform existing benchmarks models. The results shows us that if provided large enough data, well devised raw sequence based deep learning models can replace feature based deep learning models. However, construction of well behaved deep learning model can be very challenging. In cased features can be easily extracted, feature-based deep learning models may be a better alternative.
ER  -


TY  - Preprint
T1  - Learning Social Image Embedding with Deep Multimodal Attention Networks
A1  - Feiran Huang
A1  - Xiaoming Zhang
A1  - Zhoujun Li
A1  - Tao Mei
A1  - Yueying He
A1  - Zhonghua Zhao
JO  - ArXiv e-prints
Y1  - 18 October, 2017
UR  - https://arxiv.org/abs/1710.06582
N2  - Learning social media data embedding by deep models has attracted extensive research interest as well as boomed a lot of applications, such as link prediction, classification, and cross-modal search. However, for social images which contain both link information and multimodal contents (e.g., text description, and visual content), simply employing the embedding learnt from network structure or data content results in sub-optimal social image representation. In this paper, we propose a novel social image embedding approach called Deep Multimodal Attention Networks (DMAN), which employs a deep model to jointly embed multimodal contents and link information. Specifically, to effectively capture the correlations between multimodal contents, we propose a multimodal attention network to encode the fine-granularity relation between image regions and textual words. To leverage the network structure for embedding learning, a novel Siamese-Triplet neural network is proposed to model the links among images. With the joint deep model, the learnt embedding can capture both the multimodal contents and the nonlinear network information. Extensive experiments are conducted to investigate the effectiveness of our approach in the applications of multi-label classification and cross-modal search. Compared to state-of-the-art image embeddings, our proposed DMAN achieves significant improvement in the tasks of multi-label classification and cross-modal search.
ER  -


TY  - Preprint
T1  - Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification
A1  - Dangwei Li
A1  - Xiaotang Chen
A1  - Zhang Zhang
A1  - Kaiqi Huang
JO  - ArXiv e-prints
Y1  - 17 October, 2017
UR  - https://arxiv.org/abs/1710.06555
N2  - Person Re-identification (ReID) is to identify the same person across different cameras. It is a challenging task due to the large variations in person pose, occlusion, background clutter, etc How to extract powerful features is a fundamental problem in ReID and is still an open problem today. In this paper, we design a Multi-Scale Context-Aware Network (MSCAN) to learn powerful features over full body and body parts, which can well capture the local context knowledge by stacking multi-scale convolutions in each layer. Moreover, instead of using predefined rigid parts, we propose to learn and localize deformable pedestrian parts using Spatial Transformer Networks (STN) with novel spatial constraints. The learned body parts can release some difficulties, eg pose variations and background clutters, in part-based representation. Finally, we integrate the representation learning processes of full body and body parts into a unified framework for person ReID through multi-class person identification tasks. Extensive evaluations on current challenging large-scale person ReID datasets, including the image-based Market1501, CUHK03 and sequence-based MARS datasets, show that the proposed method achieves the state-of-the-art results.
ER  -


TY  - Preprint
T1  - Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from Simulation
A1  - Kuan Fang
A1  - Yunfei Bai
A1  - Stefan Hinterstoisser
A1  - Silvio Savarese
A1  - Mrinal Kalakrishnan
JO  - ArXiv e-prints
Y1  - 3 March, 2018
UR  - https://arxiv.org/abs/1710.06422
N2  - Learning-based approaches to robotic manipulation are limited by the scalability of data collection and accessibility of labels. In this paper, we present a multi-task domain adaptation framework for instance grasping in cluttered scenes by utilizing simulated robot experiments. Our neural network takes monocular RGB images and the instance segmentation mask of a specified target object as inputs, and predicts the probability of successfully grasping the specified object for each candidate motor command. The proposed transfer learning framework trains a model for instance grasping in simulation and uses a domain-adversarial loss to transfer the trained model to real robots using indiscriminate grasping data, which is available both in simulation and the real world. We evaluate our model in real-world robot experiments, comparing it with alternative model architectures as well as an indiscriminate grasping baseline.
ER  -


TY  - Preprint
T1  - Deep Spectral Descriptors: Learning the point-wise correspondence metric via Siamese deep neural networks
A1  - Zhiyu Sun
A1  - Yusen He
A1  - Andrey Gritsenko
A1  - Amaury Lendasse
A1  - Stephen Baek
JO  - ArXiv e-prints
Y1  - 25 June, 2018
UR  - https://arxiv.org/abs/1710.06368
N2  - A robust and informative local shape descriptor plays an important role in mesh registration. In this regard, spectral descriptors that are based on the spectrum of the Laplace-Beltrami operator have gained a spotlight among the researchers for the last decade due to their desirable properties, such as isometry invariance. Despite such, however, spectral descriptors often fail to give a correct similarity measure for non-isometric cases where the metric distortion between the models is large. Hence, they are in general not suitable for the registration problems, except for the special cases when the models are near-isometry. In this paper, we investigate a way to develop shape descriptors for non-isometric registration tasks by embedding the spectral shape descriptors into a different metric space where the Euclidean distance between the elements directly indicates the geometric dissimilarity. We design and train a Siamese deep neural network to find such an embedding, where the embedded descriptors are promoted to rearrange based on the geometric similarity. We found our approach can significantly enhance the performance of the conventional spectral descriptors for the non-isometric registration tasks, and outperforms recent state-of-the-art method reported in literature.
ER  -


TY  - Preprint
T1  - Towards CT-quality Ultrasound Imaging using Deep Learning
A1  - Sanketh Vedula
A1  - Ortal Senouf
A1  - Alex M. Bronstein
A1  - Oleg V. Michailovich
A1  - Michael Zibulevsky
JO  - ArXiv e-prints
Y1  - 17 October, 2017
UR  - https://arxiv.org/abs/1710.06304
N2  - The cost-effectiveness and practical harmlessness of ultrasound imaging have made it one of the most widespread tools for medical diagnosis. Unfortunately, the beam-forming based image formation produces granular speckle noise, blurring, shading and other artifacts. To overcome these effects, the ultimate goal would be to reconstruct the tissue acoustic properties by solving a full wave propagation inverse problem. In this work, we make a step towards this goal, using Multi-Resolution Convolutional Neural Networks (CNN). As a result, we are able to reconstruct CT-quality images from the reflected ultrasound radio-frequency(RF) data obtained by simulation from real CT scans of a human body. We also show that CNN is able to imitate existing computationally heavy despeckling methods, thereby saving orders of magnitude in computations and making them amenable to real-time applications.
ER  -


TY  - Preprint
T1  - Map-based Multi-Policy Reinforcement Learning: Enhancing Adaptability of Robots by Deep Reinforcement Learning
A1  - Ayaka Kume
A1  - Eiichi Matsumoto
A1  - Kuniyuki Takahashi
A1  - Wilson Ko
A1  - Jethro Tan
JO  - ArXiv e-prints
Y1  - 18 October, 2017
UR  - https://arxiv.org/abs/1710.06117
N2  - In order for robots to perform mission-critical tasks, it is essential that they are able to quickly adapt to changes in their environment as well as to injuries and or other bodily changes. Deep reinforcement learning has been shown to be successful in training robot control policies for operation in complex environments. However, existing methods typically employ only a single policy. This can limit the adaptability since a large environmental modification might require a completely different behavior compared to the learning environment. To solve this problem, we propose Map-based Multi-Policy Reinforcement Learning (MMPRL), which aims to search and store multiple policies that encode different behavioral features while maximizing the expected reward in advance of the environment change. Thanks to these policies, which are stored into a multi-dimensional discrete map according to its behavioral feature, adaptation can be performed within reasonable time without retraining the robot. An appropriate pre-trained policy from the map can be recalled using Bayesian optimization. Our experiments show that MMPRL enables robots to quickly adapt to large changes without requiring any prior knowledge on the type of injuries that could occur. A highlight of the learned behaviors can be found here: https://youtu.be/QwInbilXNOE .
ER  -


TY  - Preprint
T1  - Deep Self-Paced Learning for Person Re-Identification
A1  - Sanping Zhou
A1  - Jinjun Wang
A1  - Deyu Meng
A1  - Xiaomeng Xin
A1  - Yubing Li
A1  - Yihong Gong
A1  - Nanning Zheng
JO  - ArXiv e-prints
Y1  - 6 October, 2017
UR  - https://arxiv.org/abs/1710.05711
N2  - Person re-identification (Re-ID) usually suffers from noisy samples with background clutter and mutual occlusion, which makes it extremely difficult to distinguish different individuals across the disjoint camera views. In this paper, we propose a novel deep self-paced learning (DSPL) algorithm to alleviate this problem, in which we apply a self-paced constraint and symmetric regularization to help the relative distance metric training the deep neural network, so as to learn the stable and discriminative features for person Re-ID. Firstly, we propose a soft polynomial regularizer term which can derive the adaptive weights to samples based on both the training loss and model age. As a result, the high-confidence fidelity samples will be emphasized and the low-confidence noisy samples will be suppressed at early stage of the whole training process. Such a learning regime is naturally implemented under a self-paced learning (SPL) framework, in which samples weights are adaptively updated based on both model age and sample loss using an alternative optimization method. Secondly, we introduce a symmetric regularizer term to revise the asymmetric gradient back-propagation derived by the relative distance metric, so as to simultaneously minimize the intra-class distance and maximize the inter-class distance in each triplet unit. Finally, we build a part-based deep neural network, in which the features of different body parts are first discriminately learned in the lower convolutional layers and then fused in the higher fully connected layers. Experiments on several benchmark datasets have demonstrated the superior performance of our method as compared with the state-of-the-art approaches.
ER  -


TY  - Preprint
T1  - Intention-Net: Integrating Planning and Deep Learning for Goal-Directed Autonomous Navigation
A1  - Wei Gao
A1  - David Hsu
A1  - Wee Sun Lee
A1  - Shengmei Shen
A1  - Karthikk Subramanian
JO  - ArXiv e-prints
Y1  - 16 October, 2017
UR  - https://arxiv.org/abs/1710.05627
N2  - How can a delivery robot navigate reliably to a destination in a new office building, with minimal prior information? To tackle this challenge, this paper introduces a two-level hierarchical approach, which integrates model-free deep learning and model-based path planning. At the low level, a neural-network motion controller, called the intention-net, is trained end-to-end to provide robust local navigation. The intention-net maps images from a single monocular camera and &#34;intentions&#34; directly to robot controls. At the high level, a path planner uses a crude map, e.g., a 2-D floor plan, to compute a path from the robot&#39;s current location to the goal. The planned path provides intentions to the intention-net. Preliminary experiments suggest that the learned motion controller is robust against perceptual uncertainty and by integrating with a path planner, it generalizes effectively to new environments and goals.
ER  -


TY  - Preprint
T1  - Using Deep Learning and Satellite Imagery to Quantify the Impact of the Built Environment on Neighborhood Crime Rates
A1  - Adyasha Maharana
A1  - Quynh C. Nguyen
A1  - Elaine O. Nsoesie
JO  - ArXiv e-prints
Y1  - 15 October, 2017
UR  - https://arxiv.org/abs/1710.05483
N2  - The built environment has been postulated to have an impact on neighborhood crime rates, however, measures of the built environment can be subjective and differ across studies leading to varying observations on its association with crime rates. Here, we illustrate an accurate and straightforward approach to quantify the impact of the built environment on neighborhood crime rates from high-resolution satellite imagery. Using geo-referenced crime reports and satellite images for three United States cities, we demonstrate how image features consistently identified using a convolutional neural network can explain up to 82% of the variation in neighborhood crime rates. Our results suggest the built environment is a strong predictor of crime rates, and this can lead to structural interventions shown to reduce crime incidence in urban settings.
ER  -


TY  - Preprint
T1  - DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations
A1  - Sanjay Krishnan
A1  - Roy Fox
A1  - Ion Stoica
A1  - Ken Goldberg
JO  - ArXiv e-prints
Y1  - 31 October, 2017
UR  - https://arxiv.org/abs/1710.05421
N2  - An option is a short-term skill consisting of a control policy for a specified region of the state space, and a termination condition recognizing leaving that region. In prior work, we proposed an algorithm called Deep Discovery of Options (DDO) to discover options to accelerate reinforcement learning in Atari games. This paper studies an extension to robot imitation learning, called Discovery of Deep Continuous Options (DDCO), where low-level continuous control skills parametrized by deep neural networks are learned from demonstrations. We extend DDO with: (1) a hybrid categorical-continuous distribution model to parametrize high-level policies that can invoke discrete options as well continuous control actions, and (2) a cross-validation method that relaxes DDO&#39;s requirement that users specify the number of options to be discovered. We evaluate DDCO in simulation of a 3-link robot in the vertical plane pushing a block with friction and gravity, and in two physical experiments on the da Vinci surgical robot, needle insertion where a needle is grasped and inserted into a silicone tissue phantom, and needle bin picking where needles and pins are grasped from a pile and categorized into bins. In the 3-link arm simulation, results suggest that DDCO can take 3x fewer demonstrations to achieve the same reward compared to a baseline imitation learning approach. In the needle insertion task, DDCO was successful 8/10 times compared to the next most accurate imitation learning baseline 6/10. In the surgical bin picking task, the learned policy successfully grasps a single object in 66 out of 99 attempted grasps, and in all but one case successfully recovered from failed grasps by retrying a second time.
ER  -


TY  - Preprint
T1  - Deep Learning for Wireless Physical Layer: Opportunities and Challenges
A1  - Tianqi Wang
A1  - Chao-Kai Wen
A1  - Hanqing Wang
A1  - Feifei Gao
A1  - Tao Jiang
A1  - Shi Jin
JO  - ArXiv e-prints
Y1  - 27 October, 2017
UR  - https://arxiv.org/abs/1710.05312
N2  - Machine learning (ML) has been widely applied to the upper layers of wireless communication systems for various purposes, such as deployment of cognitive radio and communication network. However, its application to the physical layer is hampered by sophisticated channel environments and limited learning ability of conventional ML algorithms. Deep learning (DL) has been recently applied for many fields, such as computer vision and natural language processing, given its expressive capacity and convenient optimization capability. The potential application of DL to the physical layer has also been increasingly recognized because of the new features for future communications, such as complex scenarios with unknown channel models, high speed and accurate processing requirements; these features challenge conventional communication theories. This paper presents a comprehensive overview of the emerging studies on DL-based physical layer processing, including leveraging DL to redesign a module of the conventional communication system (for modulation recognition, channel decoding, and detection) and replace the communication system with a radically new architecture based on an autoencoder. These DL-based methods show promising performance improvements but have certain limitations, such as lack of solid analytical tools and use of architectures that are specifically designed for communication and implementation research, thereby motivating future research in this field.
ER  -


TY  - Preprint
T1  - Data-Driven and Deep Learning Methodology for Deceptive Advertising and Phone Scams Detection
A1  - TonTon Hsien-De Huang
A1  - Chia-Mu Yu
A1  - Hung-Yu Kao
JO  - ArXiv e-prints
Y1  - 15 October, 2017
UR  - https://arxiv.org/abs/1710.05305
N2  - The advance of smartphones and cellular networks boosts the need of mobile advertising and targeted marketing. However, it also triggers the unseen security threats. We found that the phone scams with fake calling numbers of very short lifetime are increasingly popular and have been used to trick the users. The harm is worldwide. On the other hand, deceptive advertising (deceptive ads), the fake ads that tricks users to install unnecessary apps via either alluring or daunting texts and pictures, is an emerging threat that seriously harms the reputation of the advertiser. To counter against these two new threats, the conventional blacklist (or whitelist) approach and the machine learning approach with predefined features have been proven useless. Nevertheless, due to the success of deep learning in developing the highly intelligent program, our system can efficiently and effectively detect phone scams and deceptive ads by taking advantage of our unified framework on deep neural network (DNN) and convolutional neural network (CNN). The proposed system has been deployed for operational use and the experimental results proved the effectiveness of our proposed system. Furthermore, we keep our research results and release experiment material on http://DeceptiveAds.TWMAN.ORG and http://PhoneScams.TWMAN.ORG if there is any update.
ER  -


TY  - Preprint
T1  - Object Classification in Images of Neoclassical Artifacts Using Deep Learning
A1  - Bernhard Bermeitinger
A1  - Maria Christoforaki
A1  - Simon Donig
A1  - Siegfried Handschuh
JO  - ArXiv e-prints
Y1  - 13 October, 2017
UR  - https://arxiv.org/abs/1710.04943
N2  - In this paper, we report on our efforts for using Deep Learning for classifying artifacts and their features in digital visuals as a part of the Neoclassica framework. It was conceived to provide scholars with new methods for analyzing and classifying artifacts and aesthetic forms from the era of Classicism. The framework accommodates both traditional knowledge representation as a formal ontology and data-driven knowledge discovery, where cultural patterns will be identified by means of algorithms in statistical analysis and machine learning. We created a Deep Learning approach trained on photographs to classify the objects inside these photographs. In a next step, we will apply a different Deep Learning approach. It is capable of locating multiple objects inside an image and classifying them with a high accuracy.
ER  -


TY  - Preprint
T1  - RADNET: Radiologist Level Accuracy using Deep Learning for HEMORRHAGE detection in CT Scans
A1  - Monika Grewal
A1  - Muktabh Mayank Srivastava
A1  - Pulkit Kumar
A1  - Srikrishna Varadarajan
JO  - ArXiv e-prints
Y1  - 3 January, 2018
UR  - https://arxiv.org/abs/1710.04934
N2  - We describe a deep learning approach for automated brain hemorrhage detection from computed tomography (CT) scans. Our model emulates the procedure followed by radiologists to analyse a 3D CT scan in real-world. Similar to radiologists, the model sifts through 2D cross-sectional slices while paying close attention to potential hemorrhagic regions. Further, the model utilizes 3D context from neighboring slices to improve predictions at each slice and subsequently, aggregates the slice-level predictions to provide diagnosis at CT level. We refer to our proposed approach as Recurrent Attention DenseNet (RADnet) as it employs original DenseNet architecture along with adding the components of attention for slice level predictions and recurrent neural network layer for incorporating 3D context. The real-world performance of RADnet has been benchmarked against independent analysis performed by three senior radiologists for 77 brain CTs. RADnet demonstrates 81.82% hemorrhage prediction accuracy at CT level that is comparable to radiologists. Further, RADnet achieves higher recall than two of the three radiologists, which is remarkable.
ER  -


TY  - Preprint
T1  - Anti-jamming Communications Using Spectrum Waterfall: A Deep Reinforcement Learning Approach
A1  - Xin Liu
A1  - Yuhua Xu
A1  - Luliang Jia
A1  - Qihui Wu
A1  - Alagan Anpalagan
JO  - ArXiv e-prints
Y1  - 13 October, 2017
UR  - https://arxiv.org/abs/1710.04830
N2  - This letter investigates the problem of anti-jamming communications in dynamic and unknown environment through on-line learning. Different from existing studies which need to know (estimate) the jamming patterns and parameters, we use the spectrum waterfall, i.e., the raw spectrum environment, directly. Firstly, to cope with the challenge of infinite state of raw spectrum information, a deep anti-jamming Q-network is constructed. Then, a deep anti-jamming reinforcement learning algorithm is proposed to obtain the optimal anti-jamming strategies. Finally, simulation results validate the the proposed approach. The proposed approach is relying only on the local observed information and does not need to estimate the jamming patterns and parameters, which implies that it can be widely used various anti-jamming scenarios.
ER  -


TY  - Preprint
T1  - Deep Learning for Case-Based Reasoning through Prototypes: A Neural Network that Explains Its Predictions
A1  - Oscar Li
A1  - Hao Liu
A1  - Chaofan Chen
A1  - Cynthia Rudin
JO  - ArXiv e-prints
Y1  - 21 November, 2017
UR  - https://arxiv.org/abs/1710.04806
N2  - Deep neural networks are widely used for classification. These deep models often suffer from a lack of interpretability -- they are particularly difficult to understand because of their non-linear nature. As a result, neural networks are often treated as &#34;black box&#34; models, and in the past, have been trained purely to optimize the accuracy of predictions. In this work, we create a novel network architecture for deep learning that naturally explains its own reasoning for each prediction. This architecture contains an autoencoder and a special prototype layer, where each unit of that layer stores a weight vector that resembles an encoded training input. The encoder of the autoencoder allows us to do comparisons within the latent space, while the decoder allows us to visualize the learned prototypes. The training objective has four terms: an accuracy term, a term that encourages every prototype to be similar to at least one encoded input, a term that encourages every encoded input to be close to at least one prototype, and a term that encourages faithful reconstruction by the autoencoder. The distances computed in the prototype layer are used as part of the classification process. Since the prototypes are learned during training, the learned network naturally comes with explanations for each prediction, and the explanations are loyal to what the network actually computes.
ER  -


TY  - Preprint
T1  - Explaining Aviation Safety Incidents Using Deep Temporal Multiple Instance Learning
A1  - Vijay Manikandan Janakiraman
JO  - ArXiv e-prints
Y1  - 12 February, 2018
UR  - https://arxiv.org/abs/1710.04749
N2  - Although aviation accidents are rare, safety incidents occur more frequently and require a careful analysis to detect and mitigate risks in a timely manner. Analyzing safety incidents using operational data and producing event-based explanations is invaluable to airline companies as well as to governing organizations such as the Federal Aviation Administration (FAA) in the United States. However, this task is challenging because of the complexity involved in mining multi-dimensional heterogeneous time series data, the lack of time-step-wise annotation of events in a flight, and the lack of scalable tools to perform analysis over a large number of events. In this work, we propose a precursor mining algorithm that identifies events in the multidimensional time series that are correlated with the safety incident. Precursors are valuable to systems health and safety monitoring and in explaining and forecasting safety incidents. Current methods suffer from poor scalability to high dimensional time series data and are inefficient in capturing temporal behavior. We propose an approach by combining multiple-instance learning (MIL) and deep recurrent neural networks (DRNN) to take advantage of MIL&#39;s ability to learn using weakly supervised data and DRNN&#39;s ability to model temporal behavior. We describe the algorithm, the data, the intuition behind taking a MIL approach, and a comparative analysis of the proposed algorithm with baseline models. We also discuss the application to a real-world aviation safety problem using data from a commercial airline company and discuss the model&#39;s abilities and shortcomings, with some final remarks about possible deployment directions.
ER  -


TY  - Preprint
T1  - Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation
A1  - Tianhao Zhang
A1  - Zoe McCarthy
A1  - Owen Jow
A1  - Dennis Lee
A1  - Xi Chen
A1  - Ken Goldberg
A1  - Pieter Abbeel
JO  - ArXiv e-prints
Y1  - 6 March, 2018
UR  - https://arxiv.org/abs/1710.04615
N2  - Imitation learning is a powerful paradigm for robot skill acquisition. However, obtaining demonstrations suitable for learning a policy that maps from raw pixels to actions can be challenging. In this paper we describe how consumer-grade Virtual Reality headsets and hand tracking hardware can be used to naturally teleoperate robots to perform complex tasks. We also describe how imitation learning can learn deep neural network policies (mapping from pixels to actions) that can acquire the demonstrated skills. Our experiments showcase the effectiveness of our approach for learning visuomotor skills.
ER  -


TY  - Preprint
T1  - NeuroTrainer: An Intelligent Memory Module for Deep Learning Training
A1  - Duckhwan Kim
A1  - Taesik Na
A1  - Sudhakar Yalamanchili
A1  - Saibal Mukhopadhyay
JO  - ArXiv e-prints
Y1  - 11 October, 2017
UR  - https://arxiv.org/abs/1710.04347
N2  - This paper presents, NeuroTrainer, an intelligent memory module with in-memory accelerators that forms the building block of a scalable architecture for energy efficient training for deep neural networks. The proposed architecture is based on integration of a homogeneous computing substrate composed of multiple processing engines in the logic layer of a 3D memory module. NeuroTrainer utilizes a programmable data flow based execution model to optimize memory mapping and data re-use during different phases of training operation. A programming model and supporting architecture utilizes the flexible data flow to efficiently accelerate training of various types of DNNs. The cycle level simulation and synthesized design in 15nm FinFET showspower efficiency of 500 GFLOPS/W, and almost similar throughput for a wide range of DNNs including convolutional, recurrent, multi-layer-perceptron, and mixed (CNN+RNN) networks
ER  -


TY  - Preprint
T1  - Interactive Medical Image Segmentation using Deep Learning with Image-specific Fine-tuning
A1  - Guotai Wang
A1  - Wenqi Li
A1  - Maria A. Zuluaga
A1  - Rosalind Pratt
A1  - Premal A. Patel
A1  - Michael Aertsen
A1  - Tom Doel
A1  - Anna L. David
A1  - Jan Deprest
A1  - Sebastien Ourselin
A1  - Tom Vercauteren
JO  - ArXiv e-prints
Y1  - 11 October, 2017
UR  - https://arxiv.org/abs/1710.04043
N2  - Convolutional neural networks (CNNs) have achieved state-of-the-art performance for automatic medical image segmentation. However, they have not demonstrated sufficiently accurate and robust results for clinical use. In addition, they are limited by the lack of image-specific adaptation and the lack of generalizability to previously unseen object classes. To address these problems, we propose a novel deep learning-based framework for interactive segmentation by incorporating CNNs into a bounding box and scribble-based segmentation pipeline. We propose image-specific fine-tuning to make a CNN model adaptive to a specific test image, which can be either unsupervised (without additional user interactions) or supervised (with additional scribbles). We also propose a weighted loss function considering network and interaction-based uncertainty for the fine-tuning. We applied this framework to two applications: 2D segmentation of multiple organs from fetal MR slices, where only two types of these organs were annotated for training; and 3D segmentation of brain tumor core (excluding edema) and whole brain tumor (including edema) from different MR sequences, where only tumor cores in one MR sequence were annotated for training. Experimental results show that 1) our model is more robust to segment previously unseen objects than state-of-the-art CNNs; 2) image-specific fine-tuning with the proposed weighted loss function significantly improves segmentation accuracy; and 3) our method leads to accurate results with fewer user interactions and less user time than traditional interactive segmentation methods.
ER  -


TY  - Preprint
T1  - Deep learning in remote sensing: a review
A1  - Xiao Xiang Zhu
A1  - Devis Tuia
A1  - Lichao Mou
A1  - Gui-Song Xia
A1  - Liangpei Zhang
A1  - Feng Xu
A1  - Friedrich Fraundorfer
JO  - ArXiv e-prints
Y1  - 11 October, 2017
UR  - https://arxiv.org/abs/1710.03959
N2  - Standing at the paradigm shift towards data-intensive science, machine learning techniques are becoming increasingly important. In particular, as a major breakthrough in the field, deep learning has proven as an extremely powerful tool in many fields. Shall we embrace deep learning as the key to all? Or, should we resist a &#39;black-box&#39; solution? There are controversial opinions in the remote sensing community. In this article, we analyze the challenges of using deep learning for remote sensing data analysis, review the recent advances, and provide resources to make deep learning in remote sensing ridiculously simple to start with. More importantly, we advocate remote sensing scientists to bring their expertise into deep learning, and use it as an implicit general model to tackle unprecedented large-scale influential challenges, such as climate change and urbanization.
ER  -


TY  - Preprint
T1  - Decision support from financial disclosures with deep neural networks and transfer learning
A1  - Mathias Kraus
A1  - Stefan Feuerriegel
JO  - ArXiv e-prints
Y1  - 11 October, 2017
UR  - https://arxiv.org/abs/1710.03954
N2  - Company disclosures greatly aid in the process of financial decision-making; therefore, they are consulted by financial investors and automated traders before exercising ownership in stocks. While humans are usually able to correctly interpret the content, the same is rarely true of computerized decision support systems, which struggle with the complexity and ambiguity of natural language. A possible remedy is represented by deep learning, which overcomes several shortcomings of traditional methods of text mining. For instance, recurrent neural networks, such as long short-term memories, employ hierarchical structures, together with a large number of hidden layers, to automatically extract features from ordered sequences of words and capture highly non-linear relationships such as context-dependent meanings. However, deep learning has only recently started to receive traction, possibly because its performance is largely untested. Hence, this paper studies the use of deep neural networks for financial decision support. We additionally experiment with transfer learning, in which we pre-train the network on a different corpus with a length of 139.1 million words. Our results reveal a higher directional accuracy as compared to traditional machine learning when predicting stock price movements in response to financial disclosures. Our work thereby helps to highlight the business value of deep learning and provides recommendations to practitioners and executives.
ER  -


TY  - Preprint
T1  - Application of Deep Learning in Neuroradiology: Automated Detection of Basal Ganglia Hemorrhage using 2D-Convolutional Neural Networks
A1  - Vishal Desai
A1  - Adam E. Flanders
A1  - Paras Lakhani
JO  - ArXiv e-prints
Y1  - 27 October, 2017
UR  - https://arxiv.org/abs/1710.03823
N2  - Background: Deep learning techniques have achieved high accuracy in image classification tasks, and there is interest in applicability to neuroimaging critical findings. This study evaluates the efficacy of 2D deep convolutional neural networks (DCNNs) for detecting basal ganglia (BG) hemorrhage on noncontrast head CT.
ER  -


TY  - Preprint
T1  - End-to-End Deep Learning for Steering Autonomous Vehicles Considering Temporal Dependencies
A1  - Hesham M. Eraqi
A1  - Mohamed N. Moustafa
A1  - Jens Honer
JO  - ArXiv e-prints
Y1  - 22 November, 2017
UR  - https://arxiv.org/abs/1710.03804
N2  - Steering a car through traffic is a complex task that is difficult to cast into algorithms. Therefore, researchers turn to training artificial neural networks from front-facing camera data stream along with the associated steering angles. Nevertheless, most existing solutions consider only the visual camera frames as input, thus ignoring the temporal relationship between frames. In this work, we propose a Convolutional Long Short-Term Memory Recurrent Neural Network (C-LSTM), that is end-to-end trainable, to learn both visual and dynamic temporal dependencies of driving. Additionally, We introduce posing the steering angle regression problem as classification while imposing a spatial relationship between the output layer neurons. Such method is based on learning a sinusoidal function that encodes steering angles. To train and validate our proposed methods, we used the publicly available Comma.ai dataset. Our solution improved steering root mean square error by 35% over recent methods, and led to a more stable steering by 87%.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning: Framework, Applications, and Embedded Implementations
A1  - Hongjia Li
A1  - Tianshu Wei
A1  - Ao Ren
A1  - Qi Zhu
A1  - Yanzhi Wang
JO  - ArXiv e-prints
Y1  - 10 October, 2017
UR  - https://arxiv.org/abs/1710.03792
N2  - The recent breakthroughs of deep reinforcement learning (DRL) technique in Alpha Go and playing Atari have set a good example in handling large state and actions spaces of complicated control problems. The DRL technique is comprised of (i) an offline deep neural network (DNN) construction phase, which derives the correlation between each state-action pair of the system and its value function, and (ii) an online deep Q-learning phase, which adaptively derives the optimal action and updates value estimates. In this paper, we first present the general DRL framework, which can be widely utilized in many applications with different optimization objectives. This is followed by the introduction of three specific applications: the cloud computing resource allocation problem, the residential smart grid task scheduling problem, and building HVAC system optimal control problem. The effectiveness of the DRL technique in these three cyber-physical applications have been validated. Finally, this paper investigates the stochastic computing-based hardware implementations of the DRL framework, which consumes a significant improvement in area efficiency and power consumption compared with binary-based implementation counterparts.
ER  -


TY  - Preprint
T1  - Joint Weakly and Semi-Supervised Deep Learning for Localization and Classification of Masses in Breast Ultrasound Images
A1  - Seung Yeon Shin
A1  - Soochahn Lee
A1  - Il Dong Yun
A1  - Kyoung Mu Lee
JO  - ArXiv e-prints
Y1  - 10 October, 2017
UR  - https://arxiv.org/abs/1710.03778
N2  - We propose a framework for localization and classification of masses in breast ultrasound (BUS) images. In particular, we simultaneously use a weakly annotated dataset and a relatively small strongly annotated dataset to train a convolutional neural network detector. We have experimentally found that mass detectors trained with small, strongly annotated datasets are easily overfitted, whereas those trained with large, weakly annotated datasets present a non-trivial problem. To overcome these problems, we jointly use datasets with different characteristics in a hybrid manner. Consequently, a sophisticated weakly and semi-supervised training scenario is introduced with appropriate training loss selection. Experimental results show that the proposed method successfully localizes and classifies masses while requiring less effort in annotation work. The influences of each component in the proposed framework are also validated by conducting an ablative analysis. Although the proposed method is intended for masses in BUS images, it can also be applied as a general framework to train computer-aided detection and diagnosis systems for a wide variety of image modalities, target organs, and diseases.
ER  -


TY  - Preprint
T1  - Function space analysis of deep learning representation layers
A1  - Oren Elisha
A1  - Shai Dekel
JO  - ArXiv e-prints
Y1  - 9 October, 2017
UR  - https://arxiv.org/abs/1710.03263
N2  - In this paper we propose a function space approach to Representation Learning and the analysis of the representation layers in deep learning architectures. We show how to compute a weak-type Besov smoothness index that quantifies the geometry of the clustering in the feature space. This approach was already applied successfully to improve the performance of machine learning algorithms such as the Random Forest and tree-based Gradient Boosting. Our experiments demonstrate that in well-known and well-performing trained networks, the Besov smoothness of the training set, measured in the corresponding hidden layer feature map representation, increases from layer to layer. We also contribute to the understanding of generalization by showing how the Besov smoothness of the representations, decreases as we add more mis-labeling to the training data. We hope this approach will contribute to the de-mystification of some aspects of deep learning.
ER  -


TY  - Preprint
T1  - Deep Learning Paradigm with Transformed Monolingual Word Embeddings for Multilingual Sentiment Analysis
A1  - Yujie Lu
A1  - Tatsunori Mori
JO  - ArXiv e-prints
Y1  - 9 October, 2017
UR  - https://arxiv.org/abs/1710.03203
N2  - The surge of social media use brings huge demand of multilingual sentiment analysis (MSA) for unveiling cultural difference. So far, traditional methods resorted to machine translation---translating texts in other languages to English, and then adopt the methods once worked in English. However, this paradigm is conditioned by the quality of machine translation. In this paper, we propose a new deep learning paradigm to assimilate the differences between languages for MSA. We first pre-train monolingual word embeddings separately, then map word embeddings in different spaces into a shared embedding space, and then finally train a parameter-sharing deep neural network for MSA. The experimental results show that our paradigm is effective. Especially, our CNN model outperforms a state-of-the-art baseline by around 2.1% in terms of classification accuracy.
ER  -


TY  - Preprint
T1  - An automatic deep learning approach for coronary artery calcium segmentation
A1  - G. Santini
A1  - D. Della Latta
A1  - N. Martini
A1  - G. Valvano
A1  - A. Gori
A1  - A. Ripoli
A1  - C. L. Susini
A1  - L. Landini
A1  - D. Chiappino
JO  - ArXiv e-prints
Y1  - 9 October, 2017
UR  - https://arxiv.org/abs/1710.03023
N2  - Coronary artery calcium (CAC) is a significant marker of atherosclerosis and cardiovascular events. In this work we present a system for the automatic quantification of calcium score in ECG-triggered non-contrast enhanced cardiac computed tomography (CT) images. The proposed system uses a supervised deep learning algorithm, i.e. convolutional neural network (CNN) for the segmentation and classification of candidate lesions as coronary or not, previously extracted in the region of the heart using a cardiac atlas. We trained our network with 45 CT volumes; 18 volumes were used to validate the model and 56 to test it. Individual lesions were detected with a sensitivity of 91.24%, a specificity of 95.37% and a positive predicted value (PPV) of 90.5%; comparing calcium score obtained by the system and calcium score manually evaluated by an expert operator, a Pearson coefficient of 0.983 was obtained. A high agreement (Cohen&#39;s k = 0.879) between manual and automatic risk prediction was also observed. These results demonstrated that convolutional neural networks can be effectively applied for the automatic segmentation and classification of coronary calcifications.
ER  -


TY  - Preprint
T1  - Face Sketch Matching via Coupled Deep Transform Learning
A1  - Shruti Nagpal
A1  - Maneet Singh
A1  - Richa Singh
A1  - Mayank Vatsa
A1  - Afzel Noore
A1  - Angshul Majumdar
JO  - ArXiv e-prints
Y1  - 8 October, 2017
UR  - https://arxiv.org/abs/1710.02914
N2  - Face sketch to digital image matching is an important challenge of face recognition that involves matching across different domains. Current research efforts have primarily focused on extracting domain invariant representations or learning a mapping from one domain to the other. In this research, we propose a novel transform learning based approach termed as DeepTransformer, which learns a transformation and mapping function between the features of two domains. The proposed formulation is independent of the input information and can be applied with any existing learned or hand-crafted feature. Since the mapping function is directional in nature, we propose two variants of DeepTransformer: (i) semi-coupled and (ii) symmetrically-coupled deep transform learning. This research also uses a novel IIIT-D Composite Sketch with Age (CSA) variations database which contains sketch images of 150 subjects along with age-separated digital photos. The performance of the proposed models is evaluated on a novel application of sketch-to-sketch matching, along with sketch-to-digital photo matching. Experimental results demonstrate the robustness of the proposed models in comparison to existing state-of-the-art sketch matching algorithms and a commercial face recognition system.
ER  -


TY  - Preprint
T1  - Protein identification with deep learning: from abc to xyz
A1  - Ngoc Hieu Tran
A1  - Zachariah Levine
A1  - Lei Xin
A1  - Baozhen Shan
A1  - Ming Li
JO  - ArXiv e-prints
Y1  - 7 October, 2017
UR  - https://arxiv.org/abs/1710.02765
N2  - Proteins are the main workhorses of biological functions in a cell, a tissue, or an organism. Identification and quantification of proteins in a given sample, e.g. a cell type under normal/disease conditions, are fundamental tasks for the understanding of human health and disease. In this paper, we present DeepNovo, a deep learning-based tool to address the problem of protein identification from tandem mass spectrometry data. The idea was first proposed in the context of de novo peptide sequencing [1] in which convolutional neural networks and recurrent neural networks were applied to predict the amino acid sequence of a peptide from its spectrum, a similar task to generating a caption from an image. We further develop DeepNovo to perform sequence database search, the main technique for peptide identification that greatly benefits from numerous existing protein databases. We combine two modules de novo sequencing and database search into a single deep learning framework for peptide identification, and integrate de Bruijn graph assembly technique to offer a complete solution to reconstruct protein sequences from tandem mass spectrometry data. This paper describes a comprehensive protocol of DeepNovo for protein identification, including training neural network models, dynamic programming search, database querying, estimation of false discovery rate, and de Bruijn graph assembly. Training and testing data, model implementations, and comprehensive tutorials in form of IPython notebooks are available in our GitHub repository (https://github.com/nh2tran/DeepNovo).
ER  -


TY  - Preprint
T1  - A Transfer-Learning Approach for Accelerated MRI using Deep Neural Networks
A1  - Salman Ul Hassan Dar
A1  - Tolga Ãukur
JO  - ArXiv e-prints
Y1  - 19 November, 2017
UR  - https://arxiv.org/abs/1710.02615
N2  - Neural network based architectures have recently been proposed for reconstruction of undersampled MR acquisitions. A deep network containing many free parameters is typically trained using a relatively large set of fully-sampled MRI data, and later used for on-line reconstruction of undersampled data. Ideally network performance should be optimized by drawing the training and testing data from the same domain. In practice, however, large datasets comprising hundreds of subjects scanned under a common protocol are rare. Here, we propose a transfer-learning approach to address the problem of data scarcity in training deep networks for accelerated MRI. The proposed approach trains neural networks using thousands of samples from a public dataset of natural images (ImageNet). The network is then fine-tuned using only few tens of MR images acquired in the testing domain (T1- or T2-weighted MRI). The ImageNet-trained network yields nearly identical reconstructions to networks trained directly in the testing domain using thousands of MR images, and it outperforms conventional compressed sensing reconstructions in terms of image quality. The proposed approach might facilitate the use of neural networks for MRI reconstruction without the need for collection of extensive imaging datasets.
ER  -


TY  - Preprint
T1  - Real-Time Illegal Parking Detection System Based on Deep Learning
A1  - Xuemei Xie
A1  - Chenye Wang
A1  - Shu Chen
A1  - Guangming Shi
A1  - Zhifu Zhao
JO  - ArXiv e-prints
Y1  - 5 October, 2017
UR  - https://arxiv.org/abs/1710.02546
N2  - The increasing illegal parking has become more and more serious. Nowadays the methods of detecting illegally parked vehicles are based on background segmentation. However, this method is weakly robust and sensitive to environment. Benefitting from deep learning, this paper proposes a novel illegal vehicle parking detection system. Illegal vehicles captured by camera are firstly located and classified by the famous Single Shot MultiBox Detector (SSD) algorithm. To improve the performance, we propose to optimize SSD by adjusting the aspect ratio of default box to accommodate with our dataset better. After that, a tracking and analysis of movement is adopted to judge the illegal vehicles in the region of interest (ROI). Experiments show that the system can achieve a 99% accuracy and real-time (25FPS) detection with strong robustness in complex environments.
ER  -


TY  - Preprint
T1  - Rainbow: Combining Improvements in Deep Reinforcement Learning
A1  - Matteo Hessel
A1  - Joseph Modayil
A1  - Hado van Hasselt
A1  - Tom Schaul
A1  - Georg Ostrovski
A1  - Will Dabney
A1  - Dan Horgan
A1  - Bilal Piot
A1  - Mohammad Azar
A1  - David Silver
JO  - ArXiv e-prints
Y1  - 6 October, 2017
UR  - https://arxiv.org/abs/1710.02298
N2  - The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.
ER  -


TY  - Preprint
T1  - Efficient K-Shot Learning with Regularized Deep Networks
A1  - Donghyun Yoo
A1  - Haoqi Fan
A1  - Vishnu Naresh Boddeti
A1  - Kris M. Kitani
JO  - ArXiv e-prints
Y1  - 6 October, 2017
UR  - https://arxiv.org/abs/1710.02277
N2  - Feature representations from pre-trained deep neural networks have been known to exhibit excellent generalization and utility across a variety of related tasks. Fine-tuning is by far the simplest and most widely used approach that seeks to exploit and adapt these feature representations to novel tasks with limited data. Despite the effectiveness of fine-tuning, itis often sub-optimal and requires very careful optimization to prevent severe over-fitting to small datasets. The problem of sub-optimality and over-fitting, is due in part to the large number of parameters used in a typical deep convolutional neural network. To address these problems, we propose a simple yet effective regularization method for fine-tuning pre-trained deep networks for the task of k-shot learning. To prevent overfitting, our key strategy is to cluster the model parameters while ensuring intra-cluster similarity and inter-cluster diversity of the parameters, effectively regularizing the dimensionality of the parameter search space. In particular, we identify groups of neurons within each layer of a deep network that shares similar activation patterns. When the network is to be fine-tuned for a classification task using only k examples, we propagate a single gradient to all of the neuron parameters that belong to the same group. The grouping of neurons is non-trivial as neuron activations depend on the distribution of the input data. To efficiently search for optimal groupings conditioned on the input data, we propose a reinforcement learning search strategy using recurrent networks to learn the optimal group assignments for each network layer. Experimental results show that our method can be easily applied to several popular convolutional neural networks and improve upon other state-of-the-art fine-tuning based k-shot learning strategies by more than10%
ER  -


TY  - Preprint
T1  - Model-free prediction of noisy chaotic time series by deep learning
A1  - Kyongmin Yeo
JO  - ArXiv e-prints
Y1  - 29 September, 2017
UR  - https://arxiv.org/abs/1710.01693
N2  - We present a deep neural network for a model-free prediction of a chaotic dynamical system from noisy observations. The proposed deep learning model aims to predict the conditional probability distribution of a state variable. The Long Short-Term Memory network (LSTM) is employed to model the nonlinear dynamics and a softmax layer is used to approximate a probability distribution. The LSTM model is trained by minimizing a regularized cross-entropy function. The LSTM model is validated against delay-time chaotic dynamical systems, Mackey-Glass and Ikeda equations. It is shown that the present LSTM makes a good prediction of the nonlinear dynamics by effectively filtering out the noise. It is found that the prediction uncertainty of a multiple-step forecast of the LSTM model is not a monotonic function of time; the predicted standard deviation may increase or decrease dynamically in time.
ER  -


TY  - Preprint
T1  - Deep learning for source camera identification on mobile devices
A1  - David Freire-ObregÃ³n
A1  - Fabio Narducci
A1  - Silvio Barra
A1  - Modesto CastrillÃ³n-Santana
JO  - ArXiv e-prints
Y1  - 13 October, 2017
UR  - https://arxiv.org/abs/1710.01257
N2  - In the present paper, we propose a source camera identification method for mobile devices based on deep learning. Recently, convolutional neural networks (CNNs) have shown a remarkable performance on several tasks such as image recognition, video analysis or natural language processing. A CNN consists on a set of layers where each layer is composed by a set of high pass filters which are applied all over the input image. This convolution process provides the unique ability to extract features automatically from data and to learn from those features. Our proposal describes a CNN architecture which is able to infer the noise pattern of mobile camera sensors (also known as camera fingerprint) with the aim at detecting and identifying not only the mobile device used to capture an image (with a 98\% of accuracy), but also from which embedded camera the image was captured. More specifically, we provide an extensive analysis on the proposed architecture considering different configurations. The experiment has been carried out using the images captured from different mobile devices cameras (MICHE-I Dataset was used) and the obtained results have proved the robustness of the proposed method.
ER  -


TY  - Preprint
T1  - Reducing Complexity of HEVC: A Deep Learning Approach
A1  - Mai Xu
A1  - Tianyi Li
A1  - Zulin Wang
A1  - Xin Deng
A1  - Ren Yang
A1  - Zhenyu Guan
JO  - ArXiv e-prints
Y1  - 22 March, 2018
UR  - https://arxiv.org/abs/1710.01218
N2  - High Efficiency Video Coding (HEVC) significantly reduces bit-rates over the proceeding H.264 standard but at the expense of extremely high encoding complexity. In HEVC, the quad-tree partition of coding unit (CU) consumes a large proportion of the HEVC encoding complexity, due to the bruteforce search for rate-distortion optimization (RDO). Therefore, this paper proposes a deep learning approach to predict the CU partition for reducing the HEVC complexity at both intra- and inter-modes, which is based on convolutional neural network (CNN) and long- and short-term memory (LSTM) network. First, we establish a large-scale database including substantial CU partition data for HEVC intra- and inter-modes. This enables deep learning on the CU partition. Second, we represent the CU partition of an entire coding tree unit (CTU) in the form of a hierarchical CU partition map (HCPM). Then, we propose an early-terminated hierarchical CNN (ETH-CNN) for learning to predict the HCPM. Consequently, the encoding complexity of intra-mode HEVC can be drastically reduced by replacing the brute-force search with ETH-CNN to decide the CU partition. Third, an early-terminated hierarchical LSTM (ETH-LSTM) is proposed to learn the temporal correlation of the CU partition. Then, we combine ETH-LSTM and ETH-CNN to predict the CU partition for reducing the HEVC complexity for inter-mode. Finally, experimental results show that our approach outperforms other state-of-the-art approaches in reducing the HEVC complexity at both intra- and inter-modes.
ER  -


TY  - Preprint
T1  - Identifying Nominals with No Head Match Co-references Using Deep Learning
A1  - M. Stone
A1  - R. Arora
JO  - ArXiv e-prints
Y1  - 2 October, 2017
UR  - https://arxiv.org/abs/1710.00936
N2  - Identifying nominals with no head match is a long-standing challenge in coreference resolution with current systems performing significantly worse than humans. In this paper we present a new neural network architecture which outperforms the current state-of-the-art system on the English portion of the CoNLL 2012 Shared Task. This is done by using a logistic regression on features produced by two submodels, one of which is has the architecture proposed in [CM16a] while the other combines domain specific embeddings of the antecedent and the mention. We also propose some simple additional features which seem to improve performance for all models substantially, increasing F1 by almost 4% on basic logistic regression and other complex models.
ER  -


TY  - Preprint
T1  - Deep Learning for Unsupervised Insider Threat Detection in Structured Cybersecurity Data Streams
A1  - Aaron Tuor
A1  - Samuel Kaplan
A1  - Brian Hutchinson
A1  - Nicole Nichols
A1  - Sean Robinson
JO  - ArXiv e-prints
Y1  - 15 December, 2017
UR  - https://arxiv.org/abs/1710.00811
N2  - Analysis of an organization&#39;s computer network activity is a key component of early detection and mitigation of insider threat, a growing concern for many organizations. Raw system logs are a prototypical example of streaming data that can quickly scale beyond the cognitive power of a human analyst. As a prospective filter for the human analyst, we present an online unsupervised deep learning approach to detect anomalous network activity from system logs in real time. Our models decompose anomaly scores into the contributions of individual user behavior features for increased interpretability to aid analysts reviewing potential cases of insider threat. Using the CERT Insider Threat Dataset v6.2 and threat detection recall as our performance metric, our novel deep and recurrent neural network models outperform Principal Component Analysis, Support Vector Machine and Isolation Forest based anomaly detection baselines. For our best model, the events labeled as insider threat activity in our dataset had an average anomaly score in the 95.53 percentile, demonstrating our approach&#39;s potential to greatly reduce analyst workloads.
ER  -


TY  - Preprint
T1  - Margin Sample Mining Loss: A Deep Learning Based Method for Person Re-identification
A1  - Qiqi Xiao
A1  - Hao Luo
A1  - Chi Zhang
JO  - ArXiv e-prints
Y1  - 6 October, 2017
UR  - https://arxiv.org/abs/1710.00478
N2  - Person re-identification (ReID) is an important task in computer vision. Recently, deep learning with a metric learning loss has become a common framework for ReID. In this paper, we also propose a new metric learning loss with hard sample mining called margin smaple mining loss (MSML) which can achieve better accuracy compared with other metric learning losses, such as triplet loss. In experi- ments, our proposed methods outperforms most of the state-of-the-art algorithms on Market1501, MARS, CUHK03 and CUHK-SYSU.
ER  -


TY  - Preprint
T1  - Parameter Sharing Deep Deterministic Policy Gradient for Cooperative Multi-agent Reinforcement Learning
A1  - Xiangxiang Chu
A1  - Hangjun Ye
JO  - ArXiv e-prints
Y1  - 2 October, 2017
UR  - https://arxiv.org/abs/1710.00336
N2  - Deep reinforcement learning for multi-agent cooperation and competition has been a hot topic recently. This paper focuses on cooperative multi-agent problem based on actor-critic methods under local observations settings. Multi agent deep deterministic policy gradient obtained state of art results for some multi-agent games, whereas, it cannot scale well with growing amount of agents. In order to boost scalability, we propose a parameter sharing deterministic policy gradient method with three variants based on neural networks, including actor-critic sharing, actor sharing and actor sharing with partially shared critic. Benchmarks from rllab show that the proposed method has advantages in learning speed and memory efficiency, well scales with growing amount of agents, and moreover, it can make full use of reward sharing and exchangeability if possible.
ER  -


TY  - Preprint
T1  - DeepWheat: Estimating Phenotypic Traits from Crop Images with Deep Learning
A1  - Shubhra Aich
A1  - Anique Josuttes
A1  - Ilya Ovsyannikov
A1  - Keegan Strueby
A1  - Imran Ahmed
A1  - Hema Sudhakar Duddu
A1  - Curtis Pozniak
A1  - Steve Shirtliffe
A1  - Ian Stavness
JO  - ArXiv e-prints
Y1  - 26 January, 2018
UR  - https://arxiv.org/abs/1710.00241
N2  - In this paper, we investigate estimating emergence and biomass traits from color images and elevation maps of wheat field plots. We employ a state-of-the-art deconvolutional network for segmentation and convolutional architectures, with residual and Inception-like layers, to estimate traits via high dimensional nonlinear regression. Evaluation was performed on two different species of wheat, grown in field plots for an experimental plant breeding study. Our framework achieves satisfactory performance with mean and standard deviation of absolute difference of 1.05 and 1.40 counts for emergence and 1.45 and 2.05 for biomass estimation. Our results for counting wheat plants from field images are better than the accuracy reported for the similar, but arguably less difficult, task of counting leaves from indoor images of rosette plants. Our results for biomass estimation, even with a very small dataset, improve upon all previously proposed approaches in the literature.
ER  -


TY  - Preprint
T1  - The Deep Ritz method: A deep learning-based numerical algorithm for solving variational problems
A1  - Weinan E
A1  - Bing Yu
JO  - ArXiv e-prints
Y1  - 30 September, 2017
UR  - https://arxiv.org/abs/1710.00211
N2  - We propose a deep learning based method, the Deep Ritz Method, for numerically solving variational problems, particularly the ones that arise from partial differential equations. The Deep Ritz method is naturally nonlinear, naturally adaptive and has the potential to work in rather high dimensions. The framework is quite simple and fits well with the stochastic gradient descent method used in deep learning. We illustrate the method on several problems including some eigenvalue problems.
ER  -


TY  - Preprint
T1  - Self-supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation
A1  - Gregory Kahn
A1  - Adam Villaflor
A1  - Bosen Ding
A1  - Pieter Abbeel
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 17 May, 2018
UR  - https://arxiv.org/abs/1709.10489
N2  - Enabling robots to autonomously navigate complex environments is essential for real-world deployment. Prior methods approach this problem by having the robot maintain an internal map of the world, and then use a localization and planning method to navigate through the internal map. However, these approaches often include a variety of assumptions, are computationally intensive, and do not learn from failures. In contrast, learning-based methods improve as the robot acts in the environment, but are difficult to deploy in the real-world due to their high sample complexity. To address the need to learn complex policies with few samples, we propose a generalized computation graph that subsumes value-based model-free methods and model-based methods, with specific instantiations interpolating between model-free and model-based. We then instantiate this graph to form a navigation model that learns from raw images and is sample efficient. Our simulated car experiments explore the design decisions of our navigation model, and show our approach outperforms single-step and $N$-step double Q-learning. We also evaluate our approach on a real-world RC car and show it can learn to navigate through a complex indoor environment with a few hours of fully autonomous, self-supervised training. Videos of the experiments and code can be found at github.com/gkahn13/gcg
ER  -


TY  - Preprint
T1  - A representer theorem for deep kernel learning
A1  - Bastian Bohn
A1  - Michael Griebel
A1  - Christian Rieger
JO  - ArXiv e-prints
Y1  - 7 June, 2018
UR  - https://arxiv.org/abs/1709.10441
N2  - In this paper we provide a finite-sample and an infinite-sample representer theorem for the concatenation of (linear combinations of) kernel functions of reproducing kernel Hilbert spaces. These results serve as mathematical foundation for the analysis of machine learning algorithms based on compositions of functions. As a direct consequence in the finite-sample case, the corresponding infinite-dimensional minimization problems can be recast into (nonlinear) finite-dimensional minimization problems, which can be tackled with nonlinear optimization algorithms. Moreover, we show how concatenated machine learning problems can be reformulated as neural networks and how our representer theorem applies to a broad class of state-of-the-art deep learning methods.
ER  -


TY  - Preprint
T1  - Impact of Three-Dimensional Video Scalability on Multi-View Activity Recognition using Deep Learning
A1  - Jun-Ho Choi
A1  - Manri Cheon
A1  - Min-Su Choi
A1  - Jong-Seok Lee
JO  - ArXiv e-prints
Y1  - 28 September, 2017
UR  - https://arxiv.org/abs/1709.10206
N2  - Human activity recognition is one of the important research topics in computer vision and video understanding. It is often assumed that high quality video sequences are available for recognition. However, relaxing such a requirement and implementing robust recognition using videos having reduced data rates can achieve efficiency in storing and transmitting video data. Three-dimensional video scalability, which refers to the possibility of reducing spatial, temporal, and quality resolutions of videos, is an effective way for flexible representation and management of video data. In this paper, we investigate the impact of the video scalability on multi-view activity recognition. We employ both a spatiotemporal feature extraction-based method and a deep learning-based method using convolutional and recurrent neural networks. The recognition performance of the two methods is examined, along with in-depth analysis regarding how their performance vary with respect to various scalability combinations. In particular, we demonstrate that the deep learning-based method can achieve significantly improved robustness in comparison to the feature-based method. Furthermore, we investigate optimal scalability combinations with respect to bitrate in order to provide useful guidelines for an optimal operation policy in resource-constrained activity recognition systems.
ER  -


TY  - Preprint
T1  - Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations
A1  - Aravind Rajeswaran
A1  - Vikash Kumar
A1  - Abhishek Gupta
A1  - Giulia Vezzani
A1  - John Schulman
A1  - Emanuel Todorov
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 26 June, 2018
UR  - https://arxiv.org/abs/1709.10087
N2  - Dexterous multi-fingered hands are extremely versatile and provide a generic way to perform a multitude of tasks in human-centric environments. However, effectively controlling them remains challenging due to their high dimensionality and large number of potential contacts. Deep reinforcement learning (DRL) provides a model-agnostic approach to control complex dynamical systems, but has not been shown to scale to high-dimensional dexterous manipulation. Furthermore, deployment of DRL on physical systems remains challenging due to sample inefficiency. Consequently, the success of DRL in robotics has thus far been limited to simpler manipulators and tasks. In this work, we show that model-free DRL can effectively scale up to complex manipulation tasks with a high-dimensional 24-DoF hand, and solve them from scratch in simulated experiments. Furthermore, with the use of a small number of human demonstrations, the sample complexity can be significantly reduced, which enables learning with sample sizes equivalent to a few hours of robot experience. The use of demonstrations result in policies that exhibit very natural movements and, surprisingly, are also substantially more robust.
ER  -


TY  - Preprint
T1  - Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning
A1  - Pinxin Long
A1  - Tingxiang Fan
A1  - Xinyi Liao
A1  - Wenxi Liu
A1  - Hao Zhang
A1  - Jia Pan
JO  - ArXiv e-prints
Y1  - 20 May, 2018
UR  - https://arxiv.org/abs/1709.10082
N2  - Developing a safe and efficient collision avoidance policy for multiple robots is challenging in the decentralized scenarios where each robot generate its paths without observing other robots&#39; states and intents. While other distributed multi-robot collision avoidance systems exist, they often require extracting agent-level features to plan a local collision-free action, which can be computationally prohibitive and not robust. More importantly, in practice the performance of these methods are much lower than their centralized counterparts.
ER  -


TY  - Preprint
T1  - Deep Learning Assisted Heuristic Tree Search for the Container Pre-marshalling Problem
A1  - AndrÃ© Hottung
A1  - Shunji Tanaka
A1  - Kevin Tierney
JO  - ArXiv e-prints
Y1  - 28 September, 2017
UR  - https://arxiv.org/abs/1709.09972
N2  - One of the key challenges for operations researchers solving real-world problems is designing and implementing high-quality heuristics to guide their search procedures. In the past, machine learning techniques have failed to play a major role in operations research approaches, especially in terms of guiding branching and pruning decisions. We integrate deep neural networks into a heuristic tree search procedure to decide which branch to choose next and to estimate a bound for pruning the search tree of an optimization problem. We call our approach Deep Learning assisted heuristic Tree Search (DLTS) and apply it to a well-known problem from the container terminals literature, the container pre-marshalling problem (CPMP). Our approach is able to learn heuristics customized to the CPMP solely through analyzing the solutions to CPMP instances, and applies this knowledge within a heuristic tree search to produce the highest quality heuristic solutions to the CPMP to date.
ER  -


TY  - Preprint
T1  - Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge
A1  - Ryota Hinami
A1  - Tao Mei
A1  - Shin&#39;ichi Satoh
JO  - ArXiv e-prints
Y1  - 26 September, 2017
UR  - https://arxiv.org/abs/1709.09121
N2  - This paper addresses the problem of joint detection and recounting of abnormal events in videos. Recounting of abnormal events, i.e., explaining why they are judged to be abnormal, is an unexplored but critical task in video surveillance, because it helps human observers quickly judge if they are false alarms or not. To describe the events in the human-understandable form for event recounting, learning generic knowledge about visual concepts (e.g., object and action) is crucial. Although convolutional neural networks (CNNs) have achieved promising results in learning such concepts, it remains an open question as to how to effectively use CNNs for abnormal event detection, mainly due to the environment-dependent nature of the anomaly detection. In this paper, we tackle this problem by integrating a generic CNN model and environment-dependent anomaly detectors. Our approach first learns CNN with multiple visual tasks to exploit semantic information that is useful for detecting and recounting abnormal events. By appropriately plugging the model into anomaly detectors, we can detect and recount abnormal events while taking advantage of the discriminative power of CNNs. Our approach outperforms the state-of-the-art on Avenue and UCSD Ped2 benchmarks for abnormal event detection and also produces promising results of abnormal event recounting.
ER  -


TY  - Preprint
T1  - Converting Your Thoughts to Texts: Enabling Brain Typing via Deep Feature Learning of EEG Signals
A1  - Xiang Zhang
A1  - Lina Yao
A1  - Quan Z. Sheng
A1  - Salil S. Kanhere
A1  - Tao Gu
A1  - Dalin Zhang
JO  - ArXiv e-prints
Y1  - 26 September, 2017
UR  - https://arxiv.org/abs/1709.08820
N2  - An electroencephalography (EEG) based Brain Computer Interface (BCI) enables people to communicate with the outside world by interpreting the EEG signals of their brains to interact with devices such as wheelchairs and intelligent robots. More specifically, motor imagery EEG (MI-EEG), which reflects a subjects active intent, is attracting increasing attention for a variety of BCI applications. Accurate classification of MI-EEG signals while essential for effective operation of BCI systems, is challenging due to the significant noise inherent in the signals and the lack of informative correlation between the signals and brain activities. In this paper, we propose a novel deep neural network based learning framework that affords perceptive insights into the relationship between the MI-EEG data and brain activities. We design a joint convolutional recurrent neural network that simultaneously learns robust high-level feature presentations through low-dimensional dense embeddings from raw MI-EEG signals. We also employ an Autoencoder layer to eliminate various artifacts such as background activities. The proposed approach has been evaluated extensively on a large- scale public MI-EEG dataset and a limited but easy-to-deploy dataset collected in our lab. The results show that our approach outperforms a series of baselines and the competitive state-of-the- art methods, yielding a classification accuracy of 95.53%. The applicability of our proposed approach is further demonstrated with a practical BCI system for typing.
ER  -


TY  - Preprint
T1  - A Deep Learning Model for Traffic Flow State Classification Based on Smart Phone Sensor Data
A1  - Wenwen Tu
A1  - Feng Xiao
A1  - Liping Fu
A1  - Guangyuan Pan
JO  - ArXiv e-prints
Y1  - 25 September, 2017
UR  - https://arxiv.org/abs/1709.08802
N2  - This study proposes a Deep Belief Network model to classify traffic flow states. The model is capable of processing massive, high-density, and noise-contaminated data sets generated from smartphone sensors. The statistical features of Vehicle acceleration, angular acceleration, and GPS speed data, recorded by smartphone software, are analyzed, and then used as input for traffic flow state classification. Data from a five-day experiment is used to train and test the proposed model. A total of 747,856 sets of data are generated and used for both traffic flow states classification and sensitivity analysis of input variables. The result shows that the proposed Deep Belief Network model is superior to traditional machine learning methods in both classification performance and computational efficiency.
ER  -


TY  - Preprint
T1  - Image similarity using Deep CNN and Curriculum Learning
A1  - Srikar Appalaraju
A1  - Vineet Chaoji
JO  - ArXiv e-prints
Y1  - 13 July, 2018
UR  - https://arxiv.org/abs/1709.08761
N2  - Image similarity involves fetching similar looking images given a reference image. Our solution called SimNet, is a deep siamese network which is trained on pairs of positive and negative images using a novel online pair mining strategy inspired by Curriculum learning. We also created a multi-scale CNN, where the final image embedding is a joint representation of top as well as lower layer embedding&#39;s. We go on to show that this multi-scale siamese network is better at capturing fine grained image similarities than traditional CNN&#39;s.
ER  -


TY  - Preprint
T1  - Generative learning for deep networks
A1  - Boris Flach
A1  - Alexander Shekhovtsov
A1  - Ondrej Fikar
JO  - ArXiv e-prints
Y1  - 25 September, 2017
UR  - https://arxiv.org/abs/1709.08524
N2  - Learning, taking into account full distribution of the data, referred to as generative, is not feasible with deep neural networks (DNNs) because they model only the conditional distribution of the outputs given the inputs. Current solutions are either based on joint probability models facing difficult estimation problems or learn two separate networks, mapping inputs to outputs (recognition) and vice-versa (generation). We propose an intermediate approach. First, we show that forward computation in DNNs with logistic sigmoid activations corresponds to a simplified approximate Bayesian inference in a directed probabilistic multi-layer model. This connection allows to interpret DNN as a probabilistic model of the output and all hidden units given the input. Second, we propose that in order for the recognition and generation networks to be more consistent with the joint model of the data, weights of the recognition and generator network should be related by transposition. We demonstrate in a tentative experiment that such a coupled pair can be learned generatively, modelling the full distribution of the data, and has enough capacity to perform well in both recognition and generation.
ER  -


TY  - Preprint
T1  - Towards continuous control of flippers for a multi-terrain robot using deep reinforcement learning
A1  - Giuseppe Paolo
A1  - Lei Tai
A1  - Ming Liu
JO  - ArXiv e-prints
Y1  - 25 September, 2017
UR  - https://arxiv.org/abs/1709.08430
N2  - In this paper we focus on developing a control algorithm for multi-terrain tracked robots with flippers using a reinforcement learning (RL) approach. The work is based on the deep deterministic policy gradient (DDPG) algorithm, proven to be very successful in simple simulation environments. The algorithm works in an end-to-end fashion in order to control the continuous position of the flippers. This end-to-end approach makes it easy to apply the controller to a wide array of circumstances, but the huge flexibility comes to the cost of an increased difficulty of solution. The complexity of the task is enlarged even more by the fact that real multi-terrain robots move in partially observable environments. Notwithstanding these complications, being able to smoothly control a multi-terrain robot can produce huge benefits in impaired people daily lives or in search and rescue situations.
ER  -


TY  - Preprint
T1  - Deep Learning Based Cryptographic Primitive Classification
A1  - Gregory D. Hill
A1  - Xavier J. A. Bellekens
JO  - ArXiv e-prints
Y1  - 25 September, 2017
UR  - https://arxiv.org/abs/1709.08385
N2  - Cryptovirological augmentations present an immediate, incomparable threat. Over the last decade, the substantial proliferation of crypto-ransomware has had widespread consequences for consumers and organisations alike. Established preventive measures perform well, however, the problem has not ceased. Reverse engineering potentially malicious software is a cumbersome task due to platform eccentricities and obfuscated transmutation mechanisms, hence requiring smarter, more efficient detection strategies. The following manuscript presents a novel approach for the classification of cryptographic primitives in compiled binary executables using deep learning. The model blueprint, a DCNN, is fittingly configured to learn from variable-length control flow diagnostics output from a dynamic trace. To rival the size and variability of contemporary data compendiums, hence feeding the model cognition, a methodology for the procedural generation of synthetic cryptographic binaries is defined, utilising core primitives from OpenSSL with multivariate obfuscation, to draw a vastly scalable distribution. The library, CryptoKnight, rendered an algorithmic pool of AES, RC4, Blowfish, MD5 and RSA to synthesis combinable variants which are automatically fed in its core model. Converging at 91% accuracy, CryptoKnight is successfully able to classify the sample algorithms with minimal loss.
ER  -


TY  - Preprint
T1  - HDLTex: Hierarchical Deep Learning for Text Classification
A1  - Kamran Kowsari
A1  - Donald E. Brown
A1  - Mojtaba Heidarysafa
A1  - Kiana Jafari Meimandi
A1  - Matthew S. Gerber
A1  - Laura E. Barnes
JO  - ArXiv e-prints
Y1  - 6 October, 2017
UR  - https://arxiv.org/abs/1709.08267
N2  - The continually increasing number of documents produced each year necessitates ever improving information processing methods for searching, retrieving, and organizing text. Central to these information processing methods is document classification, which has become an important application for supervised learning. Recently the performance of these traditional classifiers has degraded as the number of documents has increased. This is because along with this growth in the number of documents has come an increase in the number of categories. This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification. Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification (HDLTex). HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy.
ER  -


TY  - Preprint
T1  - A Hybrid DSP/Deep Learning Approach to Real-Time Full-Band Speech Enhancement
A1  - Jean-Marc Valin
JO  - ArXiv e-prints
Y1  - 31 May, 2018
UR  - https://arxiv.org/abs/1709.08243
N2  - Despite noise suppression being a mature area in signal processing, it remains highly dependent on fine tuning of estimator algorithms and parameters. In this paper, we demonstrate a hybrid DSP/deep learning approach to noise suppression. A deep neural network with four hidden layers is used to estimate ideal critical band gains, while a more traditional pitch filter attenuates noise between pitch harmonics. The approach achieves significantly higher quality than a traditional minimum mean squared error spectral estimator, while keeping the complexity low enough for real-time operation at 48 kHz on a low-power processor.
ER  -


TY  - Preprint
T1  - Constrained Deep Transfer Feature Learning and its Applications
A1  - Yue Wu
A1  - Qiang Ji
JO  - ArXiv e-prints
Y1  - 23 September, 2017
UR  - https://arxiv.org/abs/1709.08128
N2  - Feature learning with deep models has achieved impressive results for both data representation and classification for various vision tasks. Deep feature learning, however, typically requires a large amount of training data, which may not be feasible for some application domains. Transfer learning can be one of the approaches to alleviate this problem by transferring data from data-rich source domain to data-scarce target domain. Existing transfer learning methods typically perform one-shot transfer learning and often ignore the specific properties that the transferred data must satisfy. To address these issues, we introduce a constrained deep transfer feature learning method to perform simultaneous transfer learning and feature learning by performing transfer learning in a progressively improving feature space iteratively in order to better narrow the gap between the target domain and the source domain for effective transfer of the data from the source domain to target domain. Furthermore, we propose to exploit the target domain knowledge and incorporate such prior knowledge as a constraint during transfer learning to ensure that the transferred data satisfies certain properties of the target domain. To demonstrate the effectiveness of the proposed constrained deep transfer feature learning method, we apply it to thermal feature learning for eye detection by transferring from the visible domain. We also applied the proposed method for cross-view facial expression recognition as a second application. The experimental results demonstrate the effectiveness of the proposed method for both applications.
ER  -


TY  - Preprint
T1  - Deep Learning for Secure Mobile Edge Computing
A1  - Yuanfang Chen
A1  - Yan Zhang
A1  - Sabita Maharjan
JO  - ArXiv e-prints
Y1  - 23 September, 2017
UR  - https://arxiv.org/abs/1709.08025
N2  - Mobile edge computing (MEC) is a promising approach for enabling cloud-computing capabilities at the edge of cellular networks. Nonetheless, security is becoming an increasingly important issue in MEC-based applications. In this paper, we propose a deep-learning-based model to detect security threats. The model uses unsupervised learning to automate the detection process, and uses location information as an important feature to improve the performance of detection. Our proposed model can be used to detect malicious applications at the edge of a cellular network, which is a serious security threat. Extensive experiments are carried out with 10 different datasets, the results of which illustrate that our deep-learning-based model achieves an average gain of 6% accuracy compared with state-of-the-art machine learning algorithms.
ER  -


TY  - Preprint
T1  - OptLayer - Practical Constrained Optimization for Deep Reinforcement Learning in the Real World
A1  - Tu-Hoa Pham
A1  - Giovanni De Magistris
A1  - Ryuki Tachibana
JO  - ArXiv e-prints
Y1  - 23 February, 2018
UR  - https://arxiv.org/abs/1709.07643
N2  - While deep reinforcement learning techniques have recently produced considerable achievements on many decision-making problems, their use in robotics has largely been limited to simulated worlds or restricted motions, since unconstrained trial-and-error interactions in the real world can have undesirable consequences for the robot or its environment. To overcome such limitations, we propose a novel reinforcement learning architecture, OptLayer, that takes as inputs possibly unsafe actions predicted by a neural network and outputs the closest actions that satisfy chosen constraints. While learning control policies often requires carefully crafted rewards and penalties while exploring the range of possible actions, OptLayer ensures that only safe actions are actually executed and unsafe predictions are penalized during training. We demonstrate the effectiveness of our approach on robot reaching tasks, both simulated and in the real world.
ER  -


TY  - Preprint
T1  - AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection
A1  - Thanh-Toan Do
A1  - Anh Nguyen
A1  - Ian Reid
JO  - ArXiv e-prints
Y1  - 4 March, 2018
UR  - https://arxiv.org/abs/1709.07326
N2  - We propose AffordanceNet, a new deep learning approach to simultaneously detect multiple objects and their affordances from RGB images. Our AffordanceNet has two branches: an object detection branch to localize and classify the object, and an affordance detection branch to assign each pixel in the object to its most probable affordance label. The proposed framework employs three key components for effectively handling the multiclass problem in the affordance mask: a sequence of deconvolutional layers, a robust resizing strategy, and a multi-task loss function. The experimental results on the public datasets show that our AffordanceNet outperforms recent state-of-the-art methods by a fair margin, while its end-to-end architecture allows the inference at the speed of 150ms per image. This makes our AffordanceNet well suitable for real-time robotic applications. Furthermore, we demonstrate the effectiveness of AffordanceNet in different testing environments and in real robotic applications. The source code is available at https://github.com/nqanh/affordance-net
ER  -


TY  - Preprint
T1  - Local Communication Protocols for Learning Complex Swarm Behaviors with Deep Reinforcement Learning
A1  - Maximilian HÃ¼ttenrauch
A1  - Adrian Å oÅ¡iÄ
A1  - Gerhard Neumann
JO  - ArXiv e-prints
Y1  - 18 July, 2018
UR  - https://arxiv.org/abs/1709.07224
N2  - Swarm systems constitute a challenging problem for reinforcement learning (RL) as the algorithm needs to learn decentralized control policies that can cope with limited local sensing and communication abilities of the agents. While it is often difficult to directly define the behavior of the agents, simple communication protocols can be defined more easily using prior knowledge about the given task. In this paper, we propose a number of simple communication protocols that can be exploited by deep reinforcement learning to find decentralized control policies in a multi-robot swarm environment. The protocols are based on histograms that encode the local neighborhood relations of the agents and can also transmit task-specific information, such as the shortest distance and direction to a desired target. In our framework, we use an adaptation of Trust Region Policy Optimization to learn complex collaborative tasks, such as formation building and building a communication link. We evaluate our findings in a simulated 2D-physics environment, and compare the implications of different communication protocols.
ER  -


TY  - Preprint
T1  - Agile Off-Road Autonomous Driving Using End-to-End Deep Imitation Learning
A1  - Yunpeng Pan
A1  - Ching-An Cheng
A1  - Kamil Saigol
A1  - Keuntaek Lee
A1  - Xinyan Yan
A1  - Evangelos Theodorou
A1  - Byron Boots
JO  - ArXiv e-prints
Y1  - 10 September, 2018
UR  - https://arxiv.org/abs/1709.07174
N2  - We present an end-to-end imitation learning system for agile, off-road autonomous driving using only low-cost on-board sensors. By imitating a model predictive controller equipped with advanced sensors, we train a deep neural network control policy to map raw, high-dimensional observations to continuous steering and throttle commands. Compared with recent approaches to similar tasks, our method requires neither state estimation nor on-the-fly planning to navigate the vehicle. Our approach relies on, and experimentally validates, recent imitation learning theory. Empirically, we show that policies trained with online imitation learning overcome well-known challenges related to covariate shift and generalize better than policies trained with batch imitation learning. Built on these insights, our autonomous driving system demonstrates successful high-speed off-road driving, matching the state-of-the-art performance.
ER  -


TY  - Preprint
T1  - A Deep-Reinforcement Learning Approach for Software-Defined Networking Routing Optimization
A1  - Giorgio Stampa
A1  - Marta Arias
A1  - David Sanchez-Charles
A1  - Victor Muntes-Mulero
A1  - Albert Cabellos
JO  - ArXiv e-prints
Y1  - 20 September, 2017
UR  - https://arxiv.org/abs/1709.07080
N2  - In this paper we design and evaluate a Deep-Reinforcement Learning agent that optimizes routing. Our agent adapts automatically to current traffic conditions and proposes tailored configurations that attempt to minimize the network delay. Experiments show very promising performance. Moreover, this approach provides important operational advantages with respect to traditional optimization algorithms.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Dexterous Manipulation with Concept Networks
A1  - Aditya Gudimella
A1  - Ross Story
A1  - Matineh Shaker
A1  - Ruofan Kong
A1  - Matthew Brown
A1  - Victor Shnayder
A1  - Marcos Campos
JO  - ArXiv e-prints
Y1  - 20 September, 2017
UR  - https://arxiv.org/abs/1709.06977
N2  - Deep reinforcement learning yields great results for a large array of problems, but models are generally retrained anew for each new problem to be solved. Prior learning and knowledge are difficult to incorporate when training new models, requiring increasingly longer training as problems become more complex. This is especially problematic for problems with sparse rewards. We provide a solution to these problems by introducing Concept Network Reinforcement Learning (CNRL), a framework which allows us to decompose problems using a multi-level hierarchy. Concepts in a concept network are reusable, and flexible enough to encapsulate feature extractors, skills, or other concept networks. With this hierarchical learning approach, deep reinforcement learning can be used to solve complex tasks in a modular way, through problem decomposition. We demonstrate the strength of CNRL by training a model to grasp a rectangular prism and precisely stack it on top of a cube using a gripper on a Kinova JACO arm, simulated in MuJoCo. Our experiments show that our use of hierarchy results in a 45x reduction in environment interactions compared to the state-of-the-art on this task.
ER  -


TY  - Preprint
T1  - Open Source Dataset and Deep Learning Models for Online Digit Gesture Recognition on Touchscreens
A1  - Philip J. Corr
A1  - Guenole C. Silvestre
A1  - Chris J. Bleakley
JO  - ArXiv e-prints
Y1  - 20 September, 2017
UR  - https://arxiv.org/abs/1709.06871
N2  - This paper presents an evaluation of deep neural networks for recognition of digits entered by users on a smartphone touchscreen. A new large dataset of Arabic numerals was collected for training and evaluation of the network. The dataset consists of spatial and temporal touch data recorded for 80 digits entered by 260 users. Two neural network models were investigated. The first model was a 2D convolutional neural (ConvNet) network applied to bitmaps of the glpyhs created by interpolation of the sensed screen touches and its topology is similar to that of previously published models for offline handwriting recognition from scanned images. The second model used a 1D ConvNet architecture but was applied to the sequence of polar vectors connecting the touch points. The models were found to provide accuracies of 98.50% and 95.86%, respectively. The second model was much simpler, providing a reduction in the number of parameters from 1,663,370 to 287,690. The dataset has been made available to the community as an open source resource.
ER  -


TY  - Preprint
T1  - UnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning
A1  - Ruihao Li
A1  - Sen Wang
A1  - Zhiqiang Long
A1  - Dongbing Gu
JO  - ArXiv e-prints
Y1  - 21 February, 2018
UR  - https://arxiv.org/abs/1709.06841
N2  - We propose a novel monocular visual odometry (VO) system called UnDeepVO in this paper. UnDeepVO is able to estimate the 6-DoF pose of a monocular camera and the depth of its view by using deep neural networks. There are two salient features of the proposed UnDeepVO: one is the unsupervised deep learning scheme, and the other is the absolute scale recovery. Specifically, we train UnDeepVO by using stereo image pairs to recover the scale but test it by using consecutive monocular images. Thus, UnDeepVO is a monocular system. The loss function defined for training the networks is based on spatial and temporal dense information. A system overview is shown in Fig. 1. The experiments on KITTI dataset show our UnDeepVO achieves good performance in terms of pose accuracy.
ER  -


TY  - Preprint
T1  - Updating the silent speech challenge benchmark with deep learning
A1  - Yan Ji
A1  - Licheng Liu
A1  - Hongcui Wang
A1  - Zhilei Liu
A1  - Zhibin Niu
A1  - Bruce Denby
JO  - ArXiv e-prints
Y1  - 20 September, 2017
UR  - https://arxiv.org/abs/1709.06818
N2  - The 2010 Silent Speech Challenge benchmark is updated with new results obtained in a Deep Learning strategy, using the same input features and decoding strategy as in the original article. A Word Error Rate of 6.4% is obtained, compared to the published value of 17.4%. Additional results comparing new auto-encoder-based features with the original features at reduced dimensionality, as well as decoding scenarios on two different language models, are also presented. The Silent Speech Challenge archive has been updated to contain both the original and the new auto-encoder features, in addition to the original raw data.
ER  -


TY  - Preprint
T1  - Dex-Net 3.0: Computing Robust Robot Vacuum Suction Grasp Targets in Point Clouds using a New Analytic Model and Deep Learning
A1  - Jeffrey Mahler
A1  - Matthew Matl
A1  - Xinyu Liu
A1  - Albert Li
A1  - David Gealy
A1  - Ken Goldberg
JO  - ArXiv e-prints
Y1  - 13 April, 2018
UR  - https://arxiv.org/abs/1709.06670
N2  - Vacuum-based end effectors are widely used in industry and are often preferred over parallel-jaw and multifinger grippers due to their ability to lift objects with a single point of contact. Suction grasp planners often target planar surfaces on point clouds near the estimated centroid of an object. In this paper, we propose a compliant suction contact model that computes the quality of the seal between the suction cup and local target surface and a measure of the ability of the suction grasp to resist an external gravity wrench. To characterize grasps, we estimate robustness to perturbations in end-effector and object pose, material properties, and external wrenches. We analyze grasps across 1,500 3D object models to generate Dex-Net 3.0, a dataset of 2.8 million point clouds, suction grasps, and grasp robustness labels. We use Dex-Net 3.0 to train a Grasp Quality Convolutional Neural Network (GQ-CNN) to classify robust suction targets in point clouds containing a single object. We evaluate the resulting system in 350 physical trials on an ABB YuMi fitted with a pneumatic suction gripper. When evaluated on novel objects that we categorize as Basic (prismatic or cylindrical), Typical (more complex geometry), and Adversarial (with few available suction-grasp points) Dex-Net 3.0 achieves success rates of 98$\%$, 82$\%$, and 58$\%$ respectively, improving to 81$\%$ in the latter case when the training set includes only adversarial objects. Code, datasets, and supplemental material can be found at http://berkeleyautomation.github.io/dex-net .
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Event-Driven Multi-Agent Decision Processes
A1  - Kunal Menda
A1  - Yi-Chun Chen
A1  - Justin Grana
A1  - James W. Bono
A1  - Brendan D. Tracey
A1  - Mykel J. Kochenderfer
A1  - David Wolpert
JO  - ArXiv e-prints
Y1  - 19 September, 2017
UR  - https://arxiv.org/abs/1709.06656
N2  - The incorporation of macro-actions (temporally extended actions) into multi-agent decision problems has the potential to address the curse of dimensionality associated with such decision problems. Since macro-actions last for stochastic durations, multiple agents executing decentralized policies in cooperative environments must act asynchronously. We present an algorithm that modifies Generalized Advantage Estimation for temporally extended actions, allowing a state-of-the-art policy optimization algorithm to optimize policies in Dec-POMDPs in which agents act asynchronously. We show that our algorithm is capable of learning optimal policies in two cooperative domains, one involving real-time bus holding control and one involving wildfire fighting with unmanned aircraft. Our algorithm works by framing problems as &#34;event-driven decision processes,&#34; which are scenarios where the sequence and timing of actions and events are random and governed by an underlying stochastic process. In addition to optimizing policies with continuous state and action spaces, our algorithm also facilitates the use of event-driven simulators, which do not require time to be discretized into time-steps. We demonstrate the benefit of using event-driven simulation in the context of multiple agents taking asynchronous actions. We show that fixed time-step simulation risks obfuscating the sequence in which closely-separated events occur, adversely affecting the policies learned. Additionally, we show that arbitrarily shrinking the time-step scales poorly with the number of agents.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning that Matters
A1  - Peter Henderson
A1  - Riashat Islam
A1  - Philip Bachman
A1  - Joelle Pineau
A1  - Doina Precup
A1  - David Meger
JO  - ArXiv e-prints
Y1  - 24 November, 2017
UR  - https://arxiv.org/abs/1709.06560
N2  - In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.
ER  -


TY  - Preprint
T1  - When 3D-Aided 2D Face Recognition Meets Deep Learning: An extended UR2D for Pose-Invariant Face Recognition
A1  - Xiang Xu
A1  - Pengfei Dou
A1  - Ha A. Le
A1  - Ioannis A. Kakadiaris
JO  - ArXiv e-prints
Y1  - 19 September, 2017
UR  - https://arxiv.org/abs/1709.06532
N2  - Most of the face recognition works focus on specific modules or demonstrate a research idea. This paper presents a pose-invariant 3D-aided 2D face recognition system (UR2D) that is robust to pose variations as large as 90? by leveraging deep learning technology. The architecture and the interface of UR2D are described, and each module is introduced in detail. Extensive experiments are conducted on the UHDB31 and IJB-A, demonstrating that UR2D outperforms existing 2D face recognition systems such as VGG-Face, FaceNet, and a commercial off-the-shelf software (COTS) by at least 9% on the UHDB31 dataset and 3% on the IJB-A dataset on average in face identification tasks. UR2D also achieves state-of-the-art performance of 85% on the IJB-A dataset by comparing the Rank-1 accuracy score from template matching. It fills a gap by providing a 3D-aided 2D face recognition system that has compatible results with 2D face recognition systems using deep learning techniques.
ER  -


TY  - Preprint
T1  - A Deep Learning-based Framework for Conducting Stealthy Attacks in Industrial Control Systems
A1  - Cheng Feng
A1  - Tingting Li
A1  - Zhanxing Zhu
A1  - Deeph Chana
JO  - ArXiv e-prints
Y1  - 19 September, 2017
UR  - https://arxiv.org/abs/1709.06397
N2  - Industrial control systems (ICS), which in many cases are components of critical national infrastructure, are increasingly being connected to other networks and the wider internet motivated by factors such as enhanced operational functionality and improved efficiency. However, set in this context, it is easy to see that the cyber attack surface of these systems is expanding, making it more important than ever that innovative solutions for securing ICS be developed and that the limitations of these solutions are well understood. The development of anomaly based intrusion detection techniques has provided capability for protecting ICS from the serious physical damage that cyber breaches are capable of delivering to them by monitoring sensor and control signals for abnormal activity. Recently, the use of so-called stealthy attacks has been demonstrated where the injection of false sensor measurements can be used to mimic normal control system signals, thereby defeating anomaly detectors whilst still delivering attack objectives. In this paper we define a deep learning-based framework which allows an attacker to conduct stealthy attacks with minimal a-priori knowledge of the target ICS. Specifically, we show that by intercepting the sensor and/or control signals in an ICS for a period of time, a malicious program is able to automatically learn to generate high-quality stealthy attacks which can achieve specific attack goals whilst bypassing a black box anomaly detector. Furthermore, we demonstrate the effectiveness of our framework for conducting stealthy attacks using two real-world ICS case studies. We contend that our results motivate greater attention on this area by the security community as we demonstrate that currently assumed barriers for the successful execution of such attacks are relaxed.
ER  -


TY  - Preprint
T1  - Guided Deep Reinforcement Learning for Swarm Systems
A1  - Maximilian HÃ¼ttenrauch
A1  - Adrian Å oÅ¡iÄ
A1  - Gerhard Neumann
JO  - ArXiv e-prints
Y1  - 18 September, 2017
UR  - https://arxiv.org/abs/1709.06011
N2  - In this paper, we investigate how to learn to control a group of cooperative agents with limited sensing capabilities such as robot swarms. The agents have only very basic sensor capabilities, yet in a group they can accomplish sophisticated tasks, such as distributed assembly or search and rescue tasks. Learning a policy for a group of agents is difficult due to distributed partial observability of the state. Here, we follow a guided approach where a critic has central access to the global state during learning, which simplifies the policy evaluation problem from a reinforcement learning point of view. For example, we can get the positions of all robots of the swarm using a camera image of a scene. This camera image is only available to the critic and not to the control policies of the robots. We follow an actor-critic approach, where the actors base their decisions only on locally sensed information. In contrast, the critic is learned based on the true global state. Our algorithm uses deep reinforcement learning to approximate both the Q-function and the policy. The performance of the algorithm is evaluated on two tasks with simple simulated 2D agents: 1) finding and maintaining a certain distance to each others and 2) locating a target.
ER  -


TY  - Preprint
T1  - Deep Learning for Automatic Stereotypical Motor Movement Detection using Wearable Sensors in Autism Spectrum Disorders
A1  - Nastaran Mohammadian Rad
A1  - Seyed Mostafa Kia
A1  - Calogero Zarbo
A1  - Twan van Laarhoven
A1  - Giuseppe Jurman
A1  - Paola Venuti
A1  - Elena Marchiori
A1  - Cesare Furlanello
JO  - ArXiv e-prints
Y1  - 14 September, 2017
UR  - https://arxiv.org/abs/1709.05956
N2  - Autism Spectrum Disorders are associated with atypical movements, of which stereotypical motor movements (SMMs) interfere with learning and social interaction. The automatic SMM detection using inertial measurement units (IMU) remains complex due to the strong intra and inter-subject variability, especially when handcrafted features are extracted from the signal. We propose a new application of the deep learning to facilitate automatic SMM detection using multi-axis IMUs. We use a convolutional neural network (CNN) to learn a discriminative feature space from raw data. We show how the CNN can be used for parameter transfer learning to enhance the detection rate on longitudinal data. We also combine the long short-term memory (LSTM) with CNN to model the temporal patterns in a sequence of multi-axis signals. Further, we employ ensemble learning to combine multiple LSTM learners into a more robust SMM detector. Our results show that: 1) feature learning outperforms handcrafted features; 2) parameter transfer learning is beneficial in longitudinal settings; 3) using LSTM to learn the temporal dynamic of signals enhances the detection rate especially for skewed training data; 4) an ensemble of LSTMs provides more accurate and stable detectors. These findings provide a significant step toward accurate SMM detection in real-time scenarios.
ER  -


TY  - Preprint
T1  - AJILE Movement Prediction: Multimodal Deep Learning for Natural Human Neural Recordings and Video
A1  - Nancy Xin Ru Wang
A1  - Ali Farhadi
A1  - Rajesh Rao
A1  - Bingni Brunton
JO  - ArXiv e-prints
Y1  - 1 March, 2018
UR  - https://arxiv.org/abs/1709.05939
N2  - Developing useful interfaces between brains and machines is a grand challenge of neuroengineering. An effective interface has the capacity to not only interpret neural signals, but predict the intentions of the human to perform an action in the near future; prediction is made even more challenging outside well-controlled laboratory experiments. This paper describes our approach to detect and to predict natural human arm movements in the future, a key challenge in brain computer interfacing that has never before been attempted. We introduce the novel Annotated Joints in Long-term ECoG (AJILE) dataset; AJILE includes automatically annotated poses of 7 upper body joints for four human subjects over 670 total hours (more than 72 million frames), along with the corresponding simultaneously acquired intracranial neural recordings. The size and scope of AJILE greatly exceeds all previous datasets with movements and electrocorticography (ECoG), making it possible to take a deep learning approach to movement prediction. We propose a multimodal model that combines deep convolutional neural networks (CNN) with long short-term memory (LSTM) blocks, leveraging both ECoG and video modalities. We demonstrate that our models are able to detect movements and predict future movements up to 800 msec before movement initiation. Further, our multimodal movement prediction models exhibit resilience to simulated ablation of input neural signals. We believe a multimodal approach to natural neural decoding that takes context into account is critical in advancing bioelectronic technologies and human neuroscience.
ER  -


TY  - Preprint
T1  - Multi-Task Learning for Segmentation of Building Footprints with Deep Neural Networks
A1  - Benjamin Bischke
A1  - Patrick Helber
A1  - Joachim Folz
A1  - Damian Borth
A1  - Andreas Dengel
JO  - ArXiv e-prints
Y1  - 18 September, 2017
UR  - https://arxiv.org/abs/1709.05932
N2  - The increased availability of high resolution satellite imagery allows to sense very detailed structures on the surface of our planet. Access to such information opens up new directions in the analysis of remote sensing imagery. However, at the same time this raises a set of new challenges for existing pixel-based prediction methods, such as semantic segmentation approaches. While deep neural networks have achieved significant advances in the semantic segmentation of high resolution images in the past, most of the existing approaches tend to produce predictions with poor boundaries. In this paper, we address the problem of preserving semantic segmentation boundaries in high resolution satellite imagery by introducing a new cascaded multi-task loss. We evaluate our approach on Inria Aerial Image Labeling Dataset which contains large-scale and high resolution images. Our results show that we are able to outperform state-of-the-art methods by 8.3\% without any additional post-processing step.
ER  -


TY  - Preprint
T1  - Institutionally Distributed Deep Learning Networks
A1  - Ken Chang
A1  - Niranjan Balachandar
A1  - Carson K Lam
A1  - Darvin Yi
A1  - James M Brown
A1  - Andrew Beers
A1  - Bruce R Rosen
A1  - Daniel L Rubin
A1  - Jayashree Kalpathy-Cramer
JO  - ArXiv e-prints
Y1  - 10 September, 2017
UR  - https://arxiv.org/abs/1709.05929
N2  - Deep learning has become a promising approach for automated medical diagnoses. When medical data samples are limited, collaboration among multiple institutions is necessary to achieve high algorithm performance. However, sharing patient data often has limitations due to technical, legal, or ethical concerns. In such cases, sharing a deep learning model is a more attractive alternative. The best method of performing such a task is unclear, however. In this study, we simulate the dissemination of learning deep learning network models across four institutions using various heuristics and compare the results with a deep learning model trained on centrally hosted patient data. The heuristics investigated include ensembling single institution models, single weight transfer, and cyclical weight transfer. We evaluated these approaches for image classification in three independent image collections (retinal fundus photos, mammography, and ImageNet). We find that cyclical weight transfer resulted in a performance (testing accuracy = 77.3%) that was closest to that of centrally hosted patient data (testing accuracy = 78.7%). We also found that there is an improvement in the performance of cyclical weight transfer heuristic with high frequency of weight transfer.
ER  -


TY  - Preprint
T1  - IBM Deep Learning Service
A1  - Bishwaranjan Bhattacharjee
A1  - Scott Boag
A1  - Chandani Doshi
A1  - Parijat Dube
A1  - Ben Herta
A1  - Vatche Ishakian
A1  - K. R. Jayaram
A1  - Rania Khalaf
A1  - Avesh Krishna
A1  - Yu Bo Li
A1  - Vinod Muthusamy
A1  - Ruchir Puri
A1  - Yufei Ren
A1  - Florian Rosenberg
A1  - Seetharami R. Seelam
A1  - Yandong Wang
A1  - Jian Ming Zhang
A1  - Li Zhang
JO  - ArXiv e-prints
Y1  - 18 September, 2017
UR  - https://arxiv.org/abs/1709.05871
N2  - Deep learning driven by large neural network models is overtaking traditional machine learning methods for understanding unstructured and perceptual data domains such as speech, text, and vision. At the same time, the &#34;as-a-Service&#34;-based business model on the cloud is fundamentally transforming the information technology industry. These two trends: deep learning, and &#34;as-a-service&#34; are colliding to give rise to a new business model for cognitive application delivery: deep learning as a service in the cloud. In this paper, we will discuss the details of the software architecture behind IBM&#39;s deep learning as a service (DLaaS). DLaaS provides developers the flexibility to use popular deep learning libraries such as Caffe, Torch and TensorFlow, in the cloud in a scalable and resilient manner with minimal effort. The platform uses a distribution and orchestration layer that facilitates learning from a large amount of data in a reasonable amount of time across compute nodes. A resource provisioning layer enables flexible job management on heterogeneous resources, such as graphics processing units (GPUs) and central processing units (CPUs), in an infrastructure as a service (IaaS) cloud.
ER  -


TY  - Preprint
T1  - Adaptive Laplace Mechanism: Differential Privacy Preservation in Deep Learning
A1  - NhatHai Phan
A1  - Xintao Wu
A1  - Han Hu
A1  - Dejing Dou
JO  - ArXiv e-prints
Y1  - 22 April, 2018
UR  - https://arxiv.org/abs/1709.05750
N2  - In this paper, we focus on developing a novel mechanism to preserve differential privacy in deep neural networks, such that: (1) The privacy budget consumption is totally independent of the number of training steps; (2) It has the ability to adaptively inject noise into features based on the contribution of each to the output; and (3) It could be applied in a variety of different deep neural networks. To achieve this, we figure out a way to perturb affine transformations of neurons, and loss functions used in deep neural networks. In addition, our mechanism intentionally adds &#34;more noise&#34; into features which are &#34;less relevant&#34; to the model output, and vice-versa. Our theoretical analysis further derives the sensitivities and error bounds of our mechanism. Rigorous experiments conducted on MNIST and CIFAR-10 datasets show that our mechanism is highly effective and outperforms existing solutions.
ER  -


TY  - Preprint
T1  - Deep Automated Multi-task Learning
A1  - Davis Liang
A1  - Yan Shu
JO  - ArXiv e-prints
Y1  - 19 September, 2017
UR  - https://arxiv.org/abs/1709.05554
N2  - Multi-task learning (MTL) has recently contributed to learning better representations in service of various NLP tasks. MTL aims at improving the performance of a primary task, by jointly training on a secondary task. This paper introduces automated tasks, which exploit the sequential nature of the input data, as secondary tasks in an MTL model. We explore next word prediction, next character prediction, and missing word completion as potential automated tasks. Our results show that training on a primary task in parallel with a secondary automated task improves both the convergence speed and accuracy for the primary task. We suggest two methods for augmenting an existing network with automated tasks and establish better performance in topic prediction, sentiment analysis, and hashtag recommendation. Finally, we show that the MTL models can perform well on datasets that are small and colloquial by nature.
ER  -


TY  - Preprint
T1  - Multi-scale Deep Learning Architectures for Person Re-identification
A1  - Xuelin Qian
A1  - Yanwei Fu
A1  - Yu-Gang Jiang
A1  - Tao Xiang
A1  - Xiangyang Xue
JO  - ArXiv e-prints
Y1  - 15 September, 2017
UR  - https://arxiv.org/abs/1709.05165
N2  - Person Re-identification (re-id) aims to match people across non-overlapping camera views in a public space. It is a challenging problem because many people captured in surveillance videos wear similar clothes. Consequently, the differences in their appearance are often subtle and only detectable at the right location and scales. Existing re-id models, particularly the recently proposed deep learning based ones match people at a single scale. In contrast, in this paper, a novel multi-scale deep learning model is proposed. Our model is able to learn deep discriminative feature representations at different scales and automatically determine the most suitable scales for matching. The importance of different spatial locations for extracting discriminative features is also learned explicitly. Experiments are carried out to demonstrate that the proposed model outperforms the state-of-the art on a number of benchmarks
ER  -


TY  - Preprint
T1  - Transforming Cooling Optimization for Green Data Center via Deep Reinforcement Learning
A1  - Yuanlong Li
A1  - Yonggang Wen
A1  - Kyle Guan
A1  - Dacheng Tao
JO  - ArXiv e-prints
Y1  - 18 July, 2018
UR  - https://arxiv.org/abs/1709.05077
N2  - Cooling system plays a critical role in a modern data center (DC). Developing an optimal control policy for DC cooling system is a challenging task. The prevailing approaches often rely on approximating system models that are built upon the knowledge of mechanical cooling, electrical and thermal management, which is difficult to design and may lead to sub-optimal or unstable performances. In this paper, we propose utilizing the large amount of monitoring data in DC to optimize the control policy. To do so, we cast the cooling control policy design into an energy cost minimization problem with temperature constraints, and tap it into the emerging deep reinforcement learning (DRL) framework. Specifically, we propose an end-to-end cooling control algorithm (CCA) that is based on the actor-critic framework and an off-policy offline version of the deep deterministic policy gradient (DDPG) algorithm. In the proposed CCA, an evaluation network is trained to predict an energy cost counter penalized by the cooling status of the DC room, and a policy network is trained to predict optimized control settings when gave the current load and weather information. The proposed algorithm is evaluated on the EnergyPlus simulation platform and on a real data trace collected from the National Super Computing Centre (NSCC) of Singapore. Our results show that the proposed CCA can achieve about 11% cooling cost saving on the simulation platform compared with a manually configured baseline control algorithm. In the trace-based study, we propose a de-underestimation validation mechanism as we cannot directly test the algorithm on a real DC. Even though with DUE the results are conservative, we can still achieve about 15% cooling energy saving on the NSCC data trace if we set the inlet temperature threshold at 26.6 degree Celsius.
ER  -


TY  - Preprint
T1  - Accelerating SGD for Distributed Deep-Learning Using Approximated Hessian Matrix
A1  - SÃ©bastien M. R. Arnold
A1  - Chunming Wang
JO  - ArXiv e-prints
Y1  - 15 September, 2017
UR  - https://arxiv.org/abs/1709.05069
N2  - We introduce a novel method to compute a rank $m$ approximation of the inverse of the Hessian matrix in the distributed regime. By leveraging the differences in gradients and parameters of multiple Workers, we are able to efficiently implement a distributed approximation of the Newton-Raphson method. We also present preliminary results which underline advantages and challenges of second-order methods for large stochastic optimization problems. In particular, our work suggests that novel strategies for combining gradients provide further information on the loss surface.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Conversational AI
A1  - Mahipal Jadeja
A1  - Neelanshi Varia
A1  - Agam Shah
JO  - ArXiv e-prints
Y1  - 15 September, 2017
UR  - https://arxiv.org/abs/1709.05067
N2  - Deep reinforcement learning is revolutionizing the artificial intelligence field. Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world. It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games. In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement learning and supervised learning and models for implementation of reinforcement are discussed. Key challenges related to the implementation of reinforcement learning in conversational AI domain are identified as well as discussed in detail. Various conversational models which are based on deep reinforcement learning (as well as deep learning) are also discussed. In summary, this paper discusses key aspects of deep reinforcement learning which are crucial for designing an efficient conversational AI.
ER  -


TY  - Preprint
T1  - Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting
A1  - Bing Yu
A1  - Haoteng Yin
A1  - Zhanxing Zhu
JO  - ArXiv e-prints
Y1  - 12 July, 2018
UR  - https://arxiv.org/abs/1709.04875
N2  - Timely accurate traffic forecast is crucial for urban traffic control and guidance. Due to the high nonlinearity and complexity of traffic flow, traditional methods cannot satisfy the requirements of mid-and-long term prediction tasks and often neglect spatial and temporal dependencies. In this paper, we propose a novel deep learning framework, Spatio-Temporal Graph Convolutional Networks (STGCN), to tackle the time series prediction problem in traffic domain. Instead of applying regular convolutional and recurrent units, we formulate the problem on graphs and build the model with complete convolutional structures, which enable much faster training speed with fewer parameters. Experiments show that our model STGCN effectively captures comprehensive spatio-temporal correlations through modeling multi-scale traffic networks and consistently outperforms state-of-the-art baselines on various real-world traffic datasets.
ER  -


TY  - Preprint
T1  - An Exploration of 2D and 3D Deep Learning Techniques for Cardiac MR Image Segmentation
A1  - Christian F. Baumgartner
A1  - Lisa M. Koch
A1  - Marc Pollefeys
A1  - Ender Konukoglu
JO  - ArXiv e-prints
Y1  - 10 October, 2017
UR  - https://arxiv.org/abs/1709.04496
N2  - Accurate segmentation of the heart is an important step towards evaluating cardiac function. In this paper, we present a fully automated framework for segmentation of the left (LV) and right (RV) ventricular cavities and the myocardium (Myo) on short-axis cardiac MR images. We investigate various 2D and 3D convolutional neural network architectures for this task. We investigate the suitability of various state-of-the art 2D and 3D convolutional neural network architectures, as well as slight modifications thereof, for this task. Experiments were performed on the ACDC 2017 challenge training dataset comprising cardiac MR images of 100 patients, where manual reference segmentations were made available for end-diastolic (ED) and end-systolic (ES) frames. We find that processing the images in a slice-by-slice fashion using 2D networks is beneficial due to a relatively large slice thickness. However, the exact network architecture only plays a minor role. We report mean Dice coefficients of $0.950$ (LV), $0.893$ (RV), and $0.899$ (Myo), respectively with an average evaluation time of 1.1 seconds per volume on a modern GPU.
ER  -


TY  - Preprint
T1  - A Tutorial on Deep Learning for Music Information Retrieval
A1  - Keunwoo Choi
A1  - GyÃ¶rgy Fazekas
A1  - Kyunghyun Cho
A1  - Mark Sandler
JO  - ArXiv e-prints
Y1  - 3 May, 2018
UR  - https://arxiv.org/abs/1709.04396
N2  - Following their success in Computer Vision and other areas, deep learning techniques have recently become widely adopted in Music Information Retrieval (MIR) research. However, the majority of works aim to adopt and assess methods that have been shown to be effective in other domains, while there is still a great need for more original research focusing on music primarily and utilising musical knowledge and insight. The goal of this paper is to boost the interest of beginners by providing a comprehensive tutorial and reducing the barriers to entry into deep learning for MIR. We lay out the basic principles and review prominent works in this hard to navigate the field. We then outline the network structures that have been successful in MIR problems and facilitate the selection of building blocks for the problems at hand. Finally, guidelines for new tasks and some advanced topics in deep learning are discussed to stimulate new research in this fascinating field.
ER  -


TY  - Preprint
T1  - Automated Cloud Provisioning on AWS using Deep Reinforcement Learning
A1  - Zhiguang Wang
A1  - Chul Gwon
A1  - Tim Oates
A1  - Adam Iezzi
JO  - ArXiv e-prints
Y1  - 19 September, 2017
UR  - https://arxiv.org/abs/1709.04305
N2  - As the use of cloud computing continues to rise, controlling cost becomes increasingly important. Yet there is evidence that 30\% - 45\% of cloud spend is wasted. Existing tools for cloud provisioning typically rely on highly trained human experts to specify what to monitor, thresholds for triggering action, and actions. In this paper we explore the use of reinforcement learning (RL) to acquire policies to balance performance and spend, allowing humans to specify what they want as opposed to how to do it, minimizing the need for cloud expertise. Empirical results with tabular, deep, and dueling double deep Q-learning with the CloudSim simulator show the utility of RL and the relative merits of the approaches. We also demonstrate effective policy transfer learning from an extremely simple simulator to CloudSim, with the next step being transfer from CloudSim to an Amazon Web Services physical environment.
ER  -


TY  - Preprint
T1  - Action Schema Networks: Generalised Policies with Deep Learning
A1  - Sam Toyer
A1  - Felipe Trevizan
A1  - Sylvie ThiÃ©baux
A1  - Lexing Xie
JO  - ArXiv e-prints
Y1  - 22 December, 2017
UR  - https://arxiv.org/abs/1709.04271
N2  - In this paper, we introduce the Action Schema Network (ASNet): a neural network architecture for learning generalised policies for probabilistic planning problems. By mimicking the relational structure of planning problems, ASNets are able to adopt a weight-sharing scheme which allows the network to be applied to any problem from a given planning domain. This allows the cost of training the network to be amortised over all problems in that domain. Further, we propose a training method which balances exploration and supervised training on small problems to produce a policy which remains robust when evaluated on larger problems. In experiments, we show that ASNet&#39;s learning capability allows it to significantly outperform traditional non-learning planners in several challenging domains.
ER  -


TY  - Preprint
T1  - Co-training for Demographic Classification Using Deep Learning from Label Proportions
A1  - Ehsan Mohammady Ardehaly
A1  - Aron Culotta
JO  - ArXiv e-prints
Y1  - 12 September, 2017
UR  - https://arxiv.org/abs/1709.04108
N2  - Deep learning algorithms have recently produced state-of-the-art accuracy in many classification tasks, but this success is typically dependent on access to many annotated training examples. For domains without such data, an attractive alternative is to train models with light, or distant supervision. In this paper, we introduce a deep neural network for the Learning from Label Proportion (LLP) setting, in which the training data consist of bags of unlabeled instances with associated label distributions for each bag. We introduce a new regularization layer, Batch Averager, that can be appended to the last layer of any deep neural network to convert it from supervised learning to LLP. This layer can be implemented readily with existing deep learning packages. To further support domains in which the data consist of two conditionally independent feature views (e.g. image and text), we propose a co-training algorithm that iteratively generates pseudo bags and refits the deep LLP model to improve classification accuracy. We demonstrate our models on demographic attribute classification (gender and race/ethnicity), which has many applications in social media analysis, public health, and marketing. We conduct experiments to predict demographics of Twitter users based on their tweets and profile image, without requiring any user-level annotations for training. We find that the deep LLP approach outperforms baselines for both text and image features separately. Additionally, we find that co-training algorithm improves image and text classification by 4% and 8% absolute F1, respectively. Finally, an ensemble of text and image classifiers further improves the absolute F1 measure by 4% on average.
ER  -


TY  - Preprint
T1  - Pre-training Neural Networks with Human Demonstrations for Deep Reinforcement Learning
A1  - Gabriel V. de la Cruz Jr
A1  - Yunshu Du
A1  - Matthew E. Taylor
JO  - ArXiv e-prints
Y1  - 12 September, 2017
UR  - https://arxiv.org/abs/1709.04083
N2  - Deep reinforcement learning (deep RL) has achieved superior performance in complex sequential tasks by using a deep neural network as its function approximator and by learning directly from raw images. A drawback of using raw images is that deep RL must learn the state feature representation from the raw images in addition to learning a policy. As a result, deep RL can require a prohibitively large amount of training time and data to reach reasonable performance, making it difficult to use deep RL in real-world applications, especially when data is expensive. In this work, we speed up training by addressing half of what deep RL is trying to solve --- learning features. Our approach is to learn some of the important features by pre-training deep RL network&#39;s hidden layers via supervised learning using a small set of human demonstrations. We empirically evaluate our approach using deep Q-network (DQN) and asynchronous advantage actor-critic (A3C) algorithms on the Atari 2600 games of Pong, Freeway, and Beamrider. Our results show that: 1) pre-training with human demonstrations in a supervised learning manner is better at discovering features relative to pre-training naively in DQN, and 2) initializing a deep RL network with a pre-trained model provides a significant improvement in training time even when pre-training from a small number of human demonstrations.
ER  -


TY  - Preprint
T1  - Explore, Exploit or Listen: Combining Human Feedback and Policy Model to Speed up Deep Reinforcement Learning in 3D Worlds
A1  - Zhiyu Lin
A1  - Brent Harrison
A1  - Aaron Keech
A1  - Mark O. Riedl
JO  - ArXiv e-prints
Y1  - 12 September, 2017
UR  - https://arxiv.org/abs/1709.03969
N2  - We describe a method to use discrete human feedback to enhance the performance of deep learning agents in virtual three-dimensional environments by extending deep-reinforcement learning to model the confidence and consistency of human feedback. This enables deep reinforcement learning algorithms to determine the most appropriate time to listen to the human feedback, exploit the current policy model, or explore the agent&#39;s environment. Managing the trade-off between these three strategies allows DRL agents to be robust to inconsistent or intermittent human feedback. Through experimentation using a synthetic oracle, we show that our technique improves the training speed and overall performance of deep reinforcement learning in navigating three-dimensional environments using Minecraft. We further show that our technique is robust to highly innacurate human feedback and can also operate when no human feedback is given.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning with Surrogate Agent-Environment Interface
A1  - Song Wang
A1  - Yu Jing
JO  - ArXiv e-prints
Y1  - 10 November, 2017
UR  - https://arxiv.org/abs/1709.03942
N2  - In this paper, we propose surrogate agent-environment interface (SAEI) in reinforcement learning. We also state that learning based on probability surrogate agent-environment interface provides optimal policy of task agent-environment interface. We introduce surrogate probability action and develop the probability surrogate action deterministic policy gradient (PSADPG) algorithm based on SAEI. This algorithm enables continuous control of discrete action. The experiments show PSADPG achieves the performance of DQN in certain tasks with the stochastic optimal policy nature in the initial training stage.
ER  -


TY  - Preprint
T1  - Interpreting Shared Deep Learning Models via Explicable Boundary Trees
A1  - Huijun Wu
A1  - Chen Wang
A1  - Jie Yin
A1  - Kai Lu
A1  - Liming Zhu
JO  - ArXiv e-prints
Y1  - 12 September, 2017
UR  - https://arxiv.org/abs/1709.03730
N2  - Despite outperforming the human in many tasks, deep neural network models are also criticized for the lack of transparency and interpretability in decision making. The opaqueness results in uncertainty and low confidence when deploying such a model in model sharing scenarios, when the model is developed by a third party. For a supervised machine learning model, sharing training process including training data provides an effective way to gain trust and to better understand model predictions. However, it is not always possible to share all training data due to privacy and policy constraints. In this paper, we propose a method to disclose a small set of training data that is just sufficient for users to get the insight of a complicated model. The method constructs a boundary tree using selected training data and the tree is able to approximate the complicated model with high fidelity. We show that traversing data points in the tree gives users significantly better understanding of the model and paves the way for trustworthy model sharing.
ER  -


TY  - Preprint
T1  - NiftyNet: a deep-learning platform for medical imaging
A1  - Eli Gibson
A1  - Wenqi Li
A1  - Carole Sudre
A1  - Lucas Fidon
A1  - Dzhoshkun I. Shakir
A1  - Guotai Wang
A1  - Zach Eaton-Rosen
A1  - Robert Gray
A1  - Tom Doel
A1  - Yipeng Hu
A1  - Tom Whyntie
A1  - Parashkev Nachev
A1  - Marc Modat
A1  - Dean C. Barratt
A1  - SÃ©bastien Ourselin
A1  - M. Jorge Cardoso
A1  - Tom Vercauteren
JO  - ArXiv e-prints
Y1  - 16 October, 2017
UR  - https://arxiv.org/abs/1709.03485
N2  - Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions. Established deep-learning platforms are flexible but do not provide specific functionality for medical image analysis and adapting them for this application requires substantial implementation effort. Thus, there has been substantial duplication of effort and incompatible infrastructure developed across many research groups. This work presents the open-source NiftyNet platform for deep learning in medical imaging. The ambition of NiftyNet is to accelerate and simplify the development of these solutions, and to provide a common mechanism for disseminating research outputs for the community to use, adapt and build upon.
ER  -


TY  - Preprint
T1  - Autonomous Quadrotor Landing using Deep Reinforcement Learning
A1  - Riccardo Polvara
A1  - Massimiliano Patacchiola
A1  - Sanjay Sharma
A1  - Jian Wan
A1  - Andrew Manning
A1  - Robert Sutton
A1  - Angelo Cangelosi
JO  - ArXiv e-prints
Y1  - 27 February, 2018
UR  - https://arxiv.org/abs/1709.03339
N2  - Landing an unmanned aerial vehicle (UAV) on a ground marker is an open problem despite the effort of the research community. Previous attempts mostly focused on the analysis of hand-crafted geometric features and the use of external sensors in order to allow the vehicle to approach the land-pad. In this article, we propose a method based on deep reinforcement learning that only requires low-resolution images taken from a down-looking camera in order to identify the position of the marker and land the UAV on it. The proposed approach is based on a hierarchy of Deep Q-Networks (DQNs) used as high-level control policy for the navigation toward the marker. We implemented different technical solutions, such as the combination of vanilla and double DQNs, and a partitioned buffer replay. Using domain randomization we trained the vehicle on uniform textures and we tested it on a large variety of simulated and real-world environments. The overall performance is comparable with a state-of-the-art algorithm and human pilots.
ER  -


TY  - Preprint
T1  - What does fault tolerant Deep Learning need from MPI?
A1  - Vinay Amatya
A1  - Abhinav Vishnu
A1  - Charles Siegel
A1  - Jeff Daily
JO  - ArXiv e-prints
Y1  - 11 September, 2017
UR  - https://arxiv.org/abs/1709.03316
N2  - Deep Learning (DL) algorithms have become the de facto Machine Learning (ML) algorithm for large scale data analysis. DL algorithms are computationally expensive - even distributed DL implementations which use MPI require days of training (model learning) time on commonly studied datasets. Long running DL applications become susceptible to faults - requiring development of a fault tolerant system infrastructure, in addition to fault tolerant DL algorithms. This raises an important question: What is needed from MPI for de- signing fault tolerant DL implementations? In this paper, we address this problem for permanent faults. We motivate the need for a fault tolerant MPI specification by an in-depth consideration of recent innovations in DL algorithms and their properties, which drive the need for specific fault tolerance features. We present an in-depth discussion on the suitability of different parallelism types (model, data and hybrid); a need (or lack thereof) for check-pointing of any critical data structures; and most importantly, consideration for several fault tolerance proposals (user-level fault mitigation (ULFM), Reinit) in MPI and their applicability to fault tolerant DL implementations. We leverage a distributed memory implementation of Caffe, currently available under the Machine Learning Toolkit for Extreme Scale (MaTEx). We implement our approaches by ex- tending MaTEx-Caffe for using ULFM-based implementation. Our evaluation using the ImageNet dataset and AlexNet, and GoogLeNet neural network topologies demonstrates the effectiveness of the proposed fault tolerant DL implementation using OpenMPI based ULFM.
ER  -


TY  - Preprint
T1  - Fully Optical Spacecraft Communications: Implementing an Omnidirectional PV-Cell Receiver and 8Mb/s LED Visible Light Downlink with Deep Learning Error Correction
A1  - Sihao Huang
A1  - Haowen Lin
JO  - ArXiv e-prints
Y1  - 2 January, 2018
UR  - https://arxiv.org/abs/1709.03222
N2  - Free space optical communication techniques have been the subject of numerous investigations in recent years, with multiple missions expected to fly in the near future. Existing methods require high pointing accuracies, drastically driving up overall system cost. Recent developments in LED-based visible light communication (VLC) and past in-orbit experiments have convinced us that the technology has reached a critical level of maturity. On these premises, we propose a new optical communication system utilizing a VLC downlink and a high throughput, omnidirectional photovoltaic cell receiver system. By performing error-correction via deep learning methods and by utilizing phase-delay interference, the system is able to deliver data rates that match those of traditional laser-based solutions. A prototype of the proposed system has been constructed, demonstrating the scheme to be a feasible alternative to laser-based methods. This creates an opportunity for the full scale development of optical communication techniques on small spacecraft as a backup telemetry beacon or as a high throughput link.
ER  -


TY  - Preprint
T1  - Robust Emotion Recognition from Low Quality and Low Bit Rate Video: A Deep Learning Approach
A1  - Bowen Cheng
A1  - Zhangyang Wang
A1  - Zhaobin Zhang
A1  - Zhu Li
A1  - Ding Liu
A1  - Jianchao Yang
A1  - Shuai Huang
A1  - Thomas S. Huang
JO  - ArXiv e-prints
Y1  - 10 September, 2017
UR  - https://arxiv.org/abs/1709.03126
N2  - Emotion recognition from facial expressions is tremendously useful, especially when coupled with smart devices and wireless multimedia applications. However, the inadequate network bandwidth often limits the spatial resolution of the transmitted video, which will heavily degrade the recognition reliability. We develop a novel framework to achieve robust emotion recognition from low bit rate video. While video frames are downsampled at the encoder side, the decoder is embedded with a deep network model for joint super-resolution (SR) and recognition. Notably, we propose a novel max-mix training strategy, leading to a single &#34;One-for-All&#34; model that is remarkably robust to a vast range of downsampling factors. That makes our framework well adapted for the varied bandwidths in real transmission scenarios, without hampering scalability or efficiency. The proposed framework is evaluated on the AVEC 2016 benchmark, and demonstrates significantly improved stand-alone recognition performance, as well as rate-distortion (R-D) performance, than either directly recognizing from LR frames, or separating SR and recognition.
ER  -


TY  - Preprint
T1  - Optimal Transport for Deep Joint Transfer Learning
A1  - Ying Lu
A1  - Liming Chen
A1  - Alexandre Saidi
JO  - ArXiv e-prints
Y1  - 9 September, 2017
UR  - https://arxiv.org/abs/1709.02995
N2  - Training a Deep Neural Network (DNN) from scratch requires a large amount of labeled data. For a classification task where only small amount of training data is available, a common solution is to perform fine-tuning on a DNN which is pre-trained with related source data. This consecutive training process is time consuming and does not consider explicitly the relatedness between different source and target tasks.
ER  -


TY  - Preprint
T1  - A Deep Structured Learning Approach Towards Automating Connectome Reconstruction from 3D Electron Micrographs
A1  - Jan Funke
A1  - Fabian David Tschopp
A1  - William Grisaitis
A1  - Arlo Sheridan
A1  - Chandan Singh
A1  - Stephan Saalfeld
A1  - Srinivas C. Turaga
JO  - ArXiv e-prints
Y1  - 24 September, 2017
UR  - https://arxiv.org/abs/1709.02974
N2  - We present a deep structured learning method for neuron segmentation from 3D electron microscopy (EM) which improves significantly upon the state of the art in terms of accuracy and scalability. Our method consists of a 3D U-Net classifier predicting affinity graphs on voxels, followed by iterative region agglomeration. We train the U-Net using a new structured loss based on MALIS that encourages topological correctness. Our extension consists of two parts: First, an $O(n\log(n))$ method to compute the loss gradient, improving over the originally proposed $O(n^2)$ algorithm. Second, we compute the gradient in two separate passes to avoid spurious contributions in early training stages. Our affinity predictions are accurate enough that simple agglomeration outperforms more involved methods used earlier on inferior predictions. We present results on three datasets (CREMI, FIB, and SegEM) of different imaging techniques and animals and achieve improvements over previous results of 27%, 15%, and 250%. Our findings suggest that a single 3D segmentation strategy can be applied to both isotropic and anisotropic EM data. The runtime of our method scales with $O(n)$ in the size of the volume and achieves a throughput of about 2.6 seconds per megavoxel, allowing processing of very large datasets.
ER  -


TY  - Preprint
T1  - Urban morphology meets deep learning: Exploring urban forms in one million cities, town and villages across the planet
A1  - Vahid Moosavi
JO  - ArXiv e-prints
Y1  - 12 September, 2017
UR  - https://arxiv.org/abs/1709.02939
N2  - Study of urban form is an important area of research in urban planning/design that contributes to our understanding of how cities function and evolve. However, classical approaches are based on very limited observations and inconsistent methods. As an alternative, availability of massive urban data collections such as Open Street Map from the one hand and the recent advancements in machine learning methods such as deep learning techniques on the other have opened up new possibilities to automatically investigate urban forms at the global scale. In this work for the first time, by collecting a large data set of street networks in more than one million cities, towns and villages all over the world, we trained a deep convolutional auto-encoder, that automatically learns the hierarchical structures of urban forms and represents them via dense and comparable vectors. We showed how the learned urban vectors could be used for different investigations. Using the learned urban vectors, one is able to easily find and compare similar urban forms all over the world, considering their overall spatial structure and other factors such as orientation, graphical structure, and density and partial deformations. Further cluster analysis reveals the distribution of the main patterns of urban forms all over the planet.
ER  -


TY  - Preprint
T1  - Deep Packet: A Novel Approach For Encrypted Traffic Classification Using Deep Learning
A1  - Mohammad Lotfollahi
A1  - Ramin Shirali Hossein Zade
A1  - Mahdi Jafari Siavoshani
A1  - Mohammdsadegh Saberian
JO  - ArXiv e-prints
Y1  - 4 July, 2018
UR  - https://arxiv.org/abs/1709.02656
N2  - Internet traffic classification has become more important with rapid growth of current Internet network and online applications. There have been numerous studies on this topic which have led to many different approaches. Most of these approaches use predefined features extracted by an expert in order to classify network traffic. In contrast, in this study, we propose a \emph{deep learning} based approach which integrates both feature extraction and classification phases into one system. Our proposed scheme, called &#34;Deep Packet,&#34; can handle both \emph{traffic characterization} in which the network traffic is categorized into major classes (\eg, FTP and P2P) and application identification in which end-user applications (\eg, BitTorrent and Skype) identification is desired. Contrary to most of the current methods, Deep Packet can identify encrypted traffic and also distinguishes between VPN and non-VPN network traffic. After an initial pre-processing phase on data, packets are fed into Deep Packet framework that embeds stacked autoencoder and convolution neural network in order to classify network traffic. Deep packet with CNN as its classification model achieved recall of $0.98$ in application identification task and $0.94$ in traffic categorization task. To the best of our knowledge, Deep Packet outperforms all of the proposed classification methods on UNB ISCX VPN-nonVPN dataset.
ER  -


TY  - Preprint
T1  - DeepFense: Online Accelerated Defense Against Adversarial Deep Learning
A1  - Bita Darvish Rouhani
A1  - Mohammad Samragh
A1  - Mojan Javaheripi
A1  - Tara Javidi
A1  - Farinaz Koushanfar
JO  - ArXiv e-prints
Y1  - 20 August, 2018
UR  - https://arxiv.org/abs/1709.02538
N2  - Recent advances in adversarial Deep Learning (DL) have opened up a largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems. With the wide-spread usage of DL in critical and time-sensitive applications, including unmanned vehicles, drones, and video surveillance systems, online detection of malicious inputs is of utmost importance. We propose DeepFense, the first end-to-end automated framework that simultaneously enables efficient and safe execution of DL models. DeepFense formalizes the goal of thwarting adversarial attacks as an optimization problem that minimizes the rarely observed regions in the latent feature space spanned by a DL network. To solve the aforementioned minimization problem, a set of complementary but disjoint modular redundancies are trained to validate the legitimacy of the input samples in parallel with the victim DL model. DeepFense leverages hardware/software/algorithm co-design and customized acceleration to achieve just-in-time performance in resource-constrained settings. The proposed countermeasure is unsupervised, meaning that no adversarial sample is leveraged to train modular redundancies. We further provide an accompanying API to reduce the non-recurring engineering cost and ensure automated adaptation to various platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders of magnitude performance improvement while enabling online adversarial sample detection.
ER  -


TY  - Preprint
T1  - Deep Learning the Physics of Transport Phenomena
A1  - Amir Barati Farimani
A1  - Joseph Gomes
A1  - Vijay S. Pande
JO  - ArXiv e-prints
Y1  - 7 September, 2017
UR  - https://arxiv.org/abs/1709.02432
N2  - We have developed a new data-driven paradigm for the rapid inference, modeling and simulation of the physics of transport phenomena by deep learning. Using conditional generative adversarial networks (cGAN), we train models for the direct generation of solutions to steady state heat conduction and incompressible fluid flow purely on observation without knowledge of the underlying governing equations. Rather than using iterative numerical methods to approximate the solution of the constitutive equations, cGANs learn to directly generate the solutions to these phenomena, given arbitrary boundary conditions and domain, with high test accuracy (MAE$&lt;$1\%) and state-of-the-art computational performance. The cGAN framework can be used to learn causal models directly from experimental observations where the underlying physical model is complex or unknown.
ER  -


TY  - Preprint
T1  - A Deep Reinforcement Learning Chatbot
A1  - Iulian V. Serban
A1  - Chinnadhurai Sankar
A1  - Mathieu Germain
A1  - Saizheng Zhang
A1  - Zhouhan Lin
A1  - Sandeep Subramanian
A1  - Taesup Kim
A1  - Michael Pieper
A1  - Sarath Chandar
A1  - Nan Rosemary Ke
A1  - Sai Rajeshwar
A1  - Alexandre de Brebisson
A1  - Jose M. R. Sotelo
A1  - Dendi Suhubdy
A1  - Vincent Michalski
A1  - Alexandre Nguyen
A1  - Joelle Pineau
A1  - Yoshua Bengio
JO  - ArXiv e-prints
Y1  - 5 November, 2017
UR  - https://arxiv.org/abs/1709.02349
N2  - We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than many competing systems. Due to its machine learning architecture, the system is likely to improve with additional data.
ER  -


TY  - Preprint
T1  - Improving Sonar Image Patch Matching via Deep Learning
A1  - Matias Valdenegro-Toro
JO  - ArXiv e-prints
Y1  - 7 September, 2017
UR  - https://arxiv.org/abs/1709.02150
N2  - Matching sonar images with high accuracy has been a problem for a long time, as sonar images are inherently hard to model due to reflections, noise and viewpoint dependence. Autonomous Underwater Vehicles require good sonar image matching capabilities for tasks such as tracking, simultaneous localization and mapping (SLAM) and some cases of object detection/recognition. We propose the use of Convolutional Neural Networks (CNN) to learn a matching function that can be trained from labeled sonar data, after pre-processing to generate matching and non-matching pairs. In a dataset of 39K training pairs, we obtain 0.91 Area under the ROC Curve (AUC) for a CNN that outputs a binary classification matching decision, and 0.89 AUC for another CNN that outputs a matching score. In comparison, classical keypoint matching methods like SIFT, SURF, ORB and AKAZE obtain AUC 0.61 to 0.68. Alternative learning methods obtain similar results, with a Random Forest Classifier obtaining AUC 0.79, and a Support Vector Machine resulting in AUC 0.66.
ER  -


TY  - Preprint
T1  - Formulation of Deep Reinforcement Learning Architecture Toward Autonomous Driving for On-Ramp Merge
A1  - Pin Wang
A1  - Ching-Yao Chan
JO  - ArXiv e-prints
Y1  - 23 April, 2018
UR  - https://arxiv.org/abs/1709.02066
N2  - Multiple automakers have in development or in production automated driving systems (ADS) that offer freeway-pilot functions. This type of ADS is typically limited to restricted-access freeways only, that is, the transition from manual to automated modes takes place only after the ramp merging process is completed manually. One major challenge to extend the automation to ramp merging is that the automated vehicle needs to incorporate and optimize long-term objectives (e.g. successful and smooth merge) when near-term actions must be safely executed. Moreover, the merging process involves interactions with other vehicles whose behaviors are sometimes hard to predict but may influence the merging vehicle optimal actions. To tackle such a complicated control problem, we propose to apply Deep Reinforcement Learning (DRL) techniques for finding an optimal driving policy by maximizing the long-term reward in an interactive environment. Specifically, we apply a Long Short-Term Memory (LSTM) architecture to model the interactive environment, from which an internal state containing historical driving information is conveyed to a Deep Q-Network (DQN). The DQN is used to approximate the Q-function, which takes the internal state as input and generates Q-values as output for action selection. With this DRL architecture, the historical impact of interactive environment on the long-term reward can be captured and taken into account for deciding the optimal control policy. The proposed architecture has the potential to be extended and applied to other autonomous driving scenarios such as driving through a complex intersection or changing lanes under varying traffic flow conditions.
ER  -


TY  - Preprint
T1  - The Mating Rituals of Deep Neural Networks: Learning Compact Feature Representations through Sexual Evolutionary Synthesis
A1  - Audrey Chung
A1  - Mohammad Javad Shafiee
A1  - Paul Fieguth
A1  - Alexander Wong
JO  - ArXiv e-prints
Y1  - 6 September, 2017
UR  - https://arxiv.org/abs/1709.02043
N2  - Evolutionary deep intelligence was recently proposed as a method for achieving highly efficient deep neural network architectures over successive generations. Drawing inspiration from nature, we propose the incorporation of sexual evolutionary synthesis. Rather than the current asexual synthesis of networks, we aim to produce more compact feature representations by synthesizing more diverse and generalizable offspring networks in subsequent generations via the combination of two parent networks. Experimental results were obtained using the MNIST and CIFAR-10 datasets, and showed improved architectural efficiency and comparable testing accuracy relative to the baseline asexual evolutionary neural networks. In particular, the network synthesized via sexual evolutionary synthesis for MNIST had approximately double the architectural efficiency (cluster efficiency of 34.29X and synaptic efficiency of 258.37X) in comparison to the network synthesized via asexual evolutionary synthesis, with both networks achieving a testing accuracy of ~97%.
ER  -


TY  - Preprint
T1  - Implicit Regularization in Deep Learning
A1  - Behnam Neyshabur
JO  - ArXiv e-prints
Y1  - 7 September, 2017
UR  - https://arxiv.org/abs/1709.01953
N2  - In an attempt to better understand generalization in deep learning, we study several possible explanations. We show that implicit regularization induced by the optimization method is playing a key role in generalization and success of deep learning models. Motivated by this view, we study how different complexity measures can ensure generalization and explain how optimization algorithms can implicitly regularize complexity measures. We empirically investigate the ability of these measures to explain different observed phenomena in deep learning. We further study the invariances in neural networks, suggest complexity measures and optimization algorithms that have similar invariances to those in neural networks and evaluate them on a number of learning tasks.
ER  -


TY  - Preprint
T1  - Boosting Deep Learning Risk Prediction with Generative Adversarial Networks for Electronic Health Records
A1  - Zhengping Che
A1  - Yu Cheng
A1  - Shuangfei Zhai
A1  - Zhaonan Sun
A1  - Yan Liu
JO  - ArXiv e-prints
Y1  - 5 September, 2017
UR  - https://arxiv.org/abs/1709.01648
N2  - The rapid growth of Electronic Health Records (EHRs), as well as the accompanied opportunities in Data-Driven Healthcare (DDH), has been attracting widespread interests and attentions. Recent progress in the design and applications of deep learning methods has shown promising results and is forcing massive changes in healthcare academia and industry, but most of these methods rely on massive labeled data. In this work, we propose a general deep learning framework which is able to boost risk prediction performance with limited EHR data. Our model takes a modified generative adversarial network namely ehrGAN, which can provide plausible labeled EHR data by mimicking real patient records, to augment the training dataset in a semi-supervised learning manner. We use this generative model together with a convolutional neural network (CNN) based prediction model to improve the onset prediction performance. Experiments on two real healthcare datasets demonstrate that our proposed framework produces realistic data samples and achieves significant improvements on classification tasks with the generated data over several stat-of-the-art baselines.
ER  -


TY  - Preprint
T1  - Deep Learning Techniques for Music Generation - A Survey
A1  - Jean-Pierre Briot
A1  - GaÃ«tan Hadjeres
A1  - FranÃ§ois Pachet
JO  - ArXiv e-prints
Y1  - 5 September, 2017
UR  - https://arxiv.org/abs/1709.01620
N2  - This book is a survey and an analysis of different ways of using deep learning (deep artificial neural networks) to generate musical content. At first, we propose a methodology based on four dimensions for our analysis: - objective - What musical content is to be generated? (e.g., melody, accompaniment...); - representation - What are the information formats used for the corpus and for the expected generated output? (e.g., MIDI, piano roll, text...); - architecture - What type of deep neural network is to be used? (e.g., recurrent network, autoencoder, generative adversarial networks...); - strategy - How to model and control the process of generation (e.g., direct feedforward, sampling, unit selection...). For each dimension, we conduct a comparative analysis of various models and techniques. For the strategy dimension, we propose some tentative typology of possible approaches and mechanisms. This classification is bottom-up, based on the analysis of many existing deep-learning based systems for music generation, which are described in this book. The last part of the book includes discussion and prospects.
ER  -


TY  - Preprint
T1  - Opening the Black Box of Financial AI with CLEAR-Trade: A CLass-Enhanced Attentive Response Approach for Explaining and Visualizing Deep Learning-Driven Stock Market Prediction
A1  - Devinder Kumar
A1  - Graham W Taylor
A1  - Alexander Wong
JO  - ArXiv e-prints
Y1  - 5 September, 2017
UR  - https://arxiv.org/abs/1709.01574
N2  - Deep learning has been shown to outperform traditional machine learning algorithms across a wide range of problem domains. However, current deep learning algorithms have been criticized as uninterpretable &#34;black-boxes&#34; which cannot explain their decision making processes. This is a major shortcoming that prevents the widespread application of deep learning to domains with regulatory processes such as finance. As such, industries such as finance have to rely on traditional models like decision trees that are much more interpretable but less effective than deep learning for complex problems. In this paper, we propose CLEAR-Trade, a novel financial AI visualization framework for deep learning-driven stock market prediction that mitigates the interpretability issue of deep learning methods. In particular, CLEAR-Trade provides a effective way to visualize and explain decisions made by deep stock market prediction models. We show the efficacy of CLEAR-Trade in enhancing the interpretability of stock market prediction by conducting experiments based on S&amp;P 500 stock index prediction. The results demonstrate that CLEAR-Trade can provide significant insight into the decision-making process of deep learning-driven financial models, particularly for regulatory processes, thus improving their potential uptake in the financial industry.
ER  -


TY  - Preprint
T1  - BOOK: Storing Algorithm-Invariant Episodes for Deep Reinforcement Learning
A1  - Simyung Chang
A1  - YoungJoon Yoo
A1  - Jaeseok Choi
A1  - Nojun Kwak
JO  - ArXiv e-prints
Y1  - 12 February, 2018
UR  - https://arxiv.org/abs/1709.01308
N2  - We introduce a novel method to train agents of reinforcement learning (RL) by sharing knowledge in a way similar to the concept of using a book. The recorded information in the form of a book is the main means by which humans learn knowledge. Nevertheless, the conventional deep RL methods have mainly focused either on experiential learning where the agent learns through interactions with the environment from the start or on imitation learning that tries to mimic the teacher. Contrary to these, our proposed book learning shares key information among different agents in a book-like manner by delving into the following two characteristic features: (1) By defining the linguistic function, input states can be clustered semantically into a relatively small number of core clusters, which are forwarded to other RL agents in a prescribed manner. (2) By defining state priorities and the contents for recording, core experiences can be selected and stored in a small container. We call this container as `BOOK&#39;. Our method learns hundreds to thousand times faster than the conventional methods by learning only a handful of core cluster information, which shows that deep RL agents can effectively learn through the shared knowledge from other agents.
ER  -


TY  - Preprint
T1  - Multi-Modal Multi-Scale Deep Learning for Large-Scale Image Annotation
A1  - Yulei Niu
A1  - Zhiwu Lu
A1  - Ji-Rong Wen
A1  - Tao Xiang
A1  - Shih-Fu Chang
JO  - ArXiv e-prints
Y1  - 4 September, 2017
UR  - https://arxiv.org/abs/1709.01220
N2  - Large-scale image annotation is a challenging task in image content analysis, which aims to annotate each image of a very large dataset with multiple class labels. In this paper, we focus on two main issues in large-scale image annotation: 1) how to learn stronger features for multifarious images; 2) how to annotate an image with an automatically-determined number of class labels. To address the first issue, we propose a multi-modal multi-scale deep learning model for extracting descriptive features from multifarious images. Specifically, the visual features extracted by a multi-scale deep learning subnetwork are refined with the textual features extracted from social tags along with images by a simple multi-layer perception subnetwork. Since we have extracted very powerful features by multi-modal multi-scale deep learning, we simplify the second issue and decompose large-scale image annotation into multi-class classification and label quantity prediction. Note that the label quantity prediction subproblem can be implicitly solved when a recurrent neural network (RNN) model is used for image annotation. However, in this paper, we choose to explicitly solve this subproblem directly using our deep learning model, resulting in that we can pay more attention to deep feature learning. Experimental results demonstrate the superior performance of our model as compared to the state-of-the-art (including RNN-based models).
ER  -


TY  - Preprint
T1  - Semi-supervised Learning with Deep Generative Models for Asset Failure Prediction
A1  - Andre S. Yoon
A1  - Taehoon Lee
A1  - Yongsub Lim
A1  - Deokwoo Jung
A1  - Philgyun Kang
A1  - Dongwon Kim
A1  - Keuntae Park
A1  - Yongjin Choi
JO  - ArXiv e-prints
Y1  - 4 September, 2017
UR  - https://arxiv.org/abs/1709.00845
N2  - This work presents a novel semi-supervised learning approach for data-driven modeling of asset failures when health status is only partially known in historical data. We combine a generative model parameterized by deep neural networks with non-linear embedding technique. It allows us to build prognostic models with the limited amount of health status information for the precise prediction of future asset reliability. The proposed method is evaluated on a publicly available dataset for remaining useful life (RUL) estimation, which shows significant improvement even when a fraction of the data with known health status is as sparse as 1% of the total. Our study suggests that the non-linear embedding based on a deep generative model can efficiently regularize a complex model with deep architectures while achieving high prediction accuracy that is far less sensitive to the availability of health status information.
ER  -


TY  - Preprint
T1  - Deep Learning-Guided Image Reconstruction from Incomplete Data
A1  - Brendan Kelly
A1  - Thomas P. Matthews
A1  - Mark A. Anastasio
JO  - ArXiv e-prints
Y1  - 2 September, 2017
UR  - https://arxiv.org/abs/1709.00584
N2  - An approach to incorporate deep learning within an iterative image reconstruction framework to reconstruct images from severely incomplete measurement data is presented. Specifically, we utilize a convolutional neural network (CNN) as a quasi-projection operator within a least squares minimization procedure. The CNN is trained to encode high level information about the class of images being imaged; this information is utilized to mitigate artifacts in intermediate images produced by use of an iterative method. The structure of the method was inspired by the proximal gradient descent method, where the proximal operator is replaced by a deep CNN and the gradient descent step is generalized by use of a linear reconstruction operator. It is demonstrated that this approach improves image quality for several cases of limited-view image reconstruction and that using a CNN in an iterative method increases performance compared to conventional image reconstruction approaches. We test our method on several limited-view image reconstruction problems. Qualitative and quantitative results demonstrate state-of-the-art performance.
ER  -


TY  - Preprint
T1  - PassGAN: A Deep Learning Approach for Password Guessing
A1  - Briland Hitaj
A1  - Paolo Gasti
A1  - Giuseppe Ateniese
A1  - Fernando Perez-Cruz
JO  - ArXiv e-prints
Y1  - 9 March, 2018
UR  - https://arxiv.org/abs/1709.00440
N2  - State-of-the-art password guessing tools, such as HashCat and John the Ripper, enable users to check billions of passwords per second against password hashes. In addition to performing straightforward dictionary attacks, these tools can expand password dictionaries using password generation rules, such as concatenation of words (e.g., &#34;password123456&#34;) and leet speak (e.g., &#34;password&#34; becomes &#34;p4s5w0rd&#34;). Although these rules work well in practice, expanding them to model further passwords is a laborious task that requires specialized expertise. To address this issue, in this paper we introduce PassGAN, a novel approach that replaces human-generated password rules with theory-grounded machine learning algorithms. Instead of relying on manual password analysis, PassGAN uses a Generative Adversarial Network (GAN) to autonomously learn the distribution of real passwords from actual password leaks, and to generate high-quality password guesses. Our experiments show that this approach is very promising. When we evaluated PassGAN on two large password datasets, we were able to surpass rule-based and state-of-the-art machine learning password guessing tools. However, in contrast with the other tools, PassGAN achieved this result without any a-priori knowledge on passwords or common password structures. Additionally, when we combined the output of PassGAN with the output of HashCat, we were able to match 51%-73% more passwords than with HashCat alone. This is remarkable, because it shows that PassGAN can autonomously extract a considerable number of password properties that current state-of-the art rules do not encode.
ER  -


TY  - Preprint
T1  - A Comprehensive Survey of Deep Learning in Remote Sensing: Theories, Tools and Challenges for the Community
A1  - John E. Ball
A1  - Derek T. Anderson
A1  - Chee Seng Chan
JO  - ArXiv e-prints
Y1  - 24 September, 2017
UR  - https://arxiv.org/abs/1709.00308
N2  - In recent years, deep learning (DL), a re-branding of neural networks (NNs), has risen to the top in numerous areas, namely computer vision (CV), speech recognition, natural language processing, etc. Whereas remote sensing (RS) possesses a number of unique challenges, primarily related to sensors and applications, inevitably RS draws from many of the same theories as CV; e.g., statistics, fusion, and machine learning, to name a few. This means that the RS community should be aware of, if not at the leading edge of, of advancements like DL. Herein, we provide the most comprehensive survey of state-of-the-art RS DL research. We also review recent new developments in the DL field that can be used in DL for RS. Namely, we focus on theories, tools and challenges for the RS community. Specifically, we focus on unsolved challenges and opportunities as it relates to (i) inadequate data sets, (ii) human-understandable solutions for modelling physical phenomena, (iii) Big Data, (iv) non-traditional heterogeneous data sources, (v) DL architectures and learning algorithms for spectral, spatial and temporal data, (vi) transfer learning, (vii) an improved theoretical understanding of DL systems, (viii) high barriers to entry, and (ix) training and optimizing the DL.
ER  -


TY  - Preprint
T1  - EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification
A1  - Patrick Helber
A1  - Benjamin Bischke
A1  - Andreas Dengel
A1  - Damian Borth
JO  - ArXiv e-prints
Y1  - 31 August, 2017
UR  - https://arxiv.org/abs/1709.00029
N2  - In this paper, we address the challenge of land use and land cover classification using remote sensing satellite images. For this challenging task, we use the openly and freely accessible Sentinel-2 satellite images provided within the scope of the Earth observation program Copernicus. The key contributions are as follows. We present a novel dataset based on satellite images covering 13 different spectral bands and consisting of 10 classes with in total 27,000 labeled images. We evaluate state-of-the-art deep Convolutional Neural Network (CNNs) on this novel dataset with its different spectral bands. We also evaluate deep CNNs on existing remote sensing datasets and compare the obtained results. With the proposed novel dataset, we achieved an overall classification accuracy of 98.57%. The classification system resulting from the proposed research opens a gate towards a number of Earth observation applications. We demonstrate how the classification system can be used for detecting land use or land cover changes and how it can assist in improving geographical maps.
ER  -


TY  - Preprint
T1  - Predicting Cardiovascular Risk Factors from Retinal Fundus Photographs using Deep Learning
A1  - Ryan Poplin
A1  - Avinash V. Varadarajan
A1  - Katy Blumer
A1  - Yun Liu
A1  - Michael V. McConnell
A1  - Greg S. Corrado
A1  - Lily Peng
A1  - Dale R. Webster
JO  - ArXiv e-prints
Y1  - 21 September, 2017
UR  - https://arxiv.org/abs/1708.09843
N2  - Traditionally, medical discoveries are made by observing associations and then designing experiments to test these hypotheses. However, observing and quantifying associations in images can be difficult because of the wide variety of features, patterns, colors, values, shapes in real data. In this paper, we use deep learning, a machine learning technique that learns its own features, to discover new knowledge from retinal fundus images. Using models trained on data from 284,335 patients, and validated on two independent datasets of 12,026 and 999 patients, we predict cardiovascular risk factors not previously thought to be present or quantifiable in retinal images, such as such as age (within 3.26 years), gender (0.97 AUC), smoking status (0.71 AUC), HbA1c (within 1.39%), systolic blood pressure (within 11.23mmHg) as well as major adverse cardiac events (0.70 AUC). We further show that our models used distinct aspects of the anatomy to generate each prediction, such as the optic disc or blood vessels, opening avenues of further research.
ER  -


TY  - Preprint
T1  - Learning Invariant Riemannian Geometric Representations Using Deep Nets
A1  - Suhas Lohit
A1  - Pavan Turaga
JO  - ArXiv e-prints
Y1  - 22 September, 2017
UR  - https://arxiv.org/abs/1708.09485
N2  - Non-Euclidean constraints are inherent in many kinds of data in computer vision and machine learning, typically as a result of specific invariance requirements that need to be respected during high-level inference. Often, these geometric constraints can be expressed in the language of Riemannian geometry, where conventional vector space machine learning does not apply directly. The central question this paper deals with is: How does one train deep neural nets whose final outputs are elements on a Riemannian manifold? To answer this, we propose a general framework for manifold-aware training of deep neural networks -- we utilize tangent spaces and exponential maps in order to convert the proposed problem into a form that allows us to bring current advances in deep learning to bear upon this problem. We describe two specific applications to demonstrate this approach: prediction of probability distributions for multi-class image classification, and prediction of illumination-invariant subspaces from a single face-image via regression on the Grassmannian. These applications show the generality of the proposed framework, and result in improved performance over baselines that ignore the geometry of the output space. In addition to solving this specific problem, we believe this paper opens new lines of enquiry centered on the implications of Riemannian geometry on deep architectures.
ER  -


TY  - Preprint
T1  - Texture and Structure Incorporated ScatterNet Hybrid Deep Learning Network (TS-SHDL) For Brain Matter Segmentation
A1  - Amarjot Singh
A1  - Devamanyu Hazarika
A1  - Aniruddha Bhattacharya
JO  - ArXiv e-prints
Y1  - 30 August, 2017
UR  - https://arxiv.org/abs/1708.09300
N2  - Automation of brain matter segmentation from MR images is a challenging task due to the irregular boundaries between the grey and white matter regions. In addition, the presence of intensity inhomogeneity in the MR images further complicates the problem. In this paper, we propose a texture and vesselness incorporated version of the ScatterNet Hybrid Deep Learning Network (TS-SHDL) that extracts hierarchical invariant mid-level features, used by fisher vector encoding and a conditional random field (CRF) to perform the desired segmentation. The performance of the proposed network is evaluated by extensive experimentation and comparison with the state-of-the-art methods on several 2D MRI scans taken from the synthetic McGill Brain Web as well as on the MRBrainS dataset of real 3D MRI scans. The advantages of the TS-SHDL network over supervised deep learning networks is also presented in addition to its superior performance over the state-of-the-art.
ER  -


TY  - Preprint
T1  - ScatterNet Hybrid Deep Learning (SHDL) Network For Object Classification
A1  - Amarjot Singh
A1  - Nick Kingsbury
JO  - ArXiv e-prints
Y1  - 30 August, 2017
UR  - https://arxiv.org/abs/1708.09212
N2  - The paper proposes the ScatterNet Hybrid Deep Learning (SHDL) network that extracts invariant and discriminative image representations for object recognition. SHDL framework is constructed with a multi-layer ScatterNet front-end, an unsupervised learning middle, and a supervised learning back-end module. Each layer of the SHDL network is automatically designed as an explicit optimization problem leading to an optimal deep learning architecture with improved computational performance as compared to the more usual deep network architectures. SHDL network produces the state-of-the-art classification performance against unsupervised and semi-supervised learning (GANs) on two image datasets. Advantages of the SHDL network over supervised methods (NIN, VGG) are also demonstrated with experiments performed on training datasets of reduced size.
ER  -


TY  - Preprint
T1  - A Deep Learning Approach for Population Estimation from Satellite Imagery
A1  - Caleb Robinson
A1  - Fred Hohman
A1  - Bistra Dilkina
JO  - ArXiv e-prints
Y1  - 29 August, 2017
UR  - https://arxiv.org/abs/1708.09086
N2  - Knowing where people live is a fundamental component of many decision making processes such as urban development, infectious disease containment, evacuation planning, risk management, conservation planning, and more. While bottom-up, survey driven censuses can provide a comprehensive view into the population landscape of a country, they are expensive to realize, are infrequently performed, and only provide population counts over broad areas. Population disaggregation techniques and population projection methods individually address these shortcomings, but also have shortcomings of their own. To jointly answer the questions of &#34;where do people live&#34; and &#34;how many people live there,&#34; we propose a deep learning model for creating high-resolution population estimations from satellite imagery. Specifically, we train convolutional neural networks to predict population in the USA at a $0.01^{\circ} \times 0.01^{\circ}$ resolution grid from 1-year composite Landsat imagery. We validate these models in two ways: quantitatively, by comparing our model&#39;s grid cell estimates aggregated at a county-level to several US Census county-level population projections, and qualitatively, by directly interpreting the model&#39;s predictions in terms of the satellite image inputs. We find that aggregating our model&#39;s estimates gives comparable results to the Census county-level population projections and that the predictions made by our model can be directly interpreted, which give it advantages over traditional population disaggregation methods. In general, our model is an example of how machine learning techniques can be an effective tool for extracting information from inherently unstructured, remotely sensed data to provide effective solutions to social problems.
ER  -


TY  - Preprint
T1  - Deep Learning for Medical Image Analysis
A1  - Mina Rezaei
A1  - Haojin Yang
A1  - Christoph Meinel
JO  - ArXiv e-prints
Y1  - 17 August, 2017
UR  - https://arxiv.org/abs/1708.08987
N2  - This report describes my research activities in the Hasso Plattner Institute and summarizes my Ph.D. plan and several novels, end-to-end trainable approaches for analyzing medical images using deep learning algorithm. In this report, as an example, we explore different novel methods based on deep learning for brain abnormality detection, recognition, and segmentation. This report prepared for the doctoral consortium in the AIME-2017 conference.
ER  -


TY  - Preprint
T1  - Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization
A1  - Luis MuÃ±oz-GonzÃ¡lez
A1  - Battista Biggio
A1  - Ambra Demontis
A1  - Andrea Paudice
A1  - Vasin Wongrassamee
A1  - Emil C. Lupu
A1  - Fabio Roli
JO  - ArXiv e-prints
Y1  - 29 August, 2017
UR  - https://arxiv.org/abs/1708.08689
N2  - A number of online services nowadays rely upon machine learning to extract valuable information from data collected in the wild. This exposes learning algorithms to the threat of data poisoning, i.e., a coordinate attack in which a fraction of the training data is controlled by the attacker and manipulated to subvert the learning process. To date, these attacks have been devised only against a limited class of binary learning algorithms, due to the inherent complexity of the gradient-based procedure used to optimize the poisoning points (a.k.a. adversarial training examples). In this work, we rst extend the de nition of poisoning attacks to multiclass problems. We then propose a novel poisoning algorithm based on the idea of back-gradient optimization, i.e., to compute the gradient of interest through automatic di erentiation, while also reversing the learning procedure to drastically reduce the attack complexity. Compared to current poisoning strategies, our approach is able to target a wider class of learning algorithms, trained with gradient- based procedures, including neural networks and deep learning architectures. We empirically evaluate its e ectiveness on several application examples, including spam ltering, malware detection, and handwritten digit recognition. We nally show that, similarly to adversarial test examples, adversarial training examples can also be transferred across di erent learning algorithms.
ER  -


TY  - Preprint
T1  - A parameterized activation function for learning fuzzy logic operations in deep neural networks
A1  - Luke B. Godfrey
A1  - Michael S. Gashler
JO  - ArXiv e-prints
Y1  - 11 September, 2017
UR  - https://arxiv.org/abs/1708.08557
N2  - We present a deep learning architecture for learning fuzzy logic expressions. Our model uses an innovative, parameterized, differentiable activation function that can learn a number of logical operations by gradient descent. This activation function allows a neural network to determine the relationships between its input variables and provides insight into the logical significance of learned network parameters. We provide a theoretical basis for this parameterization and demonstrate its effectiveness and utility by successfully applying our model to five classification problems from the UCI Machine Learning Repository.
ER  -


TY  - Preprint
T1  - Deep Learning for Accelerated Reliability Analysis of Infrastructure Networks
A1  - Mohammad Amin Nabian
A1  - Hadi Meidani
JO  - ArXiv e-prints
Y1  - 28 August, 2017
UR  - https://arxiv.org/abs/1708.08551
N2  - Natural disasters can have catastrophic impacts on the functionality of infrastructure systems and cause severe physical and socio-economic losses. Given budget constraints, it is crucial to optimize decisions regarding mitigation, preparedness, response, and recovery practices for these systems. This requires accurate and efficient means to evaluate the infrastructure system reliability. While numerous research efforts have addressed and quantified the impact of natural disasters on infrastructure systems, typically using the Monte Carlo approach, they still suffer from high computational cost and, thus, are of limited applicability to large systems. This paper presents a deep learning framework for accelerating infrastructure system reliability analysis. In particular, two distinct deep neural network surrogates are constructed and studied: (1) A classifier surrogate which speeds up the connectivity determination of networks, and (2) An end-to-end surrogate that replaces a number of components such as roadway status realization, connectivity determination, and connectivity averaging. The proposed approach is applied to a simulation-based study of the two-terminal connectivity of a California transportation network subject to extreme probabilistic earthquake events. Numerical results highlight the effectiveness of the proposed approach in accelerating the transportation system two-terminal reliability analysis with extremely high prediction accuracy.
ER  -


TY  - Preprint
T1  - Power of Deep Learning for Channel Estimation and Signal Detection in OFDM Systems
A1  - Hao Ye
A1  - Geoffrey Ye Li
A1  - Biing-Hwang Fred Juang
JO  - ArXiv e-prints
Y1  - 28 August, 2017
UR  - https://arxiv.org/abs/1708.08514
N2  - This article presents our initial results in deep learning for channel estimation and signal detection in orthogonal frequency-division multiplexing (OFDM). OFDM has been widely adopted in wireless broadband communications to combat frequency-selective fading in wireless channels. In this article, we take advantage of deep learning in handling wireless OFDM channels in an end-to-end approach. Different from existing OFDM receivers that first estimate CSI explicitly and then detect/recover the transmitted symbols with the estimated CSI, our deep learning based approach estimates CSI implicitly and recovers the transmitted symbols directly. To address channel distortion, a deep learning model is first trained offline using the data generated from the simulation based on the channel statistics and then used for recovering the online transmitted data directly. From our simulation results, the deep learning based approach has the ability to address channel distortions and detect the transmitted symbols with performance comparable to minimum mean-square error (MMSE) estimator. Furthermore, the deep learning based approach is more robust than conventional methods when fewer training pilots are used, the cyclic prefix (CP) is omitted, and nonlinear clipping noise is presented. In summary, deep learning is a promising tool for channel estimation and signal detection in wireless communications with complicated channel distortions and interferences.
ER  -


TY  - Preprint
T1  - Deep Learning Sparse Ternary Projections for Compressed Sensing of Images
A1  - Duc Minh Nguyen
A1  - Evaggelia Tsiligianni
A1  - Nikos Deligiannis
JO  - ArXiv e-prints
Y1  - 28 August, 2017
UR  - https://arxiv.org/abs/1708.08311
N2  - Compressed sensing (CS) is a sampling theory that allows reconstruction of sparse (or compressible) signals from an incomplete number of measurements, using of a sensing mechanism implemented by an appropriate projection matrix. The CS theory is based on random Gaussian projection matrices, which satisfy recovery guarantees with high probability; however, sparse ternary {0, -1, +1} projections are more suitable for hardware implementation. In this paper, we present a deep learning approach to obtain very sparse ternary projections for compressed sensing. Our deep learning architecture jointly learns a pair of a projection matrix and a reconstruction operator in an end-to-end fashion. The experimental results on real images demonstrate the effectiveness of the proposed approach compared to state-of-the-art methods, with significant advantage in terms of complexity.
ER  -


TY  - Preprint
T1  - Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models
A1  - Wojciech Samek
A1  - Thomas Wiegand
A1  - Klaus-Robert MÃ¼ller
JO  - ArXiv e-prints
Y1  - 28 August, 2017
UR  - https://arxiv.org/abs/1708.08296
N2  - With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks.
ER  -


TY  - Preprint
T1  - ChainerCV: a Library for Deep Learning in Computer Vision
A1  - Yusuke Niitani
A1  - Toru Ogawa
A1  - Shunta Saito
A1  - Masaki Saito
JO  - ArXiv e-prints
Y1  - 27 August, 2017
UR  - https://arxiv.org/abs/1708.08169
N2  - Despite significant progress of deep learning in the field of computer vision, there has not been a software library that covers these methods in a unifying manner. We introduce ChainerCV, a software library that is intended to fill this gap. ChainerCV supports numerous neural network models as well as software components needed to conduct research in computer vision. These implementations emphasize simplicity, flexibility and good software engineering practices. The library is designed to perform on par with the results reported in published papers and its tools can be used as a baseline for future research in computer vision. Our implementation includes sophisticated models like Faster R-CNN and SSD, and covers tasks such as object detection and semantic segmentation.
ER  -


TY  - Preprint
T1  - Facial Expression Recognition using Visual Saliency and Deep Learning
A1  - Viraj Mavani
A1  - Shanmuganathan Raman
A1  - Krishna P Miyapuram
JO  - ArXiv e-prints
Y1  - 26 August, 2017
UR  - https://arxiv.org/abs/1708.08016
N2  - We have developed a convolutional neural network for the purpose of recognizing facial expressions in human beings. We have fine-tuned the existing convolutional neural network model trained on the visual recognition dataset used in the ILSVRC2012 to two widely used facial expression datasets - CFEE and RaFD, which when trained and tested independently yielded test accuracies of 74.79% and 95.71%, respectively. Generalization of results was evident by training on one dataset and testing on the other. Further, the image product of the cropped faces and their visual saliency maps were computed using Deep Multi-Layer Network for saliency prediction and were fed to the facial expression recognition CNN. In the most generalized experiment, we observed the top-1 accuracy in the test set to be 65.39%. General confusion trends between different facial expressions as exhibited by humans were also observed.
ER  -


TY  - Preprint
T1  - Deep learning with convolutional neural networks for decoding and visualization of EEG pathology
A1  - Robin Tibor Schirrmeister
A1  - Lukas Gemein
A1  - Katharina Eggensperger
A1  - Frank Hutter
A1  - Tonio Ball
JO  - ArXiv e-prints
Y1  - 11 January, 2018
UR  - https://arxiv.org/abs/1708.08012
N2  - We apply convolutional neural networks (ConvNets) to the task of distinguishing pathological from normal EEG recordings in the Temple University Hospital EEG Abnormal Corpus. We use two basic, shallow and deep ConvNet architectures recently shown to decode task-related information from EEG at least as well as established algorithms designed for this purpose. In decoding EEG pathology, both ConvNets reached substantially better accuracies (about 6% better, ~85% vs. ~79%) than the only published result for this dataset, and were still better when using only 1 minute of each recording for training and only six seconds of each recording for testing. We used automated methods to optimize architectural hyperparameters and found intriguingly different ConvNet architectures, e.g., with max pooling as the only nonlinearity. Visualizations of the ConvNet decoding behavior showed that they used spectral power changes in the delta (0-4 Hz) and theta (4-8 Hz) frequency range, possibly alongside other features, consistent with expectations derived from spectral analysis of the EEG data and from the textual medical reports. Analysis of the textual medical reports also highlighted the potential for accuracy increases by integrating contextual information, such as the age of subjects. In summary, the ConvNets and visualization techniques used in this study constitute a next step towards clinically useful automated EEG diagnosis and establish a new baseline for future work on this topic.
ER  -


TY  - Preprint
T1  - Deep Learning for Target Classification from SAR Imagery: Data Augmentation and Translation Invariance
A1  - Hidetoshi Furukawa
JO  - ArXiv e-prints
Y1  - 25 August, 2017
UR  - https://arxiv.org/abs/1708.07920
N2  - This report deals with translation invariance of convolutional neural networks (CNNs) for automatic target recognition (ATR) from synthetic aperture radar (SAR) imagery. In particular, the translation invariance of CNNs for SAR ATR represents the robustness against misalignment of target chips extracted from SAR images. To understand the translation invariance of the CNNs, we trained CNNs which classify the target chips from the MSTAR into the ten classes under the condition of with and without data augmentation, and then visualized the translation invariance of the CNNs. According to our results, even if we use a deep residual network, the translation invariance of the CNN without data augmentation using the aligned images such as the MSTAR target chips is not so large. A more important factor of translation invariance is the use of augmented training data. Furthermore, our CNN using augmented training data achieved a state-of-the-art classification accuracy of 99.6%. These results show an importance of domain-specific data augmentation.
ER  -


TY  - Preprint
T1  - Robust Task Clustering for Deep Many-Task Learning
A1  - Mo Yu
A1  - Xiaoxiao Guo
A1  - Jinfeng Yi
A1  - Shiyu Chang
A1  - Saloni Potdar
A1  - Gerald Tesauro
A1  - Haoyu Wang
A1  - Bowen Zhou
JO  - ArXiv e-prints
Y1  - 17 May, 2018
UR  - https://arxiv.org/abs/1708.07918
N2  - We investigate task clustering for deep-learning based multi-task and few-shot learning in a many-task setting. We propose a new method to measure task similarities with cross-task transfer performance matrix for the deep learning scenario. Although this matrix provides us critical information regarding similarity between tasks, its asymmetric property and unreliable performance scores can affect conventional clustering methods adversely. Additionally, the uncertain task-pairs, i.e., the ones with extremely asymmetric transfer scores, may collectively mislead clustering algorithms to output an inaccurate task-partition. To overcome these limitations, we propose a novel task-clustering algorithm by using the matrix completion technique. The proposed algorithm constructs a partially-observed similarity matrix based on the certainty of cluster membership of the task-pairs. We then use a matrix completion algorithm to complete the similarity matrix. Our theoretical analysis shows that under mild constraints, the proposed algorithm will perfectly recover the underlying &#34;true&#34; similarity matrix with a high probability. Our results show that the new task clustering method can discover task clusters for training flexible and superior neural network models in a multi-task learning setup for sentiment classification and dialog intent classification tasks. Our task clustering approach also extends metric-based few-shot learning methods to adapt multiple metrics, which demonstrates empirical advantages when the tasks are diverse.
ER  -


TY  - Preprint
T1  - Deep Learning for Video Game Playing
A1  - Niels Justesen
A1  - Philip Bontrager
A1  - Julian Togelius
A1  - Sebastian Risi
JO  - ArXiv e-prints
Y1  - 30 October, 2017
UR  - https://arxiv.org/abs/1708.07902
N2  - In this article, we review recent Deep Learning advances in the context of how they have been applied to play different types of video games such as first-person shooters, arcade games, and real-time strategy games. We analyze the unique requirements that different game genres pose to a deep learning system and highlight important open challenges in the context of applying these machine learning methods to video games, such as general game playing, dealing with extremely large decision spaces and sparse rewards.
ER  -


TY  - Preprint
T1  - Exploit imaging through opaque wall via deep learning
A1  - Meng Lyu
A1  - Hao Wang
A1  - Guowei Li
A1  - Guohai Situ
JO  - ArXiv e-prints
Y1  - 9 August, 2017
UR  - https://arxiv.org/abs/1708.07881
N2  - Imaging through scattering media is encountered in many disciplines or sciences, ranging from biology, mesescopic physics and astronomy. But it is still a big challenge because light suffers from multiple scattering is such media and can be totally decorrelated. Here, we propose a deep-learning-based method that can retrieve the image of a target behind a thick scattering medium. The method uses a trained deep neural network to fit the way of mapping of objects at one side of a thick scattering medium to the corresponding speckle patterns observed at the other side. For demonstration, we retrieve the images of a set of objects hidden behind a 3mm thick white polystyrene slab, the optical depth of which is 13.4 times of the scattering mean free path. Our work opens up a new way to tackle the longstanding challenge by using the technique of deep learning.
ER  -


TY  - Preprint
T1  - Evaluation of Deep Learning on an Abstract Image Classification Dataset
A1  - Sebastian Stabinger
A1  - Antonio Rodriguez-Sanchez
JO  - ArXiv e-prints
Y1  - 25 August, 2017
UR  - https://arxiv.org/abs/1708.07770
N2  - Convolutional Neural Networks have become state of the art methods for image classification over the last couple of years. By now they perform better than human subjects on many of the image classification datasets. Most of these datasets are based on the notion of concrete classes (i.e. images are classified by the type of object in the image). In this paper we present a novel image classification dataset, using abstract classes, which should be easy to solve for humans, but variations of it are challenging for CNNs. The classification performance of popular CNN architectures is evaluated on this dataset and variations of the dataset that might be interesting for further research are identified.
ER  -


TY  - Preprint
T1  - Supervised Speech Separation Based on Deep Learning: An Overview
A1  - DeLiang Wang
A1  - Jitong Chen
JO  - ArXiv e-prints
Y1  - 14 June, 2018
UR  - https://arxiv.org/abs/1708.07524
N2  - Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This article provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement (speech-nonspeech separation), speaker separation (multi-talker separation), and speech dereverberation, as well as multi-microphone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target source.
ER  -


TY  - Preprint
T1  - Automatic Myocardial Segmentation by Using A Deep Learning Network in Cardiac MRI
A1  - Ariel H. Curiale
A1  - Flavio D. Colavecchia
A1  - Pablo Kaluza
A1  - Roberto A. Isoardi
A1  - German Mato
JO  - ArXiv e-prints
Y1  - 24 August, 2017
UR  - https://arxiv.org/abs/1708.07452
N2  - Cardiac function is of paramount importance for both prognosis and treatment of different pathologies such as mitral regurgitation, ischemia, dyssynchrony and myocarditis. Cardiac behavior is determined by structural and functional features. In both cases, the analysis of medical imaging studies requires to detect and segment the myocardium. Nowadays, magnetic resonance imaging (MRI) is one of the most relevant and accurate non-invasive diagnostic tools for cardiac structure and function.
ER  -


TY  - Preprint
T1  - Learning 6-DOF Grasping Interaction via Deep Geometry-aware 3D Representations
A1  - Xinchen Yan
A1  - Jasmine Hsu
A1  - Mohi Khansari
A1  - Yunfei Bai
A1  - Arkanath Pathak
A1  - Abhinav Gupta
A1  - James Davidson
A1  - Honglak Lee
JO  - ArXiv e-prints
Y1  - 14 June, 2018
UR  - https://arxiv.org/abs/1708.07303
N2  - This paper focuses on the problem of learning 6-DOF grasping with a parallel jaw gripper in simulation. We propose the notion of a geometry-aware representation in grasping based on the assumption that knowledge of 3D geometry is at the heart of interaction. Our key idea is constraining and regularizing grasping interaction learning through 3D geometry prediction. Specifically, we formulate the learning of deep geometry-aware grasping model in two steps: First, we learn to build mental geometry-aware representation by reconstructing the scene (i.e., 3D occupancy grid) from RGBD input via generative 3D shape modeling. Second, we learn to predict grasping outcome with its internal geometry-aware representation. The learned outcome prediction model is used to sequentially propose grasping solutions via analysis-by-synthesis optimization. Our contributions are fourfold: (1) To best of our knowledge, we are presenting for the first time a method to learn a 6-DOF grasping net from RGBD input; (2) We build a grasping dataset from demonstrations in virtual reality with rich sensory and interaction annotations. This dataset includes 101 everyday objects spread across 7 categories, additionally, we propose a data augmentation strategy for effective learning; (3) We demonstrate that the learned geometry-aware representation leads to about 10 percent relative performance improvement over the baseline CNN on grasping objects from our dataset. (4) We further demonstrate that the model generalizes to novel viewpoints and object instances.
ER  -


TY  - Preprint
T1  - Learning Generalized Reactive Policies using Deep Neural Networks
A1  - Edward Groshev
A1  - Maxwell Goldstein
A1  - Aviv Tamar
A1  - Siddharth Srivastava
A1  - Pieter Abbeel
JO  - ArXiv e-prints
Y1  - 24 July, 2018
UR  - https://arxiv.org/abs/1708.07280
N2  - We present a new approach to learning for planning, where knowledge acquired while solving a given set of planning problems is used to plan faster in related, but new problem instances. We show that a deep neural network can be used to learn and represent a \emph{generalized reactive policy} (GRP) that maps a problem instance and a state to an action, and that the learned GRPs efficiently solve large classes of challenging problem instances. In contrast to prior efforts in this direction, our approach significantly reduces the dependence of learning on handcrafted domain knowledge or feature selection. Instead, the GRP is trained from scratch using a set of successful execution traces. We show that our approach can also be used to automatically learn a heuristic function that can be used in directed search algorithms. We evaluate our approach using an extensive suite of experiments on two challenging planning problem domains and show that our approach facilitates learning complex decision making policies and powerful heuristic functions with minimal human input. Videos of our results are available at goo.gl/Hpy4e3.
ER  -


TY  - Preprint
T1  - Is Deep Learning Safe for Robot Vision? Adversarial Examples against the iCub Humanoid
A1  - Marco Melis
A1  - Ambra Demontis
A1  - Battista Biggio
A1  - Gavin Brown
A1  - Giorgio Fumera
A1  - Fabio Roli
JO  - ArXiv e-prints
Y1  - 23 August, 2017
UR  - https://arxiv.org/abs/1708.06939
N2  - Deep neural networks have been widely adopted in recent years, exhibiting impressive performances in several application domains. It has however been shown that they can be fooled by adversarial examples, i.e., images altered by a barely-perceivable adversarial noise, carefully crafted to mislead classification. In this work, we aim to evaluate the extent to which robot-vision systems embodying deep-learning algorithms are vulnerable to adversarial examples, and propose a computationally efficient countermeasure to mitigate this threat, based on rejecting classification of anomalous inputs. We then provide a clearer understanding of the safety properties of deep networks through an intuitive empirical analysis, showing that the mapping learned by such networks essentially violates the smoothness assumption of learning algorithms. We finally discuss the main limitations of this work, including the creation of real-world adversarial examples, and sketch promising research directions.
ER  -


TY  - Preprint
T1  - Learning Deep Neural Network Representations for Koopman Operators of Nonlinear Dynamical Systems
A1  - Enoch Yeung
A1  - Soumya Kundu
A1  - Nathan Hodas
JO  - ArXiv e-prints
Y1  - 17 November, 2017
UR  - https://arxiv.org/abs/1708.06850
N2  - The Koopman operator has recently garnered much attention for its value in dynamical systems analysis and data-driven model discovery. However, its application has been hindered by the computational complexity of extended dynamic mode decomposition; this requires a combinatorially large basis set to adequately describe many nonlinear systems of interest, e.g. cyber-physical infrastructure systems, biological networks, social systems, and fluid dynamics. Often the dictionaries generated for these problems are manually curated, requiring domain-specific knowledge and painstaking tuning. In this paper we introduce a deep learning framework for learning Koopman operators of nonlinear dynamical systems. We show that this novel method automatically selects efficient deep dictionaries, outperforming state-of-the-art methods. We benchmark this method on partially observed nonlinear systems, including the glycolytic oscillator and show it is able to predict quantitatively 100 steps into the future, using only a single timepoint, and qualitative oscillatory behavior 400 steps into the future.
ER  -


TY  - Preprint
T1  - Automated Website Fingerprinting through Deep Learning
A1  - Vera Rimmer
A1  - Davy Preuveneers
A1  - Marc Juarez
A1  - Tom Van Goethem
A1  - Wouter Joosen
JO  - ArXiv e-prints
Y1  - 5 December, 2017
UR  - https://arxiv.org/abs/1708.06376
N2  - Several studies have shown that the network traffic that is generated by a visit to a website over Tor reveals information specific to the website through the timing and sizes of network packets. By capturing traffic traces between users and their Tor entry guard, a network eavesdropper can leverage this meta-data to reveal which website Tor users are visiting. The success of such attacks heavily depends on the particular set of traffic features that are used to construct the fingerprint. Typically, these features are manually engineered and, as such, any change introduced to the Tor network can render these carefully constructed features ineffective. In this paper, we show that an adversary can automate the feature engineering process, and thus automatically deanonymize Tor traffic by applying our novel method based on deep learning. We collect a dataset comprised of more than three million network traces, which is the largest dataset of web traffic ever used for website fingerprinting, and find that the performance achieved by our deep learning approaches is comparable to known methods which include various research efforts spanning over multiple years. The obtained success rate exceeds 96% for a closed world of 100 websites and 94% for our biggest closed world of 900 classes. In our open world evaluation, the most performant deep learning model is 2% more accurate than the state-of-the-art attack. Furthermore, we show that the implicit features automatically learned by our approach are far more resilient to dynamic changes of web content over time. We conclude that the ability to automatically construct the most relevant traffic features and perform accurate traffic recognition makes our deep learning based approach an efficient, flexible and robust technique for website fingerprinting.
ER  -


TY  - Preprint
T1  - nuts-flow/ml: data pre-processing for deep learning
A1  - S. Maetschke
A1  - R. Tennakoon
A1  - C. Vecchiola
A1  - R. Garnavi
JO  - ArXiv e-prints
Y1  - 9 January, 2018
UR  - https://arxiv.org/abs/1708.06046
N2  - Data preprocessing is a fundamental part of any machine learning application and frequently the most time-consuming aspect when developing a machine learning solution. Preprocessing for deep learning is characterized by pipelines that lazily load data and perform data transformation, augmentation, batching and logging. Many of these functions are common across applications but require different arrangements for training, testing or inference. Here we introduce a novel software framework named nuts-flow/ml that encapsulates common preprocessing operations as components, which can be flexibly arranged to rapidly construct efficient preprocessing pipelines for deep learning.
ER  -


TY  - Preprint
T1  - DeepBreath: Deep Learning of Breathing Patterns for Automatic Stress Recognition using Low-Cost Thermal Imaging in Unconstrained Settings
A1  - Youngjun Cho
A1  - Nadia Bianchi-Berthouze
A1  - Simon J. Julier
JO  - ArXiv e-prints
Y1  - 20 August, 2017
UR  - https://arxiv.org/abs/1708.06026
N2  - We propose DeepBreath, a deep learning model which automatically recognises people&#39;s psychological stress level (mental overload) from their breathing patterns. Using a low cost thermal camera, we track a person&#39;s breathing patterns as temperature changes around his/her nostril. The paper&#39;s technical contribution is threefold. First of all, instead of creating hand-crafted features to capture aspects of the breathing patterns, we transform the uni-dimensional breathing signals into two dimensional respiration variability spectrogram (RVS) sequences. The spectrograms easily capture the complexity of the breathing dynamics. Second, a spatial pattern analysis based on a deep Convolutional Neural Network (CNN) is directly applied to the spectrogram sequences without the need of hand-crafting features. Finally, a data augmentation technique, inspired from solutions for over-fitting problems in deep learning, is applied to allow the CNN to learn with a small-scale dataset from short-term measurements (e.g., up to a few hours). The model is trained and tested with data collected from people exposed to two types of cognitive tasks (Stroop Colour Word Test, Mental Computation test) with sessions of different difficulty levels. Using normalised self-report as ground truth, the CNN reaches 84.59% accuracy in discriminating between two levels of stress and 56.52% in discriminating between three levels. In addition, the CNN outperformed powerful shallow learning methods based on a single layer neural network. Finally, the dataset of labelled thermal images will be open to the community.
ER  -


TY  - Preprint
T1  - Improving Deep Learning using Generic Data Augmentation
A1  - Luke Taylor
A1  - Geoff Nitschke
JO  - ArXiv e-prints
Y1  - 20 August, 2017
UR  - https://arxiv.org/abs/1708.06020
N2  - Deep artificial neural networks require a large corpus of training data in order to effectively learn, where collection of such training data is often expensive and laborious. Data augmentation overcomes this issue by artificially inflating the training set with label preserving transformations. Recently there has been extensive use of generic data augmentation to improve Convolutional Neural Network (CNN) task performance. This study benchmarks various popular data augmentation schemes to allow researchers to make informed decisions as to which training methods are most appropriate for their data sets. Various geometric and photometric schemes are evaluated on a coarse-grained data set using a relatively simple CNN. Experimental results, run using 4-fold cross-validation and reported in terms of Top-1 and Top-5 accuracy, indicate that cropping in geometric augmentation significantly increases CNN task performance.
ER  -


TY  - Preprint
T1  - Perceptual audio loss function for deep learning
A1  - Dan Elbaz
A1  - Michael Zibulevsky
JO  - ArXiv e-prints
Y1  - 20 August, 2017
UR  - https://arxiv.org/abs/1708.05987
N2  - PESQ and POLQA , are standards are standards for automated assessment of voice quality of speech as experienced by human beings. The predictions of those objective measures should come as close as possible to subjective quality scores as obtained in subjective listening tests. Wavenet is a deep neural network originally developed as a deep generative model of raw audio wave-forms. Wavenet architecture is based on dilated causal convolutions, which exhibit very large receptive fields. In this short paper we suggest using the Wavenet architecture, in particular its large receptive filed in order to learn PESQ algorithm. By doing so we can use it as a differentiable loss function for speech enhancement.
ER  -


TY  - Preprint
T1  - Applying Data Augmentation to Handwritten Arabic Numeral Recognition Using Deep Learning Neural Networks
A1  - Akm Ashiquzzaman
A1  - Abdul Kawsar Tushar
A1  - Ashiqur Rahman
JO  - ArXiv e-prints
Y1  - 27 September, 2017
UR  - https://arxiv.org/abs/1708.05969
N2  - Handwritten character recognition has been the center of research and a benchmark problem in the sector of pattern recognition and artificial intelligence, and it continues to be a challenging research topic. Due to its enormous application many works have been done in this field focusing on different languages. Arabic, being a diversified language has a huge scope of research with potential challenges. A convolutional neural network model for recognizing handwritten numerals in Arabic language is proposed in this paper, where the dataset is subject to various augmentation in order to add robustness needed for deep learning approach. The proposed method is empowered by the presence of dropout regularization to do away with the problem of data overfitting. Moreover, suitable change is introduced in activation function to overcome the problem of vanishing gradient. With these modifications, the proposed system achieves an accuracy of 99.4\% which performs better than every previous work on the dataset.
ER  -


TY  - Preprint
T1  - Solving a New 3D Bin Packing Problem with Deep Reinforcement Learning Method
A1  - Haoyuan Hu
A1  - Xiaodong Zhang
A1  - Xiaowei Yan
A1  - Longfei Wang
A1  - Yinghui Xu
JO  - ArXiv e-prints
Y1  - 19 August, 2017
UR  - https://arxiv.org/abs/1708.05930
N2  - In this paper, a new type of 3D bin packing problem (BPP) is proposed, in which a number of cuboid-shaped items must be put into a bin one by one orthogonally. The objective is to find a way to place these items that can minimize the surface area of the bin. This problem is based on the fact that there is no fixed-sized bin in many real business scenarios and the cost of a bin is proportional to its surface area. Our research shows that this problem is NP-hard. Based on previous research on 3D BPP, the surface area is determined by the sequence, spatial locations and orientations of items. Among these factors, the sequence of items plays a key role in minimizing the surface area. Inspired by recent achievements of deep reinforcement learning (DRL) techniques, especially Pointer Network, on combinatorial optimization problems such as TSP, a DRL-based method is applied to optimize the sequence of items to be packed into the bin. Numerical results show that the method proposed in this paper achieve about 5% improvement than heuristic method.
ER  -


TY  - Preprint
T1  - A Deep Q-Network for the Beer Game: A Reinforcement Learning algorithm to Solve Inventory Optimization Problems
A1  - Afshin Oroojlooyjadid
A1  - MohammadReza Nazari
A1  - Lawrence Snyder
A1  - Martin TakÃ¡Ä
JO  - ArXiv e-prints
Y1  - 8 March, 2018
UR  - https://arxiv.org/abs/1708.05924
N2  - The beer game is a widely used in-class game that is played in supply chain management classes to demonstrate the bullwhip effect. The game is a decentralized, multi-agent, cooperative problem that can be modeled as a serial supply chain network in which agents cooperatively attempt to minimize the total cost of the network even though each agent can only observe its own local information. Each agent chooses order quantities to replenish its stock. Under some conditions, a base-stock replenishment policy is known to be optimal. However, in a decentralized supply chain in which some agents (stages) may act irrationally (as they do in the beer game), there is no known optimal policy for an agent wishing to act optimally.
ER  -


TY  - Preprint
T1  - A Brief Survey of Deep Reinforcement Learning
A1  - Kai Arulkumaran
A1  - Marc Peter Deisenroth
A1  - Miles Brundage
A1  - Anil Anthony Bharath
JO  - ArXiv e-prints
Y1  - 28 September, 2017
UR  - https://arxiv.org/abs/1708.05866
N2  - Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep $Q$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.
ER  -


TY  - Preprint
T1  - Simultaneous Detection and Quantification of Retinal Fluid with Deep Learning
A1  - Dustin Morley
A1  - Hassan Foroosh
A1  - Saad Shaikh
A1  - Ulas Bagci
JO  - ArXiv e-prints
Y1  - 17 August, 2017
UR  - https://arxiv.org/abs/1708.05464
N2  - We propose a new deep learning approach for automatic detection and segmentation of fluid within retinal OCT images. The proposed framework utilizes both ResNet and Encoder-Decoder neural network architectures. When training the network, we apply a novel data augmentation method called myopic warping together with standard rotation-based augmentation to increase the training set size to 45 times the original amount. Finally, the network output is post-processed with an energy minimization algorithm (graph cut) along with a few other knowledge guided morphological operations to finalize the segmentation process. Based on OCT imaging data and its ground truth from the RETOUCH challenge, the proposed system achieves dice indices of 0.522, 0.682, and 0.612, and average absolute volume differences of 0.285, 0.115, and 0.156 mm$^3$ for intaretinal fluid, subretinal fluid, and pigment epithelial detachment respectively.
ER  -


TY  - Preprint
T1  - Deep Learning at 15PF: Supervised and Semi-Supervised Classification for Scientific Data
A1  - Thorsten Kurth
A1  - Jian Zhang
A1  - Nadathur Satish
A1  - Ioannis Mitliagkas
A1  - Evan Racah
A1  - Mostofa Ali Patwary
A1  - Tareq Malas
A1  - Narayanan Sundaram
A1  - Wahid Bhimji
A1  - Mikhail Smorkalov
A1  - Jack Deslippe
A1  - Mikhail Shiryaev
A1  - Srinivas Sridharan
A1  -  Prabhat
A1  - Pradeep Dubey
JO  - ArXiv e-prints
Y1  - 17 August, 2017
UR  - https://arxiv.org/abs/1708.05256
N2  - This paper presents the first, 15-PetaFLOP Deep Learning system for solving scientific pattern classification problems on contemporary HPC architectures. We develop supervised convolutional architectures for discriminating signals in high-energy physics data as well as semi-supervised architectures for localizing and classifying extreme weather in climate data. Our Intelcaffe-based implementation obtains $\sim$2TFLOP/s on a single Cori Phase-II Xeon-Phi node. We use a hybrid strategy employing synchronous node-groups, while using asynchronous communication across groups. We use this strategy to scale training of a single model to $\sim$9600 Xeon-Phi nodes; obtaining peak performance of 11.73-15.07 PFLOP/s and sustained performance of 11.41-13.27 PFLOP/s. At scale, our HEP architecture produces state-of-the-art classification accuracy on a dataset with 10M images, exceeding that achieved by selections on high-level physics-motivated features. Our semi-supervised architecture successfully extracts weather patterns in a 15TB climate dataset. Our results demonstrate that Deep Learning can be optimized and scaled effectively on many-core, HPC systems.
ER  -


TY  - Preprint
T1  - Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation
A1  - Yuhuai Wu
A1  - Elman Mansimov
A1  - Shun Liao
A1  - Roger Grosse
A1  - Jimmy Ba
JO  - ArXiv e-prints
Y1  - 18 August, 2017
UR  - https://arxiv.org/abs/1708.05144
N2  - In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines
ER  -


TY  - Preprint
T1  - Deep Residual Learning and PDEs on Manifold
A1  - Zhen Li
A1  - Zuoqiang Shi
JO  - ArXiv e-prints
Y1  - 24 January, 2018
UR  - https://arxiv.org/abs/1708.05115
N2  - In this paper, we formulate the deep residual network (ResNet) as a control problem of transport equation. In ResNet, the transport equation is solved along the characteristics. Based on this observation, deep neural network is closely related to the control problem of PDEs on manifold. We propose several models based on transport equation and Hamilton-Jacobi equation. The discretization of these PDEs on point cloud is also discussed.
ER  -


TY  - Preprint
T1  - DARVIZ: Deep Abstract Representation, Visualization, and Verification of Deep Learning Models
A1  - Anush Sankaran
A1  - Rahul Aralikatte
A1  - Senthil Mani
A1  - Shreya Khare
A1  - Naveen Panwar
A1  - Neelamadhav Gantayat
JO  - ArXiv e-prints
Y1  - 16 August, 2017
UR  - https://arxiv.org/abs/1708.04915
N2  - Traditional software engineering programming paradigms are mostly object or procedure oriented, driven by deterministic algorithms. With the advent of deep learning and cognitive sciences there is an emerging trend for data-driven programming, creating a shift in the programming paradigm among the software engineering communities. Visualizing and interpreting the execution of a current large scale data-driven software development is challenging. Further, for deep learning development there are many libraries in multiple programming languages such as TensorFlow (Python), CAFFE (C++), Theano (Python), Torch (Lua), and Deeplearning4j (Java), driving a huge need for interoperability across libraries.
ER  -


TY  - Preprint
T1  - Deep Learning for Passive Synthetic Aperture Radar
A1  - Bariscan Yonel
A1  - Eric Mason
A1  - Birsen YazÄ±cÄ±
JO  - ArXiv e-prints
Y1  - 11 August, 2017
UR  - https://arxiv.org/abs/1708.04682
N2  - We introduce a deep learning (DL) framework for inverse problems in imaging, and demonstrate the advantages and applicability of this approach in passive synthetic aperture radar (SAR) image reconstruction. We interpret image recon- struction as a machine learning task and utilize deep networks as forward and inverse solvers for imaging. Specifically, we design a recurrent neural network (RNN) architecture as an inverse solver based on the iterations of proximal gradient descent optimization methods. We further adapt the RNN architecture to image reconstruction problems by transforming the network into a recurrent auto-encoder, thereby allowing for unsupervised training. Our DL based inverse solver is particularly suitable for a class of image formation problems in which the forward model is only partially known. The ability to learn forward models and hyper parameters combined with unsupervised training approach establish our recurrent auto-encoder suitable for real world applications. We demonstrate the performance of our method in passive SAR image reconstruction. In this regime a source of opportunity, with unknown location and transmitted waveform, is used to illuminate a scene of interest. We investigate recurrent auto- encoder architecture based on the 1 and 0 constrained least- squares problem. We present a projected stochastic gradient descent based training scheme which incorporates constraints of the unknown model parameters. We demonstrate through extensive numerical simulations that our DL based approach out performs conventional sparse coding methods in terms of computation and reconstructed image quality, specifically, when no information about the transmitter is available.
ER  -


TY  - Preprint
T1  - Acoustic Feature Learning via Deep Variational Canonical Correlation Analysis
A1  - Qingming Tang
A1  - Weiran Wang
A1  - Karen Livescu
JO  - ArXiv e-prints
Y1  - 31 August, 2017
UR  - https://arxiv.org/abs/1708.04673
N2  - We study the problem of acoustic feature learning in the setting where we have access to another (non-acoustic) modality for feature learning but not at test time. We use deep variational canonical correlation analysis (VCCA), a recently proposed deep generative method for multi-view representation learning. We also extend VCCA with improved latent variable priors and with adversarial learning. Compared to other techniques for multi-view feature learning, VCCA&#39;s advantages include an intuitive latent variable interpretation and a variational lower bound objective that can be trained end-to-end efficiently. We compare VCCA and its extensions with previous feature learning methods on the University of Wisconsin X-ray Microbeam Database, and show that VCCA-based feature learning improves over previous methods for speaker-independent phonetic recognition.
ER  -


TY  - Preprint
T1  - Extractive Summarization using Deep Learning
A1  - Sukriti Verma
A1  - Vagisha Nidhi
JO  - ArXiv e-prints
Y1  - 15 August, 2017
UR  - https://arxiv.org/abs/1708.04439
N2  - This paper proposes a text summarization approach for factual reports using a deep learning model. This approach consists of three phases: feature extraction, feature enhancement, and summary generation, which work together to assimilate core information and generate a coherent, understandable summary. We are exploring various features to improve the set of sentences selected for the summary, and are using a Restricted Boltzmann Machine to enhance and abstract those features to improve resultant accuracy without losing any important information. The sentences are scored based on those enhanced features and an extractive summary is constructed. Experimentation carried out on several articles demonstrates the effectiveness of the proposed approach.
ER  -


TY  - Preprint
T1  - Graph Classification via Deep Learning with Virtual Nodes
A1  - Trang Pham
A1  - Truyen Tran
A1  - Hoa Dam
A1  - Svetha Venkatesh
JO  - ArXiv e-prints
Y1  - 14 August, 2017
UR  - https://arxiv.org/abs/1708.04357
N2  - Learning representation for graph classification turns a variable-size graph into a fixed-size vector (or matrix). Such a representation works nicely with algebraic manipulations. Here we introduce a simple method to augment an attributed graph with a virtual node that is bidirectionally connected to all existing nodes. The virtual node represents the latent aspects of the graph, which are not immediately available from the attributes and local connectivity structures. The expanded graph is then put through any node representation method. The representation of the virtual node is then the representation of the entire graph. In this paper, we use the recently introduced Column Network for the expanded graph, resulting in a new end-to-end graph classification model dubbed Virtual Column Network (VCN). The model is validated on two tasks: (i) predicting bio-activity of chemical compounds, and (ii) finding software vulnerability from source code. Results demonstrate that VCN is competitive against well-established rivals.
ER  -


TY  - Preprint
T1  - Deep Object-Centric Representations for Generalizable Robot Learning
A1  - Coline Devin
A1  - Pieter Abbeel
A1  - Trevor Darrell
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 26 September, 2017
UR  - https://arxiv.org/abs/1708.04225
N2  - Robotic manipulation in complex open-world scenarios requires both reliable physical manipulation skills and effective and generalizable perception. In this paper, we propose a method where general purpose pretrained visual models serve as an object-centric prior for the perception system of a learned policy. We devise an object-level attentional mechanism that can be used to determine relevant objects from a few trajectories or demonstrations, and then immediately incorporate those objects into a learned policy. A task-independent meta-attention locates possible objects in the scene, and a task-specific attention identifies which objects are predictive of the trajectories. The scope of the task-specific attention is easily adjusted by showing demonstrations with distractor objects or with diverse relevant objects. Our results indicate that this approach exhibits good generalization across object instances using very few samples, and can be used to learn a variety of manipulation tasks using reinforcement learning.
ER  -


TY  - Preprint
T1  - Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control
A1  - Riashat Islam
A1  - Peter Henderson
A1  - Maziar Gomrokchi
A1  - Doina Precup
JO  - ArXiv e-prints
Y1  - 10 August, 2017
UR  - https://arxiv.org/abs/1708.04133
N2  - Policy gradient methods in reinforcement learning have become increasingly prevalent for state-of-the-art performance in continuous control tasks. Novel methods typically benchmark against a few key algorithms such as deep deterministic policy gradients and trust region policy optimization. As such, it is important to present and use consistent baselines experiments. However, this can be difficult due to general variance in the algorithms, hyper-parameter tuning, and environment stochasticity. We investigate and discuss: the significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results. We provide guidelines on reporting novel results as comparisons against baseline methods such that future researchers can make informed decisions when investigating novel methods.
ER  -


TY  - Preprint
T1  - Kinship Verification from Videos using Spatio-Temporal Texture Features and Deep Learning
A1  - Elhocine Boutellaa
A1  - Miguel Bordallo LÃ³pez
A1  - Samy Ait-Aoudia
A1  - Xiaoyi Feng
A1  - Abdenour Hadid
JO  - ArXiv e-prints
Y1  - 14 August, 2017
UR  - https://arxiv.org/abs/1708.04069
N2  - Automatic kinship verification using facial images is a relatively new and challenging research problem in computer vision. It consists in automatically predicting whether two persons have a biological kin relation by examining their facial attributes. While most of the existing works extract shallow handcrafted features from still face images, we approach this problem from spatio-temporal point of view and explore the use of both shallow texture features and deep features for characterizing faces. Promising results, especially those of deep features, are obtained on the benchmark UvA-NEMO Smile database. Our extensive experiments also show the superiority of using videos over still images, hence pointing out the important role of facial dynamics in kinship verification. Furthermore, the fusion of the two types of features (i.e. shallow spatio-temporal texture features and deep features) shows significant performance improvements compared to state-of-the-art methods.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for High Precision Assembly Tasks
A1  - Tadanobu Inoue
A1  - Giovanni De Magistris
A1  - Asim Munawar
A1  - Tsuyoshi Yokoya
A1  - Ryuki Tachibana
JO  - ArXiv e-prints
Y1  - 21 September, 2017
UR  - https://arxiv.org/abs/1708.04033
N2  - High precision assembly of mechanical parts requires accuracy exceeding the robot precision. Conventional part mating methods used in the current manufacturing requires tedious tuning of numerous parameters before deployment. We show how the robot can successfully perform a tight clearance peg-in-hole task through training a recurrent neural network with reinforcement learning. In addition to saving the manual effort, the proposed technique also shows robustness against position and angle errors for the peg-in-hole task. The neural network learns to take the optimal action by observing the robot sensors to estimate the system state. The advantages of our proposed method is validated experimentally on a 7-axis articulated robot arm.
ER  -


TY  - Preprint
T1  - Towards Speech Emotion Recognition &#34;in the wild&#34; using Aggregated Corpora and Deep Multi-Task Learning
A1  - Jaebok Kim
A1  - Gwenn Englebienne
A1  - Khiet P. Truong
A1  - Vanessa Evers
JO  - ArXiv e-prints
Y1  - 13 August, 2017
UR  - https://arxiv.org/abs/1708.03920
N2  - One of the challenges in Speech Emotion Recognition (SER) &#34;in the wild&#34; is the large mismatch between training and test data (e.g. speakers and tasks). In order to improve the generalisation capabilities of the emotion models, we propose to use Multi-Task Learning (MTL) and use gender and naturalness as auxiliary tasks in deep neural networks. This method was evaluated in within-corpus and various cross-corpus classification experiments that simulate conditions &#34;in the wild&#34;. In comparison to Single-Task Learning (STL) based state of the art methods, we found that our MTL method proposed improved performance significantly. Particularly, models using both gender and naturalness achieved more gains than those using either gender or naturalness separately. This benefit was also found in the high-level representations of the feature space, obtained from our method proposed, where discriminative emotional clusters could be observed.
ER  -


TY  - Preprint
T1  - Learning Deep Neural Networks for Vehicle Re-ID with Visual-spatio-temporal Path Proposals
A1  - Yantao Shen
A1  - Tong Xiao
A1  - Hongsheng Li
A1  - Shuai Yi
A1  - Xiaogang Wang
JO  - ArXiv e-prints
Y1  - 13 August, 2017
UR  - https://arxiv.org/abs/1708.03918
N2  - Vehicle re-identification is an important problem and has many applications in video surveillance and intelligent transportation. It gains increasing attention because of the recent advances of person re-identification techniques. However, unlike person re-identification, the visual differences between pairs of vehicle images are usually subtle and even challenging for humans to distinguish. Incorporating additional spatio-temporal information is vital for solving the challenging re-identification task. Existing vehicle re-identification methods ignored or used over-simplified models for the spatio-temporal relations between vehicle images. In this paper, we propose a two-stage framework that incorporates complex spatio-temporal information for effectively regularizing the re-identification results. Given a pair of vehicle images with their spatio-temporal information, a candidate visual-spatio-temporal path is first generated by a chain MRF model with a deeply learned potential function, where each visual-spatio-temporal state corresponds to an actual vehicle image with its spatio-temporal information. A Siamese-CNN+Path-LSTM model takes the candidate path as well as the pairwise queries to generate their similarity score. Extensive experiments and analysis show the effectiveness of our proposed method and individual components.
ER  -


TY  - Preprint
T1  - IoT Data Analytics Using Deep Learning
A1  - Xiaofeng Xie
A1  - Di Wu
A1  - Siping Liu
A1  - Renfa Li
JO  - ArXiv e-prints
Y1  - 13 August, 2017
UR  - https://arxiv.org/abs/1708.03854
N2  - Deep learning is a popular machine learning approach which has achieved a lot of progress in all traditional machine learning areas. Internet of thing (IoT) and Smart City deployments are generating large amounts of time-series sensor data in need of analysis. Applying deep learning to these domains has been an important topic of research. The Long-Short Term Memory (LSTM) network has been proven to be well suited for dealing with and predicting important events with long intervals and delays in the time series. LTSM networks have the ability to maintain long-term memory. In an LTSM network, a stacked LSTM hidden layer also makes it possible to learn a high level temporal feature without the need of any fine tuning and preprocessing which would be required by other techniques. In this paper, we construct a long-short term memory (LSTM) recurrent neural network structure, use the normal time series training set to build the prediction model. And then we use the predicted error from the prediction model to construct a Gaussian naive Bayes model to detect whether the original sample is abnormal. This method is called LSTM-Gauss-NBayes for short. We use three real-world data sets, each of which involve long-term time-dependence or short-term time-dependence, even very weak time dependence. The experimental results show that LSTM-Gauss-NBayes is an effective and robust model.
ER  -


TY  - Preprint
T1  - Deep Steering: Learning End-to-End Driving Model from Spatial and Temporal Visual Cues
A1  - Lu Chi
A1  - Yadong Mu
JO  - ArXiv e-prints
Y1  - 12 August, 2017
UR  - https://arxiv.org/abs/1708.03798
N2  - In recent years, autonomous driving algorithms using low-cost vehicle-mounted cameras have attracted increasing endeavors from both academia and industry. There are multiple fronts to these endeavors, including object detection on roads, 3-D reconstruction etc., but in this work we focus on a vision-based model that directly maps raw input images to steering angles using deep networks. This represents a nascent research topic in computer vision. The technical contributions of this work are three-fold. First, the model is learned and evaluated on real human driving videos that are time-synchronized with other vehicle sensors. This differs from many prior models trained from synthetic data in racing games. Second, state-of-the-art models, such as PilotNet, mostly predict the wheel angles independently on each video frame, which contradicts common understanding of driving as a stateful process. Instead, our proposed model strikes a combination of spatial and temporal cues, jointly investigating instantaneous monocular camera observations and vehicle&#39;s historical states. This is in practice accomplished by inserting carefully-designed recurrent units (e.g., LSTM and Conv-LSTM) at proper network layers. Third, to facilitate the interpretability of the learned model, we utilize a visual back-propagation scheme for discovering and visualizing image regions crucially influencing the final steering prediction. Our experimental study is based on about 6 hours of human driving data provided by Udacity. Comprehensive quantitative evaluations demonstrate the effectiveness and robustness of our model, even under scenarios like drastic lighting changes and abrupt turning. The comparison with other state-of-the-art models clearly reveals its superior performance in predicting the due wheel angle for a self-driving car.
ER  -


TY  - Preprint
T1  - Unsupervised Incremental Learning of Deep Descriptors From Video Streams
A1  - Federico Pernici
A1  - Alberto Del Bimbo
JO  - ArXiv e-prints
Y1  - 11 August, 2017
UR  - https://arxiv.org/abs/1708.03615
N2  - We present a novel unsupervised method for face identity learning from video sequences. The method exploits the ResNet deep network for face detection and VGGface fc7 face descriptors together with a smart learning mechanism that exploits the temporal coherence of visual data in video streams. We present a novel feature matching solution based on Reverse Nearest Neighbour and a feature forgetting strategy that supports incremental learning with memory size control, while time progresses. It is shown that the proposed learning procedure is asymptotically stable and can be effectively applied to relevant applications like multiple face tracking.
ER  -


TY  - Preprint
T1  - Attention-Aware Face Hallucination via Deep Reinforcement Learning
A1  - Qingxing Cao
A1  - Liang Lin
A1  - Yukai Shi
A1  - Xiaodan Liang
A1  - Guanbin Li
JO  - ArXiv e-prints
Y1  - 10 August, 2017
UR  - https://arxiv.org/abs/1708.03132
N2  - Face hallucination is a domain-specific super-resolution problem with the goal to generate high-resolution (HR) faces from low-resolution (LR) input images. In contrast to existing methods that often learn a single patch-to-patch mapping from LR to HR images and are regardless of the contextual interdependency between patches, we propose a novel Attention-aware Face Hallucination (Attention-FH) framework which resorts to deep reinforcement learning for sequentially discovering attended patches and then performing the facial part enhancement by fully exploiting the global interdependency of the image. Specifically, in each time step, the recurrent policy network is proposed to dynamically specify a new attended region by incorporating what happened in the past. The state (i.e., face hallucination result for the whole image) can thus be exploited and updated by the local enhancement network on the selected region. The Attention-FH approach jointly learns the recurrent policy network and local enhancement network through maximizing the long-term reward that reflects the hallucination performance over the whole image. Therefore, our proposed Attention-FH is capable of adaptively personalizing an optimal searching path for each face image according to its own characteristic. Extensive experiments show our approach significantly surpasses the state-of-the-arts on in-the-wild faces with large pose and illumination variations.
ER  -


TY  - Preprint
T1  - Scaling Deep Learning on GPU and Knights Landing clusters
A1  - Yang You
A1  - Aydin Buluc
A1  - James Demmel
JO  - ArXiv e-prints
Y1  - 9 August, 2017
UR  - https://arxiv.org/abs/1708.02983
N2  - The speed of deep neural networks training has become a big bottleneck of deep learning research and development. For example, training GoogleNet by ImageNet dataset on one Nvidia K20 GPU needs 21 days. To speed up the training process, the current deep learning systems heavily rely on the hardware accelerators. However, these accelerators have limited on-chip memory compared with CPUs. To handle large datasets, they need to fetch data from either CPU memory or remote processors. We use both self-hosted Intel Knights Landing (KNL) clusters and multi-GPU clusters as our target platforms. From an algorithm aspect, current distributed machine learning systems are mainly designed for cloud systems. These methods are asynchronous because of the slow network and high fault-tolerance requirement on cloud systems. We focus on Elastic Averaging SGD (EASGD) to design algorithms for HPC clusters. Original EASGD used round-robin method for communication and updating. The communication is ordered by the machine rank ID, which is inefficient on HPC clusters.
ER  -


TY  - Preprint
T1  - Learning Policies for Adaptive Tracking with Deep Feature Cascades
A1  - Chen Huang
A1  - Simon Lucey
A1  - Deva Ramanan
JO  - ArXiv e-prints
Y1  - 13 September, 2017
UR  - https://arxiv.org/abs/1708.02973
N2  - Visual object tracking is a fundamental and time-critical vision task. Recent years have seen many shallow tracking methods based on real-time pixel-based correlation filters, as well as deep methods that have top performance but need a high-end GPU. In this paper, we learn to improve the speed of deep trackers without losing accuracy. Our fundamental insight is to take an adaptive approach, where easy frames are processed with cheap features (such as pixel values), while challenging frames are processed with invariant but expensive deep features. We formulate the adaptive tracking problem as a decision-making process, and learn an agent to decide whether to locate objects with high confidence on an early layer, or continue processing subsequent layers of a network. This significantly reduces the feed-forward cost for easy frames with distinct or slow-moving objects. We train the agent offline in a reinforcement learning fashion, and further demonstrate that learning all deep layers (so as to provide good features for adaptive tracking) can lead to near real-time average tracking speed of 23 fps on a single CPU while achieving state-of-the-art performance. Perhaps most tellingly, our approach provides a 100X speedup for almost 50% of the time, indicating the power of an adaptive approach.
ER  -


TY  - Preprint
T1  - Probabilistic Neural Network with Complex Exponential Activation Functions in Image Recognition using Deep Learning Framework
A1  - Andrey Savchenko
JO  - ArXiv e-prints
Y1  - 9 August, 2017
UR  - https://arxiv.org/abs/1708.02733
N2  - If the training dataset is not very large, image recognition is usually implemented with the transfer learning methods. In these methods the features are extracted using a deep convolutional neural network, which was preliminarily trained with an external very-large dataset. In this paper we consider the nonparametric classification of extracted feature vectors with the probabilistic neural network (PNN). The number of neurons at the pattern layer of the PNN is equal to the database size, which causes the low recognition performance and high memory space complexity of this network. We propose to overcome these drawbacks by replacing the exponential activation function in the Gaussian Parzen kernel to the complex exponential functions in the FejÃ©r kernel. We demonstrate that in this case it is possible to implement the network with the number of neurons in the pattern layer proportional to the cubic root of the database size. Thus, the proposed modification of the PNN makes it possible to significantly decrease runtime and memory complexities without loosing its main advantages, namely, extremely fast training procedure and the convergence to the optimal Bayesian decision. An experimental study in visual object category classification and unconstrained face recognition with contemporary deep neural networks have shown, that our approach obtains very efficient and rather accurate decisions for the small training sample in comparison with the well-known classifiers.
ER  -


TY  - Preprint
T1  - Weakly- and Self-Supervised Learning for Content-Aware Deep Image Retargeting
A1  - Donghyeon Cho
A1  - Jinsun Park
A1  - Tae-Hyun Oh
A1  - Yu-Wing Tai
A1  - In So Kweon
JO  - ArXiv e-prints
Y1  - 9 August, 2017
UR  - https://arxiv.org/abs/1708.02731
N2  - This paper proposes a weakly- and self-supervised deep convolutional neural network (WSSDCNN) for content-aware image retargeting. Our network takes a source image and a target aspect ratio, and then directly outputs a retargeted image. Retargeting is performed through a shift map, which is a pixel-wise mapping from the source to the target grid. Our method implicitly learns an attention map, which leads to a content-aware shift map for image retargeting. As a result, discriminative parts in an image are preserved, while background regions are adjusted seamlessly. In the training phase, pairs of an image and its image-level annotation are used to compute content and structure losses. We demonstrate the effectiveness of our proposed method for a retargeting application with insightful analyses.
ER  -


TY  - Preprint
T1  - Sequential Dual Deep Learning with Shape and Texture Features for Sketch Recognition
A1  - Qi Jia
A1  - Meiyu Yu
A1  - Xin Fan
A1  - Haojie Li
JO  - ArXiv e-prints
Y1  - 9 August, 2017
UR  - https://arxiv.org/abs/1708.02716
N2  - Recognizing freehand sketches with high arbitrariness is greatly challenging. Most existing methods either ignore the geometric characteristics or treat sketches as handwritten characters with fixed structural ordering. Consequently, they can hardly yield high recognition performance even though sophisticated learning techniques are employed. In this paper, we propose a sequential deep learning strategy that combines both shape and texture features. A coded shape descriptor is exploited to characterize the geometry of sketch strokes with high flexibility, while the outputs of constitutional neural networks (CNN) are taken as the abstract texture feature. We develop dual deep networks with memorable gated recurrent units (GRUs), and sequentially feed these two types of features into the dual networks, respectively. These dual networks enable the feature fusion by another gated recurrent unit (GRU), and thus accurately recognize sketches invariant to stroke ordering. The experiments on the TU-Berlin data set show that our method outperforms the average of human and state-of-the-art algorithms even when significant shape and appearance variations occur.
ER  -


TY  - Preprint
T1  - Recent Trends in Deep Learning Based Natural Language Processing
A1  - Tom Young
A1  - Devamanyu Hazarika
A1  - Soujanya Poria
A1  - Erik Cambria
JO  - ArXiv e-prints
Y1  - 4 August, 2018
UR  - https://arxiv.org/abs/1708.02709
N2  - Deep learning methods employ multiple processing layers to learn hierarchical representations of data and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.
ER  -


TY  - Preprint
T1  - Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning
A1  - Anusha Nagabandi
A1  - Gregory Kahn
A1  - Ronald S. Fearing
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 1 December, 2017
UR  - https://arxiv.org/abs/1708.02596
N2  - Model-free deep reinforcement learning algorithms have been shown to be capable of learning a wide range of robotic skills, but typically require a very large number of samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks. We also propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent sample efficiency, and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents. Videos can be found at https://sites.google.com/view/mbmf
ER  -


TY  - Preprint
T1  - Learning how to Active Learn: A Deep Reinforcement Learning Approach
A1  - Meng Fang
A1  - Yuan Li
A1  - Trevor Cohn
JO  - ArXiv e-prints
Y1  - 8 August, 2017
UR  - https://arxiv.org/abs/1708.02383
N2  - Active learning aims to select a small subset of data for annotation such that a classifier learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic. Importantly, our method allows the selection policy learned using simulation on one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning.
ER  -


TY  - Preprint
T1  - Multibiometric Secure System Based on Deep Learning
A1  - Veeru Talreja
A1  - Matthew C. Valenti
A1  - Nasser M. Nasrabadi
JO  - ArXiv e-prints
Y1  - 7 August, 2017
UR  - https://arxiv.org/abs/1708.02314
N2  - In this paper, we propose a secure multibiometric system that uses deep neural networks and error-correction coding. We present a feature-level fusion framework to generate a secure multibiometric template from each user&#39;s multiple biometrics. Two fusion architectures, fully connected architecture and bilinear architecture, are implemented to develop a robust multibiometric shared representation. The shared representation is used to generate a cancelable biometric template that involves the selection of a different set of reliable and discriminative features for each user. This cancelable template is a binary vector and is passed through an appropriate error-correcting decoder to find a closest codeword and this codeword is hashed to generate the final secure template. The efficacy of the proposed approach is shown using a multimodal database where we achieve state-of-the-art matching performance, along with cancelability and security.
ER  -


TY  - Preprint
T1  - Regularised Deep Reinforcement Learning with Guaranteed Convergence
A1  - Felix Leibfried
A1  - Rasul Tutunov
A1  - Jordi Grau-Moya
A1  - Haitham Bou-Ammar
JO  - ArXiv e-prints
Y1  - 6 September, 2018
UR  - https://arxiv.org/abs/1708.01867
N2  - Deep Q-networks (DQNs) suffer from two important challenges hindering their application in real-world scenarios. First, DQNs overestimate Q-values which leads to increased sample complexity, and second, no theoretical convergence guarantees have been established. In this paper, we address both problems by introducing an intrinsic penalty signal arising from a Kullback-Leibler (KL) constraint that encourages reduced Q-value estimates. We then prove, for the first time, convergence to a stationary point under a specific scheduling of the penalisation magnitude. Our proofs operate in the deep reinforcement learning setting that considers convolutional and dense layers for Q-function approximation. Furthermore, we prove divergence of standard DQNs using a counter example that relates to the non-optimal choice of the history-scheduling parameter adopted by `vanilla&#39; DQNs. We believe this can shed the light on some of the difficulties reported by researchers and practitioners in the field.
ER  -


TY  - Preprint
T1  - Deep Metric Learning with Angular Loss
A1  - Jian Wang
A1  - Feng Zhou
A1  - Shilei Wen
A1  - Xiao Liu
A1  - Yuanqing Lin
JO  - ArXiv e-prints
Y1  - 4 August, 2017
UR  - https://arxiv.org/abs/1708.01682
N2  - The modern image search system requires semantic understanding of image, and a key yet under-addressed problem is to learn a good metric for measuring the similarity between images. While deep metric learning has yielded impressive performance gains by extracting high level abstractions from image data, a proper objective loss function becomes the central issue to boost the performance. In this paper, we propose a novel angular loss, which takes angle relationship into account, for learning better similarity metric. Whereas previous metric learning methods focus on optimizing the similarity (contrastive loss) or relative similarity (triplet loss) of image pairs, our proposed method aims at constraining the angle at the negative point of triplet triangles. Several favorable properties are observed when compared with conventional methods. First, scale invariance is introduced, improving the robustness of objective against feature variance. Second, a third-order geometric constraint is inherently imposed, capturing additional local structure of triplet triangles than contrastive loss or triplet loss. Third, better convergence has been demonstrated by experiments on three publicly available datasets.
ER  -


TY  - Preprint
T1  - Hashtag Healthcare: From Tweets to Mental Health Journals Using Deep Transfer Learning
A1  - Benjamin Shickel
A1  - Martin Heesacker
A1  - Sherry Benton
A1  - Parisa Rashidi
JO  - ArXiv e-prints
Y1  - 3 August, 2017
UR  - https://arxiv.org/abs/1708.01372
N2  - As the popularity of social media platforms continues to rise, an ever-increasing amount of human communication and self- expression takes place online. Most recent research has focused on mining social media for public user opinion about external entities such as product reviews or sentiment towards political news. However, less attention has been paid to analyzing users&#39; internalized thoughts and emotions from a mental health perspective. In this paper, we quantify the semantic difference between public Tweets and private mental health journals used in online cognitive behavioral therapy. We will use deep transfer learning techniques for analyzing the semantic gap between the two domains. We show that for the task of emotional valence prediction, social media can be successfully harnessed to create more accurate, robust, and personalized mental health models. Our results suggest that the semantic gap between public and private self-expression is small, and that utilizing the abundance of available social media is one way to overcome the small sample sizes of mental health data, which are commonly limited by availability and privacy concerns.
ER  -


TY  - Preprint
T1  - Learning Accurate Low-Bit Deep Neural Networks with Stochastic Quantization
A1  - Yinpeng Dong
A1  - Renkun Ni
A1  - Jianguo Li
A1  - Yurong Chen
A1  - Jun Zhu
A1  - Hang Su
JO  - ArXiv e-prints
Y1  - 3 August, 2017
UR  - https://arxiv.org/abs/1708.01001
N2  - Low-bit deep neural networks (DNNs) become critical for embedded applications due to their low storage requirement and computing efficiency. However, they suffer much from the non-negligible accuracy drop. This paper proposes the stochastic quantization (SQ) algorithm for learning accurate low-bit DNNs. The motivation is due to the following observation. Existing training algorithms approximate the real-valued elements/filters with low-bit representation all together in each iteration. The quantization errors may be small for some elements/filters, while are remarkable for others, which lead to inappropriate gradient direction during training, and thus bring notable accuracy drop. Instead, SQ quantizes a portion of elements/filters to low-bit with a stochastic probability inversely proportional to the quantization error, while keeping the other portion unchanged with full-precision. The quantized and full-precision portions are updated with corresponding gradients separately in each iteration. The SQ ratio is gradually increased until the whole network is quantized. This procedure can greatly compensate the quantization error and thus yield better accuracy for low-bit DNNs. Experiments show that SQ can consistently and significantly improve the accuracy for different low-bit DNNs on various datasets and various network structures.
ER  -


TY  - Preprint
T1  - Adversarial-Playground: A Visualization Suite Showing How Adversarial Examples Fool Deep Learning
A1  - Andrew P. Norton
A1  - Yanjun Qi
JO  - ArXiv e-prints
Y1  - 1 August, 2017
UR  - https://arxiv.org/abs/1708.00807
N2  - Recent studies have shown that attackers can force deep learning models to misclassify so-called &#34;adversarial examples&#34;: maliciously generated images formed by making imperceptible modifications to pixel values. With growing interest in deep learning for security applications, it is important for security experts and users of machine learning to recognize how learning systems may be attacked. Due to the complex nature of deep learning, it is challenging to understand how deep models can be fooled by adversarial examples. Thus, we present a web-based visualization tool, Adversarial-Playground, to demonstrate the efficacy of common adversarial methods against a convolutional neural network (CNN) system. Adversarial-Playground is educational, modular and interactive. (1) It enables non-experts to compare examples visually and to understand why an adversarial example can fool a CNN-based image classifier. (2) It can help security experts explore more vulnerability of deep learning as a software module. (3) Building an interactive visualization is challenging in this domain due to the large feature space of image classification (generating adversarial examples is slow in general and visualizing images are costly). Through multiple novel design choices, our tool can provide fast and accurate responses to user requests. Empirically, we find that our client-server division strategy reduced the response time by an average of 1.5 seconds per sample. Our other innovation, a faster variant of JSMA evasion algorithm, empirically performed twice as fast as JSMA and yet maintains a comparable evasion rate.
ER  -


TY  - Preprint
T1  - OmniArt: Multi-task Deep Learning for Artistic Data Analysis
A1  - Gjorgji Strezoski
A1  - Marcel Worring
JO  - ArXiv e-prints
Y1  - 2 August, 2017
UR  - https://arxiv.org/abs/1708.00684
N2  - Vast amounts of artistic data is scattered on-line from both museums and art applications. Collecting, processing and studying it with respect to all accompanying attributes is an expensive process. With a motivation to speed up and improve the quality of categorical analysis in the artistic domain, in this paper we propose an efficient and accurate method for multi-task learning with a shared representation applied in the artistic domain. We continue to show how different multi-task configurations of our method behave on artistic data and outperform handcrafted feature approaches as well as convolutional neural networks. In addition to the method and analysis, we propose a challenge like nature to the new aggregated data set with almost half a million samples and structured meta-data to encourage further research and societal engagement.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Inquiry Dialog Policies with Logical Formula Embeddings
A1  - Takuya Hiraoka
A1  - Masaaki Tsuchida
A1  - Yotaro Watanabe
JO  - ArXiv e-prints
Y1  - 2 August, 2017
UR  - https://arxiv.org/abs/1708.00667
N2  - This paper is the first attempt to learn the policy of an inquiry dialog system (IDS) by using deep reinforcement learning (DRL). Most IDS frameworks represent dialog states and dialog acts with logical formulae. In order to make learning inquiry dialog policies more effective, we introduce a logical formula embedding framework based on a recursive neural network. The results of experiments to evaluate the effect of 1) the DRL and 2) the logical formula embedding framework show that the combination of the two are as effective or even better than existing rule-based methods for inquiry dialog policies.
ER  -


TY  - Preprint
T1  - ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural Projections
A1  - Sujith Ravi
JO  - ArXiv e-prints
Y1  - 9 August, 2017
UR  - https://arxiv.org/abs/1708.00630
N2  - Deep neural networks have become ubiquitous for applications related to visual recognition and language understanding tasks. However, it is often prohibitive to use typical neural networks on devices like mobile phones or smart watches since the model sizes are huge and cannot fit in the limited memory available on such devices. While these devices could make use of machine learning models running on high-performance data centers with CPUs or GPUs, this is not feasible for many applications because data can be privacy sensitive and inference needs to be performed directly &#34;on&#34; device.
ER  -


TY  - Preprint
T1  - Learning Deep Convolutional Embeddings for Face Representation Using Joint Sample- and Set-based Supervision
A1  - Baris Gecer
A1  - Vassileios Balntas
A1  - Tae-Kyun Kim
JO  - ArXiv e-prints
Y1  - 12 January, 2018
UR  - https://arxiv.org/abs/1708.00277
N2  - In this work, we investigate several methods and strategies to learn deep embeddings for face recognition, using joint sample- and set-based optimization. We explain our framework that expands traditional learning with set-based supervision together with the strategies used to maintain set characteristics. We, then, briefly review the related set-based loss functions, and subsequently propose a novel Max-Margin Loss which maximizes maximum possible inter-class margin with assistance of Support Vector Machines (SVMs). It implicitly pushes all the samples towards correct side of the margin with a vector perpendicular to the hyperplane and a strength exponentially growing towards to negative side of the hyperplane. We show that the introduced loss outperform the previous sample-based and set-based ones in terms verification of faces on two commonly used benchmarks.
ER  -


TY  - Preprint
T1  - Deep Asymmetric Multi-task Feature Learning
A1  - Hae Beom Lee
A1  - Eunho Yang
A1  - Sung Ju Hwang
JO  - ArXiv e-prints
Y1  - 30 June, 2018
UR  - https://arxiv.org/abs/1708.00260
N2  - We propose Deep Asymmetric Multitask Feature Learning (Deep-AMTFL) which can learn deep representations shared across multiple tasks while effectively preventing negative transfer that may happen in the feature sharing process. Specifically, we introduce an asymmetric autoencoder term that allows reliable predictors for the easy tasks to have high contribution to the feature learning while suppressing the influences of unreliable predictors for more difficult tasks. This allows the learning of less noisy representations, and enables unreliable predictors to exploit knowledge from the reliable predictors via the shared latent features. Such asymmetric knowledge transfer through shared features is also more scalable and efficient than inter-task asymmetric transfer. We validate our Deep-AMTFL model on multiple benchmark datasets for multitask learning and image classification, on which it significantly outperforms existing symmetric and asymmetric multitask learning models, by effectively preventing negative transfer in deep feature learning.
ER  -


TY  - Preprint
T1  - Deep Transfer in Reinforcement Learning by Language Grounding
A1  - Karthik Narasimhan
A1  - Regina Barzilay
A1  - Tommi Jaakkola
JO  - ArXiv e-prints
Y1  - 31 July, 2017
UR  - https://arxiv.org/abs/1708.00133
N2  - In this paper, we explore the utilization of natural language to drive transfer for reinforcement learning (RL). Despite the wide-spread application of deep RL techniques, learning generalized policy representations that work across domains remains a challenging problem. We demonstrate that textual descriptions of environments provide a compact intermediate channel to facilitate effective policy transfer. We employ a model-based RL approach consisting of a differentiable planning module, a model-free component and a factorized representation to effectively utilize entity descriptions. Our model outperforms prior work on both transfer and multi-task scenarios in a variety of different environments.
ER  -


TY  - Preprint
T1  - Compiling Deep Learning Models for Custom Hardware Accelerators
A1  - Andre Xian Ming Chang
A1  - Aliasger Zaidy
A1  - Vinayak Gokhale
A1  - Eugenio Culurciello
JO  - ArXiv e-prints
Y1  - 10 December, 2017
UR  - https://arxiv.org/abs/1708.00117
N2  - Convolutional neural networks (CNNs) are the core of most state-of-the-art deep learning algorithms specialized for object detection and classification. CNNs are both computationally complex and embarrassingly parallel. Two properties that leave room for potential software and hardware optimizations for embedded systems. Given a programmable hardware accelerator with a CNN oriented custom instructions set, the compiler&#39;s task is to exploit the hardware&#39;s full potential, while abiding with the hardware constraints and maintaining generality to run different CNN models with varying workload properties. Snowflake is an efficient and scalable hardware accelerator implemented on programmable logic devices. It implements a control pipeline for a custom instruction set. The goal of this paper is to present Snowflake&#39;s compiler that generates machine level instructions from Torch7 model description files. The main software design points explored in this work are: model structure parsing, CNN workload breakdown, loop rearrangement for memory bandwidth optimizations and memory access balancing. The performance achieved by compiler generated instructions matches against hand optimized code for convolution layers. Generated instructions also efficiently execute AlexNet and ResNet18 inference on Snowflake. Snowflake with $256$ processing units was synthesized on Xilinx&#39;s Zynq XC7Z045 FPGA. At $250$ MHz, AlexNet achieved in $93.6$ frames/s and $1.2$ GB/s of off-chip memory bandwidth, and $21.4$ frames/s and $2.2$ GB/s for ResNet18. Total on-chip power is $5$ W.
ER  -


TY  - Preprint
T1  - Vision-Based Assessment of Parkinsonism and Levodopa-Induced Dyskinesia with Deep Learning Pose Estimation
A1  - Michael H. Li
A1  - Tiago A. Mestre
A1  - Susan H. Fox
A1  - Babak Taati
JO  - ArXiv e-prints
Y1  - 1 August, 2017
UR  - https://arxiv.org/abs/1707.09416
N2  - Objective: To apply deep learning pose estimation algorithms for vision-based assessment of parkinsonism and levodopa-induced dyskinesia (LID). Methods: Nine participants with Parkinson&#39;s disease (PD) and LID completed a levodopa infusion protocol, where symptoms were assessed at regular intervals using the Unified Dyskinesia Rating Scale (UDysRS) and Unified Parkinson&#39;s Disease Rating Scale (UPDRS). A state-of-the-art deep learning pose estimation method was used to extract movement trajectories from videos of PD assessments. Features of the movement trajectories were used to detect and estimate the severity of parkinsonism and LID using random forest. Communication and drinking tasks were used to assess LID, while leg agility and toe tapping tasks were used to assess parkinsonism. Feature sets from tasks were also combined to predict total UDysRS and UPDRS Part III scores. Results: For LID, the communication task yielded the best results for dyskinesia (severity estimation: r = 0.661, detection: AUC = 0.930). For parkinsonism, leg agility had better results for severity estimation (r = 0.618), while toe tapping was better for detection (AUC = 0.773). UDysRS and UPDRS Part III scores were predicted with r = 0.741 and 0.530, respectively. Conclusion: This paper presents the first application of deep learning for vision-based assessment of parkinsonism and LID and demonstrates promising performance for the future translation of deep learning to PD clinical practices. Significance: The proposed system provides insight into the potential of computer vision and deep learning for clinical application in PD.
ER  -


TY  - Preprint
T1  - Optimized Broadcast for Deep Learning Workloads on Dense-GPU InfiniBand Clusters: MPI or NCCL?
A1  - Ammar Ahmad Awan
A1  - Ching-Hsiang Chu
A1  - Hari Subramoni
A1  - Dhabaleswar K. Panda
JO  - ArXiv e-prints
Y1  - 28 July, 2017
UR  - https://arxiv.org/abs/1707.09414
N2  - Dense Multi-GPU systems have recently gained a lot of attention in the HPC arena. Traditionally, MPI runtimes have been primarily designed for clusters with a large number of nodes. However, with the advent of MPI+CUDA applications and CUDA-Aware MPI runtimes like MVAPICH2 and OpenMPI, it has become important to address efficient communication schemes for such dense Multi-GPU nodes. This coupled with new application workloads brought forward by Deep Learning frameworks like Caffe and Microsoft CNTK pose additional design constraints due to very large message communication of GPU buffers during the training phase. In this context, special-purpose libraries like NVIDIA NCCL have been proposed for GPU-based collective communication on dense GPU systems. In this paper, we propose a pipelined chain (ring) design for the MPI_Bcast collective operation along with an enhanced collective tuning framework in MVAPICH2-GDR that enables efficient intra-/inter-node multi-GPU communication. We present an in-depth performance landscape for the proposed MPI_Bcast schemes along with a comparative analysis of NVIDIA NCCL Broadcast and NCCL-based MPI_Bcast. The proposed designs for MVAPICH2-GDR enable up to 14X and 16.6X improvement, compared to NCCL-based solutions, for intra- and inter-node broadcast latency, respectively. In addition, the proposed designs provide up to 7% improvement over NCCL-based solutions for data parallel training of the VGG network on 128 GPUs using Microsoft CNTK.
ER  -


TY  - Preprint
T1  - Deep Co-Space: Sample Mining Across Feature Transformation for Semi-Supervised Learning
A1  - Ziliang Chen
A1  - Keze Wang
A1  - Xiao Wang
A1  - Pai Peng
A1  - Ebroul Izquierdo
A1  - Liang Lin
JO  - ArXiv e-prints
Y1  - 28 July, 2017
UR  - https://arxiv.org/abs/1707.09119
N2  - Aiming at improving performance of visual classification in a cost-effective manner, this paper proposes an incremental semi-supervised learning paradigm called Deep Co-Space (DCS). Unlike many conventional semi-supervised learning methods usually performing within a fixed feature space, our DCS gradually propagates information from labeled samples to unlabeled ones along with deep feature learning. We regard deep feature learning as a series of steps pursuing feature transformation, i.e., projecting the samples from a previous space into a new one, which tends to select the reliable unlabeled samples with respect to this setting. Specifically, for each unlabeled image instance, we measure its reliability by calculating the category variations of feature transformation from two different neighborhood variation perspectives, and merged them into an unified sample mining criterion deriving from Hellinger distance. Then, those samples keeping stable correlation to their neighboring samples (i.e., having small category variation in distribution) across the successive feature space transformation, are automatically received labels and incorporated into the model for incrementally training in terms of classification. Our extensive experiments on standard image classification benchmarks (e.g., Caltech-256 and SUN-397) demonstrate that the proposed framework is capable of effectively mining from large-scale unlabeled images, which boosts image classification performance and achieves promising results compared to other semi-supervised learning methods.
ER  -


TY  - Preprint
T1  - Tartan: Accelerating Fully-Connected and Convolutional Layers in Deep Learning Networks by Exploiting Numerical Precision Variability
A1  - Alberto Delmas
A1  - Sayeh Sharify
A1  - Patrick Judd
A1  - Andreas Moshovos
JO  - ArXiv e-prints
Y1  - 27 July, 2017
UR  - https://arxiv.org/abs/1707.09068
N2  - Tartan (TRT), a hardware accelerator for inference with Deep Neural Networks (DNNs), is presented and evaluated on Convolutional Neural Networks. TRT exploits the variable per layer precision requirements of DNNs to deliver execution time that is proportional to the precision p in bits used per layer for convolutional and fully-connected layers. Prior art has demonstrated an accelerator with the same execution performance only for convolutional layers. Experiments on image classification CNNs show that on average across all networks studied, TRT outperforms a state-of-the-art bit-parallel accelerator by 1:90x without any loss in accuracy while it is 1:17x more energy efficient. TRT requires no network retraining while it enables trading off accuracy for additional improvements in execution performance and energy efficiency. For example, if a 1% relative loss in accuracy is acceptable, TRT is on average 2:04x faster and 1:25x more energy efficient than a conventional bit-parallel accelerator. A Tartan configuration that processes 2-bits at time, requires less area than the 1-bit configuration, improves efficiency to 1:24x over the bit-parallel baseline while being 73% faster for convolutional layers and 60% faster for fully-connected layers is also presented.
ER  -


TY  - Preprint
T1  - Robust Physical-World Attacks on Deep Learning Models
A1  - Kevin Eykholt
A1  - Ivan Evtimov
A1  - Earlence Fernandes
A1  - Bo Li
A1  - Amir Rahmati
A1  - Chaowei Xiao
A1  - Atul Prakash
A1  - Tadayoshi Kohno
A1  - Dawn Song
JO  - ArXiv e-prints
Y1  - 10 April, 2018
UR  - https://arxiv.org/abs/1707.08945
N2  - Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations.Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm,Robust Physical Perturbations (RP2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. Witha perturbation in the form of only black and white stickers,we attack a real stop sign, causing targeted misclassification in 100% of the images obtained in lab settings, and in 84.8%of the captured video frames obtained on a moving vehicle(field test) for the target classifier.
ER  -


TY  - Preprint
T1  - Distributed Deep Learning Models for Wireless Signal Classification with Low-Cost Spectrum Sensors
A1  - Sreeraj Rajendran
A1  - Wannes Meert
A1  - Domenico Giustiniano
A1  - Vincent Lenders
A1  - Sofie Pollin
JO  - ArXiv e-prints
Y1  - 11 July, 2018
UR  - https://arxiv.org/abs/1707.08908
N2  - This paper looks into the technology classification problem for a distributed wireless spectrum sensing network. First, a new data-driven model for Automatic Modulation Classification (AMC) based on long short term memory (LSTM) is proposed. The model learns from the time domain amplitude and phase information of the modulation schemes present in the training data without requiring expert features like higher order cyclic moments. Analyses show that the proposed model yields an average classification accuracy of close to 90% at varying SNR conditions ranging from 0dB to 20dB. Further, we explore the utility of this LSTM model for a variable symbol rate scenario. We show that a LSTM based model can learn good representations of variable length time domain sequences, which is useful in classifying modulation signals with different symbol rates. The achieved accuracy of 75% on an input sample length of 64 for which it was not trained, substantiates the representation power of the model. To reduce the data communication overhead from distributed sensors, the feasibility of classification using averaged magnitude spectrum data, or online classification on the low cost sensors is studied. Furthermore, quantized realizations of the proposed models are analyzed for deployment on sensors with low processing power.
ER  -


TY  - Preprint
T1  - Deep Residual Learning for Weakly-Supervised Relation Extraction
A1  - Yi Yao Huang
A1  - William Yang Wang
JO  - ArXiv e-prints
Y1  - 27 July, 2017
UR  - https://arxiv.org/abs/1707.08866
N2  - Deep residual learning (ResNet) is a new method for training very deep neural networks using identity map-ping for shortcut connections. ResNet has won the ImageNet ILSVRC 2015 classification task, and achieved state-of-the-art performances in many computer vision tasks. However, the effect of residual learning on noisy natural language processing tasks is still not well understood. In this paper, we design a novel convolutional neural network (CNN) with residual learning, and investigate its impacts on the task of distantly supervised noisy relation extraction. In contradictory to popular beliefs that ResNet only works well for very deep networks, we found that even with 9 layers of CNNs, using identity mapping could significantly improve the performance for distantly-supervised relation extraction.
ER  -


TY  - Preprint
T1  - Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards
A1  - Mel Vecerik
A1  - Todd Hester
A1  - Jonathan Scholz
A1  - Fumin Wang
A1  - Olivier Pietquin
A1  - Bilal Piot
A1  - Nicolas Heess
A1  - Thomas RothÃ¶rl
A1  - Thomas Lampe
A1  - Martin Riedmiller
JO  - ArXiv e-prints
Y1  - 8 October, 2018
UR  - https://arxiv.org/abs/1707.08817
N2  - We propose a general and model-free approach for Reinforcement Learning (RL) on real robotics with sparse rewards. We build upon the Deep Deterministic Policy Gradient (DDPG) algorithm to use demonstrations. Both demonstrations and actual interactions are used to fill a replay buffer and the sampling ratio between demonstrations and transitions is automatically tuned via a prioritized replay mechanism. Typically, carefully engineered shaping rewards are required to enable the agents to efficiently explore on high dimensional control problems such as robotics. They are also required for model-based acceleration methods relying on local solvers such as iLQG (e.g. Guided Policy Search and Normalized Advantage Function). The demonstrations replace the need for carefully engineered rewards, and reduce the exploration problem encountered by classical RL approaches in these domains. Demonstrations are collected by a robot kinesthetically force-controlled by a human demonstrator. Results on four simulated insertion tasks show that DDPG from demonstrations out-performs DDPG, and does not require engineered rewards. Finally, we demonstrate the method on a real robotics task consisting of inserting a clip (flexible object) into a rigid object.
ER  -


TY  - Preprint
T1  - A Jointly Learned Deep Architecture for Facial Attribute Analysis and Face Detection in the Wild
A1  - Keke He
A1  - Yanwei Fu
A1  - Xiangyang Xue
JO  - ArXiv e-prints
Y1  - 27 July, 2017
UR  - https://arxiv.org/abs/1707.08705
N2  - Facial attribute analysis in the real world scenario is very challenging mainly because of complex face variations. Existing works of analyzing face attributes are mostly based on the cropped and aligned face images. However, this result in the capability of attribute prediction heavily relies on the preprocessing of face detector. To address this problem, we present a novel jointly learned deep architecture for both facial attribute analysis and face detection. Our framework can process the natural images in the wild and our experiments on CelebA and LFWA datasets clearly show that the state-of-the-art performance is obtained.
ER  -


TY  - Preprint
T1  - Direct Load Control of Thermostatically Controlled Loads Based on Sparse Observations Using Deep Reinforcement Learning
A1  - Frederik Ruelens
A1  - Bert J. Claessens
A1  - Peter Vrancx
A1  - Fred Spiessens
A1  - Geert Deconinck
JO  - ArXiv e-prints
Y1  - 26 July, 2017
UR  - https://arxiv.org/abs/1707.08553
N2  - This paper considers a demand response agent that must find a near-optimal sequence of decisions based on sparse observations of its environment. Extracting a relevant set of features from these observations is a challenging task and may require substantial domain knowledge. One way to tackle this problem is to store sequences of past observations and actions in the state vector, making it high dimensional, and apply techniques from deep learning. This paper investigates the capabilities of different deep learning techniques, such as convolutional neural networks and recurrent neural networks, to extract relevant features for finding near-optimal policies for a residential heating system and electric water heater that are hindered by sparse observations. Our simulation results indicate that in this specific scenario, feeding sequences of time-series to an LSTM network, which is a specific type of recurrent neural network, achieved a higher performance than stacking these time-series in the input of a convolutional neural network or deep neural network.
ER  -


TY  - Preprint
T1  - TensorLayer: A Versatile Library for Efficient Deep Learning Development
A1  - Hao Dong
A1  - Akara Supratak
A1  - Luo Mai
A1  - Fangde Liu
A1  - Axel Oehmichen
A1  - Simiao Yu
A1  - Yike Guo
JO  - ArXiv e-prints
Y1  - 3 August, 2017
UR  - https://arxiv.org/abs/1707.08551
N2  - Deep learning has enabled major advances in the fields of computer vision, natural language processing, and multimedia among many others. Developing a deep learning system is arduous and complex, as it involves constructing neural network architectures, managing training/trained models, tuning optimization process, preprocessing and organizing data, etc. TensorLayer is a versatile Python library that aims at helping researchers and engineers efficiently develop deep learning systems. It offers rich abstractions for neural networks, model and data management, and parallel workflow mechanism. While boosting efficiency, TensorLayer maintains both performance and scalability. TensorLayer was released in September 2016 on GitHub, and has helped people from academia and industry develop real-world applications of deep learning.
ER  -


TY  - Preprint
T1  - Detecting and classifying lesions in mammograms with Deep Learning
A1  - DezsÅ Ribli
A1  - Anna HorvÃ¡th
A1  - Zsuzsa Unger
A1  - PÃ©ter Pollner
A1  - IstvÃ¡n Csabai
JO  - ArXiv e-prints
Y1  - 9 November, 2017
UR  - https://arxiv.org/abs/1707.08401
N2  - In the last two decades Computer Aided Diagnostics (CAD) systems were developed to help radiologists analyze screening mammograms. The benefits of current CAD technologies appear to be contradictory and they should be improved to be ultimately considered useful. Since 2012 deep convolutional neural networks (CNN) have been a tremendous success in image recognition, reaching human performance. These methods have greatly surpassed the traditional approaches, which are similar to currently used CAD solutions. Deep CNN-s have the potential to revolutionize medical image analysis. We propose a CAD system based on one of the most successful object detection frameworks, Faster R-CNN. The system detects and classifies malignant or benign lesions on a mammogram without any human intervention. The proposed method sets the state of the art classification performance on the public INbreast database, AUC = 0.95 . The approach described here has achieved the 2nd place in the Digital Mammography DREAM Challenge with AUC = 0.85 . When used as a detector, the system reaches high sensitivity with very few false positive marks per image on the INbreast dataset. Source code, the trained model and an OsiriX plugin are availaible online at https://github.com/riblidezso/frcnn_cad .
ER  -


TY  - Preprint
T1  - Reduction of Overfitting in Diabetes Prediction Using Deep Learning Neural Network
A1  - Akm Ashiquzzaman
A1  - Abdul Kawsar Tushar
A1  - Md. Rashedul Islam
A1  - Jong-Myon Kim
JO  - ArXiv e-prints
Y1  - 26 July, 2017
UR  - https://arxiv.org/abs/1707.08386
N2  - Augmented accuracy in prediction of diabetes will open up new frontiers in health prognostics. Data overfitting is a performance-degrading issue in diabetes prognosis. In this study, a prediction system for the disease of diabetes is pre-sented where the issue of overfitting is minimized by using the dropout method. Deep learning neural network is used where both fully connected layers are fol-lowed by dropout layers. The output performance of the proposed neural network is shown to have outperformed other state-of-art methods and it is recorded as by far the best performance for the Pima Indians Diabetes Data Set.
ER  -


TY  - Preprint
T1  - Dragon: A Computation Graph Virtual Machine Based Deep Learning Framework
A1  - Ting Pan
JO  - ArXiv e-prints
Y1  - 25 July, 2017
UR  - https://arxiv.org/abs/1707.08265
N2  - Deep Learning has made a great progress for these years. However, it is still difficult to master the implement of various models because different researchers may release their code based on different frameworks or interfaces. In this paper, we proposed a computation graph based framework which only aims to introduce well-known interfaces. It will help a lot when reproducing a newly model or transplanting models that were implemented by other frameworks. Additionally, we implement numerous recent models covering both Computer Vision and Nature Language Processing. We demonstrate that our framework will not suffer from model-starving because it is much easier to make full use of the works that are already done.
ER  -


TY  - Preprint
T1  - SLEEPNET: Automated Sleep Staging System via Deep Learning
A1  - Siddharth Biswal
A1  - Joshua Kulas
A1  - Haoqi Sun
A1  - Balaji Goparaju
A1  - M Brandon Westover
A1  - Matt T Bianchi
A1  - Jimeng Sun
JO  - ArXiv e-prints
Y1  - 25 July, 2017
UR  - https://arxiv.org/abs/1707.08262
N2  - Sleep disorders, such as sleep apnea, parasomnias, and hypersomnia, affect 50-70 million adults in the United States (Hillman et al., 2006). Overnight polysomnography (PSG), including brain monitoring using electroencephalography (EEG), is a central component of the diagnostic evaluation for sleep disorders. While PSG is conventionally performed by trained technologists, the recent rise of powerful neural network learning algorithms combined with large physiological datasets offers the possibility of automation, potentially making expert-level sleep analysis more widely available. We propose SLEEPNET (Sleep EEG neural network), a deployed annotation tool for sleep staging. SLEEPNET uses a deep recurrent neural network trained on the largest sleep physiology database assembled to date, consisting of PSGs from over 10,000 patients from the Massachusetts General Hospital (MGH) Sleep Laboratory. SLEEPNET achieves human-level annotation performance on an independent test set of 1,000 EEGs, with an average accuracy of 85.76% and algorithm-expert inter-rater agreement (IRA) of kappa = 79.46%, comparable to expert-expert IRA.
ER  -


TY  - Preprint
T1  - Deep Forecast: Deep Learning-based Spatio-Temporal Forecasting
A1  - Amir Ghaderi
A1  - Borhan M. Sanandaji
A1  - Faezeh Ghaderi
JO  - ArXiv e-prints
Y1  - 24 July, 2017
UR  - https://arxiv.org/abs/1707.08110
N2  - The paper presents a spatio-temporal wind speed forecasting algorithm using Deep Learning (DL)and in particular, Recurrent Neural Networks(RNNs). Motivated by recent advances in renewable energy integration and smart grids, we apply our proposed algorithm for wind speed forecasting. Renewable energy resources (wind and solar)are random in nature and, thus, their integration is facilitated with accurate short-term forecasts. In our proposed framework, we model the spatiotemporal information by a graph whose nodes are data generating entities and its edges basically model how these nodes are interacting with each other. One of the main contributions of our work is the fact that we obtain forecasts of all nodes of the graph at the same time based on one framework. Results of a case study on recorded time series data from a collection of wind mills in the north-east of the U.S. show that the proposed DL-based forecasting algorithm significantly improves the short-term forecasts compared to a set of widely-used benchmarks models.
ER  -


TY  - Preprint
T1  - Learning Bag-of-Features Pooling for Deep Convolutional Neural Networks
A1  - Nikolaos Passalis
A1  - Anastasios Tefas
JO  - ArXiv e-prints
Y1  - 26 July, 2017
UR  - https://arxiv.org/abs/1707.08105
N2  - Convolutional Neural Networks (CNNs) are well established models capable of achieving state-of-the-art classification accuracy for various computer vision tasks. However, they are becoming increasingly larger, using millions of parameters, while they are restricted to handling images of fixed size. In this paper, a quantization-based approach, inspired from the well-known Bag-of-Features model, is proposed to overcome these limitations. The proposed approach, called Convolutional BoF (CBoF), uses RBF neurons to quantize the information extracted from the convolutional layers and it is able to natively classify images of various sizes as well as to significantly reduce the number of parameters in the network. In contrast to other global pooling operators and CNN compression techniques the proposed method utilizes a trainable pooling layer that it is end-to-end differentiable, allowing the network to be trained using regular back-propagation and to achieve greater distribution shift invariance than competitive methods. The ability of the proposed method to reduce the parameters of the network and increase the classification accuracy over other state-of-the-art techniques is demonstrated using three image datasets.
ER  -


TY  - Preprint
T1  - Deep Learning Based MIMO Communications
A1  - Timothy J. O&#39;Shea
A1  - Tugba Erpek
A1  - T. Charles Clancy
JO  - ArXiv e-prints
Y1  - 25 July, 2017
UR  - https://arxiv.org/abs/1707.07980
N2  - We introduce a novel physical layer scheme for single user Multiple-Input Multiple-Output (MIMO) communications based on unsupervised deep learning using an autoencoder. This method extends prior work on the joint optimization of physical layer representation and encoding and decoding processes as a single end-to-end task by expanding transmitter and receivers to the multi-antenna case. We introduce a widely used domain appropriate wireless channel impairment model (Rayleigh fading channel), into the autoencoder optimization problem in order to directly learn a system which optimizes for it. We considered both spatial diversity and spatial multiplexing techniques in our implementation. Our deep learning-based approach demonstrates significant potential for learning schemes which approach and exceed the performance of the methods which are widely used in existing wireless MIMO systems. We discuss how the proposed scheme can be easily adapted for open-loop and closed-loop operation in spatial diversity and multiplexing modes and extended use with only compact binary channel state information (CSI) as feedback.
ER  -


TY  - Preprint
T1  - Functional connectivity patterns of autism spectrum disorder identified by deep feature learning
A1  - Hongyoon Choi
JO  - ArXiv e-prints
Y1  - 25 July, 2017
UR  - https://arxiv.org/abs/1707.07932
N2  - Autism spectrum disorder (ASD) is regarded as a brain disease with globally disrupted neuronal networks. Even though fMRI studies have revealed abnormal functional connectivity in ASD, they have not reached a consensus of the disrupted patterns. Here, a deep learning-based feature extraction method identifies multivariate and nonlinear functional connectivity patterns of ASD. Resting-state fMRI data of 972 subjects (465 ASD 507 normal controls) acquired from the Autism Brain Imaging Data Exchange were used. A functional connectivity matrix of each subject was generated using 90 predefined brain regions. As a data-driven feature extraction method without prior knowledge such as subjects diagnosis, variational autoencoder (VAE) summarized the functional connectivity matrix into 2 features. Those feature values of ASD patients were statistically compared with those of controls. A feature was significantly different between ASD and normal controls. The extracted features were visualized by VAE-based generator which can produce virtual functional connectivity matrices. The ASD-related feature was associated with frontoparietal connections, interconnections of the dorsal medial frontal cortex and corticostriatal connections. It also showed a trend of negative correlation with full-scale IQ. A data-driven feature extraction based on deep learning could identify complex patterns of functional connectivity of ASD. This approach will help discover complex patterns of abnormalities in brain connectivity in various brain disorders.
ER  -


TY  - Preprint
T1  - Deep Feature Learning via Structured Graph Laplacian Embedding for Person Re-Identification
A1  - De Cheng
A1  - Yihong Gong
A1  - Zhihui Li
A1  - Weiwei Shi
A1  - Alexander G. Hauptmann
A1  - Nanning Zheng
JO  - ArXiv e-prints
Y1  - 24 July, 2017
UR  - https://arxiv.org/abs/1707.07791
N2  - Learning the distance metric between pairs of examples is of great importance for visual recognition, especially for person re-identification (Re-Id). Recently, the contrastive and triplet loss are proposed to enhance the discriminative power of the deeply learned features, and have achieved remarkable success. As can be seen, either the contrastive or triplet loss is just one special case of the Euclidean distance relationships among these training samples. Therefore, we propose a structured graph Laplacian embedding algorithm, which can formulate all these structured distance relationships into the graph Laplacian form. The proposed method can take full advantages of the structured distance relationships among these training samples, with the constructed complete graph. Besides, this formulation makes our method easy-to-implement and super-effective. When embedding the proposed algorithm with the softmax loss for the CNN training, our method can obtain much more robust and discriminative deep features with inter-personal dispersion and intra-personal compactness, which is essential to person Re-Id. We illustrate the effectiveness of our proposed method on top of three popular networks, namely AlexNet, DGDNet and ResNet50, on recent four widely used Re-Id benchmark datasets. Our proposed method achieves state-of-the-art performances.
ER  -


TY  - Preprint
T1  - A Deep Learning Approach to Digitally Stain Optical Coherence Tomography Images of the Optic Nerve Head
A1  - Sripad Krishna Devalla
A1  - Jean-Martial Mari
A1  - Tin A. Tun
A1  - Nicholas G. Strouthidis
A1  - Tin Aung
A1  - Alexandre H. Thiery
A1  - Michael J. A. Girard
JO  - ArXiv e-prints
Y1  - 24 July, 2017
UR  - https://arxiv.org/abs/1707.07609
N2  - Purpose: To develop a deep learning approach to digitally-stain optical coherence tomography (OCT) images of the optic nerve head (ONH).
ER  -


TY  - Preprint
T1  - Deep Learning based Recommender System: A Survey and New Perspectives
A1  - Shuai Zhang
A1  - Lina Yao
A1  - Aixin Sun
A1  - Yi Tay
JO  - ArXiv e-prints
Y1  - 4 September, 2018
UR  - https://arxiv.org/abs/1707.07435
N2  - With the ever-growing volume of online information, recommender systems have been an effective strategy to overcome such information overload. The utility of recommender systems cannot be overstated, given its widespread adoption in many web applications, along with its potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. Evidently, the field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning based recommender systems. More concretely, we provide and devise a taxonomy of deep learning based recommendation models, along with providing a comprehensive summary of the state-of-the-art. Finally, we expand on current trends and provide new perspectives pertaining to this new exciting development of the field.
ER  -


TY  - Preprint
T1  - Deep Optical Flow Estimation Via Multi-Scale Correspondence Structure Learning
A1  - Shanshan Zhao
A1  - Xi Li
A1  - Omar El Farouk Bourahla
JO  - ArXiv e-prints
Y1  - 23 July, 2017
UR  - https://arxiv.org/abs/1707.07301
N2  - As an important and challenging problem in computer vision, learning based optical flow estimation aims to discover the intrinsic correspondence structure between two adjacent video frames through statistical learning. Therefore, a key issue to solve in this area is how to effectively model the multi-scale correspondence structure properties in an adaptive end-to-end learning fashion. Motivated by this observation, we propose an end-to-end multi-scale correspondence structure learning (MSCSL) approach for optical flow estimation. In principle, the proposed MSCSL approach is capable of effectively capturing the multi-scale inter-image-correlation correspondence structures within a multi-level feature space from deep learning. Moreover, the proposed MSCSL approach builds a spatial Conv-GRU neural network model to adaptively model the intrinsic dependency relationships among these multi-scale correspondence structures. Finally, the above procedures for correspondence structure learning and multi-scale dependency modeling are implemented in a unified end-to-end deep learning framework. Experimental results on several benchmark datasets demonstrate the effectiveness of the proposed approach.
ER  -


TY  - Preprint
T1  - Deep Learning in Robotics: A Review of Recent Research
A1  - Harry A. Pierson
A1  - Michael S. Gashler
JO  - ArXiv e-prints
Y1  - 22 July, 2017
UR  - https://arxiv.org/abs/1707.07217
N2  - Advances in deep learning over the last decade have led to a flurry of research in the application of deep artificial neural networks to robotic systems, with at least thirty papers published on the subject between 2014 and the present. This review discusses the applications, benefits, and limitations of deep learning vis-Ã -vis physical robotic systems, using contemporary research as exemplars. It is intended to communicate recent advances to the wider robotics community and inspire additional interest in and application of deep learning in robotics.
ER  -


TY  - Preprint
T1  - Multi-kernel learning of deep convolutional features for action recognition
A1  - Biswa Sengupta
A1  - Yu Qian
JO  - ArXiv e-prints
Y1  - 12 November, 2017
UR  - https://arxiv.org/abs/1707.06923
N2  - Image understanding using deep convolutional network has reached human-level performance, yet a closely related problem of video understanding especially, action recognition has not reached the requisite level of maturity. We combine multi-kernels based support-vector-machines (SVM) with a multi-stream deep convolutional neural network to achieve close to state-of-the-art performance on a 51-class activity recognition problem (HMDB-51 dataset); this specific dataset has proved to be particularly challenging for deep neural networks due to the heterogeneity in camera viewpoints, video quality, etc. The resulting architecture is named pillar networks as each (very) deep neural network acts as a pillar for the hierarchical classifiers. In addition, we illustrate that hand-crafted features such as improved dense trajectories (iDT) and Multi-skip Feature Stacking (MIFS), as additional pillars, can further supplement the performance.
ER  -


TY  - Preprint
T1  - Shallow reading with Deep Learning: Predicting popularity of online content using only its title
A1  - Wociech Stokowiec
A1  - Tomasz Trzcinski
A1  - Krzysztof Wolk
A1  - Krzysztof Marasek
A1  - Przemyslaw Rokita
JO  - ArXiv e-prints
Y1  - 21 July, 2017
UR  - https://arxiv.org/abs/1707.06806
N2  - With the ever decreasing attention span of contemporary Internet users, the title of online content (such as a news article or video) can be a major factor in determining its popularity. To take advantage of this phenomenon, we propose a new method based on a bidirectional Long Short-Term Memory (LSTM) neural network designed to predict the popularity of online content using only its title. We evaluate the proposed architecture on two distinct datasets of news articles and news videos distributed in social media that contain over 40,000 samples in total. On those datasets, our approach improves the performance over traditional shallow approaches by a margin of 15%. Additionally, we show that using pre-trained word vectors in the embedding layer improves the results of LSTM models, especially when the training set is small. To our knowledge, this is the first attempt of applying popularity prediction using only textual information from the title.
ER  -


TY  - Preprint
T1  - 3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-scale 3D Point Clouds
A1  - Fangyu Liu
A1  - Shuaipeng Li
A1  - Liqiang Zhang
A1  - Chenghu Zhou
A1  - Rongtian Ye
A1  - Yuebin Wang
A1  - Jiwen Lu
JO  - ArXiv e-prints
Y1  - 21 July, 2017
UR  - https://arxiv.org/abs/1707.06783
N2  - Semantic parsing of large-scale 3D point clouds is an important research topic in computer vision and remote sensing fields. Most existing approaches utilize hand-crafted features for each modality independently and combine them in a heuristic manner. They often fail to consider the consistency and complementary information among features adequately, which makes them difficult to capture high-level semantic structures. The features learned by most of the current deep learning methods can obtain high-quality image classification results. However, these methods are hard to be applied to recognize 3D point clouds due to unorganized distribution and various point density of data. In this paper, we propose a 3DCNN-DQN-RNN method which fuses the 3D convolutional neural network (CNN), Deep Q-Network (DQN) and Residual recurrent neural network (RNN) for an efficient semantic parsing of large-scale 3D point clouds. In our method, an eye window under control of the 3D CNN and DQN can localize and segment the points of the object class efficiently. The 3D CNN and Residual RNN further extract robust and discriminative features of the points in the eye window, and thus greatly enhance the parsing accuracy of large-scale point clouds. Our method provides an automatic process that maps the raw data to the classification results. It also integrates object localization, segmentation and classification into one framework. Experimental results demonstrate that the proposed method outperforms the state-of-the-art point cloud classification methods.
ER  -


TY  - Preprint
T1  - Adaptive Learning Rule for Hardware-based Deep Neural Networks Using Electronic Synapse Devices
A1  - Suhwan Lim
A1  - Jong-Ho Bae
A1  - Jai-Ho Eum
A1  - Sungtae Lee
A1  - Chul-Heung Kim
A1  - Dongseok Kwon
A1  - Byung-Gook Park
A1  - Jong-Ho Lee
JO  - ArXiv e-prints
Y1  - 19 August, 2017
UR  - https://arxiv.org/abs/1707.06381
N2  - In this paper, we propose a learning rule based on a back-propagation (BP) algorithm that can be applied to a hardware-based deep neural network (HW-DNN) using electronic devices that exhibit discrete and limited conductance characteristics. This adaptive learning rule, which enables forward, backward propagation, as well as weight updates in hardware, is helpful during the implementation of power-efficient and high-speed deep neural networks. In simulations using a three-layer perceptron network, we evaluate the learning performance according to various conductance responses of electronic synapse devices and weight-updating methods. It is shown that the learning accuracy is comparable to that obtained when using a software-based BP algorithm when the electronic synapse device has a linear conductance response with a high dynamic range. Furthermore, the proposed unidirectional weight-updating method is suitable for electronic synapse devices which have nonlinear and finite conductance responses. Because this weight-updating method can compensate the demerit of asymmetric weight updates, we can obtain better accuracy compared to other methods. This adaptive learning rule, which can be applied to full hardware implementation, can also compensate the degradation of learning accuracy due to the probable device-to-device variation in an actual electronic synapse device.
ER  -


TY  - Preprint
T1  - Imagination-Augmented Agents for Deep Reinforcement Learning
A1  - ThÃ©ophane Weber
A1  - SÃ©bastien RacaniÃ¨re
A1  - David P. Reichert
A1  - Lars Buesing
A1  - Arthur Guez
A1  - Danilo Jimenez Rezende
A1  - Adria PuigdomÃ¨nech Badia
A1  - Oriol Vinyals
A1  - Nicolas Heess
A1  - Yujia Li
A1  - Razvan Pascanu
A1  - Peter Battaglia
A1  - Demis Hassabis
A1  - David Silver
A1  - Daan Wierstra
JO  - ArXiv e-prints
Y1  - 14 February, 2018
UR  - https://arxiv.org/abs/1707.06203
N2  - We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.
ER  -


TY  - Preprint
T1  - Orthogonal and Idempotent Transformations for Learning Deep Neural Networks
A1  - Jingdong Wang
A1  - Yajie Xing
A1  - Kexin Zhang
A1  - Cha Zhang
JO  - ArXiv e-prints
Y1  - 19 July, 2017
UR  - https://arxiv.org/abs/1707.05974
N2  - Identity transformations, used as skip-connections in residual networks, directly connect convolutional layers close to the input and those close to the output in deep neural networks, improving information flow and thus easing the training. In this paper, we introduce two alternative linear transforms, orthogonal transformation and idempotent transformation. According to the definition and property of orthogonal and idempotent matrices, the product of multiple orthogonal (same idempotent) matrices, used to form linear transformations, is equal to a single orthogonal (idempotent) matrix, resulting in that information flow is improved and the training is eased. One interesting point is that the success essentially stems from feature reuse and gradient reuse in forward and backward propagation for maintaining the information during flow and eliminating the gradient vanishing problem because of the express way through skip-connections. We empirically demonstrate the effectiveness of the proposed two transformations: similar performance in single-branch networks and even superior in multi-branch networks in comparison to identity transformations.
ER  -


TY  - Preprint
T1  - Deep Active Learning for Named Entity Recognition
A1  - Yanyao Shen
A1  - Hyokun Yun
A1  - Zachary C. Lipton
A1  - Yakov Kronrod
A1  - Animashree Anandkumar
JO  - ArXiv e-prints
Y1  - 3 February, 2018
UR  - https://arxiv.org/abs/1707.05928
N2  - Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\% of the original training data.
ER  -


TY  - Preprint
T1  - On-line Building Energy Optimization using Deep Reinforcement Learning
A1  - Elena Mocanu
A1  - Decebal Constantin Mocanu
A1  - Phuong H. Nguyen
A1  - Antonio Liotta
A1  - Michael E. Webber
A1  - Madeleine Gibescu
A1  - J. G. Slootweg
JO  - ArXiv e-prints
Y1  - 18 July, 2017
UR  - https://arxiv.org/abs/1707.05878
N2  - Unprecedented high volumes of data are becoming available with the growth of the advanced metering infrastructure. These are expected to benefit planning and operation of the future power system, and to help the customers transition from a passive to an active role. In this paper, we explore for the first time in the smart grid context the benefits of using Deep Reinforcement Learning, a hybrid type of methods that combines Reinforcement Learning with Deep Learning, to perform on-line optimization of schedules for building energy management systems. The learning procedure was explored using two methods, Deep Q-learning and Deep Policy Gradient, both of them being extended to perform multiple actions simultaneously. The proposed approach was validated on the large-scale Pecan Street Inc. database. This highly-dimensional database includes information about photovoltaic power generation, electric vehicles as well as buildings appliances. Moreover, these on-line energy scheduling strategies could be used to provide real-time feedback to consumers to encourage more efficient use of electricity.
ER  -


TY  - Preprint
T1  - A deep learning approach to diabetic blood glucose prediction
A1  - H. N. Mhaskar
A1  - S. V. Pereverzyev
A1  - M. D. van der Walt
JO  - ArXiv e-prints
Y1  - 18 July, 2017
UR  - https://arxiv.org/abs/1707.05828
N2  - We consider the question of 30-minute prediction of blood glucose levels measured by continuous glucose monitoring devices, using clinical data. While most studies of this nature deal with one patient at a time, we take a certain percentage of patients in the data set as training data, and test on the remainder of the patients; i.e., the machine need not re-calibrate on the new patients in the data set. We demonstrate how deep learning can outperform shallow networks in this example. One novelty is to demonstrate how a parsimonious deep representation can be constructed using domain knowledge.
ER  -


TY  - Preprint
T1  - A Novel Deep Learning Architecture for Testis Histology Image Classification
A1  - Chia-Yu Kao
A1  - Leonard McMillan
JO  - ArXiv e-prints
Y1  - 18 July, 2017
UR  - https://arxiv.org/abs/1707.05809
N2  - Unlike other histology analysis, classification of tubule status in testis histology is very challenging due to their high similarity of texture and shape. Traditional deep learning networks have difficulties to capture nuance details among different tubule categories. In this paper, we propose a novel deep learning architecture for feature learning, image classification, and image reconstruction. It is based on stacked auto-encoders with an additional layer, called a hyperlayer, which is created to capture features of an image at different layers in the network. This addition effectively combines features at different scales and thus provides a more complete profile for further classification. Evaluation is performed on a set of 10,542 tubule image patches. We demonstrate our approach with two experiments on two different subsets of the dataset. The results show that the features learned from our architecture achieve more than 98% accuracy and represent an improvement over traditional deep network architectures.
ER  -


TY  - Preprint
T1  - TensorLog: Deep Learning Meets Probabilistic DBs
A1  - William W. Cohen
A1  - Fan Yang
A1  - Kathryn Rivard Mazaitis
JO  - ArXiv e-prints
Y1  - 17 July, 2017
UR  - https://arxiv.org/abs/1707.05390
N2  - We present an implementation of a probabilistic first-order logic called TensorLog, in which classes of logical queries are compiled into differentiable functions in a neural-network infrastructure such as Tensorflow or Theano. This leads to a close integration of probabilistic logical reasoning with deep-learning infrastructure: in particular, it enables high-performance deep learning frameworks to be used for tuning the parameters of a probabilistic logic. Experimental results show that TensorLog scales to problems involving hundreds of thousands of knowledge-base triples and tens of thousands of examples.
ER  -


TY  - Preprint
T1  - Unsupervised Iterative Deep Learning of Speech Features and Acoustic Tokens with Applications to Spoken Term Detection
A1  - Cheng-Tao Chung
A1  - Cheng-Yu Tsai
A1  - Chia-Hsiang Liu
A1  - Lin-Shan Lee
JO  - ArXiv e-prints
Y1  - 17 July, 2017
UR  - https://arxiv.org/abs/1707.05315
N2  - In this paper we aim to automatically discover high quality frame-level speech features and acoustic tokens directly from unlabeled speech data. A Multi-granular Acoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters describing the model configuration. These different sets of acoustic tokens carry different characteristics for the given corpus and the language behind, thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target Deep Neural Network (MDNN) trained on frame-level acoustic features. Bottleneck features extracted from the MDNN are then used as the feedback input to the MAT and the MDNN itself in the next iteration. The multi-granular acoustic token sets and the frame-level speech features can be iteratively optimized in the iterative deep learning framework. We call this framework the Multi-granular Acoustic Tokenizing Deep Neural Network (MATDNN). The results were evaluated using the metrics and corpora defined in the Zero Resource Speech Challenge organized at Interspeech 2015, and improved performance was obtained with a set of experiments of query-by-example spoken term detection on the same corpora. Visualization for the discovered tokens against the English phonemes was also shown.
ER  -


TY  - Preprint
T1  - Deep Learning to Attend to Risk in ICU
A1  - Phuoc Nguyen
A1  - Truyen Tran
A1  - Svetha Venkatesh
JO  - ArXiv e-prints
Y1  - 17 July, 2017
UR  - https://arxiv.org/abs/1707.05010
N2  - Modeling physiological time-series in ICU is of high clinical importance. However, data collected within ICU are irregular in time and often contain missing measurements. Since absence of a measure would signify its lack of importance, the missingness is indeed informative and might reflect the decision making by the clinician. Here we propose a deep learning architecture that can effectively handle these challenges for predicting ICU mortality outcomes. The model is based on Long Short-Term Memory, and has layered attention mechanisms. At the sensing layer, the model decides whether to observe and incorporate parts of the current measurements. At the reasoning layer, evidences across time steps are weighted and combined. The model is evaluated on the PhysioNet 2012 dataset showing competitive and interpretable results.
ER  -


TY  - Preprint
T1  - Improving Deep Pancreas Segmentation in CT and MRI Images via Recurrent Neural Contextual Learning and Direct Loss Function
A1  - Jinzheng Cai
A1  - Le Lu
A1  - Yuanpu Xie
A1  - Fuyong Xing
A1  - Lin Yang
JO  - ArXiv e-prints
Y1  - 17 July, 2017
UR  - https://arxiv.org/abs/1707.04912
N2  - Deep neural networks have demonstrated very promising performance on accurate segmentation of challenging organs (e.g., pancreas) in abdominal CT and MRI scans. The current deep learning approaches conduct pancreas segmentation by processing sequences of 2D image slices independently through deep, dense per-pixel masking for each image, without explicitly enforcing spatial consistency constraint on segmentation of successive slices. We propose a new convolutional/recurrent neural network architecture to address the contextual learning and segmentation consistency problem. A deep convolutional sub-network is first designed and pre-trained from scratch. The output layer of this network module is then connected to recurrent layers and can be fine-tuned for contextual learning, in an end-to-end manner. Our recurrent sub-network is a type of Long short-term memory (LSTM) network that performs segmentation on an image by integrating its neighboring slice segmentation predictions, in the form of a dependent sequence processing. Additionally, a novel segmentation-direct loss function (named Jaccard Loss) is proposed and deep networks are trained to optimize Jaccard Index (JI) directly. Extensive experiments are conducted to validate our proposed deep models, on quantitative pancreas segmentation using both CT and MRI scans. Our method outperforms the state-of-the-art work on CT [11] and MRI pancreas segmentation [1], respectively.
ER  -


TY  - Preprint
T1  - Listening while Speaking: Speech Chain by Deep Learning
A1  - Andros Tjandra
A1  - Sakriani Sakti
A1  - Satoshi Nakamura
JO  - ArXiv e-prints
Y1  - 16 July, 2017
UR  - https://arxiv.org/abs/1707.04879
N2  - Despite the close relationship between speech perception and production, research in automatic speech recognition (ASR) and text-to-speech synthesis (TTS) has progressed more or less independently without exerting much mutual influence on each other. In human communication, on the other hand, a closed-loop speech chain mechanism with auditory feedback from the speaker&#39;s mouth to her ear is crucial. In this paper, we take a step further and develop a closed-loop speech chain model based on deep learning. The sequence-to-sequence model in close-loop architecture allows us to train our model on the concatenation of both labeled and unlabeled data. While ASR transcribes the unlabeled speech features, TTS attempts to reconstruct the original speech waveform based on the text from ASR. In the opposite direction, ASR also attempts to reconstruct the original text transcription given the synthesized speech. To the best of our knowledge, this is the first deep learning model that integrates human speech perception and production behaviors. Our experimental results show that the proposed approach significantly improved the performance more than separate systems that were only trained with labeled data.
ER  -


TY  - Preprint
T1  - Sorting and Transforming Program Repair Ingredients via Deep Learning Code Similarities
A1  - Martin White
A1  - Michele Tufano
A1  - Matias Martinez
A1  - Martin Monperrus
A1  - Denys Poshyvanyk
JO  - ArXiv e-prints
Y1  - 15 July, 2017
UR  - https://arxiv.org/abs/1707.04742
N2  - In the field of automated program repair, the redundancy assumption claims large programs contain the seeds of their own repair. However, most redundancy-based program repair techniques do not reason about the repair ingredients---the code that is reused to craft a patch. We aim to reason about the repair ingredients by using code similarities to prioritize and transform statements in a codebase for patch generation. Our approach, DeepRepair, relies on deep learning to reason about code similarities. Code fragments at well-defined levels of granularity in a codebase can be sorted according to their similarity to suspicious elements (i.e., code elements that contain suspicious statements) and statements can be transformed by mapping out-of-scope identifiers to similar identifiers in scope. We examined these new search strategies for patch generation with respect to effectiveness from the viewpoint of a software maintainer. Our comparative experiments were executed on six open-source Java projects including 374 buggy program revisions and consisted of 19,949 trials spanning 2,616 days of computation time. DeepRepair&#39;s search strategy using code similarities generally found compilable ingredients faster than the baseline, jGenProg, but this improvement neither yielded test-adequate patches in fewer attempts (on average) nor found significantly more patches than the baseline. Although the patch counts were not statistically different, there were notable differences between the nature of DeepRepair patches and baseline patches. The results demonstrate that our learning-based approach finds patches that cannot be found by existing redundancy-based repair techniques.
ER  -


TY  - Preprint
T1  - Recognizing Abnormal Heart Sounds Using Deep Learning
A1  - Jonathan Rubin
A1  - Rui Abreu
A1  - Anurag Ganguli
A1  - Saigopal Nelaturi
A1  - Ion Matei
A1  - Kumar Sricharan
JO  - ArXiv e-prints
Y1  - 19 October, 2017
UR  - https://arxiv.org/abs/1707.04642
N2  - The work presented here applies deep learning to the task of automated cardiac auscultation, i.e. recognizing abnormalities in heart sounds. We describe an automated heart sound classification algorithm that combines the use of time-frequency heat map representations with a deep convolutional neural network (CNN). Given the cost-sensitive nature of misclassification, our CNN architecture is trained using a modified loss function that directly optimizes the trade-off between sensitivity and specificity. We evaluated our algorithm at the 2016 PhysioNet Computing in Cardiology challenge where the objective was to accurately classify normal and abnormal heart sounds from single, short, potentially noisy recordings. Our entry to the challenge achieved a final specificity of 0.95, sensitivity of 0.73 and overall score of 0.84. We achieved the greatest specificity score out of all challenge entries and, using just a single CNN, our algorithm differed in overall score by only 0.02 compared to the top place finisher, which used an ensemble approach.
ER  -


TY  - Preprint
T1  - Lenient Multi-Agent Deep Reinforcement Learning
A1  - Gregory Palmer
A1  - Karl Tuyls
A1  - Daan Bloembergen
A1  - Rahul Savani
JO  - ArXiv e-prints
Y1  - 27 February, 2018
UR  - https://arxiv.org/abs/1707.04402
N2  - Much of the success of single agent deep reinforcement learning (DRL) in recent years can be attributed to the use of experience replay memories (ERM), which allow Deep Q-Networks (DQNs) to be trained efficiently through sampling stored state transitions. However, care is required when using ERMs for multi-agent deep reinforcement learning (MA-DRL), as stored transitions can become outdated because agents update their policies in parallel [11]. In this work we apply leniency [23] to MA-DRL. Lenient agents map state-action pairs to decaying temperature values that control the amount of leniency applied towards negative policy updates that are sampled from the ERM. This introduces optimism in the value-function update, and has been shown to facilitate cooperation in tabular fully-cooperative multi-agent reinforcement learning problems. We evaluate our Lenient-DQN (LDQN) empirically against the related Hysteretic-DQN (HDQN) algorithm [22] as well as a modified version we call scheduled-HDQN, that uses average reward learning near terminal states. Evaluations take place in extended variations of the Coordinated Multi-Agent Object Transportation Problem (CMOTP) [8] which include fully-cooperative sub-tasks and stochastic rewards. We find that LDQN agents are more likely to converge to the optimal policy in a stochastic reward CMOTP compared to standard and scheduled-HDQN agents.
ER  -


TY  - Preprint
T1  - Deep Learning with Topological Signatures
A1  - Christoph Hofer
A1  - Roland Kwitt
A1  - Marc Niethammer
A1  - Andreas Uhl
JO  - ArXiv e-prints
Y1  - 16 February, 2018
UR  - https://arxiv.org/abs/1707.04041
N2  - Inferring topological and geometrical information from data can offer an alternative perspective on machine learning problems. Methods from topological data analysis, e.g., persistent homology, enable us to obtain such information, typically in the form of summary representations of topological features. However, such topological signatures often come with an unusual structure (e.g., multisets of intervals) that is highly impractical for most machine learning techniques. While many strategies have been proposed to map these topological signatures into machine learning compatible representations, they suffer from being agnostic to the target learning task. In contrast, we propose a technique that enables us to input topological signatures to deep neural networks and learn a task-optimal representation during training. Our approach is realized as a novel input layer with favorable theoretical properties. Classification experiments on 2D object shapes and social network graphs demonstrate the versatility of the approach and, in case of the latter, we even outperform the state-of-the-art by a large margin.
ER  -


TY  - Preprint
T1  - Learning Photography Aesthetics with Deep CNNs
A1  - Gautam Malu
A1  - Raju S. Bapi
A1  - Bipin Indurkhya
JO  - ArXiv e-prints
Y1  - 13 July, 2017
UR  - https://arxiv.org/abs/1707.03981
N2  - Automatic photo aesthetic assessment is a challenging artificial intelligence task. Existing computational approaches have focused on modeling a single aesthetic score or a class (good or bad), however these do not provide any details on why the photograph is good or bad, or which attributes contribute to the quality of the photograph. To obtain both accuracy and human interpretation of the score, we advocate learning the aesthetic attributes along with the prediction of the overall score. For this purpose, we propose a novel multitask deep convolution neural network, which jointly learns eight aesthetic attributes along with the overall aesthetic score. We report near human performance in the prediction of the overall aesthetic score. To understand the internal representation of these attributes in the learned model, we also develop the visualization technique using back propagation of gradients. These visualizations highlight the important image regions for the corresponding attributes, thus providing insights about model&#39;s representation of these attributes. We showcase the diversity and complexity associated with different attributes through a qualitative analysis of the activation maps.
ER  -


TY  - Preprint
T1  - DeepProf: Performance Analysis for Deep Learning Applications via Mining GPU Execution Patterns
A1  - Jiazhen Gu
A1  - Huan Liu
A1  - Yangfan Zhou
A1  - Xin Wang
JO  - ArXiv e-prints
Y1  - 12 July, 2017
UR  - https://arxiv.org/abs/1707.03750
N2  - Deep learning applications are computation-intensive and often employ GPU as the underlying computing devices. Deep learning frameworks provide powerful programming interfaces, but the gap between source codes and practical GPU operations make it difficult to analyze the performance of deep learning applications. In this paper, through examing the features of GPU traces and deep learning applications, we use the suffix tree structure to extract the repeated patten in GPU traces. Performance analysis graphs can be generated from the preprocessed GPU traces. We further present \texttt{DeepProf}, a novel tool to automatically process GPU traces and generate performance analysis reports for deep learning applications. Empirical study verifies the effectiveness of \texttt{DeepProf} in performance analysis and diagnosis. We also find out some interesting properties of Tensorflow, which can be used to guide the deep learning system setup.
ER  -


TY  - Preprint
T1  - Learning Macromanagement in StarCraft from Replays using Deep Learning
A1  - Niels Justesen
A1  - Sebastian Risi
JO  - ArXiv e-prints
Y1  - 12 July, 2017
UR  - https://arxiv.org/abs/1707.03743
N2  - The real-time strategy game StarCraft has proven to be a challenging environment for artificial intelligence techniques, and as a result, current state-of-the-art solutions consist of numerous hand-crafted modules. In this paper, we show how macromanagement decisions in StarCraft can be learned directly from game replays using deep learning. Neural networks are trained on 789,571 state-action pairs extracted from 2,005 replays of highly skilled players, achieving top-1 and top-3 error rates of 54.6% and 22.9% in predicting the next build action. By integrating the trained network into UAlbertaBot, an open source StarCraft bot, the system can significantly outperform the game&#39;s built-in Terran bot, and play competitively against UAlbertaBot with a fixed rush strategy. To our knowledge, this is the first time macromanagement tasks are learned directly from replays in StarCraft. While the best hand-crafted strategies are still the state-of-the-art, the deep network approach is able to express a wide range of different strategies and thus improving the network&#39;s performance further with deep reinforcement learning is an immediately promising avenue for future research. Ultimately this approach could lead to strong StarCraft bots that are less reliant on hard-coded strategies.
ER  -


TY  - Preprint
T1  - Deep Fisher Discriminant Learning for Mobile Hand Gesture Recognition
A1  - Chunyu Xie
A1  - Ce Li
A1  - Baochang Zhang
A1  - Chen Chen
A1  - Jungong Han
JO  - ArXiv e-prints
Y1  - 12 July, 2017
UR  - https://arxiv.org/abs/1707.03692
N2  - Gesture recognition is a challenging problem in the field of biometrics. In this paper, we integrate Fisher criterion into Bidirectional Long-Short Term Memory (BLSTM) network and Bidirectional Gated Recurrent Unit (BGRU),thus leading to two new deep models termed as F-BLSTM and F-BGRU. BothFisher discriminative deep models can effectively classify the gesture based on analyzing the acceleration and angular velocity data of the human gestures. Moreover, we collect a large Mobile Gesture Database (MGD) based on the accelerations and angular velocities containing 5547 sequences of 12 gestures. Extensive experiments are conducted to validate the superior performance of the proposed networks as compared to the state-of-the-art BLSTM and BGRU on MGD database and two benchmark databases (i.e. BUAA mobile gesture and SmartWatch gesture).
ER  -


TY  - Preprint
T1  - A Deep Learning Approach for Blind Drift Calibration of Sensor Networks
A1  - Yuzhi Wang
A1  - Anqi Yang
A1  - Xiaoming Chen
A1  - Pengjun Wang
A1  - Yu Wang
A1  - Huazhong Yang
JO  - ArXiv e-prints
Y1  - 16 June, 2017
UR  - https://arxiv.org/abs/1707.03682
N2  - Temporal drift of sensory data is a severe problem impacting the data quality of wireless sensor networks (WSNs). With the proliferation of large-scale and long-term WSNs, it is becoming more important to calibrate sensors when the ground truth is unavailable. This problem is called &#34;blind calibration&#34;. In this paper, we propose a novel deep learning method named projection-recovery network (PRNet) to blindly calibrate sensor measurements online. The PRNet first projects the drifted data to a feature space, and uses a powerful deep convolutional neural network to recover the estimated drift-free measurements. We deploy a 24-sensor testbed and provide comprehensive empirical evidence showing that the proposed method significantly improves the sensing accuracy and drifted sensor detection. Compared with previous methods, PRNet can calibrate 2x of drifted sensors at the recovery rate of 80% under the same level of accuracy requirement. We also provide helpful insights for designing deep neural networks for sensor calibration. We hope our proposed simple and effective approach will serve as a solid baseline in blind drift calibration of sensor networks.
ER  -


TY  - Preprint
T1  - Elephant Search with Deep Learning for Microarray Data Analysis
A1  - Mrutyunjaya Panda
JO  - ArXiv e-prints
Y1  - 12 July, 2017
UR  - https://arxiv.org/abs/1707.03604
N2  - Even though there is a plethora of research in Microarray gene expression data analysis, still, it poses challenges for researchers to effectively and efficiently analyze the large yet complex expression of genes. The feature (gene) selection method is of paramount importance for understanding the differences in biological and non-biological variation between samples. In order to address this problem, a novel elephant search (ES) based optimization is proposed to select best gene expressions from the large volume of microarray data. Further, a promising machine learning method is envisioned to leverage such high dimensional and complex microarray dataset for extracting hidden patterns inside to make a meaningful prediction and most accurate classification. In particular, stochastic gradient descent based Deep learning (DL) with softmax activation function is then used on the reduced features (genes) for better classification of different samples according to their gene expression levels. The experiments are carried out on nine most popular Cancer microarray gene selection datasets, obtained from UCI machine learning repository. The empirical results obtained by the proposed elephant search based deep learning (ESDL) approach are compared with most recent published article for its suitability in future Bioinformatics research.
ER  -


TY  - Preprint
T1  - Deep Learning for Sensor-based Activity Recognition: A Survey
A1  - Jindong Wang
A1  - Yiqiang Chen
A1  - Shuji Hao
A1  - Xiaohui Peng
A1  - Lisha Hu
JO  - ArXiv e-prints
Y1  - 13 December, 2017
UR  - https://arxiv.org/abs/1707.03502
N2  - Sensor-based activity recognition seeks the profound high-level knowledge about human activities from multitudes of low-level sensor readings. Conventional pattern recognition approaches have made tremendous progress in the past years. However, those methods often heavily rely on heuristic hand-crafted feature extraction, which could hinder their generalization performance. Additionally, existing methods are undermined for unsupervised and incremental learning tasks. Recently, the recent advancement of deep learning makes it possible to perform automatic high-level feature extraction thus achieves promising performance in many areas. Since then, deep learning based methods have been widely adopted for the sensor-based activity recognition tasks. This paper surveys the recent advance of deep learning based sensor-based activity recognition. We summarize existing literature from three aspects: sensor modality, deep model, and application. We also present detailed insights on existing work and propose grand challenges for future research.
ER  -


TY  - Preprint
T1  - Creatism: A deep-learning photographer capable of creating professional work
A1  - Hui Fang
A1  - Meng Zhang
JO  - ArXiv e-prints
Y1  - 11 July, 2017
UR  - https://arxiv.org/abs/1707.03491
N2  - Machine-learning excels in many areas with well-defined goals. However, a clear goal is usually not available in art forms, such as photography. The success of a photograph is measured by its aesthetic value, a very subjective concept. This adds to the challenge for a machine learning approach.
ER  -


TY  - Preprint
T1  - Individual Recognition in Schizophrenia using Deep Learning Methods with Random Forest and Voting Classifiers: Insights from Resting State EEG Streams
A1  - Lei Chu
A1  - Robert Qiu
A1  - Haichun Liu
A1  - Zenan Ling
A1  - Tianhong Zhang
A1  - Jijun Wang
JO  - ArXiv e-prints
Y1  - 17 January, 2018
UR  - https://arxiv.org/abs/1707.03467
N2  - Recently, there has been a growing interest in monitoring brain activity for individual recognition system. So far these works are mainly focussing on single channel data or fragment data collected by some advanced brain monitoring modalities. In this study we propose new individual recognition schemes based on spatio-temporal resting state Electroencephalography (EEG) data. Besides, instead of using features derived from artificially-designed procedures, modified deep learning architectures which aim to automatically extract an individual&#39;s unique features are developed to conduct classification. Our designed deep learning frameworks are proved of a small but consistent advantage of replacing the $softmax$ layer with Random Forest. Additionally, a voting layer is added at the top of designed neural networks in order to tackle the classification problem arisen from EEG streams. Lastly, various experiments are implemented to evaluate the performance of the designed deep learning architectures; Results indicate that the proposed EEG-based individual recognition scheme yields a high degree of classification accuracy: $81.6\%$ for characteristics in high risk (CHR) individuals, $96.7\%$ for clinically stable first episode patients with schizophrenia (FES) and $99.2\%$ for healthy controls (HC).
ER  -


TY  - Preprint
T1  - Learning like humans with Deep Symbolic Networks
A1  - Qunzhi Zhang
A1  - Didier Sornette
JO  - ArXiv e-prints
Y1  - 13 July, 2017
UR  - https://arxiv.org/abs/1707.03377
N2  - We introduce the Deep Symbolic Network (DSN) model, which aims at becoming the white-box version of Deep Neural Networks (DNN). The DSN model provides a simple, universal yet powerful structure, similar to DNN, to represent any knowledge of the world, which is transparent to humans. The conjecture behind the DSN model is that any type of real world objects sharing enough common features are mapped into human brains as a symbol. Those symbols are connected by links, representing the composition, correlation, causality, or other relationships between them, forming a deep, hierarchical symbolic network structure. Powered by such a structure, the DSN model is expected to learn like humans, because of its unique characteristics. First, it is universal, using the same structure to store any knowledge. Second, it can learn symbols from the world and construct the deep symbolic networks automatically, by utilizing the fact that real world objects have been naturally separated by singularities. Third, it is symbolic, with the capacity of performing causal deduction and generalization. Fourth, the symbols and the links between them are transparent to us, and thus we will know what it has learned or not - which is the key for the security of an AI system. Fifth, its transparency enables it to learn with relatively small data. Sixth, its knowledge can be accumulated. Last but not least, it is more friendly to unsupervised learning than DNN. We present the details of the model, the algorithm powering its automatic learning ability, and describe its usefulness in different use cases. The purpose of this paper is to generate broad interest to develop it within an open source project centered on the Deep Symbolic Network (DSN) model towards the development of general AI.
ER  -


TY  - Preprint
T1  - Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations
A1  - Carole H Sudre
A1  - Wenqi Li
A1  - Tom Vercauteren
A1  - SÃ©bastien Ourselin
A1  - M. Jorge Cardoso
JO  - ArXiv e-prints
Y1  - 14 July, 2017
UR  - https://arxiv.org/abs/1707.03237
N2  - Deep-learning has proved in recent years to be a powerful tool for image analysis and is now widely used to segment both 2D and 3D medical images. Deep-learning segmentation frameworks rely not only on the choice of network architecture but also on the choice of loss function. When the segmentation process targets rare observations, a severe class imbalance is likely to occur between candidate labels, thus resulting in sub-optimal performance. In order to mitigate this issue, strategies such as the weighted cross-entropy function, the sensitivity function or the Dice loss function, have been proposed. In this work, we investigate the behavior of these loss functions and their sensitivity to learning rate tuning in the presence of different rates of label imbalance across 2D and 3D segmentation tasks. We also propose to use the class re-balancing properties of the Generalized Dice overlap, a known metric for segmentation assessment, as a robust and accurate deep-learning loss function for unbalanced tasks.
ER  -


TY  - Preprint
T1  - Revisiting Unreasonable Effectiveness of Data in Deep Learning Era
A1  - Chen Sun
A1  - Abhinav Shrivastava
A1  - Saurabh Singh
A1  - Abhinav Gupta
JO  - ArXiv e-prints
Y1  - 3 August, 2017
UR  - https://arxiv.org/abs/1707.02968
N2  - The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data&#39; and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.
ER  -


TY  - Preprint
T1  - Deep Bilateral Learning for Real-Time Image Enhancement
A1  - MichaÃ«l Gharbi
A1  - Jiawen Chen
A1  - Jonathan T. Barron
A1  - Samuel W. Hasinoff
A1  - FrÃ©do Durand
JO  - ArXiv e-prints
Y1  - 22 August, 2017
UR  - https://arxiv.org/abs/1707.02880
N2  - Performance is a critical challenge in mobile image processing. Given a reference imaging pipeline, or even human-adjusted pairs of images, we seek to reproduce the enhancements and enable real-time evaluation. For this, we introduce a new neural network architecture inspired by bilateral grid processing and local affine color transforms. Using pairs of input/output images, we train a convolutional neural network to predict the coefficients of a locally-affine model in bilateral space. Our architecture learns to make local, global, and content-dependent decisions to approximate the desired image transformation. At runtime, the neural network consumes a low-resolution version of the input image, produces a set of affine transformations in bilateral space, upsamples those transformations in an edge-preserving fashion using a new slicing node, and then applies those upsampled transformations to the full-resolution image. Our algorithm processes high-resolution images on a smartphone in milliseconds, provides a real-time viewfinder at 1080p resolution, and matches the quality of state-of-the-art approximation techniques on a large class of image operators. Unlike previous work, our model is trained off-line from data and therefore does not require access to the original operator at runtime. This allows our model to learn complex, scene-dependent transformations for which no reference implementation is available, such as the photographic edits of a human retoucher.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning Attention Selection for Person Re-Identification
A1  - Xu Lan
A1  - Hanxiao Wang
A1  - Shaogang Gong
A1  - Xiatian Zhu
JO  - ArXiv e-prints
Y1  - 7 July, 2018
UR  - https://arxiv.org/abs/1707.02785
N2  - Existing person re-identification (re-id) methods assume the provision of accurately cropped person bounding boxes with minimum background noise, mostly by manually cropping. This is significantly breached in practice when person bounding boxes must be detected automatically given a very large number of images and/or videos processed. Compared to carefully cropped manually, auto-detected bounding boxes are far less accurate with random amount of background clutter which can degrade notably person re-id matching accuracy. In this work, we develop a joint learning deep model that optimises person re-id attention selection within any auto-detected person bounding boxes by reinforcement learning of background clutter minimisation subject to re-id label pairwise constraints. Specifically, we formulate a novel unified re-id architecture called Identity DiscriminativE Attention reinforcement Learning (IDEAL) to accurately select re-id attention in auto-detected bounding boxes for optimising re-id performance. Our model can improve re-id accuracy comparable to that from exhaustive human manual cropping of bounding boxes with additional advantages from identity discriminative attention selection that specially benefits re-id tasks beyond human knowledge. Extensive comparative evaluations demonstrate the re-id advantages of the proposed IDEAL model over a wide range of state-of-the-art re-id methods on two auto-detected re-id benchmarks CUHK03 and Market-1501.
ER  -


TY  - Preprint
T1  - Deep Learning for Vanishing Point Detection Using an Inverse Gnomonic Projection
A1  - Florian Kluger
A1  - Hanno Ackermann
A1  - Michael Ying Yang
A1  - Bodo Rosenhahn
JO  - ArXiv e-prints
Y1  - 16 November, 2017
UR  - https://arxiv.org/abs/1707.02427
N2  - We present a novel approach for vanishing point detection from uncalibrated monocular images. In contrast to state-of-the-art, we make no a priori assumptions about the observed scene. Our method is based on a convolutional neural network (CNN) which does not use natural images, but a Gaussian sphere representation arising from an inverse gnomonic projection of lines detected in an image. This allows us to rely on synthetic data for training, eliminating the need for labelled images. Our method achieves competitive performance on three horizon estimation benchmark datasets. We further highlight some additional use cases for which our vanishing point detection algorithm can be used.
ER  -


TY  - Preprint
T1  - Deep Q-Learning for Self-Organizing Networks Fault Management and Radio Performance Improvement
A1  - Faris B. Mismar
A1  - Brian L. Evans
JO  - ArXiv e-prints
Y1  - 31 July, 2018
UR  - https://arxiv.org/abs/1707.02329
N2  - We propose an algorithm to automate fault management in an outdoor cellular network using deep reinforcement learning (RL) against wireless impairments. This algorithm enables the cellular network cluster to self-heal by allowing RL to learn how to improve the downlink signal to interference plus noise ratio and spectral efficiency through exploration and exploitation of various alarm corrective actions. The main contributions of this paper are to 1) introduce a deep RL-based fault handling algorithm which self-organizing networks can implement in a polynomial runtime and 2) show that this fault management method can improve the radio link performance in a realistic network setup. Simulation results show that our proposed learns an action sequence to clear alarms and improve the performance in the cellular cluster better than existing algorithms, even against the randomness of the network fault occurrences and user movements.
ER  -


TY  - Preprint
T1  - An Embedded Deep Learning based Word Prediction
A1  - Seunghak Yu
A1  - Nilesh Kulkarni
A1  - Haejun Lee
A1  - Jihie Kim
JO  - ArXiv e-prints
Y1  - 6 July, 2017
UR  - https://arxiv.org/abs/1707.01662
N2  - Recent developments in deep learning with application to language modeling have led to success in tasks of text processing, summarizing and machine translation. However, deploying huge language models for mobile device such as on-device keyboards poses computation as a bottle-neck due to their puny computation capacities. In this work we propose an embedded deep learning based word prediction method that optimizes run-time memory and also provides a real time prediction environment. Our model size is 7.40MB and has average prediction time of 6.47 ms. We improve over the existing methods for word prediction in terms of key stroke savings and word prediction rate.
ER  -


TY  - Preprint
T1  - DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer
A1  - Yuntao Chen
A1  - Naiyan Wang
A1  - Zhaoxiang Zhang
JO  - ArXiv e-prints
Y1  - 18 December, 2017
UR  - https://arxiv.org/abs/1707.01220
N2  - We have witnessed rapid evolution of deep neural network architecture design in the past years. These latest progresses greatly facilitate the developments in various areas such as computer vision and natural language processing. However, along with the extraordinary performance, these state-of-the-art models also bring in expensive computational cost. Directly deploying these models into applications with real-time requirement is still infeasible. Recently, Hinton etal. have shown that the dark knowledge within a powerful teacher model can significantly help the training of a smaller and faster student network. These knowledge are vastly beneficial to improve the generalization ability of the student model. Inspired by their work, we introduce a new type of knowledge -- cross sample similarities for model compression and acceleration. This knowledge can be naturally derived from deep metric learning model. To transfer them, we bring the &#34;learning to rank&#34; technique into deep metric learning formulation. We test our proposed DarkRank method on various metric learning tasks including pedestrian re-identification, image retrieval and image clustering. The results are quite encouraging. Our method can improve over the baseline method by a large margin. Moreover, it is fully compatible with other existing methods. When combined, the performance can be further boosted.
ER  -


TY  - Preprint
T1  - Maintaining cooperation in complex social dilemmas using deep reinforcement learning
A1  - Adam Lerer
A1  - Alexander Peysakhovich
JO  - ArXiv e-prints
Y1  - 2 March, 2018
UR  - https://arxiv.org/abs/1707.01068
N2  - Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.
ER  -


TY  - Preprint
T1  - PBODL : Parallel Bayesian Online Deep Learning for Click-Through Rate Prediction in Tencent Advertising System
A1  - Xun Liu
A1  - Wei Xue
A1  - Lei Xiao
A1  - Bo Zhang
JO  - ArXiv e-prints
Y1  - 9 July, 2017
UR  - https://arxiv.org/abs/1707.00802
N2  - We describe a parallel bayesian online deep learning framework (PBODL) for click-through rate (CTR) prediction within today&#39;s Tencent advertising system, which provides quick and accurate learning of user preferences. We first explain the framework with a deep probit regression model, which is trained with probabilistic back-propagation in the mode of assumed Gaussian density filtering. Then we extend the model family to a variety of bayesian online models with increasing feature embedding capabilities, such as Sparse-MLP, FM-MLP and FFM-MLP. Finally, we implement a parallel training system based on a stream computing infrastructure and parameter servers. Experiments with public available datasets and Tencent industrial datasets show that models within our framework perform better than several common online models, such as AdPredictor, FTRL-Proximal and MatchBox. Online A/B test within Tencent advertising system further proves that our framework could achieve CTR and CPM lift by learning more quickly and accurately.
ER  -


TY  - Preprint
T1  - Deep Representation Learning with Part Loss for Person Re-Identification
A1  - Hantao Yao
A1  - Shiliang Zhang
A1  - Yongdong Zhang
A1  - Jintao Li
A1  - Qi Tian
JO  - ArXiv e-prints
Y1  - 16 November, 2017
UR  - https://arxiv.org/abs/1707.00798
N2  - Learning discriminative representations for unseen person images is critical for person Re-Identification (ReID). Most of current approaches learn deep representations in classification tasks, which essentially minimize the empirical classification risk on the training set. As shown in our experiments, such representations commonly focus on several body parts discriminative to the training set, rather than the entire human body. Inspired by the structural risk minimization principle in SVM, we revise the traditional deep representation learning procedure to minimize both the empirical classification risk and the representation learning risk. The representation learning risk is evaluated by the proposed part loss, which automatically generates several parts for an image, and computes the person classification loss on each part separately. Compared with traditional global classification loss, simultaneously considering multiple part loss enforces the deep network to focus on the entire human body and learn discriminative representations for different parts. Experimental results on three datasets, i.e., Market1501, CUHK03, VIPeR, show that our representation outperforms the existing deep representations.
ER  -


TY  - Preprint
T1  - Zero-Shot Fine-Grained Classification by Deep Feature Learning with Semantics
A1  - Aoxue Li
A1  - Zhiwu Lu
A1  - Liwei Wang
A1  - Tao Xiang
A1  - Xinqi Li
A1  - Ji-Rong Wen
JO  - ArXiv e-prints
Y1  - 3 July, 2017
UR  - https://arxiv.org/abs/1707.00785
N2  - Fine-grained image classification, which aims to distinguish images with subtle distinctions, is a challenging task due to two main issues: lack of sufficient training data for every class and difficulty in learning discriminative features for representation. In this paper, to address the two issues, we propose a two-phase framework for recognizing images from unseen fine-grained classes, i.e. zero-shot fine-grained classification. In the first feature learning phase, we finetune deep convolutional neural networks using hierarchical semantic structure among fine-grained classes to extract discriminative deep visual features. Meanwhile, a domain adaptation structure is induced into deep convolutional neural networks to avoid domain shift from training data to test data. In the second label inference phase, a semantic directed graph is constructed over attributes of fine-grained classes. Based on this graph, we develop a label propagation algorithm to infer the labels of images in the unseen classes. Experimental results on two benchmark datasets demonstrate that our model outperforms the state-of-the-art zero-shot learning models. In addition, the features obtained by our feature learning model also yield significant gains when they are used by other zero-shot learning models, which shows the flexility of our model in zero-shot fine-grained classification.
ER  -


TY  - Preprint
T1  - Deep-learning-based data page classification for holographic memory
A1  - Tomoyoshi Shimobaba
A1  - Naoki Kuwata
A1  - Mizuha Homma
A1  - Takayuki Takahashi
A1  - Yuki Nagahama
A1  - Marie Sano
A1  - Satoki Hasegawa
A1  - Ryuji Hirayama
A1  - Takashi Kakue
A1  - Atsushi Shiraki
A1  - Naoki Takada
A1  - Tomoyoshi Ito
JO  - ArXiv e-prints
Y1  - 2 July, 2017
UR  - https://arxiv.org/abs/1707.00684
N2  - We propose a deep-learning-based classification of data pages used in holographic memory. We numerically investigated the classification performance of a conventional multi-layer perceptron (MLP) and a deep neural network, under the condition that reconstructed page data are contaminated by some noise and are randomly laterally shifted. The MLP was found to have a classification accuracy of 91.58%, whereas the deep neural network was able to classify data pages at an accuracy of 99.98%. The accuracy of the deep neural network is two orders of magnitude better than the MLP.
ER  -


TY  - Preprint
T1  - Hashing over Predicted Future Frames for Informed Exploration of Deep Reinforcement Learning
A1  - Haiyan Yin
A1  - Jianda Chen
A1  - Sinno Jialin Pan
JO  - ArXiv e-prints
Y1  - 27 April, 2018
UR  - https://arxiv.org/abs/1707.00524
N2  - In deep reinforcement learning (RL) tasks, an efficient exploration mechanism should be able to encourage an agent to take actions that lead to less frequent states which may yield higher accumulative future return. However, both knowing about the future and evaluating the frequentness of states are non-trivial tasks, especially for deep RL domains, where a state is represented by high-dimensional image frames. In this paper, we propose a novel informed exploration framework for deep RL, where we build the capability for an RL agent to predict over the future transitions and evaluate the frequentness for the predicted future frames in a meaningful manner. To this end, we train a deep prediction model to predict future frames given a state-action pair, and a convolutional autoencoder model to hash over the seen frames. In addition, to utilize the counts derived from the seen frames to evaluate the frequentness for the predicted frames, we tackle the challenge of matching the predicted future frames and their corresponding seen frames at the latent feature level. In this way, we derive a reliable metric for evaluating the novelty of the future direction pointed by each action, and hence inform the agent to explore the least frequent one.
ER  -


TY  - Preprint
T1  - Detection and Localization of Image Forgeries using Resampling Features and Deep Learning
A1  - Jason Bunk
A1  - Jawadul H. Bappy
A1  - Tajuddin Manhar Mohammed
A1  - Lakshmanan Nataraj
A1  - Arjuna Flenner
A1  - B. S. Manjunath
A1  - Shivkumar Chandrasekaran
A1  - Amit K. Roy-Chowdhury
A1  - Lawrence Peterson
JO  - ArXiv e-prints
Y1  - 3 July, 2017
UR  - https://arxiv.org/abs/1707.00433
N2  - Resampling is an important signature of manipulated images. In this paper, we propose two methods to detect and localize image manipulations based on a combination of resampling features and deep learning. In the first method, the Radon transform of resampling features are computed on overlapping image patches. Deep learning classifiers and a Gaussian conditional random field model are then used to create a heatmap. Tampered regions are located using a Random Walker segmentation method. In the second method, resampling features computed on overlapping image patches are passed through a Long short-term memory (LSTM) based network for classification and localization. We compare the performance of detection/localization of both these methods. Our experimental results show that both techniques are effective in detecting and localizing digital image forgeries.
ER  -


TY  - Preprint
T1  - Learning Deep Latent Spaces for Multi-Label Classification
A1  - Chih-Kuan Yeh
A1  - Wei-Chieh Wu
A1  - Wei-Jen Ko
A1  - Yu-Chiang Frank Wang
JO  - ArXiv e-prints
Y1  - 3 July, 2017
UR  - https://arxiv.org/abs/1707.00418
N2  - Multi-label classification is a practical yet challenging task in machine learning related fields, since it requires the prediction of more than one label category for each input instance. We propose a novel deep neural networks (DNN) based model, Canonical Correlated AutoEncoder (C2AE), for solving this task. Aiming at better relating feature and label domain data for improved classification, we uniquely perform joint feature and label embedding by deriving a deep latent space, followed by the introduction of label-correlation sensitive loss function for recovering the predicted label outputs. Our C2AE is achieved by integrating the DNN architectures of canonical correlation analysis and autoencoder, which allows end-to-end learning and prediction with the ability to exploit label dependency. Moreover, our C2AE can be easily extended to address the learning problem with missing labels. Our experiments on multiple datasets with different scales confirm the effectiveness and robustness of our proposed method, which is shown to perform favorably against state-of-the-art methods for multi-label classification.
ER  -


TY  - Preprint
T1  - Deep Ranking Model by Large Adaptive Margin Learning for Person Re-identification
A1  - Jiayun Wang
A1  - Sanping Zhou
A1  - Jinjun Wang
A1  - Qiqi Hou
JO  - ArXiv e-prints
Y1  - 17 September, 2017
UR  - https://arxiv.org/abs/1707.00409
N2  - Person re-identification aims to match images of the same person across disjoint camera views, which is a challenging problem in video surveillance. The major challenge of this task lies in how to preserve the similarity of the same person against large variations caused by complex backgrounds, mutual occlusions and different illuminations, while discriminating the different individuals. In this paper, we present a novel deep ranking model with feature learning and fusion by learning a large adaptive margin between the intra-class distance and inter-class distance to solve the person re-identification problem. Specifically, we organize the training images into a batch of pairwise samples. Treating these pairwise samples as inputs, we build a novel part-based deep convolutional neural network (CNN) to learn the layered feature representations by preserving a large adaptive margin. As a result, the final learned model can effectively find out the matched target to the anchor image among a number of candidates in the gallery image set by learning discriminative and stable feature representations. Overcoming the weaknesses of conventional fixed-margin loss functions, our adaptive margin loss function is more appropriate for the dynamic feature space. On four benchmark datasets, PRID2011, Market1501, CUHK01 and 3DPeS, we extensively conduct comparative evaluations to demonstrate the advantages of the proposed method over the state-of-the-art approaches in person re-identification.
ER  -


TY  - Preprint
T1  - Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes
A1  - Lei Wu
A1  - Zhanxing Zhu
A1  - Weinan E
JO  - ArXiv e-prints
Y1  - 27 November, 2017
UR  - https://arxiv.org/abs/1706.10239
N2  - It is widely observed that deep learning models with learned parameters generalize well, even with much more model parameters than the number of training samples. We systematically investigate the underlying reasons why deep neural networks often generalize well, and reveal the difference between the minima (with the same training error) that generalize well and those they don&#39;t. We show that it is the characteristics the landscape of the loss function that explains the good generalization capability. For the landscape of loss function for deep networks, the volume of basin of attraction of good minima dominates over that of poor minima, which guarantees optimization methods with random initialization to converge to good minima. We theoretically justify our findings through analyzing 2-layer neural networks; and show that the low-complexity solutions have a small norm of Hessian matrix with respect to model parameters. For deeper networks, extensive numerical evidence helps to support our arguments.
ER  -


TY  - Preprint
T1  - Towards Monocular Vision based Obstacle Avoidance through Deep Reinforcement Learning
A1  - Linhai Xie
A1  - Sen Wang
A1  - Andrew Markham
A1  - Niki Trigoni
JO  - ArXiv e-prints
Y1  - 29 June, 2017
UR  - https://arxiv.org/abs/1706.09829
N2  - Obstacle avoidance is a fundamental requirement for autonomous robots which operate in, and interact with, the real world. When perception is limited to monocular vision avoiding collision becomes significantly more challenging due to the lack of 3D information. Conventional path planners for obstacle avoidance require tuning a number of parameters and do not have the ability to directly benefit from large datasets and continuous use. In this paper, a dueling architecture based deep double-Q network (D3QN) is proposed for obstacle avoidance, using only monocular RGB vision. Based on the dueling and double-Q mechanisms, D3QN can efficiently learn how to avoid obstacles in a simulator even with very noisy depth information predicted from RGB image. Extensive experiments show that D3QN enables twofold acceleration on learning compared with a normal deep Q network and the models trained solely in virtual environments can be directly transferred to real robots, generalizing well to various new environments with previously unseen dynamic objects.
ER  -


TY  - Preprint
T1  - Chord Label Personalization through Deep Learning of Integrated Harmonic Interval-based Representations
A1  - H. V. Koops
A1  - W. B. de Haas
A1  - J. Bransen
A1  - A. Volk
JO  - ArXiv e-prints
Y1  - 28 June, 2017
UR  - https://arxiv.org/abs/1706.09552
N2  - The increasing accuracy of automatic chord estimation systems, the availability of vast amounts of heterogeneous reference annotations, and insights from annotator subjectivity research make chord label personalization increasingly important. Nevertheless, automatic chord estimation systems are historically exclusively trained and evaluated on a single reference annotation. We introduce a first approach to automatic chord label personalization by modeling subjectivity through deep learning of a harmonic interval-based chord label representation. After integrating these representations from multiple annotators, we can accurately personalize chord labels for individual annotators from a single model and the annotators&#39; chord label vocabulary. Furthermore, we show that chord personalization using multiple reference annotations outperforms using a single reference annotation.
ER  -


TY  - Preprint
T1  - Deep Learning Based Large-Scale Automatic Satellite Crosswalk Classification
A1  - Rodrigo F. Berriel
A1  - Andre Teixeira Lopes
A1  - Alberto F. de Souza
A1  - Thiago Oliveira-Santos
JO  - ArXiv e-prints
Y1  - 5 July, 2017
UR  - https://arxiv.org/abs/1706.09302
N2  - High-resolution satellite imagery have been increasingly used on remote sensing classification problems. One of the main factors is the availability of this kind of data. Even though, very little effort has been placed on the zebra crossing classification problem. In this letter, crowdsourcing systems are exploited in order to enable the automatic acquisition and annotation of a large-scale satellite imagery database for crosswalks related tasks. Then, this dataset is used to train deep-learning-based models in order to accurately classify satellite images that contains or not zebra crossings. A novel dataset with more than 240,000 images from 3 continents, 9 countries and more than 20 cities was used in the experiments. Experimental results showed that freely available crowdsourcing data can be used to accurately (97.11%) train robust models to perform crosswalk classification on a global scale.
ER  -


TY  - Preprint
T1  - Classification of Medical Images and Illustrations in the Biomedical Literature Using Synergic Deep Learning
A1  - Jianpeng Zhang
A1  - Yong Xia
A1  - Qi Wu
A1  - Yutong Xie
JO  - ArXiv e-prints
Y1  - 27 June, 2017
UR  - https://arxiv.org/abs/1706.09092
N2  - The Classification of medical images and illustrations in the literature aims to label a medical image according to the modality it was produced or label an illustration according to its production attributes. It is an essential and challenging research hotspot in the area of automated literature review, retrieval and mining. The significant intra-class variation and inter-class similarity caused by the diverse imaging modalities and various illustration types brings a great deal of difficulties to the problem. In this paper, we propose a synergic deep learning (SDL) model to address this issue. Specifically, a dual deep convolutional neural network with a synergic signal system is designed to mutually learn image representation. The synergic signal is used to verify whether the input image pair belongs to the same category and to give the corrective feedback if a synergic error exists. Our SDL model can be trained &#39;end to end&#39;. In the test phase, the class label of an input can be predicted by averaging the likelihood probabilities obtained by two convolutional neural network components. Experimental results on the ImageCLEF2016 Subfigure Classification Challenge suggest that our proposed SDL model achieves the state-of-the art performance in this medical image classification problem and its accuracy is higher than that of the first place solution on the Challenge leader board so far.
ER  -


TY  - Preprint
T1  - Super-Resolution via Deep Learning
A1  - Khizar Hayat
JO  - ArXiv e-prints
Y1  - 27 June, 2017
UR  - https://arxiv.org/abs/1706.09077
N2  - The recent phenomenal interest in convolutional neural networks (CNNs) must have made it inevitable for the super-resolution (SR) community to explore its potential. The response has been immense and in the last three years, since the advent of the pioneering work, there appeared too many works not to warrant a comprehensive survey. This paper surveys the SR literature in the context of deep learning. We focus on the three important aspects of multimedia - namely image, video and multi-dimensions, especially depth maps. In each case, first relevant benchmarks are introduced in the form of datasets and state of the art SR methods, excluding deep learning. Next is a detailed analysis of the individual works, each including a short description of the method and a critique of the results with special reference to the benchmarking done. This is followed by minimum overall benchmarking in the form of comparison on some common dataset, while relying on the results reported in various works.
ER  -


TY  - Preprint
T1  - Exploring Generalization in Deep Learning
A1  - Behnam Neyshabur
A1  - Srinadh Bhojanapalli
A1  - David McAllester
A1  - Nathan Srebro
JO  - ArXiv e-prints
Y1  - 6 July, 2017
UR  - https://arxiv.org/abs/1706.08947
N2  - With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.
ER  -


TY  - Preprint
T1  - Cross-Country Skiing Gears Classification using Deep Learning
A1  - Aliaa Rassem
A1  - Mohammed El-Beltagy
A1  - Mohamed Saleh
JO  - ArXiv e-prints
Y1  - 27 June, 2017
UR  - https://arxiv.org/abs/1706.08924
N2  - Human Activity Recognition has witnessed a significant progress in the last decade. Although a great deal of work in this field goes in recognizing normal human activities, few studies focused on identifying motion in sports. Recognizing human movements in different sports has high impact on understanding the different styles of humans in the play and on improving their performance. As deep learning models proved to have good results in many classification problems, this paper will utilize deep learning to classify cross-country skiing movements, known as gears, collected using a 3D accelerometer. It will also provide a comparison between different deep learning models such as convolutional and recurrent neural networks versus standard multi-layer perceptron. Results show that deep learning is more effective and has the highest classification accuracy.
ER  -


TY  - Preprint
T1  - Topometric Localization with Deep Learning
A1  - Gabriel L. Oliveira
A1  - Noha Radwan
A1  - Wolfram Burgard
A1  - Thomas Brox
JO  - ArXiv e-prints
Y1  - 27 June, 2017
UR  - https://arxiv.org/abs/1706.08775
N2  - Compared to LiDAR-based localization methods, which provide high accuracy but rely on expensive sensors, visual localization approaches only require a camera and thus are more cost-effective while their accuracy and reliability typically is inferior to LiDAR-based methods. In this work, we propose a vision-based localization approach that learns from LiDAR-based localization methods by using their output as training data, thus combining a cheap, passive sensor with an accuracy that is on-par with LiDAR-based localization. The approach consists of two deep networks trained on visual odometry and topological localization, respectively, and a successive optimization to combine the predictions of these two networks. We evaluate the approach on a new challenging pedestrian-based dataset captured over the course of six months in varying weather conditions with a high degree of noise. The experiments demonstrate that the localization errors are up to 10 times smaller than with traditional vision-based localization methods.
ER  -


TY  - Preprint
T1  - Proceedings of the First International Workshop on Deep Learning and Music
A1  - Dorien Herremans
A1  - Ching-Hua Chuan
JO  - ArXiv e-prints
Y1  - 27 June, 2017
UR  - https://arxiv.org/abs/1706.08675
N2  - Proceedings of the First International Workshop on Deep Learning and Music, joint with IJCNN, Anchorage, US, May 17-18, 2017
ER  -


TY  - Preprint
T1  - Fast and accurate classification of echocardiograms using deep learning
A1  - Ali Madani
A1  - Ramy Arnaout
A1  - Mohammad Mofrad
A1  - Rima Arnaout
JO  - ArXiv e-prints
Y1  - 26 June, 2017
UR  - https://arxiv.org/abs/1706.08658
N2  - Echocardiography is essential to modern cardiology. However, human interpretation limits high throughput analysis, limiting echocardiography from reaching its full clinical and research potential for precision medicine. Deep learning is a cutting-edge machine-learning technique that has been useful in analyzing medical images but has not yet been widely applied to echocardiography, partly due to the complexity of echocardiograms&#39; multi view, multi modality format. The essential first step toward comprehensive computer assisted echocardiographic interpretation is determining whether computers can learn to recognize standard views. To this end, we anonymized 834,267 transthoracic echocardiogram (TTE) images from 267 patients (20 to 96 years, 51 percent female, 26 percent obese) seen between 2000 and 2017 and labeled them according to standard views. Images covered a range of real world clinical variation. We built a multilayer convolutional neural network and used supervised learning to simultaneously classify 15 standard views. Eighty percent of data used was randomly chosen for training and 20 percent reserved for validation and testing on never seen echocardiograms. Using multiple images from each clip, the model classified among 12 video views with 97.8 percent overall test accuracy without overfitting. Even on single low resolution images, test accuracy among 15 views was 91.7 percent versus 70.2 to 83.5 percent for board-certified echocardiographers. Confusional matrices, occlusion experiments, and saliency mapping showed that the model finds recognizable similarities among related views and classifies using clinically relevant image features. In conclusion, deep neural networks can classify essential echocardiographic views simultaneously and with high accuracy. Our results provide a foundation for more complex deep learning assisted echocardiographic interpretation.
ER  -


TY  - Preprint
T1  - Cross-lingual Speaker Verification with Deep Feature Learning
A1  - Lantian Li
A1  - Dong Wang
A1  - Askar Rozi
A1  - Thomas Fang Zheng
JO  - ArXiv e-prints
Y1  - 22 June, 2017
UR  - https://arxiv.org/abs/1706.07861
N2  - Existing speaker verification (SV) systems often suffer from performance degradation if there is any language mismatch between model training, speaker enrollment, and test. A major cause of this degradation is that most existing SV methods rely on a probabilistic model to infer the speaker factor, so any significant change on the distribution of the speech signal will impact the inference. Recently, we proposed a deep learning model that can learn how to extract the speaker factor by a deep neural network (DNN). By this feature learning, an SV system can be constructed with a very simple back-end model. In this paper, we investigate the robustness of the feature-based SV system in situations with language mismatch. Our experiments were conducted on a complex cross-lingual scenario, where the model training was in English, and the enrollment and test were in Chinese or Uyghur. The experiments demonstrated that the feature-based system outperformed the i-vector system with a large margin, particularly with language mismatch between enrollment and test.
ER  -


TY  - Preprint
T1  - Personalized Acoustic Modeling by Weakly Supervised Multi-Task Deep Learning using Acoustic Tokens Discovered from Unlabeled Data
A1  - Cheng-Kuan Wei
A1  - Cheng-Tao Chung
A1  - Hung-Yi Lee
A1  - Lin-Shan Lee
JO  - ArXiv e-prints
Y1  - 23 June, 2017
UR  - https://arxiv.org/abs/1706.07793
N2  - It is well known that recognizers personalized to each user are much more effective than user-independent recognizers. With the popularity of smartphones today, although it is not difficult to collect a large set of audio data for each user, it is difficult to transcribe it. However, it is now possible to automatically discover acoustic tokens from unlabeled personal data in an unsupervised way. We therefore propose a multi-task deep learning framework called a phoneme-token deep neural network (PTDNN), jointly trained from unsupervised acoustic tokens discovered from unlabeled data and very limited transcribed data for personalized acoustic modeling. We term this scenario &#34;weakly supervised&#34;. The underlying intuition is that the high degree of similarity between the HMM states of acoustic token models and phoneme models may help them learn from each other in this multi-task learning framework. Initial experiments performed over a personalized audio data set recorded from Facebook posts demonstrated that very good improvements can be achieved in both frame accuracy and word accuracy over popularly-considered baselines such as fDLR, speaker code and lightly supervised adaptation. This approach complements existing speaker adaptation approaches and can be used jointly with such techniques to yield improved results.
ER  -


TY  - Preprint
T1  - Sampling Matters in Deep Embedding Learning
A1  - Chao-Yuan Wu
A1  - R. Manmatha
A1  - Alexander J. Smola
A1  - Philipp KrÃ¤henbÃ¼hl
JO  - ArXiv e-prints
Y1  - 16 January, 2018
UR  - https://arxiv.org/abs/1706.07567
N2  - Deep embeddings answer one simple question: How similar are two images? Learning these embeddings is the bedrock of verification, zero-shot learning, and visual search. The most prominent approaches optimize a deep convolutional network with a suitable loss function, such as contrastive loss or triplet loss. While a rich line of work focuses solely on the loss functions, we show in this paper that selecting training examples plays an equally important role. We propose distance weighted sampling, which selects more informative and stable examples than traditional approaches. In addition, we show that a simple margin based loss is sufficient to outperform all other loss functions. We evaluate our approach on the Stanford Online Products, CAR196, and the CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset for face verification. Our method achieves state-of-the-art performance on all of them.
ER  -


TY  - Preprint
T1  - Deep Learning Methods for Improved Decoding of Linear Codes
A1  - Eliya Nachmani
A1  - Elad Marciano
A1  - Loren Lugosch
A1  - Warren J. Gross
A1  - David Burshtein
A1  - Yair Beery
JO  - ArXiv e-prints
Y1  - 1 January, 2018
UR  - https://arxiv.org/abs/1706.07043
N2  - The problem of low complexity, close to optimal, channel decoding of linear codes with short to moderate block length is considered. It is shown that deep learning methods can be used to improve a standard belief propagation decoder, despite the large example space. Similar improvements are obtained for the min-sum algorithm. It is also shown that tying the parameters of the decoders across iterations, so as to form a recurrent neural network architecture, can be implemented with comparable results. The advantage is that significantly less parameters are required. We also introduce a recurrent neural decoder architecture based on the method of successive relaxation. Improvements over standard belief propagation are also observed on sparser Tanner graph representations of the codes. Furthermore, we demonstrate that the neural belief propagation decoder can be used to improve the performance, or alternatively reduce the computational complexity, of a close to optimal decoder of short BCH codes.
ER  -


TY  - Preprint
T1  - Structure Learning in Motor Control:A Deep Reinforcement Learning Model
A1  - Ari Weinstein
A1  - Matthew M. Botvinick
JO  - ArXiv e-prints
Y1  - 13 July, 2017
UR  - https://arxiv.org/abs/1706.06827
N2  - Motor adaptation displays a structure-learning effect: adaptation to a new perturbation occurs more quickly when the subject has prior exposure to perturbations with related structure. Although this `learning-to-learn&#39; effect is well documented, its underlying computational mechanisms are poorly understood. We present a new model of motor structure learning, approaching it from the point of view of deep reinforcement learning. Previous work outside of motor control has shown how recurrent neural networks can account for learning-to-learn effects. We leverage this insight to address motor learning, by importing it into the setting of model-based reinforcement learning. We apply the resulting processing architecture to empirical findings from a landmark study of structure learning in target-directed reaching (Braun et al., 2009), and discuss its implications for a wider range of learning-to-learn phenomena.
ER  -


TY  - Preprint
T1  - Deep Learning Autoencoder Approach for Handwritten Arabic Digits Recognition
A1  - Mohamed Loey
A1  - Ahmed El-Sawy
A1  - Hazem EL-Bakry
JO  - ArXiv e-prints
Y1  - 20 June, 2017
UR  - https://arxiv.org/abs/1706.06720
N2  - This paper presents a new unsupervised learning approach with stacked autoencoder (SAE) for Arabic handwritten digits categorization. Recently, Arabic handwritten digits recognition has been an important area due to its applications in several fields. This work is focusing on the recognition part of handwritten Arabic digits recognition that face several challenges, including the unlimited variation in human handwriting and the large public databases. Arabic digits contains ten numbers that were descended from the Indian digits system. Stacked autoencoder (SAE) tested and trained the MADBase database (Arabic handwritten digits images) that contain 10000 testing images and 60000 training images. We show that the use of SAE leads to significant improvements across different machine-learning classification algorithms. SAE is giving an average accuracy of 98.5%.
ER  -


TY  - Preprint
T1  - Advanced Steel Microstructural Classification by Deep Learning Methods
A1  - Seyed Majid Azimi
A1  - Dominik Britz
A1  - Michael Engstler
A1  - Mario Fritz
A1  - Frank MÃ¼cklich
JO  - ArXiv e-prints
Y1  - 15 February, 2018
UR  - https://arxiv.org/abs/1706.06480
N2  - The inner structure of a material is called microstructure. It stores the genesis of a material and determines all its physical and chemical properties. While microstructural characterization is widely spread and well known, the microstructural classification is mostly done manually by human experts, which gives rise to uncertainties due to subjectivity. Since the microstructure could be a combination of different phases or constituents with complex substructures its automatic classification is very challenging and only a few prior studies exist. Prior works focused on designed and engineered features by experts and classified microstructures separately from the feature extraction step. Recently, Deep Learning methods have shown strong performance in vision applications by learning the features from data together with the classification step. In this work, we propose a Deep Learning method for microstructural classification in the examples of certain microstructural constituents of low carbon steel. This novel method employs pixel-wise segmentation via Fully Convolutional Neural Networks (FCNN) accompanied by a max-voting scheme. Our system achieves 93.94% classification accuracy, drastically outperforming the state-of-the-art method of 48.89% accuracy. Beyond the strong performance of our method, this line of research offers a more robust and first of all objective way for the difficult task of steel quality appreciation.
ER  -


TY  - Preprint
T1  - Short-Term Forecasting of Passenger Demand under On-Demand Ride Services: A Spatio-Temporal Deep Learning Approach
A1  - Jintao Ke
A1  - Hongyu Zheng
A1  - Hai Yang
A1  -  Xiqun
A1  -  Chen
JO  - ArXiv e-prints
Y1  - 20 June, 2017
UR  - https://arxiv.org/abs/1706.06279
N2  - Short-term passenger demand forecasting is of great importance to the on-demand ride service platform, which can incentivize vacant cars moving from over-supply regions to over-demand regions. The spatial dependences, temporal dependences, and exogenous dependences need to be considered simultaneously, however, which makes short-term passenger demand forecasting challenging. We propose a novel deep learning (DL) approach, named the fusion convolutional long short-term memory network (FCL-Net), to address these three dependences within one end-to-end learning architecture. The model is stacked and fused by multiple convolutional long short-term memory (LSTM) layers, standard LSTM layers, and convolutional layers. The fusion of convolutional techniques and the LSTM network enables the proposed DL approach to better capture the spatio-temporal characteristics and correlations of explanatory variables. A tailored spatially aggregated random forest is employed to rank the importance of the explanatory variables. The ranking is then used for feature selection. The proposed DL approach is applied to the short-term forecasting of passenger demand under an on-demand ride service platform in Hangzhou, China. Experimental results, validated on real-world data provided by DiDi Chuxing, show that the FCL-Net achieves better predictive performance than traditional approaches including both classical time-series prediction models and neural network based algorithms (e.g., artificial neural network and LSTM). This paper is one of the first DL studies to forecast the short-term passenger demand of an on-demand ride service platform by examining the spatio-temporal correlations.
ER  -


TY  - Preprint
T1  - meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting
A1  - Xu Sun
A1  - Xuancheng Ren
A1  - Shuming Ma
A1  - Houfeng Wang
JO  - ArXiv e-prints
Y1  - 30 October, 2017
UR  - https://arxiv.org/abs/1706.06197
N2  - We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-$k$ elements (in terms of magnitude) are kept. As a result, only $k$ rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction ($k$ divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1--4\% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given. The code is available at https://github.com/jklj077/meProp
ER  -


TY  - Preprint
T1  - Deep learning with spatiotemporal consistency for nerve segmentation in ultrasound images
A1  - Adel Hafiane
A1  - Pierre Vieyres
A1  - Alain Delbos
JO  - ArXiv e-prints
Y1  - 19 June, 2017
UR  - https://arxiv.org/abs/1706.05870
N2  - Ultrasound-Guided Regional Anesthesia (UGRA) has been gaining importance in the last few years, offering numerous advantages over alternative methods of nerve localization (neurostimulation or paraesthesia). However, nerve detection is one of the most tasks that anaesthetists can encounter in the UGRA procedure. Computer aided system that can detect automatically region of nerve, would help practitioner to concentrate more in anaesthetic delivery. In this paper we propose a new method based on deep learning combined with spatiotemporal information to robustly segment the nerve region. The proposed method is based on two phases, localisation and segmentation. The first phase, consists in using convolutional neural network combined with spatial and temporal consistency to detect the nerve zone. The second phase utilises active contour model to delineate the region of interest. Obtained results show the validity of the proposed approach and its robustness.
ER  -


TY  - Preprint
T1  - Addressing Item-Cold Start Problem in Recommendation Systems using Model Based Approach and Deep Learning
A1  - Ivica ObadiÄ
A1  - Gjorgji Madjarov
A1  - Ivica Dimitrovski
A1  - Dejan Gjorgjevikj
JO  - ArXiv e-prints
Y1  - 18 June, 2017
UR  - https://arxiv.org/abs/1706.05730
N2  - Traditional recommendation systems rely on past usage data in order to generate new recommendations. Those approaches fail to generate sensible recommendations for new users and items into the system due to missing information about their past interactions. In this paper, we propose a solution for successfully addressing item-cold start problem which uses model-based approach and recent advances in deep learning. In particular, we use latent factor model for recommendation, and predict the latent factors from item&#39;s descriptions using convolutional neural network when they cannot be obtained from usage data. Latent factors obtained by applying matrix factorization to the available usage data are used as ground truth to train the convolutional neural network. To create latent factor representations for the new items, the convolutional neural network uses their textual description. The results from the experiments reveal that the proposed approach significantly outperforms several baseline estimators.
ER  -


TY  - Preprint
T1  - Towards the Improvement of Automated Scientific Document Categorization by Deep Learning
A1  - Thomas Krause
JO  - ArXiv e-prints
Y1  - 18 June, 2017
UR  - https://arxiv.org/abs/1706.05719
N2  - This master thesis describes an algorithm for automated categorization of scientific documents using deep learning techniques and compares the results to the results of existing classification algorithms. As an additional goal a reusable API is to be developed allowing the automation of classification tasks in existing software. A design will be proposed using a convolutional neural network as a classifier and integrating this into a REST based API. This is then used as the basis for an actual proof of concept implementation presented as well in this thesis. It will be shown that the deep learning classifier provides very good result in the context of multi-class document categorization and that it is feasible to integrate such classifiers into a larger ecosystem using REST based services.
ER  -


TY  - Preprint
T1  - An Overview of Multi-Task Learning in Deep Neural Networks
A1  - Sebastian Ruder
JO  - ArXiv e-prints
Y1  - 15 June, 2017
UR  - https://arxiv.org/abs/1706.05098
N2  - Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.
ER  -


TY  - Preprint
T1  - Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning
A1  - Junhyuk Oh
A1  - Satinder Singh
A1  - Honglak Lee
A1  - Pushmeet Kohli
JO  - ArXiv e-prints
Y1  - 6 November, 2017
UR  - https://arxiv.org/abs/1706.05064
N2  - As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.
ER  -


TY  - Preprint
T1  - Learning Deep ResNet Blocks Sequentially using Boosting Theory
A1  - Furong Huang
A1  - Jordan Ash
A1  - John Langford
A1  - Robert Schapire
JO  - ArXiv e-prints
Y1  - 14 June, 2018
UR  - https://arxiv.org/abs/1706.04964
N2  - Deep neural networks are known to be difficult to train due to the instability of back-propagation. A deep \emph{residual network} (ResNet) with identity loops remedies this by stabilizing gradient computations. We prove a boosting theory for the ResNet architecture. We construct $T$ weak module classifiers, each contains two of the $T$ layers, such that the combined strong learner is a ResNet. Therefore, we introduce an alternative Deep ResNet training algorithm, \emph{BoostResNet}, which is particularly suitable in non-differentiable architectures. Our proposed algorithm merely requires a sequential training of $T$ &#34;shallow ResNets&#34; which are inexpensive. We prove that the training error decays exponentially with the depth $T$ if the \emph{weak module classifiers} that we train perform slightly better than some weak baseline. In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition. Our results apply to general multi-class ResNets. A generalization error bound based on margin theory is proved and suggests ResNet&#39;s resistant to overfitting under network with $l_1$ norm bounded weights.
ER  -


TY  - Preprint
T1  - Suggestive Annotation: A Deep Active Learning Framework for Biomedical Image Segmentation
A1  - Lin Yang
A1  - Yizhe Zhang
A1  - Jianxu Chen
A1  - Siyuan Zhang
A1  - Danny Z. Chen
JO  - ArXiv e-prints
Y1  - 15 June, 2017
UR  - https://arxiv.org/abs/1706.04737
N2  - Image segmentation is a fundamental problem in biomedical image analysis. Recent advances in deep learning have achieved promising results on many biomedical image segmentation benchmarks. However, due to large variations in biomedical images (different modalities, image settings, objects, noise, etc), to utilize deep learning on a new application, it usually needs a new set of training data. This can incur a great deal of annotation effort and cost, because only biomedical experts can annotate effectively, and often there are too many instances in images (e.g., cells) to annotate. In this paper, we aim to address the following question: With limited effort (e.g., time) for annotation, what instances should be annotated in order to attain the best performance? We present a deep active learning framework that combines fully convolutional network (FCN) and active learning to significantly reduce annotation effort by making judicious suggestions on the most effective annotation areas. We utilize uncertainty and similarity information provided by FCN and formulate a generalized version of the maximum set cover problem to determine the most representative and uncertain areas for annotation. Extensive experiments using the 2015 MICCAI Gland Challenge dataset and a lymph node ultrasound image segmentation dataset show that, using annotation suggestions by our method, state-of-the-art segmentation performance can be achieved by using only 50% of training data.
ER  -


TY  - Preprint
T1  - Modeling Multimodal Clues in a Hybrid Deep Learning Framework for Video Classification
A1  - Yu-Gang Jiang
A1  - Zuxuan Wu
A1  - Jinhui Tang
A1  - Zechao Li
A1  - Xiangyang Xue
A1  - Shih-Fu Chang
JO  - ArXiv e-prints
Y1  - 14 June, 2017
UR  - https://arxiv.org/abs/1706.04508
N2  - Videos are inherently multimodal. This paper studies the problem of how to fully exploit the abundant multimodal clues for improved video categorization. We introduce a hybrid deep learning framework that integrates useful clues from multiple modalities, including static spatial appearance information, motion patterns within a short time window, audio information as well as long-range temporal dynamics. More specifically, we utilize three Convolutional Neural Networks (CNNs) operating on appearance, motion and audio signals to extract their corresponding features. We then employ a feature fusion network to derive a unified representation with an aim to capture the relationships among features. Furthermore, to exploit the long-range temporal dynamics in videos, we apply two Long Short Term Memory networks with extracted appearance and motion features as inputs. Finally, we also propose to refine the prediction scores by leveraging contextual relationships among video semantics. The hybrid deep learning framework is able to exploit a comprehensive set of multimodal features for video classification. Through an extensive set of experiments, we demonstrate that (1) LSTM networks which model sequences in an explicitly recurrent manner are highly complementary with CNN models; (2) the feature fusion network which produces a fused representation through modeling feature relationships outperforms alternative fusion strategies; (3) the semantic context of video classes can help further refine the predictions for improved performance. Experimental results on two challenging benchmarks, the UCF-101 and the Columbia Consumer Videos (CCV), provide strong quantitative evidence that our framework achieves promising results: $93.1\%$ on the UCF-101 and $84.5\%$ on the CCV, outperforming competing methods with clear margins.
ER  -


TY  - Preprint
T1  - Learning and Evaluating Musical Features with Deep Autoencoders
A1  - Mason Bretan
A1  - Sageev Oore
A1  - Doug Eck
A1  - Larry Heck
JO  - ArXiv e-prints
Y1  - 15 June, 2017
UR  - https://arxiv.org/abs/1706.04486
N2  - In this work we describe and evaluate methods to learn musical embeddings. Each embedding is a vector that represents four contiguous beats of music and is derived from a symbolic representation. We consider autoencoding-based methods including denoising autoencoders, and context reconstruction, and evaluate the resulting embeddings on a forward prediction and a classification task.
ER  -


TY  - Preprint
T1  - $Î½$-net: Deep Learning for Generalized Biventricular Cardiac Mass and Function Parameters
A1  - Hinrich B Winther
A1  - Christian Hundt
A1  - Bertil Schmidt
A1  - Christoph Czerner
A1  - Johann Bauersachs
A1  - Frank Wacker
A1  - Jens Vogel-Claussen
JO  - ArXiv e-prints
Y1  - 14 June, 2017
UR  - https://arxiv.org/abs/1706.04397
N2  - Background: Cardiac MRI derived biventricular mass and function parameters, such as end-systolic volume (ESV), end-diastolic volume (EDV), ejection fraction (EF), stroke volume (SV), and ventricular mass (VM) are clinically well established. Image segmentation can be challenging and time-consuming, due to the complex anatomy of the human heart.
ER  -


TY  - Preprint
T1  - When Image Denoising Meets High-Level Vision Tasks: A Deep Learning Approach
A1  - Ding Liu
A1  - Bihan Wen
A1  - Xianming Liu
A1  - Zhangyang Wang
A1  - Thomas S. Huang
JO  - ArXiv e-prints
Y1  - 16 April, 2018
UR  - https://arxiv.org/abs/1706.04284
N2  - Conventionally, image denoising and high-level vision tasks are handled separately in computer vision. In this paper, we cope with the two jointly and explore the mutual influence between them. First we propose a convolutional neural network for image denoising which achieves the state-of-the-art performance. Second we propose a deep neural network solution that cascades two modules for image denoising and various high-level tasks, respectively, and use the joint loss for updating only the denoising network via back-propagation. We demonstrate that on one hand, the proposed denoiser has the generality to overcome the performance degradation of different high-level vision tasks. On the other hand, with the guidance of high-level vision information, the denoising network can generate more visually appealing results. To the best of our knowledge, this is the first work investigating the benefit of exploiting image semantics simultaneously for image denoising and high-level vision tasks via deep learning. The code is available online https://github.com/Ding-Liu/DeepDenoising.
ER  -


TY  - Preprint
T1  - von Mises-Fisher Mixture Model-based Deep learning: Application to Face Verification
A1  - Md. Abul Hasnat
A1  - Julien BohnÃ©
A1  - Jonathan Milgram
A1  - StÃ©phane Gentric
A1  - Liming Chen
JO  - ArXiv e-prints
Y1  - 31 December, 2017
UR  - https://arxiv.org/abs/1706.04264
N2  - A number of pattern recognition tasks, \textit{e.g.}, face verification, can be boiled down to classification or clustering of unit length directional feature vectors whose distance can be simply computed by their angle. In this paper, we propose the von Mises-Fisher (vMF) mixture model as the theoretical foundation for an effective deep-learning of such directional features and derive a novel vMF Mixture Loss and its corresponding vMF deep features. The proposed vMF feature learning achieves the characteristics of discriminative learning, \textit{i.e.}, compacting the instances of the same class while increasing the distance of instances from different classes. Moreover, it subsumes a number of popular loss functions as well as an effective method in deep learning, namely normalization. We conduct extensive experiments on face verification using 4 different challenging face datasets, \textit{i.e.}, LFW, YouTube faces, CACD and IJB-A. Results show the effectiveness and excellent generalization ability of the proposed approach as it achieves state-of-the-art results on the LFW, YouTube faces and CACD datasets and competitive results on the IJB-A dataset.
ER  -


TY  - Preprint
T1  - Temporally Efficient Deep Learning with Spikes
A1  - Peter O&#39;Connor
A1  - Efstratios Gavves
A1  - Max Welling
JO  - ArXiv e-prints
Y1  - 13 June, 2017
UR  - https://arxiv.org/abs/1706.04159
N2  - The vast majority of natural sensory data is temporally redundant. Video frames or audio samples which are sampled at nearby points in time tend to have similar values. Typically, deep learning algorithms take no advantage of this redundancy to reduce computation. This can be an obscene waste of energy. We present a variant on backpropagation for neural networks in which computation scales with the rate of change of the data - not the rate at which we process the data. We do this by having neurons communicate a combination of their state, and their temporal change in state. Intriguingly, this simple communication rule give rise to units that resemble biologically-inspired leaky integrate-and-fire neurons, and to a weight-update rule that is equivalent to a form of Spike-Timing Dependent Plasticity (STDP), a synaptic learning rule observed in the brain. We demonstrate that on MNIST and a temporal variant of MNIST, our algorithm performs about as well as a Multilayer Perceptron trained with backpropagation, despite only communicating discrete values between layers.
ER  -


TY  - Preprint
T1  - Deep Learning-Based Food Calorie Estimation Method in Dietary Assessment
A1  - Yanchao Liang
A1  - Jianhua Li
JO  - ArXiv e-prints
Y1  - 18 February, 2018
UR  - https://arxiv.org/abs/1706.04062
N2  - Obesity treatment requires obese patients to record all food intakes per day. Computer vision has been introduced to estimate calories from food images. In order to increase accuracy of detection and reduce the error of volume estimation in food calorie estimation, we present our calorie estimation method in this paper. To estimate calorie of food, a top view and side view is needed. Faster R-CNN is used to detect the food and calibration object. GrabCut algorithm is used to get each food&#39;s contour. Then the volume is estimated with the food and corresponding object. Finally we estimate each food&#39;s calorie. And the experiment results show our estimation method is effective.
ER  -


TY  - Preprint
T1  - Optimal Auctions through Deep Learning
A1  - Paul DÃ¼tting
A1  - Zhe Feng
A1  - Harikrishna Narasimhan
A1  - David C. Parkes
JO  - ArXiv e-prints
Y1  - 19 March, 2018
UR  - https://arxiv.org/abs/1706.03459
N2  - Designing an auction that maximizes expected revenue is an intricate task. Indeed, as of today--despite major efforts and impressive progress over the past few years--only the single-item case is fully understood. In this work, we initiate the exploration of the use of tools from deep learning on this topic. The design objective is revenue optimal, dominant-strategy incentive compatible auctions. We show that multi-layer neural networks can learn almost-optimal auctions for settings for which there are analytical solutions, such as Myerson&#39;s auction for a single item, Manelli and Vincent&#39;s mechanism for a single bidder with additive preferences over two items, or Yao&#39;s auction for two additive bidders with binary support distributions and multiple items, even if no prior knowledge about the form of optimal auctions is encoded in the network and the only feedback during training is revenue and regret. We further show how characterization results, even rather implicit ones such as Rochet&#39;s characterization through induced utilities and their gradients, can be leveraged to obtain more precise fits to the optimal design. We conclude by demonstrating the potential of deep learning for deriving optimal auctions with high revenue for poorly understood problems.
ER  -


TY  - Preprint
T1  - Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model
A1  - Xingjian Shi
A1  - Zhihan Gao
A1  - Leonard Lausen
A1  - Hao Wang
A1  - Dit-Yan Yeung
A1  - Wai-kin Wong
A1  - Wang-chun Woo
JO  - ArXiv e-prints
Y1  - 5 October, 2017
UR  - https://arxiv.org/abs/1706.03458
N2  - With the goal of making high-resolution forecasts of regional rainfall, precipitation nowcasting has become an important and fundamental technology underlying various public services ranging from rainstorm warnings to flight safety. Recently, the Convolutional LSTM (ConvLSTM) model has been shown to outperform traditional optical flow based methods for precipitation nowcasting, suggesting that deep learning models have a huge potential for solving the problem. However, the convolutional recurrence structure in ConvLSTM-based models is location-invariant while natural motion and transformation (e.g., rotation) are location-variant in general. Furthermore, since deep-learning-based precipitation nowcasting is a newly emerging area, clear evaluation protocols have not yet been established. To address these problems, we propose both a new model and a benchmark for precipitation nowcasting. Specifically, we go beyond ConvLSTM and propose the Trajectory GRU (TrajGRU) model that can actively learn the location-variant structure for recurrent connections. Besides, we provide a benchmark that includes a real-world large-scale dataset from the Hong Kong Observatory, a new training loss, and a comprehensive evaluation protocol to facilitate future research and gauge the state of the art.
ER  -


TY  - Preprint
T1  - Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis
A1  - Benjamin Shickel
A1  - Patrick Tighe
A1  - Azra Bihorac
A1  - Parisa Rashidi
JO  - ArXiv e-prints
Y1  - 23 February, 2018
UR  - https://arxiv.org/abs/1706.03446
N2  - The past decade has seen an explosion in the amount of digital information stored in electronic health records (EHR). While primarily designed for archiving patient clinical information and administrative healthcare tasks, many researchers have found secondary use of these records for various clinical informatics tasks. Over the same period, the machine learning community has seen widespread advances in deep learning techniques, which also have been successfully applied to the vast amount of EHR data. In this paper, we review these deep EHR systems, examining architectures, technical aspects, and clinical applications. We also identify shortcomings of current techniques and discuss avenues of future research for EHR-based deep learning.
ER  -


TY  - Preprint
T1  - Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters
A1  - Hao Zhang
A1  - Zeyu Zheng
A1  - Shizhen Xu
A1  - Wei Dai
A1  - Qirong Ho
A1  - Xiaodan Liang
A1  - Zhiting Hu
A1  - Jinliang Wei
A1  - Pengtao Xie
A1  - Eric P. Xing
JO  - ArXiv e-prints
Y1  - 10 June, 2017
UR  - https://arxiv.org/abs/1706.03292
N2  - Deep learning models can take weeks to train on a single GPU-equipped machine, necessitating scaling out DL training to a GPU-cluster. However, current distributed DL implementations can scale poorly due to substantial parameter synchronization over the network, because the high throughput of GPUs allows more data batches to be processed per unit time than CPUs, leading to more frequent network synchronization. We present Poseidon, an efficient communication architecture for distributed DL on GPUs. Poseidon exploits the layered model structures in DL programs to overlap communication and computation, reducing bursty network communication. Moreover, Poseidon uses a hybrid communication scheme that optimizes the number of bytes required to synchronize each layer, according to layer properties and the number of machines. We show that Poseidon is applicable to different DL frameworks by plugging Poseidon into Caffe and TensorFlow. We show that Poseidon enables Caffe and TensorFlow to achieve 15.5x speed-up on 16 single-GPU machines, even with limited bandwidth (10GbE) and the challenging VGG19-22K network for image classification. Moreover, Poseidon-enabled TensorFlow achieves 31.5x speed-up with 32 single-GPU machines on Inception-V3, a 50% improvement over the open-source TensorFlow (20x speed-up).
ER  -


TY  - Preprint
T1  - ACCNet: Actor-Coordinator-Critic Net for &#34;Learning-to-Communicate&#34; with Deep Multi-agent Reinforcement Learning
A1  - Hangyu Mao
A1  - Zhibo Gong
A1  - Yan Ni
A1  - Zhen Xiao
JO  - ArXiv e-prints
Y1  - 29 October, 2017
UR  - https://arxiv.org/abs/1706.03235
N2  - Communication is a critical factor for the big multi-agent world to stay organized and productive. Typically, most previous multi-agent &#34;learning-to-communicate&#34; studies try to predefine the communication protocols or use technologies such as tabular reinforcement learning and evolutionary algorithm, which can not generalize to changing environment or large collection of agents.
ER  -


TY  - Preprint
T1  - Toward Optimal Run Racing: Application to Deep Learning Calibration
A1  - Olivier Bousquet
A1  - Sylvain Gelly
A1  - Karol Kurach
A1  - Marc Schoenauer
A1  - Michele Sebag
A1  - Olivier Teytaud
A1  - Damien Vincent
JO  - ArXiv e-prints
Y1  - 20 June, 2017
UR  - https://arxiv.org/abs/1706.03199
N2  - This paper aims at one-shot learning of deep neural nets, where a highly parallel setting is considered to address the algorithm calibration problem - selecting the best neural architecture and learning hyper-parameter values depending on the dataset at hand. The notoriously expensive calibration problem is optimally reduced by detecting and early stopping non-optimal runs. The theoretical contribution regards the optimality guarantees within the multiple hypothesis testing framework. Experimentations on the Cifar10, PTB and Wiki benchmarks demonstrate the relevance of the approach with a principled and consistent improvement on the state of the art with no extra hyper-parameter.
ER  -


TY  - Preprint
T1  - Direct detection of pixel-level myocardial infarction areas via a deep-learning algorithm
A1  - Chenchu Xu
A1  - Lei Xu
A1  - Zhifan Gao
A1  - Shen zhao
A1  - Heye Zhang
A1  - Yanping Zhang
A1  - Xiuquan Du
A1  - Shu Zhao
A1  - Dhanjoo Ghista
A1  - Shuo Li
JO  - ArXiv e-prints
Y1  - 10 June, 2017
UR  - https://arxiv.org/abs/1706.03182
N2  - Accurate detection of the myocardial infarction (MI) area is crucial for early diagnosis planning and follow-up management. In this study, we propose an end-to-end deep-learning algorithm framework (OF-RNN ) to accurately detect the MI area at the pixel level. Our OF-RNN consists of three different function layers: the heart localization layers, which can accurately and automatically crop the region-of-interest (ROI) sequences, including the left ventricle, using the whole cardiac magnetic resonance image sequences; the motion statistical layers, which are used to build a time-series architecture to capture two types of motion features (at the pixel-level) by integrating the local motion features generated by long short-term memory-recurrent neural networks and the global motion features generated by deep optical flows from the whole ROI sequence, which can effectively characterize myocardial physiologic function; and the fully connected discriminate layers, which use stacked auto-encoders to further learn these features, and they use a softmax classifier to build the correspondences from the motion features to the tissue identities (infarction or not) for each pixel. Through the seamless connection of each layer, our OF-RNN can obtain the area, position, and shape of the MI for each patient. Our proposed framework yielded an overall classification accuracy of 94.35% at the pixel level, from 114 clinical subjects. These results indicate the potential of our proposed method in aiding standardized MI assessments.
ER  -


TY  - Preprint
T1  - Deep Learning for Isotropic Super-Resolution from Non-Isotropic 3D Electron Microscopy
A1  - Larissa Heinrich
A1  - John A. Bogovic
A1  - Stephan Saalfeld
JO  - ArXiv e-prints
Y1  - 9 June, 2017
UR  - https://arxiv.org/abs/1706.03142
N2  - The most sophisticated existing methods to generate 3D isotropic super-resolution (SR) from non-isotropic electron microscopy (EM) are based on learned dictionaries. Unfortunately, none of the existing methods generate practically satisfying results. For 2D natural images, recently developed super-resolution methods that use deep learning have been shown to significantly outperform the previous state of the art.
ER  -


TY  - Preprint
T1  - An Ensemble Deep Learning Based Approach for Red Lesion Detection in Fundus Images
A1  - JosÃ© Ignacio Orlando
A1  - Elena Prokofyeva
A1  - Mariana del Fresno
A1  - Matthew B. Blaschko
JO  - ArXiv e-prints
Y1  - 12 October, 2017
UR  - https://arxiv.org/abs/1706.03008
N2  - Diabetic retinopathy is one of the leading causes of preventable blindness in the world. Its earliest sign are red lesions, a general term that groups both microaneurysms and hemorrhages. In daily clinical practice, these lesions are manually detected by physicians using fundus photographs. However, this task is tedious and time consuming, and requires an intensive effort due to the small size of the lesions and their lack of contrast. Computer-assisted diagnosis of DR based on red lesion detection is being actively explored due to its improvement effects both in clinicians consistency and accuracy. Several methods for detecting red lesions have been proposed in the literature, most of them based on characterizing lesion candidates using hand crafted features, and classifying them into true or false positive detections. Deep learning based approaches, by contrast, are scarce in this domain due to the high expense of annotating the lesions manually. In this paper we propose a novel method for red lesion detection based on combining both deep learned and domain knowledge. Features learned by a CNN are augmented by incorporating hand crafted features. Such ensemble vector of descriptors is used afterwards to identify true lesion candidates using a Random Forest classifier. We empirically observed that combining both sources of information significantly improve results with respect to using each approach separately. Furthermore, our method reported the highest performance on a per-lesion basis on DIARETDB1 and e-ophtha, and for screening and need for referral on MESSIDOR compared to a second human expert. Results highlight the fact that integrating manually engineered approaches with deep learned features is relevant to improve results when the networks are trained from lesion-level annotated data. An open source implementation of our system is publicly available online.
ER  -


TY  - Preprint
T1  - Learning Deep Representations for Scene Labeling with Semantic Context Guided Supervision
A1  - Zhe Wang
A1  - Hongsheng Li
A1  - Wanli Ouyang
A1  - Xiaogang Wang
JO  - ArXiv e-prints
Y1  - 9 June, 2017
UR  - https://arxiv.org/abs/1706.02493
N2  - Scene labeling is a challenging classification problem where each input image requires a pixel-level prediction map. Recently, deep-learning-based methods have shown their effectiveness on solving this problem. However, we argue that the large intra-class variation provides ambiguous training information and hinders the deep models&#39; ability to learn more discriminative deep feature representations. Unlike existing methods that mainly utilize semantic context for regularizing or smoothing the prediction map, we design novel supervisions from semantic context for learning better deep feature representations. Two types of semantic context, scene names of images and label map statistics of image patches, are exploited to create label hierarchies between the original classes and newly created subclasses as the learning supervisions. Such subclasses show lower intra-class variation, and help CNN detect more meaningful visual patterns and learn more effective deep features. Novel training strategies and network structure that take advantages of such label hierarchies are introduced. Our proposed method is evaluated extensively on four popular datasets, Stanford Background (8 classes), SIFTFlow (33 classes), Barcelona (170 classes) and LM+Sun datasets (232 classes) with 3 different networks structures, and show state-of-the-art performance. The experiments show that our proposed method makes deep models learn more discriminative feature representations without increasing model size or complexity.
ER  -


TY  - Preprint
T1  - Predictive Coding-based Deep Dynamic Neural Network for Visuomotor Learning
A1  - Jungsik Hwang
A1  - Jinhyung Kim
A1  - Ahmadreza Ahmadi
A1  - Minkyu Choi
A1  - Jun Tani
JO  - ArXiv e-prints
Y1  - 7 June, 2017
UR  - https://arxiv.org/abs/1706.02444
N2  - This study presents a dynamic neural network model based on the predictive coding framework for perceiving and predicting the dynamic visuo-proprioceptive patterns. In our previous study [1], we have shown that the deep dynamic neural network model was able to coordinate visual perception and action generation in a seamless manner. In the current study, we extended the previous model under the predictive coding framework to endow the model with a capability of perceiving and predicting dynamic visuo-proprioceptive patterns as well as a capability of inferring intention behind the perceived visuomotor information through minimizing prediction error. A set of synthetic experiments were conducted in which a robot learned to imitate the gestures of another robot in a simulation environment. The experimental results showed that with given intention states, the model was able to mentally simulate the possible incoming dynamic visuo-proprioceptive patterns in a top-down process without the inputs from the external environment. Moreover, the results highlighted the role of minimizing prediction error in inferring underlying intention of the perceived visuo-proprioceptive patterns, supporting the predictive coding account of the mirror neuron systems. The results also revealed that minimizing prediction error in one modality induced the recall of the corresponding representation of another modality acquired during the consolidative learning of raw-level visuo-proprioceptive patterns.
ER  -


TY  - Preprint
T1  - Seamless Integration and Coordination of Cognitive Skills in Humanoid Robots: A Deep Learning Approach
A1  - Jungsik Hwang
A1  - Jun Tani
JO  - ArXiv e-prints
Y1  - 7 June, 2017
UR  - https://arxiv.org/abs/1706.02423
N2  - This study investigates how adequate coordination among the different cognitive processes of a humanoid robot can be developed through end-to-end learning of direct perception of visuomotor stream. We propose a deep dynamic neural network model built on a dynamic vision network, a motor generation network, and a higher-level network. The proposed model was designed to process and to integrate direct perception of dynamic visuomotor patterns in a hierarchical model characterized by different spatial and temporal constraints imposed on each level. We conducted synthetic robotic experiments in which a robot learned to read human&#39;s intention through observing the gestures and then to generate the corresponding goal-directed actions. Results verify that the proposed model is able to learn the tutored skills and to generalize them to novel situations. The model showed synergic coordination of perception, action and decision making, and it integrated and coordinated a set of cognitive skills including visual perception, intention reading, attention switching, working memory, action preparation and execution in a seamless manner. Analysis reveals that coherent internal representations emerged at each level of the hierarchy. Higher-level representation reflecting actional intention developed by means of continuous integration of the lower-level visuo-proprioceptive stream.
ER  -


TY  - Preprint
T1  - PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space
A1  - Charles R. Qi
A1  - Li Yi
A1  - Hao Su
A1  - Leonidas J. Guibas
JO  - ArXiv e-prints
Y1  - 7 June, 2017
UR  - https://arxiv.org/abs/1706.02413
N2  - Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.
ER  -


TY  - Preprint
T1  - DeepSketch2Face: A Deep Learning Based Sketching System for 3D Face and Caricature Modeling
A1  - Xiaoguang Han
A1  - Chang Gao
A1  - Yizhou Yu
JO  - ArXiv e-prints
Y1  - 7 June, 2017
UR  - https://arxiv.org/abs/1706.02042
N2  - Face modeling has been paid much attention in the field of visual computing. There exist many scenarios, including cartoon characters, avatars for social media, 3D face caricatures as well as face-related art and design, where low-cost interactive face modeling is a popular approach especially among amateur users. In this paper, we propose a deep learning based sketching system for 3D face and caricature modeling. This system has a labor-efficient sketching interface, that allows the user to draw freehand imprecise yet expressive 2D lines representing the contours of facial features. A novel CNN based deep regression network is designed for inferring 3D face models from 2D sketches. Our network fuses both CNN and shape based features of the input sketch, and has two independent branches of fully connected layers generating independent subsets of coefficients for a bilinear face representation. Our system also supports gesture based interactions for users to further manipulate initial face models. Both user studies and numerical results indicate that our sketching system can help users create face models quickly and effectively. A significantly expanded face database with diverse identities, expressions and levels of exaggeration is constructed to promote further research and evaluation of face modeling techniques.
ER  -


TY  - Preprint
T1  - Deep Learning: Generalization Requires Deep Compositional Feature Space Design
A1  - Mrinal Haloi
JO  - ArXiv e-prints
Y1  - 8 July, 2017
UR  - https://arxiv.org/abs/1706.01983
N2  - Generalization error defines the discriminability and the representation power of a deep model. In this work, we claim that feature space design using deep compositional function plays a significant role in generalization along with explicit and implicit regularizations. Our claims are being established with several image classification experiments. We show that the information loss due to convolution and max pooling can be marginalized with the compositional design, improving generalization performance. Also, we will show that learning rate decay acts as an implicit regularizer in deep model training.
ER  -


TY  - Preprint
T1  - Full Quantification of Left Ventricle via Deep Multitask Learning Network Respecting Intra- and Inter-Task Relatedness
A1  - Wufeng Xue
A1  - Andrea Lum
A1  - Ashley Mercado
A1  - Mark Landis
A1  - James Warringto
A1  - Shuo Li
JO  - ArXiv e-prints
Y1  - 14 June, 2017
UR  - https://arxiv.org/abs/1706.01912
N2  - Cardiac left ventricle (LV) quantification is among the most clinically important tasks for identification and diagnosis of cardiac diseases, yet still a challenge due to the high variability of cardiac structure and the complexity of temporal dynamics. Full quantification, i.e., to simultaneously quantify all LV indices including two areas (cavity and myocardium), six regional wall thicknesses (RWT), three LV dimensions, and one cardiac phase, is even more challenging since the uncertain relatedness intra and inter each type of indices may hinder the learning procedure from better convergence and generalization. In this paper, we propose a newly-designed multitask learning network (FullLVNet), which is constituted by a deep convolution neural network (CNN) for expressive feature embedding of cardiac structure; two followed parallel recurrent neural network (RNN) modules for temporal dynamic modeling; and four linear models for the final estimation. During the final estimation, both intra- and inter-task relatedness are modeled to enforce improvement of generalization: 1) respecting intra-task relatedness, group lasso is applied to each of the regression tasks for sparse and common feature selection and consistent prediction; 2) respecting inter-task relatedness, three phase-guided constraints are proposed to penalize violation of the temporal behavior of the obtained LV indices. Experiments on MR sequences of 145 subjects show that FullLVNet achieves high accurate prediction with our intra- and inter-task relatedness, leading to MAE of 190mm$^2$, 1.41mm, 2.68mm for average areas, RWT, dimensions and error rate of 10.4\% for the phase classification. This endows our method a great potential in comprehensive clinical assessment of global, regional and dynamic cardiac function.
ER  -


TY  - Preprint
T1  - Deep learning for extracting protein-protein interactions from biomedical literature
A1  - Yifan Peng
A1  - Zhiyong Lu
JO  - ArXiv e-prints
Y1  - 6 June, 2017
UR  - https://arxiv.org/abs/1706.01556
N2  - State-of-the-art methods for protein-protein interaction (PPI) extraction are primarily feature-based or kernel-based by leveraging lexical and syntactic information. But how to incorporate such knowledge in the recent deep learning methods remains an open question. In this paper, we propose a multichannel dependency-based convolutional neural network model (McDepCNN). It applies one channel to the embedding vector of each word in the sentence, and another channel to the embedding vector of the head of the corresponding word. Therefore, the model can use richer information obtained from different channels. Experiments on two public benchmarking datasets, AIMed and BioInfer, demonstrate that McDepCNN compares favorably to the state-of-the-art rich-feature and single-kernel based methods. In addition, McDepCNN achieves 24.4% relative improvement in F1-score over the state-of-the-art methods on cross-corpus evaluation and 12% improvement in F1-score over kernel-based methods on &#34;difficult&#34; instances. These results suggest that McDepCNN generalizes more easily over different corpora, and is capable of capturing long distance features in the sentences.
ER  -


TY  - Preprint
T1  - Time-Varying Formation Controllers for Unmanned Aerial Vehicles Using Deep Reinforcement Learning
A1  - Ronny Conde
A1  - JosÃ© RamÃ³n Llata
A1  - Carlos Torre-Ferrero
JO  - ArXiv e-prints
Y1  - 5 June, 2017
UR  - https://arxiv.org/abs/1706.01384
N2  - We consider the problem of designing scalable and portable controllers for unmanned aerial vehicles (UAVs) to reach time-varying formations as quickly as possible. This brief confirms that deep reinforcement learning can be used in a multi-agent fashion to drive UAVs to reach any formation while taking into account optimality and portability. We use a deep neural network to estimate how good a state is, so the agent can choose actions accordingly. The system is tested with different non-high-dimensional sensory inputs without any change in the neural network architecture, algorithm or hyperparameters, just with additional training.
ER  -


TY  - Preprint
T1  - Yeah, Right, Uh-Huh: A Deep Learning Backchannel Predictor
A1  - Robin Ruede
A1  - Markus MÃ¼ller
A1  - Sebastian StÃ¼ker
A1  - Alex Waibel
JO  - ArXiv e-prints
Y1  - 2 June, 2017
UR  - https://arxiv.org/abs/1706.01340
N2  - Using supporting backchannel (BC) cues can make human-computer interaction more social. BCs provide a feedback from the listener to the speaker indicating to the speaker that he is still listened to. BCs can be expressed in different ways, depending on the modality of the interaction, for example as gestures or acoustic cues. In this work, we only considered acoustic cues. We are proposing an approach towards detecting BC opportunities based on acoustic input features like power and pitch. While other works in the field rely on the use of a hand-written rule set or specialized features, we made use of artificial neural networks. They are capable of deriving higher order features from input features themselves. In our setup, we first used a fully connected feed-forward network to establish an updated baseline in comparison to our previously proposed setup. We also extended this setup by the use of Long Short-Term Memory (LSTM) networks which have shown to outperform feed-forward based setups on various tasks. Our best system achieved an F1-Score of 0.37 using power and pitch features. Adding linguistic information using word2vec, the score increased to 0.39.
ER  -


TY  - Preprint
T1  - Deep learning evaluation using deep linguistic processing
A1  - Alexander Kuhnle
A1  - Ann Copestake
JO  - ArXiv e-prints
Y1  - 12 May, 2018
UR  - https://arxiv.org/abs/1706.01322
N2  - We discuss problems with the standard approaches to evaluation for tasks like visual question answering, and argue that artificial data can be used to address these as a complement to current practice. We demonstrate that with the help of existing &#39;deep&#39; linguistic processing technology we are able to create challenging abstract datasets, which enable us to investigate the language understanding abilities of multimodal deep learning models in detail, as compared to a single performance value on a static and monolithic dataset.
ER  -


TY  - Preprint
T1  - Deep-Learning Convolutional Neural Networks for scattered shrub detection with Google Earth Imagery
A1  - Emilio Guirado
A1  - Siham Tabik
A1  - Domingo Alcaraz-Segura
A1  - Javier Cabello
A1  - Francisco Herrera
JO  - ArXiv e-prints
Y1  - 3 June, 2017
UR  - https://arxiv.org/abs/1706.00917
N2  - There is a growing demand for accurate high-resolution land cover maps in many fields, e.g., in land-use planning and biodiversity conservation. Developing such maps has been performed using Object-Based Image Analysis (OBIA) methods, which usually reach good accuracies, but require a high human supervision and the best configuration for one image can hardly be extrapolated to a different image. Recently, the deep learning Convolutional Neural Networks (CNNs) have shown outstanding results in object recognition in the field of computer vision. However, they have not been fully explored yet in land cover mapping for detecting species of high biodiversity conservation interest. This paper analyzes the potential of CNNs-based methods for plant species detection using free high-resolution Google Earth T M images and provides an objective comparison with the state-of-the-art OBIA-methods. We consider as case study the detection of Ziziphus lotus shrubs, which are protected as a priority habitat under the European Union Habitats Directive. According to our results, compared to OBIA-based methods, the proposed CNN-based detection model, in combination with data-augmentation, transfer learning and pre-processing, achieves higher performance with less human intervention and the knowledge it acquires in the first image can be transferred to other images, which makes the detection process very fast. The provided methodology can be systematically reproduced for other species detection.
ER  -


TY  - Preprint
T1  - Heterogeneous Face Attribute Estimation: A Deep Multi-Task Learning Approach
A1  - Hu Han
A1  - Anil K. Jain
A1  - Fang Wang
A1  - Shiguang Shan
A1  - Xilin Chen
JO  - ArXiv e-prints
Y1  - 28 September, 2017
UR  - https://arxiv.org/abs/1706.00906
N2  - Face attribute estimation has many potential applications in video surveillance, face retrieval, and social media. While a number of methods have been proposed for face attribute estimation, most of them did not explicitly consider the attribute correlation and heterogeneity (e.g., ordinal vs. nominal and holistic vs. local) during feature representation learning. In this paper, we present a Deep Multi-Task Learning (DMTL) approach to jointly estimate multiple heterogeneous attributes from a single face image. In DMTL, we tackle attribute correlation and heterogeneity with convolutional neural networks (CNNs) consisting of shared feature learning for all the attributes, and category-specific feature learning for heterogeneous attributes. We also introduce an unconstrained face database (LFW+), an extension of public-domain LFW, with heterogeneous demographic attributes (age, gender, and race) obtained via crowdsourcing. Experimental results on benchmarks with multiple face attributes (MORPH II, LFW+, CelebA, LFWA, and FotW) show that the proposed approach has superior performance compared to state of the art. Finally, evaluations on a public-domain face database (LAP) with a single attribute show that the proposed approach has excellent generalization ability.
ER  -


TY  - Preprint
T1  - IDK Cascades: Fast Deep Learning by Learning not to Overthink
A1  - Xin Wang
A1  - Yujia Luo
A1  - Daniel Crankshaw
A1  - Alexey Tumanov
A1  - Fisher Yu
A1  - Joseph E. Gonzalez
JO  - ArXiv e-prints
Y1  - 27 June, 2018
UR  - https://arxiv.org/abs/1706.00885
N2  - Advances in deep learning have led to substantial increases in prediction accuracy but have been accompanied by increases in the cost of rendering predictions. We conjecture that fora majority of real-world inputs, the recent advances in deep learning have created models that effectively &#34;overthink&#34; on simple inputs. In this paper, we revisit the classic question of building model cascades that primarily leverage class asymmetry to reduce cost. We introduce the &#34;I Don&#39;t Know&#34;(IDK) prediction cascades framework, a general framework to systematically compose a set of pre-trained models to accelerate inference without a loss in prediction accuracy. We propose two search based methods for constructing cascades as well as a new cost-aware objective within this framework. The proposed IDK cascade framework can be easily adopted in the existing model serving systems without additional model re-training. We evaluate the proposed techniques on a range of benchmarks to demonstrate the effectiveness of the proposed framework.
ER  -


TY  - Preprint
T1  - Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning
A1  - Shixiang Gu
A1  - Timothy Lillicrap
A1  - Zoubin Ghahramani
A1  - Richard E. Turner
A1  - Bernhard SchÃ¶lkopf
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 1 June, 2017
UR  - https://arxiv.org/abs/1706.00387
N2  - Off-policy model-free deep reinforcement learning methods using previously collected data can improve sample efficiency over on-policy policy gradient techniques. On the other hand, on-policy algorithms are often more stable and easier to use. This paper examines, both theoretically and empirically, approaches to merging on- and off-policy updates for deep reinforcement learning. Theoretical results show that off-policy updates with a value function estimator can be interpolated with on-policy policy gradient updates whilst still satisfying performance bounds. Our analysis uses control variate methods to produce a family of policy gradient algorithms, with several recently proposed algorithms being special cases of this family. We then provide an empirical comparison of these techniques with the remaining algorithmic details fixed, and show how different mixing of off-policy gradient estimates with on-policy samples contribute to improvements in empirical performance. The final algorithm provides a generalization and unification of existing deep policy gradient techniques, has theoretical guarantees on the bias introduced by off-policy updates, and improves on the state-of-the-art model-free deep RL methods on a number of OpenAI Gym continuous control benchmarks.
ER  -


TY  - Preprint
T1  - Deep Mutual Learning
A1  - Ying Zhang
A1  - Tao Xiang
A1  - Timothy M. Hospedales
A1  - Huchuan Lu
JO  - ArXiv e-prints
Y1  - 1 June, 2017
UR  - https://arxiv.org/abs/1706.00384
N2  - Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network. The typical application is to transfer from a powerful large network or ensemble to a small network, that is better suited to low-memory or fast execution requirements. In this paper, we present a deep mutual learning (DML) strategy where, rather than one way transfer between a static pre-defined teacher and a student, an ensemble of students learn collaboratively and teach each other throughout the training process. Our experiments show that a variety of network architectures benefit from mutual learning and achieve compelling results on CIFAR-100 recognition and Market-1501 person re-identification benchmarks. Surprisingly, it is revealed that no prior powerful teacher network is necessary -- mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher.
ER  -


TY  - Preprint
T1  - Deep Learning for Hate Speech Detection in Tweets
A1  - Pinkesh Badjatiya
A1  - Shashank Gupta
A1  - Manish Gupta
A1  - Vasudeva Varma
JO  - ArXiv e-prints
Y1  - 1 June, 2017
UR  - https://arxiv.org/abs/1706.00188
N2  - Hate speech detection on Twitter is critical for applications like controversial event extraction, building AI chatterbots, content recommendation, and sentiment analysis. We define this task as being able to classify a tweet as racist, sexist or neither. The complexity of the natural language constructs makes this task very challenging. We perform extensive experiments with multiple deep learning architectures to learn semantic word embeddings to handle this complexity. Our experiments on a benchmark dataset of 16K annotated tweets show that such deep learning methods outperform state-of-the-art char/word n-gram methods by ~18 F1 points.
ER  -


TY  - Preprint
T1  - Learning Time/Memory-Efficient Deep Architectures with Budgeted Super Networks
A1  - Tom Veniat
A1  - Ludovic Denoyer
JO  - ArXiv e-prints
Y1  - 22 May, 2018
UR  - https://arxiv.org/abs/1706.00046
N2  - We propose to focus on the problem of discovering neural network architectures efficient in terms of both prediction quality and cost. For instance, our approach is able to solve the following tasks: learn a neural network able to predict well in less than 100 milliseconds or learn an efficient model that fits in a 50 Mb memory. Our contribution is a novel family of models called Budgeted Super Networks (BSN). They are learned using gradient descent techniques applied on a budgeted learning objective function which integrates a maximum authorized cost, while making no assumption on the nature of this cost. We present a set of experiments on computer vision problems and analyze the ability of our technique to deal with three different costs: the computation cost, the memory consumption cost and a distributed computation cost. We particularly show that our model can discover neural network architectures that have a better accuracy than the ResNet and Convolutional Neural Fabrics architectures on CIFAR-10 and CIFAR-100, at a lower cost.
ER  -


TY  - Preprint
T1  - Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments
A1  - Zixing Zhang
A1  - JÃ¼rgen Geiger
A1  - Jouni Pohjalainen
A1  - Amr El-Desoky Mousa
A1  - Wenyu Jin
A1  - BjÃ¶rn Schuller
JO  - ArXiv e-prints
Y1  - 21 September, 2018
UR  - https://arxiv.org/abs/1705.10874
N2  - Eliminating the negative effect of non-stationary environmental noise is a long-standing research topic for automatic speech recognition that stills remains an important challenge. Data-driven supervised approaches, including ones based on deep neural networks, have recently emerged as potential alternatives to traditional unsupervised approaches and with sufficient training, can alleviate the shortcomings of the unsupervised methods in various real-life acoustic environments. In this light, we review recently developed, representative deep learning approaches for tackling non-stationary additive and convolutional degradation of speech with the aim of providing guidelines for those involved in the development of environmentally robust speech recognition systems. We separately discuss single- and multi-channel techniques developed for the front-end and back-end of speech recognition systems, as well as joint front-end and back-end training frameworks.
ER  -


TY  - Preprint
T1  - ResnetCrowd: A Residual Deep Learning Architecture for Crowd Counting, Violent Behaviour Detection and Crowd Density Level Classification
A1  - Mark Marsden
A1  - Kevin McGuinness
A1  - Suzanne Little
A1  - Noel E. O&#39;Connor
JO  - ArXiv e-prints
Y1  - 30 May, 2017
UR  - https://arxiv.org/abs/1705.10698
N2  - In this paper we propose ResnetCrowd, a deep residual architecture for simultaneous crowd counting, violent behaviour detection and crowd density level classification. To train and evaluate the proposed multi-objective technique, a new 100 image dataset referred to as Multi Task Crowd is constructed. This new dataset is the first computer vision dataset fully annotated for crowd counting, violent behaviour detection and density level classification. Our experiments show that a multi-task approach boosts individual task performance for all tasks and most notably for violent behaviour detection which receives a 9\% boost in ROC curve AUC (Area under the curve). The trained ResnetCrowd model is also evaluated on several additional benchmarks highlighting the superior generalisation of crowd analysis models trained for multiple objectives.
ER  -


TY  - Preprint
T1  - Deep Learning is Robust to Massive Label Noise
A1  - David Rolnick
A1  - Andreas Veit
A1  - Serge Belongie
A1  - Nir Shavit
JO  - ArXiv e-prints
Y1  - 26 February, 2018
UR  - https://arxiv.org/abs/1705.10694
N2  - Deep neural networks trained on large supervised datasets have led to impressive results in image classification and other tasks. However, well-annotated datasets can be time-consuming and expensive to collect, lending increased interest to larger but noisy datasets that are more easily obtained. In this paper, we show that deep neural networks are capable of generalizing from training data for which true labels are massively outnumbered by incorrect labels. We demonstrate remarkably high test performance after training on corrupted data from MNIST, CIFAR, and ImageNet. For example, on MNIST we obtain test accuracy above 90 percent even after each clean training example has been diluted with 100 randomly-labeled examples. Such behavior holds across multiple patterns of label noise, even when erroneous labels are biased towards confusing classes. We show that training in this regime requires a significant but manageable increase in dataset size that is related to the factor by which correct labels have been diluted. Finally, we provide an analysis of our results that shows how increasing noise decreases the effective batch size.
ER  -


TY  - Preprint
T1  - Fine-grained acceleration control for autonomous intersection management using deep reinforcement learning
A1  - Hamid Mirzaei
A1  - Tony Givargis
JO  - ArXiv e-prints
Y1  - 29 May, 2017
UR  - https://arxiv.org/abs/1705.10432
N2  - Recent advances in combining deep learning and Reinforcement Learning have shown a promising path for designing new control agents that can learn optimal policies for challenging control tasks. These new methods address the main limitations of conventional Reinforcement Learning methods such as customized feature engineering and small action/state space dimension requirements. In this paper, we leverage one of the state-of-the-art Reinforcement Learning methods, known as Trust Region Policy Optimization, to tackle intersection management for autonomous vehicles. We show that using this method, we can perform fine-grained acceleration control of autonomous vehicles in a grid street plan to achieve a global design objective.
ER  -


TY  - Preprint
T1  - Collaborative Deep Learning for Speech Enhancement: A Run-Time Model Selection Method Using Autoencoders
A1  - Minje Kim
JO  - ArXiv e-prints
Y1  - 29 May, 2017
UR  - https://arxiv.org/abs/1705.10385
N2  - We show that a Modular Neural Network (MNN) can combine various speech enhancement modules, each of which is a Deep Neural Network (DNN) specialized on a particular enhancement job. Differently from an ordinary ensemble technique that averages variations in models, the propose MNN selects the best module for the unseen test signal to produce a greedy ensemble. We see this as Collaborative Deep Learning (CDL), because it can reuse various already-trained DNN models without any further refining. In the proposed MNN selecting the best module during run time is challenging. To this end, we employ a speech AutoEncoder (AE) as an arbitrator, whose input and output are trained to be as similar as possible if its input is clean speech. Therefore, the AE can gauge the quality of the module-specific denoised result by seeing its AE reconstruction error, e.g. low error means that the module output is similar to clean speech. We propose an MNN structure with various modules that are specialized on dealing with a specific noise type, gender, and input Signal-to-Noise Ratio (SNR) value, and empirically prove that it almost always works better than an arbitrarily chosen DNN module and sometimes as good as an oracle result.
ER  -


TY  - Preprint
T1  - Deep Learning for Ontology Reasoning
A1  - Patrick Hohenecker
A1  - Thomas Lukasiewicz
JO  - ArXiv e-prints
Y1  - 29 May, 2017
UR  - https://arxiv.org/abs/1705.10342
N2  - In this work, we present a novel approach to ontology reasoning that is based on deep learning rather than logic-based formal reasoning. To this end, we introduce a new model for statistical relational learning that is built upon deep recursive neural networks, and give experimental evidence that it can easily compete with, or even outperform, existing logic-based reasoners on the task of ontology reasoning. More precisely, we compared our implemented system with one of the best logic-based ontology reasoners at present, RDFox, on a number of large standard benchmark datasets, and found that our system attained high reasoning quality, while being up to two orders of magnitude faster.
ER  -


TY  - Preprint
T1  - Who&#39;s to say what&#39;s funny? A computer using Language Models and Deep Learning, That&#39;s Who!
A1  - Xinru Yan
A1  - Ted Pedersen
JO  - ArXiv e-prints
Y1  - 29 May, 2017
UR  - https://arxiv.org/abs/1705.10272
N2  - Humor is a defining characteristic of human beings. Our goal is to develop methods that automatically detect humorous statements and rank them on a continuous scale. In this paper we report on results using a Language Model approach, and outline our plans for using methods from Deep Learning.
ER  -


TY  - Preprint
T1  - Deep Learning for Patient-Specific Kidney Graft Survival Analysis
A1  - Margaux Luck
A1  - Tristan Sylvain
A1  - HÃ©loÃ¯se Cardinal
A1  - Andrea Lodi
A1  - Yoshua Bengio
JO  - ArXiv e-prints
Y1  - 29 May, 2017
UR  - https://arxiv.org/abs/1705.10245
N2  - An accurate model of patient-specific kidney graft survival distributions can help to improve shared-decision making in the treatment and care of patients. In this paper, we propose a deep learning method that directly models the survival function instead of estimating the hazard function to predict survival times for graft patients based on the principle of multi-task learning. By learning to jointly predict the time of the event, and its rank in the cox partial log likelihood framework, our deep learning approach outperforms, in terms of survival time prediction quality and concordance index, other common methods for survival analysis, including the Cox Proportional Hazards model and a network trained on the cox partial log-likelihood.
ER  -


TY  - Preprint
T1  - Deep Learning for User Comment Moderation
A1  - John Pavlopoulos
A1  - Prodromos Malakasiotis
A1  - Ion Androutsopoulos
JO  - ArXiv e-prints
Y1  - 17 July, 2017
UR  - https://arxiv.org/abs/1705.09993
N2  - Experimenting with a new dataset of 1.6M user comments from a Greek news portal and existing datasets of English Wikipedia comments, we show that an RNN outperforms the previous state of the art in moderation. A deep, classification-specific attention mechanism improves further the overall performance of the RNN. We also compare against a CNN and a word-list baseline, considering both fully automatic and semi-automatic moderation.
ER  -


TY  - Preprint
T1  - A Deep Multi-View Learning Framework for City Event Extraction from Twitter Data Streams
A1  - Nazli Farajidavar
A1  - Sefki Kolozali
A1  - Payam Barnaghi
JO  - ArXiv e-prints
Y1  - 28 May, 2017
UR  - https://arxiv.org/abs/1705.09975
N2  - Cities have been a thriving place for citizens over the centuries due to their complex infrastructure. The emergence of the Cyber-Physical-Social Systems (CPSS) and context-aware technologies boost a growing interest in analysing, extracting and eventually understanding city events which subsequently can be utilised to leverage the citizen observations of their cities. In this paper, we investigate the feasibility of using Twitter textual streams for extracting city events. We propose a hierarchical multi-view deep learning approach to contextualise citizen observations of various city systems and services. Our goal has been to build a flexible architecture that can learn representations useful for tasks, thus avoiding excessive task-specific feature engineering. We apply our approach on a real-world dataset consisting of event reports and tweets of over four months from San Francisco Bay Area dataset and additional datasets collected from London. The results of our evaluations show that our proposed solution outperforms the existing models and can be used for extracting city related events with an averaged accuracy of 81% over all classes. To further evaluate the impact of our Twitter event extraction model, we have used two sources of authorised reports through collecting road traffic disruptions data from Transport for London API, and parsing the Time Out London website for sociocultural events. The analysis showed that 49.5% of the Twitter traffic comments are reported approximately five hours prior to the authorities official records. Moreover, we discovered that amongst the scheduled sociocultural event topics; tweets reporting transportation, cultural and social events are 31.75% more likely to influence the distribution of the Twitter comments than sport, weather and crime topics.
ER  -


TY  - Preprint
T1  - Deep Metric Learning and Image Classification with Nearest Neighbour Gaussian Kernels
A1  - Benjamin J. Meyer
A1  - Ben Harwood
A1  - Tom Drummond
JO  - ArXiv e-prints
Y1  - 2 July, 2018
UR  - https://arxiv.org/abs/1705.09780
N2  - We present a Gaussian kernel loss function and training algorithm for convolutional neural networks that can be directly applied to both distance metric learning and image classification problems. Our method treats all training features from a deep neural network as Gaussian kernel centres and computes loss by summing the influence of a feature&#39;s nearby centres in the feature embedding space. Our approach is made scalable by treating it as an approximate nearest neighbour search problem. We show how to make end-to-end learning feasible, resulting in a well formed embedding space, in which semantically related instances are likely to be located near one another, regardless of whether or not the network was trained on those classes. Our approach outperforms state-of-the-art deep metric learning approaches on embedding learning challenges, as well as conventional softmax classification on several datasets.
ER  -


TY  - Preprint
T1  - Deep Learning for Lung Cancer Detection: Tackling the Kaggle Data Science Bowl 2017 Challenge
A1  - Kingsley Kuan
A1  - Mathieu Ravaut
A1  - Gaurav Manek
A1  - Huiling Chen
A1  - Jie Lin
A1  - Babar Nazir
A1  - Cen Chen
A1  - Tse Chiang Howe
A1  - Zeng Zeng
A1  - Vijay Chandrasekhar
JO  - ArXiv e-prints
Y1  - 26 May, 2017
UR  - https://arxiv.org/abs/1705.09435
N2  - We present a deep learning framework for computer-aided lung cancer diagnosis. Our multi-stage framework detects nodules in 3D lung CAT scans, determines if each nodule is malignant, and finally assigns a cancer probability based on these results. We discuss the challenges and advantages of our framework. In the Kaggle Data Science Bowl 2017, our framework ranked 41st out of 1972 teams.
ER  -


TY  - Preprint
T1  - Learning to Optimize: Training Deep Neural Networks for Wireless Resource Management
A1  - Haoran Sun
A1  - Xiangyi Chen
A1  - Qingjiang Shi
A1  - Mingyi Hong
A1  - Xiao Fu
A1  - Nicholas D. Sidiropoulos
JO  - ArXiv e-prints
Y1  - 25 October, 2017
UR  - https://arxiv.org/abs/1705.09412
N2  - For the past couple of decades, numerical optimization has played a central role in addressing wireless resource management problems such as power control and beamformer design. However, optimization algorithms often entail considerable complexity, which creates a serious gap between theoretical design/analysis and real-time processing. To address this challenge, we propose a new learning-based approach. The key idea is to treat the input and output of a resource allocation algorithm as an unknown non-linear mapping and use a deep neural network (DNN) to approximate it. If the non-linear mapping can be learned accurately by a DNN of moderate size, then resource allocation can be done in almost real time -- since passing the input through a DNN only requires a small number of simple operations.
ER  -


TY  - Preprint
T1  - State Space Decomposition and Subgoal Creation for Transfer in Deep Reinforcement Learning
A1  - Himanshu Sahni
A1  - Saurabh Kumar
A1  - Farhan Tejani
A1  - Yannick Schroecker
A1  - Charles Isbell
JO  - ArXiv e-prints
Y1  - 24 May, 2017
UR  - https://arxiv.org/abs/1705.08997
N2  - Typical reinforcement learning (RL) agents learn to complete tasks specified by reward functions tailored to their domain. As such, the policies they learn do not generalize even to similar domains. To address this issue, we develop a framework through which a deep RL agent learns to generalize policies from smaller, simpler domains to more complex ones using a recurrent attention mechanism. The task is presented to the agent as an image and an instruction specifying the goal. This meta-controller guides the agent towards its goal by designing a sequence of smaller subtasks on the part of the state space within the attention, effectively decomposing it. As a baseline, we consider a setup without attention as well. Our experiments show that the meta-controller learns to create subgoals within the attention.
ER  -


TY  - Preprint
T1  - DeepSecure: Scalable Provably-Secure Deep Learning
A1  - Bita Darvish Rouhani
A1  - M. Sadegh Riazi
A1  - Farinaz Koushanfar
JO  - ArXiv e-prints
Y1  - 24 May, 2017
UR  - https://arxiv.org/abs/1705.08963
N2  - This paper proposes DeepSecure, a novel framework that enables scalable execution of the state-of-the-art Deep Learning (DL) models in a privacy-preserving setting. DeepSecure targets scenarios in which neither of the involved parties including the cloud servers that hold the DL model parameters or the delegating clients who own the data is willing to reveal their information. Our framework is the first to empower accurate and scalable DL analysis of data generated by distributed clients without sacrificing the security to maintain efficiency. The secure DL computation in DeepSecure is performed using Yao&#39;s Garbled Circuit (GC) protocol. We devise GC-optimized realization of various components used in DL. Our optimized implementation achieves more than 58-fold higher throughput per sample compared with the best-known prior solution. In addition to our optimized GC realization, we introduce a set of novel low-overhead pre-processing techniques which further reduce the GC overall runtime in the context of deep learning. Extensive evaluations of various DL applications demonstrate up to two orders-of-magnitude additional runtime improvement achieved as a result of our pre-processing methodology. This paper also provides mechanisms to securely delegate GC computations to a third party in constrained embedded settings.
ER  -


TY  - Preprint
T1  - Dynamic Occupancy Grid Prediction for Urban Autonomous Driving: A Deep Learning Approach with Fully Automatic Labeling
A1  - Stefan Hoermann
A1  - Martin Bach
A1  - Klaus Dietmayer
JO  - ArXiv e-prints
Y1  - 7 November, 2017
UR  - https://arxiv.org/abs/1705.08781
N2  - Long-term situation prediction plays a crucial role in the development of intelligent vehicles. A major challenge still to overcome is the prediction of complex downtown scenarios with multiple road users, e.g., pedestrians, bikes, and motor vehicles, interacting with each other. This contribution tackles this challenge by combining a Bayesian filtering technique for environment representation, and machine learning as long-term predictor. More specifically, a dynamic occupancy grid map is utilized as input to a deep convolutional neural network. This yields the advantage of using spatially distributed velocity estimates from a single time step for prediction, rather than a raw data sequence, alleviating common problems dealing with input time series of multiple sensors. Furthermore, convolutional neural networks have the inherent characteristic of using context information, enabling the implicit modeling of road user interaction. Pixel-wise balancing is applied in the loss function counteracting the extreme imbalance between static and dynamic cells. One of the major advantages is the unsupervised learning character due to fully automatic label generation. The presented algorithm is trained and evaluated on multiple hours of recorded sensor data and compared to Monte-Carlo simulation.
ER  -


TY  - Preprint
T1  - Continual Learning with Deep Generative Replay
A1  - Hanul Shin
A1  - Jung Kwon Lee
A1  - Jaehong Kim
A1  - Jiwon Kim
JO  - ArXiv e-prints
Y1  - 11 December, 2017
UR  - https://arxiv.org/abs/1705.08690
N2  - Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model (&#34;generator&#34;) and a task solving model (&#34;solver&#34;). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.
ER  -


TY  - Preprint
T1  - Deep Learning Improves Template Matching by Normalized Cross Correlation
A1  - Davit Buniatyan
A1  - Thomas Macrina
A1  - Dodam Ih
A1  - Jonathan Zung
A1  - H. Sebastian Seung
JO  - ArXiv e-prints
Y1  - 23 May, 2017
UR  - https://arxiv.org/abs/1705.08593
N2  - Template matching by normalized cross correlation (NCC) is widely used for finding image correspondences. We improve the robustness of this algorithm by preprocessing images with &#34;siamese&#34; convolutional networks trained to maximize the contrast between NCC values of true and false matches. The improvement is quantified using patches of brain images from serial section electron microscopy. Relative to a parameter-tuned bandpass filter, siamese convolutional networks significantly reduce false matches. Furthermore, all false matches can be eliminated by removing a tiny fraction of all matches based on NCC values. The improved accuracy of our method could be essential for connectomics, because emerging petascale datasets may require billions of template matches to assemble 2D images of serial sections into a 3D image stack. Our method is also expected to generalize to many other computer vision applications that use NCC template matching to find image correspondences.
ER  -


TY  - Preprint
T1  - Input Fast-Forwarding for Better Deep Learning
A1  - Ahmed Ibrahim
A1  - A. Lynn Abbott
A1  - Mohamed E. Hussein
JO  - ArXiv e-prints
Y1  - 23 May, 2017
UR  - https://arxiv.org/abs/1705.08479
N2  - This paper introduces a new architectural framework, known as input fast-forwarding, that can enhance the performance of deep networks. The main idea is to incorporate a parallel path that sends representations of input values forward to deeper network layers. This scheme is substantially different from &#34;deep supervision&#34; in which the loss layer is re-introduced to earlier layers. The parallel path provided by fast-forwarding enhances the training process in two ways. First, it enables the individual layers to combine higher-level information (from the standard processing path) with lower-level information (from the fast-forward path). Second, this new architecture reduces the problem of vanishing gradients substantially because the fast-forwarding path provides a shorter route for gradient backpropagation. In order to evaluate the utility of the proposed technique, a Fast-Forward Network (FFNet), with 20 convolutional layers along with parallel fast-forward paths, has been created and tested. The paper presents empirical results that demonstrate improved learning capacity of FFNet due to fast-forwarding, as compared to GoogLeNet (with deep supervision) and CaffeNet, which are 4x and 18x larger in size, respectively. All of the source code and deep learning models described in this paper will be made available to the entire research community
ER  -


TY  - Preprint
T1  - Thinking Fast and Slow with Deep Learning and Tree Search
A1  - Thomas Anthony
A1  - Zheng Tian
A1  - David Barber
JO  - ArXiv e-prints
Y1  - 3 December, 2017
UR  - https://arxiv.org/abs/1705.08439
N2  - Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (ExIt), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that ExIt outperforms REINFORCE for training a neural network to play the board game Hex, and our final tree search agent, trained tabula rasa, defeats MoHex 1.0, the most recent Olympiad Champion player to be publicly released.
ER  -


TY  - Preprint
T1  - Continuous State-Space Models for Optimal Sepsis Treatment - a Deep Reinforcement Learning Approach
A1  - Aniruddh Raghu
A1  - Matthieu Komorowski
A1  - Leo Anthony Celi
A1  - Peter Szolovits
A1  - Marzyeh Ghassemi
JO  - ArXiv e-prints
Y1  - 23 May, 2017
UR  - https://arxiv.org/abs/1705.08422
N2  - Sepsis is a leading cause of mortality in intensive care units (ICUs) and costs hospitals billions annually. Treating a septic patient is highly challenging, because individual patients respond very differently to medical interventions and there is no universally agreed-upon treatment for sepsis. Understanding more about a patient&#39;s physiological state at a given time could hold the key to effective treatment policies. In this work, we propose a new approach to deduce optimal treatment policies for septic patients by using continuous state-space models and deep reinforcement learning. Learning treatment policies over continuous spaces is important, because we retain more of the patient&#39;s physiological information. Our model is able to learn clinically interpretable treatment policies, similar in important aspects to the treatment policies of physicians. Evaluating our algorithm on past ICU patient data, we find that our model could reduce patient mortality in the hospital by up to 3.6% over observed clinical policies, from a baseline mortality of 13.7%. The learned treatment policies could be used to aid intensive care clinicians in medical decision making and improve the likelihood of patient survival.
ER  -


TY  - Preprint
T1  - Wasserstein Learning of Deep Generative Point Process Models
A1  - Shuai Xiao
A1  - Mehrdad Farajtabar
A1  - Xiaojing Ye
A1  - Junchi Yan
A1  - Le Song
A1  - Hongyuan Zha
JO  - ArXiv e-prints
Y1  - 22 May, 2017
UR  - https://arxiv.org/abs/1705.08051
N2  - Point processes are becoming very popular in modeling asynchronous sequential data due to their sound mathematical foundation and strength in modeling a variety of real-world phenomena. Currently, they are often characterized via intensity function which limits model&#39;s expressiveness due to unrealistic assumptions on its parametric form used in practice. Furthermore, they are learned via maximum likelihood approach which is prone to failure in multi-modal distributions of sequences. In this paper, we propose an intensity-free approach for point processes modeling that transforms nuisance processes to a target one. Furthermore, we train the model using a likelihood-free leveraging Wasserstein distance between point processes. Experiments on various synthetic and real-world data substantiate the superiority of the proposed point process model over conventional ones.
ER  -


TY  - Preprint
T1  - Detection Algorithms for Communication Systems Using Deep Learning
A1  - Nariman Farsad
A1  - Andrea Goldsmith
JO  - ArXiv e-prints
Y1  - 30 July, 2017
UR  - https://arxiv.org/abs/1705.08044
N2  - The design and analysis of communication systems typically rely on the development of mathematical models that describe the underlying communication channel, which dictates the relationship between the transmitted and the received signals. However, in some systems, such as molecular communication systems where chemical signals are used for transfer of information, it is not possible to accurately model this relationship. In these scenarios, because of the lack of mathematical channel models, a completely new approach to design and analysis is required. In this work, we focus on one important aspect of communication systems, the detection algorithms, and demonstrate that by borrowing tools from deep learning, it is possible to train detectors that perform well, without any knowledge of the underlying channel models. We evaluate these algorithms using experimental data that is collected by a chemical communication platform, where the channel model is unknown and difficult to model analytically. We show that deep learning algorithms perform significantly better than a simple detector that was used in previous works, which also did not assume any knowledge of the channel.
ER  -


TY  - Preprint
T1  - TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning
A1  - Wei Wen
A1  - Cong Xu
A1  - Feng Yan
A1  - Chunpeng Wu
A1  - Yandan Wang
A1  - Yiran Chen
A1  - Hai Li
JO  - ArXiv e-prints
Y1  - 28 December, 2017
UR  - https://arxiv.org/abs/1705.07878
N2  - High network communication cost for synchronizing gradients and parameters is the well-known bottleneck of distributed training. In this work, we propose TernGrad that uses ternary gradients to accelerate distributed deep learning in data parallelism. Our approach requires only three numerical levels {-1,0,1}, which can aggressively reduce the communication time. We mathematically prove the convergence of TernGrad under the assumption of a bound on gradients. Guided by the bound, we propose layer-wise ternarizing and gradient clipping to improve its convergence. Our experiments show that applying TernGrad on AlexNet does not incur any accuracy loss and can even improve accuracy. The accuracy loss of GoogLeNet induced by TernGrad is less than 2% on average. Finally, a performance model is proposed to study the scalability of TernGrad. Experiments show significant speed gains for various deep neural networks. Our source code is available.
ER  -


TY  - Preprint
T1  - Training Deep Networks without Learning Rates Through Coin Betting
A1  - Francesco Orabona
A1  - Tatiana Tommasi
JO  - ArXiv e-prints
Y1  - 4 November, 2017
UR  - https://arxiv.org/abs/1705.07795
N2  - Deep learning methods achieve state-of-the-art performance in many application scenarios. Yet, these methods require a significant amount of hyperparameters tuning in order to achieve the best results. In particular, tuning the learning rates in the stochastic optimization process is still one of the main bottlenecks. In this paper, we propose a new stochastic gradient descent procedure for deep networks that does not require any learning rate setting. Contrary to previous methods, we do not adapt the learning rates nor we make use of the assumed curvature of the objective function. Instead, we reduce the optimization process to a game of betting on a coin and propose a learning-rate-free optimal algorithm for this scenario. Theoretical convergence is proven for convex and quasi-convex functions and empirical evidence shows the advantage of our algorithm over popular stochastic gradient algorithms.
ER  -


TY  - Preprint
T1  - Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon
A1  - Xin Dong
A1  - Shangyu Chen
A1  - Sinno Jialin Pan
JO  - ArXiv e-prints
Y1  - 9 November, 2017
UR  - https://arxiv.org/abs/1705.07565
N2  - How to develop slim and accurate deep neural networks has become crucial for real- world applications, especially for those employed in embedded systems. Though previous work along this research line has shown some promising results, most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance. In this paper, we propose a new layer-wise pruning method for deep neural networks. In our proposed method, parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters. We prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer. Therefore, there is a guarantee that one only needs to perform a light retraining process on the pruned network to resume its original prediction performance. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods.
ER  -


TY  - Preprint
T1  - Boosting the accuracy of multi-spectral image pan-sharpening by learning a deep residual network
A1  - Yancong Wei
A1  - Qiangqiang Yuan
A1  - Huanfeng Shen
A1  - Liangpei Zhang
JO  - ArXiv e-prints
Y1  - 23 May, 2017
UR  - https://arxiv.org/abs/1705.07556
N2  - In the field of fusing multi-spectral and panchromatic images (Pan-sharpening), the impressive effectiveness of deep neural networks has been recently employed to overcome the drawbacks of traditional linear models and boost the fusing accuracy. However, to the best of our knowledge, existing research works are mainly based on simple and flat networks with relatively shallow architecture, which severely limited their performances. In this paper, the concept of residual learning has been introduced to form a very deep convolutional neural network to make a full use of the high non-linearity of deep learning models. By both quantitative and visual assessments on a large number of high quality multi-spectral images from various sources, it has been supported that our proposed model is superior to all mainstream algorithms included in the comparison, and achieved the highest spatial-spectral unified accuracy.
ER  -


TY  - Preprint
T1  - Shallow Updates for Deep Reinforcement Learning
A1  - Nir Levine
A1  - Tom Zahavy
A1  - Daniel J. Mankowitz
A1  - Aviv Tamar
A1  - Shie Mannor
JO  - ArXiv e-prints
Y1  - 2 November, 2017
UR  - https://arxiv.org/abs/1705.07461
N2  - Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN) have achieved state-of-the-art results in a variety of challenging, high-dimensional domains. This success is mainly attributed to the power of deep neural networks to learn rich domain representations for approximating the value function or policy. Batch reinforcement learning methods with linear representations, on the other hand, are more stable and require less hyper parameter tuning. Yet, substantial feature engineering is necessary to achieve good results. In this work we propose a hybrid approach -- the Least Squares Deep Q-Network (LS-DQN), which combines rich feature representations learned by a DRL algorithm with the stability of a linear least squares method. We do this by periodically re-training the last hidden layer of a DRL network with a batch least squares update. Key to our approach is a Bayesian regularization term for the least squares update, which prevents over-fitting to the more recent data. We tested LS-DQN on five Atari games and demonstrate significant improvement over vanilla DQN and Double-DQN. We also investigated the reasons for the superior performance of our method. Interestingly, we found that the performance improvement can be attributed to the large batch size used by the LS method when optimizing the last layer.
ER  -


TY  - Preprint
T1  - Learning to Mix n-Step Returns: Generalizing lambda-Returns for Deep Reinforcement Learning
A1  - Sahil Sharma
A1  - Girish Raguvir J
A1  - Srivatsan Ramesh
A1  - Balaraman Ravindran
JO  - ArXiv e-prints
Y1  - 5 November, 2017
UR  - https://arxiv.org/abs/1705.07445
N2  - Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks. A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using next state&#39;s value function. $Î»$-returns generalize beyond 1-step returns and strike a balance between Monte Carlo and TD learning methods. While lambda-returns have been extensively studied in RL, they haven&#39;t been explored a lot in Deep RL. This paper&#39;s first contribution is an exhaustive benchmarking of lambda-returns. Although mathematically tractable, the use of exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice. Our second major contribution is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner. This allows the agent to learn to decide how much it wants to weigh the n-step returns based targets. In contrast, lambda-returns restrict RL agents to use an exponentially decaying weighting scheme. Autodidactic returns can be used for improving any RL algorithm which uses TD learning. We empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns. We perform our experiments on the Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.
ER  -


TY  - Preprint
T1  - CrossNets: Cross-Information Flow in Deep Learning Architectures
A1  - Chirag Agarwal
A1  - Joe Klobusicky
A1  - Mehdi Sharifzhadeh
A1  - Dan Schonfeld
JO  - ArXiv e-prints
Y1  - 15 July, 2018
UR  - https://arxiv.org/abs/1705.07404
N2  - We propose a novel neural network structure called CrossNets, which considers architectures on directed acyclic graphs. This structure builds on previous generalization of sequential feed-forward models, such as ResNets, by allowing for all forward cross-connections between both adjacent and non-adjacent layers. The addition of cross-connections within the network increases the information flow across the whole network, leading to better training and testing performances. The superior performance of the network is tested against both image classification and compression tasks using various datasets, such as MNIST, FER, CIFAR-10, CIFAR-100, and SVHN. We conclude with a proof of convergence for CrossNets to a local minimum for error, where weights for connections are chosen through backpropagation with momentum.
ER  -


TY  - Preprint
T1  - Learning to Factor Policies and Action-Value Functions: Factored Action Space Representations for Deep Reinforcement learning
A1  - Sahil Sharma
A1  - Aravind Suresh
A1  - Rahul Ramesh
A1  - Balaraman Ravindran
JO  - ArXiv e-prints
Y1  - 20 May, 2017
UR  - https://arxiv.org/abs/1705.07269
N2  - Deep Reinforcement Learning (DRL) methods have performed well in an increasing numbering of high-dimensional visual decision making domains. Among all such visual decision making problems, those with discrete action spaces often tend to have underlying compositional structure in the said action space. Such action spaces often contain actions such as go left, go up as well as go diagonally up and left (which is a composition of the former two actions). The representations of control policies in such domains have traditionally been modeled without exploiting this inherent compositional structure in the action spaces. We propose a new learning paradigm, Factored Action space Representations (FAR) wherein we decompose a control policy learned using a Deep Reinforcement Learning Algorithm into independent components, analogous to decomposing a vector in terms of some orthogonal basis vectors. This architectural modification of the control policy representation allows the agent to learn about multiple actions simultaneously, while executing only one of them. We demonstrate that FAR yields considerable improvements on top of two DRL algorithms in Atari 2600: FARA3C outperforms A3C (Asynchronous Advantage Actor Critic) in 9 out of 14 tasks and FARAQL outperforms AQL (Asynchronous n-step Q-Learning) in 9 out of 13 tasks.
ER  -


TY  - Preprint
T1  - Simultaneous Multiple Surface Segmentation Using Deep Learning
A1  - Abhay Shah
A1  - Michael Abramoff
A1  - Xiaodong Wu
JO  - ArXiv e-prints
Y1  - 19 May, 2017
UR  - https://arxiv.org/abs/1705.07142
N2  - The task of automatically segmenting 3-D surfaces representing boundaries of objects is important for quantitative analysis of volumetric images, and plays a vital role in biomedical image analysis. Recently, graph-based methods with a global optimization property have been developed and optimized for various medical imaging applications. Despite their widespread use, these require human experts to design transformations, image features, surface smoothness priors, and re-design for a different tissue, organ or imaging modality. Here, we propose a Deep Learning based approach for segmentation of the surfaces in volumetric medical images, by learning the essential features and transformations from training data, without any human expert intervention. We employ a regional approach to learn the local surface profiles. The proposed approach was evaluated on simultaneous intraretinal layer segmentation of optical coherence tomography (OCT) images of normal retinas and retinas affected by age related macular degeneration (AMD). The proposed approach was validated on 40 retina OCT volumes including 20 normal and 20 AMD subjects. The experiments showed statistically significant improvement in accuracy for our approach compared to state-of-the-art graph based optimal surface segmentation with convex priors (G-OSC). A single Convolution Neural Network (CNN) was used to learn the surfaces for both normal and diseased images. The mean unsigned surface positioning errors obtained by G-OSC method 2.31 voxels (95% CI 2.02-2.60 voxels) was improved to $1.27$ voxels (95% CI 1.14-1.40 voxels) using our new approach. On average, our approach takes 94.34 s, requiring 95.35 MB memory, which is much faster than the 2837.46 s and 6.87 GB memory required by the G-OSC method on the same computer system.
ER  -


TY  - Preprint
T1  - Deep Learning as a Tool to Predict Flow Patterns in Two-Phase Flow
A1  - Mohammadmehdi Ezzatabadipour
A1  - Parth Singh
A1  - Melvin D. Robinson
A1  - Pablo Guillen-Rondon
A1  - Carlos Torres
JO  - ArXiv e-prints
Y1  - 18 May, 2017
UR  - https://arxiv.org/abs/1705.07117
N2  - In order to better model complex real-world data such as multiphase flow, one approach is to develop pattern recognition techniques and robust features that capture the relevant information. In this paper, we use deep learning methods, and in particular employ the multilayer perceptron, to build an algorithm that can predict flow pattern in twophase flow from fluid properties and pipe conditions. The preliminary results show excellent performance when compared with classical methods of flow pattern prediction.
ER  -


TY  - Preprint
T1  - ADMM-Net: A Deep Learning Approach for Compressive Sensing MRI
A1  - Yan Yang
A1  - Jian Sun
A1  - Huibin Li
A1  - Zongben Xu
JO  - ArXiv e-prints
Y1  - 19 May, 2017
UR  - https://arxiv.org/abs/1705.06869
N2  - Compressive sensing (CS) is an effective approach for fast Magnetic Resonance Imaging (MRI). It aims at reconstructing MR images from a small number of under-sampled data in k-space, and accelerating the data acquisition in MRI. To improve the current MRI system in reconstruction accuracy and speed, in this paper, we propose two novel deep architectures, dubbed ADMM-Nets in basic and generalized versions. ADMM-Nets are defined over data flow graphs, which are derived from the iterative procedures in Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing a general CS-based MRI model. They take the sampled k-space data as inputs and output reconstructed MR images. Moreover, we extend our network to cope with complex-valued MR images. In the training phase, all parameters of the nets, e.g., transforms, shrinkage functions, etc., are discriminatively trained end-to-end. In the testing phase, they have computational overhead similar to ADMM algorithm but use optimized parameters learned from the data for CS-based reconstruction task. We investigate different configurations in network structures and conduct extensive experiments on MR image reconstruction under different sampling rates. Due to the combination of the advantages in model-based approach and deep learning approach, the ADMM-Nets achieve state-of-the-art reconstruction accuracies with fast computational speed.
ER  -


TY  - Preprint
T1  - DeepXplore: Automated Whitebox Testing of Deep Learning Systems
A1  - Kexin Pei
A1  - Yinzhi Cao
A1  - Junfeng Yang
A1  - Suman Jana
JO  - ArXiv e-prints
Y1  - 24 September, 2017
UR  - https://arxiv.org/abs/1705.06640
N2  - Deep learning (DL) systems are increasingly deployed in safety- and security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system&#39;s behavior for corner case inputs are of great importance. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose erroneous behaviors for rare inputs.
ER  -


TY  - Preprint
T1  - Learning a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks
A1  - Matthias Plappert
A1  - Christian Mandery
A1  - Tamim Asfour
JO  - ArXiv e-prints
Y1  - 2 August, 2018
UR  - https://arxiv.org/abs/1705.06400
N2  - Linking human whole-body motion and natural language is of great interest for the generation of semantic representations of observed human behaviors as well as for the generation of robot behaviors based on natural language input. While there has been a large body of research in this area, most approaches that exist today require a symbolic representation of motions (e.g. in the form of motion primitives), which have to be defined a-priori or require complex segmentation algorithms. In contrast, recent advances in the field of neural networks and especially deep learning have demonstrated that sub-symbolic representations that can be learned end-to-end usually outperform more traditional approaches, for applications such as machine translation. In this paper we propose a generative model that learns a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks (RNNs) and sequence-to-sequence learning. Our approach does not require any segmentation or manual feature engineering and learns a distributed representation, which is shared for all motions and descriptions. We evaluate our approach on 2,846 human whole-body motions and 6,187 natural language descriptions thereof from the KIT Motion-Language Dataset. Our results clearly demonstrate the effectiveness of the proposed model: We show that our model generates a wide variety of realistic motions only from descriptions thereof in form of a single sentence. Conversely, our model is also capable of generating correct and detailed natural language descriptions from human motions.
ER  -


TY  - Preprint
T1  - Optimizing and Visualizing Deep Learning for Benign/Malignant Classification in Breast Tumors
A1  - Darvin Yi
A1  - Rebecca Lynn Sawyer
A1  - David Cohn III
A1  - Jared Dunnmon
A1  - Carson Lam
A1  - Xuerong Xiao
A1  - Daniel Rubin
JO  - ArXiv e-prints
Y1  - 17 May, 2017
UR  - https://arxiv.org/abs/1705.06362
N2  - Breast cancer has the highest incidence and second highest mortality rate for women in the US. Our study aims to utilize deep learning for benign/malignant classification of mammogram tumors using a subset of cases from the Digital Database of Screening Mammography (DDSM). Though it was a small dataset from the view of Deep Learning (about 1000 patients), we show that currently state of the art architectures of deep learning can find a robust signal, even when trained from scratch. Using convolutional neural networks (CNNs), we are able to achieve an accuracy of 85% and an ROC AUC of 0.91, while leading hand-crafted feature based methods are only able to achieve an accuracy of 71%. We investigate an amalgamation of architectures to show that our best result is reached with an ensemble of the lightweight GoogLe Nets tasked with interpreting both the coronal caudal view and the mediolateral oblique view, simply averaging the probability scores of both views to make the final prediction. In addition, we have created a novel method to visualize what features the neural network detects for the benign/malignant classification, and have correlated those features with well known radiological features, such as spiculation. Our algorithm significantly improves existing classification methods for mammography lesions and identifies features that correlate with established clinical markers.
ER  -


TY  - Preprint
T1  - Practical Processing of Mobile Sensor Data for Continual Deep Learning Predictions
A1  - Kleomenis Katevas
A1  - Ilias Leontiadis
A1  - Martin Pielot
A1  - Joan SerrÃ 
JO  - ArXiv e-prints
Y1  - 17 May, 2017
UR  - https://arxiv.org/abs/1705.06224
N2  - We present a practical approach for processing mobile sensor time series data for continual deep learning predictions. The approach comprises data cleaning, normalization, capping, time-based compression, and finally classification with a recurrent neural network. We demonstrate the effectiveness of the approach in a case study with 279 participants. On the basis of sparse sensor events, the network continually predicts whether the participants would attend to a notification within 10 minutes. Compared to a random baseline, the classifier achieves a 40% performance increase (AUC of 0.702) on a withheld test set. This approach allows to forgo resource-intensive, domain-specific, error-prone feature engineering, which may drastically increase the applicability of machine learning to mobile phone sensor data.
ER  -


TY  - Preprint
T1  - Subregular Complexity and Deep Learning
A1  - Enes Avcu
A1  - Chihiro Shibata
A1  - Jeffrey Heinz
JO  - ArXiv e-prints
Y1  - 14 October, 2017
UR  - https://arxiv.org/abs/1705.05940
N2  - This paper argues that the judicial use of formal language theory and grammatical inference are invaluable tools in understanding how deep neural networks can and cannot represent and learn long-term dependencies in temporal sequences. Learning experiments were conducted with two types of Recurrent Neural Networks (RNNs) on six formal languages drawn from the Strictly Local (SL) and Strictly Piecewise (SP) classes. The networks were Simple RNNs (s-RNNs) and Long Short-Term Memory RNNs (LSTMs) of varying sizes. The SL and SP classes are among the simplest in a mathematically well-understood hierarchy of subregular classes. They encode local and long-term dependencies, respectively. The grammatical inference algorithm Regular Positive and Negative Inference (RPNI) provided a baseline. According to earlier research, the LSTM architecture should be capable of learning long-term dependencies and should outperform s-RNNs. The results of these experiments challenge this narrative. First, the LSTMs&#39; performance was generally worse in the SP experiments than in the SL ones. Second, the s-RNNs out-performed the LSTMs on the most complex SP experiment and performed comparably to them on the others.
ER  -


TY  - Preprint
T1  - Learning Features for Offline Handwritten Signature Verification using Deep Convolutional Neural Networks
A1  - Luiz G. Hafemann
A1  - Robert Sabourin
A1  - Luiz S. Oliveira
JO  - ArXiv e-prints
Y1  - 16 May, 2017
UR  - https://arxiv.org/abs/1705.05787
N2  - Verifying the identity of a person using handwritten signatures is challenging in the presence of skilled forgeries, where a forger has access to a person&#39;s signature and deliberately attempt to imitate it. In offline (static) signature verification, the dynamic information of the signature writing process is lost, and it is difficult to design good feature extractors that can distinguish genuine signatures and skilled forgeries. This reflects in a relatively poor performance, with verification errors around 7% in the best systems in the literature. To address both the difficulty of obtaining good features, as well as improve system performance, we propose learning the representations from signature images, in a Writer-Independent format, using Convolutional Neural Networks. In particular, we propose a novel formulation of the problem that includes knowledge of skilled forgeries from a subset of users in the feature learning process, that aims to capture visual cues that distinguish genuine signatures and forgeries regardless of the user. Extensive experiments were conducted on four datasets: GPDS, MCYT, CEDAR and Brazilian PUC-PR datasets. On GPDS-160, we obtained a large improvement in state-of-the-art performance, achieving 1.72% Equal Error Rate, compared to 6.97% in the literature. We also verified that the features generalize beyond the GPDS dataset, surpassing the state-of-the-art performance in the other datasets, without requiring the representation to be fine-tuned to each particular dataset.
ER  -


TY  - Preprint
T1  - Research on Bi-mode Biometrics Based on Deep Learning
A1  - Hao Jiang
JO  - ArXiv e-prints
Y1  - 16 May, 2017
UR  - https://arxiv.org/abs/1705.05619
N2  - In view of the fact that biological characteristics have excellent independent distinguishing characteristics,biometric identification technology involves almost all the relevant areas of human distinction. Fingerprints, iris, face, voice-print and other biological features have been widely used in the public security departments to detect detection, mobile equipment unlock, target tracking and other fields. With the use of electronic devices more and more widely and the frequency is getting higher and higher. Only the Biometrics identification technology with excellent recognition rate can guarantee the long-term development of these fields.
ER  -


TY  - Preprint
T1  - A Deep Learning Based 6 Degree-of-Freedom Localization Method for Endoscopic Capsule Robots
A1  - Mehmet Turan
A1  - Yasin Almalioglu
A1  - Ender Konukoglu
A1  - Metin Sitti
JO  - ArXiv e-prints
Y1  - 15 May, 2017
UR  - https://arxiv.org/abs/1705.05435
N2  - We present a robust deep learning based 6 degrees-of-freedom (DoF) localization system for endoscopic capsule robots. Our system mainly focuses on localization of endoscopic capsule robots inside the GI tract using only visual information captured by a mono camera integrated to the robot. The proposed system is a 23-layer deep convolutional neural network (CNN) that is capable to estimate the pose of the robot in real time using a standard CPU. The dataset for the evaluation of the system was recorded inside a surgical human stomach model with realistic surface texture, softness, and surface liquid properties so that the pre-trained CNN architecture can be transferred confidently into a real endoscopic scenario. An average error of 7:1% and 3:4% for translation and rotation has been obtained, respectively. The results accomplished from the experiments demonstrate that a CNN pre-trained with raw 2D endoscopic images performs accurately inside the GI tract and is robust to various challenges posed by reflection distortions, lens imperfections, vignetting, noise, motion blur, low resolution, and lack of unique landmarks to track.
ER  -


TY  - Preprint
T1  - Efficient Parallel Methods for Deep Reinforcement Learning
A1  - Alfredo V. Clemente
A1  - Humberto N. CastejÃ³n
A1  - Arjun Chandra
JO  - ArXiv e-prints
Y1  - 16 May, 2017
UR  - https://arxiv.org/abs/1705.04862
N2  - We propose a novel framework for efficient parallelization of deep reinforcement learning algorithms, enabling these algorithms to learn from multiple actors on a single machine. The framework is algorithm agnostic and can be applied to on-policy, off-policy, value based and policy gradient based algorithms. Given its inherent parallelism, the framework can be efficiently implemented on a GPU, allowing the usage of powerful models while significantly reducing training time. We demonstrate the effectiveness of our framework by implementing an advantage actor-critic algorithm on a GPU, using on-policy experiences and employing synchronous updates. Our algorithm achieves state-of-the-art performance on the Atari domain after only a few hours of training. Our framework thus opens the door for much faster experimentation on demanding problem domains. Our implementation is open-source and is made public at https://github.com/alfredvc/paac
ER  -


TY  - Preprint
T1  - Revisiting IM2GPS in the Deep Learning Era
A1  - Nam Vo
A1  - Nathan Jacobs
A1  - James Hays
JO  - ArXiv e-prints
Y1  - 13 May, 2017
UR  - https://arxiv.org/abs/1705.04838
N2  - Image geolocalization, inferring the geographic location of an image, is a challenging computer vision problem with many potential applications. The recent state-of-the-art approach to this problem is a deep image classification approach in which the world is spatially divided into cells and a deep network is trained to predict the correct cell for a given image. We propose to combine this approach with the original Im2GPS approach in which a query image is matched against a database of geotagged images and the location is inferred from the retrieved set. We estimate the geographic location of a query image by applying kernel density estimation to the locations of its nearest neighbors in the reference database. Interestingly, we find that the best features for our retrieval task are derived from networks trained with classification loss even though we do not use a classification approach at test time. Training with classification loss outperforms several deep feature learning methods (e.g. Siamese networks with contrastive of triplet loss) more typical for retrieval applications. Our simple approach achieves state-of-the-art geolocalization accuracy while also requiring significantly less training data.
ER  -


TY  - Preprint
T1  - Person Re-Identification by Deep Joint Learning of Multi-Loss Classification
A1  - Wei Li
A1  - Xiatian Zhu
A1  - Shaogang Gong
JO  - ArXiv e-prints
Y1  - 22 May, 2017
UR  - https://arxiv.org/abs/1705.04724
N2  - Existing person re-identification (re-id) methods rely mostly on either localised or global feature representation alone. This ignores their joint benefit and mutual complementary effects. In this work, we show the advantages of jointly learning local and global features in a Convolutional Neural Network (CNN) by aiming to discover correlated local and global features in different context. Specifically, we formulate a method for joint learning of local and global feature selection losses designed to optimise person re-id when using only generic matching metrics such as the L2 distance. We design a novel CNN architecture for Jointly Learning Multi-Loss (JLML) of local and global discriminative feature optimisation subject concurrently to the same re-id labelled information. Extensive comparative evaluations demonstrate the advantages of this new JLML model for person re-id over a wide range of state-of-the-art re-id methods on five benchmarks (VIPeR, GRID, CUHK01, CUHK03, Market-1501).
ER  -


TY  - Preprint
T1  - Deep Learning Microscopy
A1  - Yair Rivenson
A1  - Zoltan Gorocs
A1  - Harun Gunaydin
A1  - Yibo Zhang
A1  - Hongda Wang
A1  - Aydogan Ozcan
JO  - ArXiv e-prints
Y1  - 12 May, 2017
UR  - https://arxiv.org/abs/1705.04709
N2  - We demonstrate that a deep neural network can significantly improve optical microscopy, enhancing its spatial resolution over a large field-of-view and depth-of-field. After its training, the only input to this network is an image acquired using a regular optical microscope, without any changes to its design. We blindly tested this deep learning approach using various tissue samples that are imaged with low-resolution and wide-field systems, where the network rapidly outputs an image with remarkably better resolution, matching the performance of higher numerical aperture lenses, also significantly surpassing their limited field-of-view and depth-of-field. These results are transformative for various fields that use microscopy tools, including e.g., life sciences, where optical microscopy is considered as one of the most widely used and deployed techniques. Beyond such applications, our presented approach is broadly applicable to other imaging modalities, also spanning different parts of the electromagnetic spectrum, and can be used to design computational imagers that get better and better as they continue to image specimen and establish new transformations among different modes of imaging.
ER  -


TY  - Preprint
T1  - Phase recovery and holographic image reconstruction using deep learning in neural networks
A1  - Yair Rivenson
A1  - Yibo Zhang
A1  - Harun Gunaydin
A1  - Da Teng
A1  - Aydogan Ozcan
JO  - ArXiv e-prints
Y1  - 9 May, 2017
UR  - https://arxiv.org/abs/1705.04286
N2  - Phase recovery from intensity-only measurements forms the heart of coherent imaging techniques and holography. Here we demonstrate that a neural network can learn to perform phase recovery and holographic image reconstruction after appropriate training. This deep learning-based approach provides an entirely new framework to conduct holographic imaging by rapidly eliminating twin-image and self-interference related spatial artifacts. Compared to existing approaches, this neural network based method is significantly faster to compute, and reconstructs improved phase and amplitude images of the objects using only one hologram, i.e., requires less number of measurements in addition to being computationally faster. We validated this method by reconstructing phase and amplitude images of various samples, including blood and Pap smears, and tissue sections. These results are broadly applicable to any phase recovery problem, and highlight that through machine learning challenging problems in imaging science can be overcome, providing new avenues to design powerful computational imaging systems.
ER  -


TY  - Preprint
T1  - Incremental Learning Through Deep Adaptation
A1  - Amir Rosenfeld
A1  - John K. Tsotsos
JO  - ArXiv e-prints
Y1  - 13 February, 2018
UR  - https://arxiv.org/abs/1705.04228
N2  - Given an existing trained neural network, it is often desirable to learn new capabilities without hindering performance of those already learned. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added domain, typically as many as the original network. We propose a method called \emph{Deep Adaptation Networks} (DAN) that constrains newly learned filters to be linear combinations of existing ones. DANs precisely preserve performance on the original domain, require a fraction (typically 13\%, dependent on network architecture) of the number of parameters compared to standard fine-tuning procedures and converge in less cycles of training to a comparable or better level of performance. When coupled with standard network quantization techniques, we further reduce the parameter cost to around 3\% of the original with negligible or no loss in accuracy. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method on a range of image classification tasks and explore different aspects of its behavior.
ER  -


TY  - Preprint
T1  - Why &amp; When Deep Learning Works: Looking Inside Deep Learnings
A1  - Ronny Ronen
JO  - ArXiv e-prints
Y1  - 10 May, 2017
UR  - https://arxiv.org/abs/1705.03921
N2  - The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012. We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of &#34;Why &amp; When Deep Learning works&#34;, with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potential. The output of this challenge resulted in five papers that address different facets of deep learning. These different facets include a high-level understating of why and when deep networks work (and do not work), the impact of geometry on the expressiveness of deep networks, and making deep networks interpretable.
ER  -


TY  - Preprint
T1  - Net2Vec: Deep Learning for the Network
A1  - Roberto Gonzalez
A1  - Filipe Manco
A1  - Alberto Garcia-Duran
A1  - Jose Mendes
A1  - Felipe Huici
A1  - Saverio Niccolini
A1  - Mathias Niepert
JO  - ArXiv e-prints
Y1  - 10 May, 2017
UR  - https://arxiv.org/abs/1705.03881
N2  - We present Net2Vec, a flexible high-performance platform that allows the execution of deep learning algorithms in the communication network. Net2Vec is able to capture data from the network at more than 60Gbps, transform it into meaningful tuples and apply predictions over the tuples in real time. This platform can be used for different purposes ranging from traffic classification to network performance analysis.
ER  -


TY  - Preprint
T1  - Deep Speaker Feature Learning for Text-independent Speaker Verification
A1  - Lantian Li
A1  - Yixiang Chen
A1  - Ying Shi
A1  - Zhiyuan Tang
A1  - Dong Wang
JO  - ArXiv e-prints
Y1  - 10 May, 2017
UR  - https://arxiv.org/abs/1705.03670
N2  - Recently deep neural networks (DNNs) have been used to learn speaker features. However, the quality of the learned features is not sufficiently good, so a complex back-end model, either neural or probabilistic, has to be used to address the residual uncertainty when applied to speaker verification, just as with raw features. This paper presents a convolutional time-delay deep neural network structure (CT-DNN) for speaker feature learning. Our experimental results on the Fisher database demonstrated that this CT-DNN can produce high-quality speaker features: even with a single feature (0.3 seconds including the context), the EER can be as low as 7.68%. This effectively confirmed that the speaker trait is largely a deterministic short-time property rather than a long-time distributional pattern, and therefore can be extracted from just dozens of frames.
ER  -


TY  - Preprint
T1  - A Survey of Deep Learning Methods for Relation Extraction
A1  - Shantanu Kumar
JO  - ArXiv e-prints
Y1  - 10 May, 2017
UR  - https://arxiv.org/abs/1705.03645
N2  - Relation Extraction is an important sub-task of Information Extraction which has the potential of employing deep learning (DL) models with the creation of large datasets using distant supervision. In this review, we compare the contributions and pitfalls of the various DL models that have been used for the task, to help guide the path ahead.
ER  -


TY  - Preprint
T1  - OMNIRank: Risk Quantification for P2P Platforms with Deep Learning
A1  - Honglun Zhang
A1  - Haiyang Wang
A1  - Xiaming Chen
A1  - Yongkun Wang
A1  - Yaohui Jin
JO  - ArXiv e-prints
Y1  - 26 April, 2017
UR  - https://arxiv.org/abs/1705.03497
N2  - P2P lending presents as an innovative and flexible alternative for conventional lending institutions like banks, where lenders and borrowers directly make transactions and benefit each other without complicated verifications. However, due to lack of specialized laws, delegated monitoring and effective managements, P2P platforms may spawn potential risks, such as withdraw failures, investigation involvements and even runaway bosses, which cause great losses to lenders and are especially serious and notorious in China. Although there are abundant public information and data available on the Internet related to P2P platforms, challenges of multi-sourcing and heterogeneity matter. In this paper, we promote a novel deep learning model, OMNIRank, which comprehends multi-dimensional features of P2P platforms for risk quantification and produces scores for ranking. We first construct a large-scale flexible crawling framework and obtain great amounts of multi-source heterogeneous data of domestic P2P platforms since 2007 from the Internet. Purifications like duplication and noise removal, null handing, format unification and fusion are applied to improve data qualities. Then we extract deep features of P2P platforms via text comprehension, topic modeling, knowledge graph and sentiment analysis, which are delivered as inputs to OMNIRank, a deep learning model for risk quantification of P2P platforms. Finally, according to rankings generated by OMNIRank, we conduct flourish data visualizations and interactions, providing lenders with comprehensive information supports, decision suggestions and safety guarantees.
ER  -


TY  - Preprint
T1  - Learning Deep Networks from Noisy Labels with Dropout Regularization
A1  - Ishan Jindal
A1  - Matthew Nokleby
A1  - Xuewen Chen
JO  - ArXiv e-prints
Y1  - 9 May, 2017
UR  - https://arxiv.org/abs/1705.03419
N2  - Large datasets often have unreliable labels-such as those obtained from Amazon&#39;s Mechanical Turk or social media platforms-and classifiers trained on mislabeled datasets often exhibit poor performance. We present a simple, effective technique for accounting for label noise when training deep neural networks. We augment a standard deep network with a softmax layer that models the label noise statistics. Then, we train the deep network and noise model jointly via end-to-end stochastic gradient descent on the (perhaps mislabeled) dataset. The augmented model is overdetermined, so in order to encourage the learning of a non-trivial noise model, we apply dropout regularization to the weights of the noise model during training. Numerical experiments on noisy versions of the CIFAR-10 and MNIST datasets show that the proposed dropout technique outperforms state-of-the-art methods.
ER  -


TY  - Preprint
T1  - Geometry of Optimization and Implicit Regularization in Deep Learning
A1  - Behnam Neyshabur
A1  - Ryota Tomioka
A1  - Ruslan Salakhutdinov
A1  - Nathan Srebro
JO  - ArXiv e-prints
Y1  - 8 May, 2017
UR  - https://arxiv.org/abs/1705.03071
N2  - We argue that the optimization plays a crucial role in generalization of deep learning models through implicit regularization. We do this by demonstrating that generalization ability is not controlled by network size but rather by some other implicit control. We then demonstrate how changing the empirical optimization procedure can improve generalization, even if actual optimization quality is not affected. We do so by studying the geometry of the parameter space of deep networks, and devising an optimization algorithm attuned to this geometry.
ER  -


TY  - Preprint
T1  - Real-Time User-Guided Image Colorization with Learned Deep Priors
A1  - Richard Zhang
A1  - Jun-Yan Zhu
A1  - Phillip Isola
A1  - Xinyang Geng
A1  - Angela S. Lin
A1  - Tianhe Yu
A1  - Alexei A. Efros
JO  - ArXiv e-prints
Y1  - 8 May, 2017
UR  - https://arxiv.org/abs/1705.02999
N2  - We propose a deep learning approach for user-guided image colorization. The system directly maps a grayscale image, along with sparse, local user &#34;hints&#34; to an output colorization with a Convolutional Neural Network (CNN). Rather than using hand-defined rules, the network propagates user edits by fusing low-level cues along with high-level semantic information, learned from large-scale data. We train on a million images, with simulated user inputs. To guide the user towards efficient input selection, the system recommends likely colors based on the input image and current user inputs. The colorization is performed in a single feed-forward pass, enabling real-time use. Even with randomly simulated user inputs, we show that the proposed system helps novice users quickly create realistic colorizations, and offers large improvements in colorization quality with just a minute of use. In addition, we demonstrate that the framework can incorporate other user &#34;hints&#34; to the desired colorization, showing an application to color histogram transfer. Our code and models are available at https://richzhang.github.io/ideepcolor.
ER  -


TY  - Preprint
T1  - Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression
A1  - Nilaksh Das
A1  - Madhuri Shanbhogue
A1  - Shang-Tse Chen
A1  - Fred Hohman
A1  - Li Chen
A1  - Michael E. Kounavis
A1  - Duen Horng Chau
JO  - ArXiv e-prints
Y1  - 8 May, 2017
UR  - https://arxiv.org/abs/1705.02900
N2  - Deep neural networks (DNNs) have achieved great success in solving a variety of machine learning (ML) problems, especially in the domain of image recognition. However, recent research showed that DNNs can be highly vulnerable to adversarially generated instances, which look seemingly normal to human observers, but completely confuse DNNs. These adversarial samples are crafted by adding small perturbations to normal, benign images. Such perturbations, while imperceptible to the human eye, are picked up by DNNs and cause them to misclassify the manipulated instances with high confidence. In this work, we explore and demonstrate how systematic JPEG compression can work as an effective pre-processing step in the classification pipeline to counter adversarial attacks and dramatically reduce their effects (e.g., Fast Gradient Sign Method, DeepFool). An important component of JPEG compression is its ability to remove high frequency signal components, inside square blocks of an image. Such an operation is equivalent to selective blurring of the image, helping remove additive perturbations. Further, we propose an ensemble-based technique that can be constructed quickly from a given well-performing DNN, and empirically show how such an ensemble that leverages JPEG compression can protect a model from multiple types of adversarial attacks, without requiring knowledge about the model.
ER  -


TY  - Preprint
T1  - Adaptive Traffic Signal Control: Deep Reinforcement Learning Algorithm with Experience Replay and Target Network
A1  - Juntao Gao
A1  - Yulong Shen
A1  - Jia Liu
A1  - Minoru Ito
A1  - Norio Shiratori
JO  - ArXiv e-prints
Y1  - 8 May, 2017
UR  - https://arxiv.org/abs/1705.02755
N2  - Adaptive traffic signal control, which adjusts traffic signal timing according to real-time traffic, has been shown to be an effective method to reduce traffic congestion. Available works on adaptive traffic signal control make responsive traffic signal control decisions based on human-crafted features (e.g. vehicle queue length). However, human-crafted features are abstractions of raw traffic data (e.g., position and speed of vehicles), which ignore some useful traffic information and lead to suboptimal traffic signal controls. In this paper, we propose a deep reinforcement learning algorithm that automatically extracts all useful features (machine-crafted features) from raw real-time traffic data and learns the optimal policy for adaptive traffic signal control. To improve algorithm stability, we adopt experience replay and target network mechanisms. Simulation results show that our algorithm reduces vehicle delay by up to 47% and 86% when compared to another two popular traffic signal control algorithms, longest queue first algorithm and fixed time control algorithm, respectively.
ER  -


TY  - Preprint
T1  - Handwritten Bangla Digit Recognition Using Deep Learning
A1  - Md Zahangir Alom
A1  - Paheding Sidike
A1  - Tarek M. Taha
A1  - Vijayan K. Asari
JO  - ArXiv e-prints
Y1  - 7 May, 2017
UR  - https://arxiv.org/abs/1705.02680
N2  - In spite of the advances in pattern recognition technology, Handwritten Bangla Character Recognition (HBCR) (such as alpha-numeric and special characters) remains largely unsolved due to the presence of many perplexing characters and excessive cursive in Bangla handwriting. Even the best existing recognizers do not lead to satisfactory performance for practical applications. To improve the performance of Handwritten Bangla Digit Recognition (HBDR), we herein present a new approach based on deep neural networks which have recently shown excellent performance in many pattern recognition and machine learning applications, but has not been throughly attempted for HBDR. We introduce Bangla digit recognition techniques based on Deep Belief Network (DBN), Convolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and Gaussian filters, and CNN with dropout and Gabor filters. These networks have the advantage of extracting and using feature information, improving the recognition of two dimensional shapes with a high degree of invariance to translation, scaling and other pattern distortions. We systematically evaluated the performance of our method on publicly available Bangla numeral image database named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition rate using the proposed method: CNN with Gabor features and dropout, which outperforms the state-of-the-art algorithms for HDBR.
ER  -


TY  - Preprint
T1  - A Study and Comparison of Human and Deep Learning Recognition Performance Under Visual Distortions
A1  - Samuel Dodge
A1  - Lina Karam
JO  - ArXiv e-prints
Y1  - 6 May, 2017
UR  - https://arxiv.org/abs/1705.02498
N2  - Deep neural networks (DNNs) achieve excellent performance on standard classification tasks. However, under image quality distortions such as blur and noise, classification accuracy becomes poor. In this work, we compare the performance of DNNs with human subjects on distorted images. We show that, although DNNs perform better than or on par with humans on good quality images, DNN performance is still much lower than human performance on distorted images. We additionally find that there is little correlation in errors between DNNs and human subjects. This could be an indication that the internal representation of images are different between DNNs and the human visual system. These comparisons with human performance could be used to guide future development of more robust DNNs.
ER  -


TY  - Preprint
T1  - Deep Patch Learning for Weakly Supervised Object Classification and Discovery
A1  - Peng Tang
A1  - Xinggang Wang
A1  - Zilong Huang
A1  - Xiang Bai
A1  - Wenyu Liu
JO  - ArXiv e-prints
Y1  - 5 May, 2017
UR  - https://arxiv.org/abs/1705.02429
N2  - Patch-level image representation is very important for object classification and detection, since it is robust to spatial transformation, scale variation, and cluttered background. Many existing methods usually require fine-grained supervisions (e.g., bounding-box annotations) to learn patch features, which requires a great effort to label images may limit their potential applications. In this paper, we propose to learn patch features via weak supervisions, i.e., only image-level supervisions. To achieve this goal, we treat images as bags and patches as instances to integrate the weakly supervised multiple instance learning constraints into deep neural networks. Also, our method integrates the traditional multiple stages of weakly supervised object classification and discovery into a unified deep convolutional neural network and optimizes the network in an end-to-end way. The network processes the two tasks object classification and discovery jointly, and shares hierarchical deep features. Through this jointly learning strategy, weakly supervised object classification and discovery are beneficial to each other. We test the proposed method on the challenging PASCAL VOC datasets. The results show that our method can obtain state-of-the-art performance on object classification, and very competitive results on object discovery, with faster testing speed than competitors.
ER  -


TY  - Preprint
T1  - Learning Representations of Emotional Speech with Deep Convolutional Generative Adversarial Networks
A1  - Jonathan Chang
A1  - Stefan Scherer
JO  - ArXiv e-prints
Y1  - 22 April, 2017
UR  - https://arxiv.org/abs/1705.02394
N2  - Automatically assessing emotional valence in human speech has historically been a difficult task for machine learning algorithms. The subtle changes in the voice of the speaker that are indicative of positive or negative emotional states are often &#34;overshadowed&#34; by voice characteristics relating to emotional intensity or emotional activation. In this work we explore a representation learning approach that automatically derives discriminative representations of emotional speech. In particular, we investigate two machine learning strategies to improve classifier performance: (1) utilization of unlabeled data using a deep convolutional generative adversarial network (DCGAN), and (2) multitask learning. Within our extensive experiments we leverage a multitask annotated emotional corpus as well as a large unlabeled meeting corpus (around 100 hours). Our speaker-independent classification experiments show that in particular the use of unlabeled data in our investigations improves performance of the classifiers and both fully supervised baseline approaches are outperformed considerably. We improve the classification of emotional valence on a discrete 5-point scale to 43.88% and on a 3-point scale to 49.80%, which is competitive to state-of-the-art performance.
ER  -


TY  - Preprint
T1  - SLDR-DL: A Framework for SLD-Resolution with Deep Learning
A1  - Cheng-Hao Cai
JO  - ArXiv e-prints
Y1  - 5 May, 2017
UR  - https://arxiv.org/abs/1705.02210
N2  - This paper introduces an SLD-resolution technique based on deep learning. This technique enables neural networks to learn from old and successful resolution processes and to use learnt experiences to guide new resolution processes. An implementation of this technique is named SLDR-DL. It includes a Prolog library of deep feedforward neural networks and some essential functions of resolution. In the SLDR-DL framework, users can define logical rules in the form of definite clauses and teach neural networks to use the rules in reasoning processes.
ER  -


TY  - Preprint
T1  - A Deep Learning Perspective on the Origin of Facial Expressions
A1  - Ran Breuer
A1  - Ron Kimmel
JO  - ArXiv e-prints
Y1  - 10 May, 2017
UR  - https://arxiv.org/abs/1705.01842
N2  - Facial expressions play a significant role in human communication and behavior. Psychologists have long studied the relationship between facial expressions and emotions. Paul Ekman et al., devised the Facial Action Coding System (FACS) to taxonomize human facial expressions and model their behavior. The ability to recognize facial expressions automatically, enables novel applications in fields like human-computer interaction, social gaming, and psychological research. There has been a tremendously active research in this field, with several recent papers utilizing convolutional neural networks (CNN) for feature extraction and inference. In this paper, we employ CNN understanding methods to study the relation between the features these computational networks are using, the FACS and Action Units (AU). We verify our findings on the Extended Cohn-Kanade (CK+), NovaEmotions and FER2013 datasets. We apply these models to various tasks and tests using transfer learning, including cross-dataset validation and cross-task performance. Finally, we exploit the nature of the FER based CNN models for the detection of micro-expressions and achieve state-of-the-art accuracy using a simple long-short-term-memory (LSTM) recurrent neural network (RNN).
ER  -


TY  - Preprint
T1  - Deep 360 Pilot: Learning a Deep Agent for Piloting through 360Â° Sports Video
A1  - Hou-Ning Hu
A1  - Yen-Chen Lin
A1  - Ming-Yu Liu
A1  - Hsien-Tzu Cheng
A1  - Yung-Ju Chang
A1  - Min Sun
JO  - ArXiv e-prints
Y1  - 4 May, 2017
UR  - https://arxiv.org/abs/1705.01759
N2  - Watching a 360Â° sports video requires a viewer to continuously select a viewing angle, either through a sequence of mouse clicks or head movements. To relieve the viewer from this &#34;360 piloting&#34; task, we propose &#34;deep 360 pilot&#34; -- a deep learning-based agent for piloting through 360Â° sports videos automatically. At each frame, the agent observes a panoramic image and has the knowledge of previously selected viewing angles. The task of the agent is to shift the current viewing angle (i.e. action) to the next preferred one (i.e., goal). We propose to directly learn an online policy of the agent from data. We use the policy gradient technique to jointly train our pipeline: by minimizing (1) a regression loss measuring the distance between the selected and ground truth viewing angles, (2) a smoothness loss encouraging smooth transition in viewing angle, and (3) maximizing an expected reward of focusing on a foreground object. To evaluate our method, we build a new 360-Sports video dataset consisting of five sports domains. We train domain-specific agents and achieve the best performance on viewing angle selection accuracy and transition smoothness compared to [51] and other baselines.
ER  -


TY  - Preprint
T1  - XES Tensorflow - Process Prediction using the Tensorflow Deep-Learning Framework
A1  - Joerg Evermann
A1  - Jana-Rebecca Rehse
A1  - Peter Fettke
JO  - ArXiv e-prints
Y1  - 3 May, 2017
UR  - https://arxiv.org/abs/1705.01507
N2  - Predicting the next activity of a running process is an important aspect of process management. Recently, artificial neural networks, so called deep-learning approaches, have been proposed to address this challenge. This demo paper describes a software application that applies the Tensorflow deep-learning framework to process prediction. The software application reads industry-standard XES files for training and presents the user with an easy-to-use graphical user interface for both training and prediction. The system provides several improvements over earlier work. This demo paper focuses on the software implementation and describes the architecture and user interface.
ER  -


TY  - Preprint
T1  - Detach and Adapt: Learning Cross-Domain Disentangled Deep Representation
A1  - Yen-Cheng Liu
A1  - Yu-Ying Yeh
A1  - Tzu-Chien Fu
A1  - Sheng-De Wang
A1  - Wei-Chen Chiu
A1  - Yu-Chiang Frank Wang
JO  - ArXiv e-prints
Y1  - 1 May, 2018
UR  - https://arxiv.org/abs/1705.01314
N2  - While representation learning aims to derive interpretable features for describing visual data, representation disentanglement further results in such features so that particular image attributes can be identified and manipulated. However, one cannot easily address this task without observing ground truth annotation for the training data. To address this problem, we propose a novel deep learning model of Cross-Domain Representation Disentangler (CDRD). By observing fully annotated source-domain data and unlabeled target-domain data of interest, our model bridges the information across data domains and transfers the attribute information accordingly. Thus, cross-domain joint feature disentanglement and adaptation can be jointly performed. In the experiments, we provide qualitative results to verify our disentanglement capability. Moreover, we further confirm that our model can be applied for solving classification tasks of unsupervised domain adaptation, and performs favorably against state-of-the-art image disentanglement and translation methods.
ER  -


TY  - Preprint
T1  - Amobee at SemEval-2017 Task 4: Deep Learning System for Sentiment Detection on Twitter
A1  - Alon Rozental
A1  - Daniel Fleischer
JO  - ArXiv e-prints
Y1  - 3 May, 2017
UR  - https://arxiv.org/abs/1705.01306
N2  - This paper describes the Amobee sentiment analysis system, adapted to compete in SemEval 2017 task 4. The system consists of two parts: a supervised training of RNN models based on a Twitter sentiment treebank, and the use of feedforward NN, Naive Bayes and logistic regression classifiers to produce predictions for the different sub-tasks. The algorithm reached the 3rd place on the 5-label classification task (sub-task C).
ER  -


TY  - Preprint
T1  - Navigating Occluded Intersections with Autonomous Vehicles using Deep Reinforcement Learning
A1  - David Isele
A1  - Reza Rahimi
A1  - Akansel Cosgun
A1  - Kaushik Subramanian
A1  - Kikuo Fujimura
JO  - ArXiv e-prints
Y1  - 26 February, 2018
UR  - https://arxiv.org/abs/1705.01196
N2  - Providing an efficient strategy to navigate safely through unsignaled intersections is a difficult task that requires determining the intent of other drivers. We explore the effectiveness of Deep Reinforcement Learning to handle intersection problems. Using recent advances in Deep RL, we are able to learn policies that surpass the performance of a commonly-used heuristic approach in several metrics including task completion time and goal success rate and have limited ability to generalize. We then explore a system&#39;s ability to learn active sensing behaviors to enable navigating safely in the case of occlusions. Our analysis, provides insight into the intersection handling problem, the solutions learned by the network point out several shortcomings of current rule-based methods, and the failures of our current deep reinforcement learning system point to future research directions.
ER  -


TY  - Preprint
T1  - Deep Learning in the Automotive Industry: Applications and Tools
A1  - Andre Luckow
A1  - Matthew Cook
A1  - Nathan Ashcraft
A1  - Edwin Weill
A1  - Emil Djerekarov
A1  - Bennie Vorster
JO  - ArXiv e-prints
Y1  - 30 April, 2017
UR  - https://arxiv.org/abs/1705.00346
N2  - Deep Learning refers to a set of machine learning techniques that utilize neural networks with many hidden layers for tasks, such as image classification, speech recognition, language understanding. Deep learning has been proven to be very effective in these domains and is pervasively used by many Internet services. In this paper, we describe different automotive uses cases for deep learning in particular in the domain of computer vision. We surveys the current state-of-the-art in libraries, tools and infrastructures (e.\,g.\ GPUs and clouds) for implementing, training and deploying deep neural networks. We particularly focus on convolutional neural networks and computer vision use cases, such as the visual inspection process in manufacturing plants and the analysis of social media data. To train neural networks, curated and labeled datasets are essential. In particular, both the availability and scope of such datasets is typically very limited. A main contribution of this paper is the creation of an automotive dataset, that allows us to learn and automatically recognize different vehicle properties. We describe an end-to-end deep learning application utilizing a mobile app for data collection and process support, and an Amazon-based cloud backend for storage and training. For training we evaluate the use of cloud and on-premises infrastructures (including multiple GPUs) in conjunction with different neural network architectures and frameworks. We assess both the training times as well as the accuracy of the classifier. Finally, we demonstrate the effectiveness of the trained classifier in a real world setting during manufacturing process.
ER  -


TY  - Preprint
T1  - Traffic Light Control Using Deep Policy-Gradient and Value-Function Based Reinforcement Learning
A1  - Seyed Sajad Mousavi
A1  - Michael Schukat
A1  - Enda Howley
JO  - ArXiv e-prints
Y1  - 27 May, 2017
UR  - https://arxiv.org/abs/1704.08883
N2  - Recent advances in combining deep neural network architectures with reinforcement learning techniques have shown promising potential results in solving complex control problems with high dimensional state and action spaces. Inspired by these successes, in this paper, we build two kinds of reinforcement learning algorithms: deep policy-gradient and value-function based agents which can predict the best possible traffic signal for a traffic intersection. At each time step, these adaptive traffic light control agents receive a snapshot of the current state of a graphical traffic simulator and produce control signals. The policy-gradient based agent maps its observation directly to the control signal, however the value-function based agent first estimates values for all legal control signals. The agent then selects the optimal control action with the highest value. Our methods show promising results in a traffic network simulated in the SUMO traffic simulator, without suffering from instability issues during the training process.
ER  -


TY  - Preprint
T1  - DeepCCI: End-to-end Deep Learning for Chemical-Chemical Interaction Prediction
A1  - Sunyoung Kwon
A1  - Sungroh Yoon
JO  - ArXiv e-prints
Y1  - 14 December, 2017
UR  - https://arxiv.org/abs/1704.08432
N2  - Chemical-chemical interaction (CCI) plays a key role in predicting candidate drugs, toxicity, therapeutic effects, and biological functions. In various types of chemical analyses, computational approaches are often required due to the amount of data that needs to be handled. The recent remarkable growth and outstanding performance of deep learning have attracted considerable research attention. However,even in state-of-the-art drug analysis methods, deep learning continues to be used only as a classifier, although deep learning is capable of not only simple classification but also automated feature extraction. In this paper, we propose the first end-to-end learning method for CCI, named DeepCCI. Hidden features are derived from a simplified molecular input line entry system (SMILES), which is a string notation representing the chemical structure, instead of learning from crafted features. To discover hidden representations for the SMILES strings, we use convolutional neural networks (CNNs). To guarantee the commutative property for homogeneous interaction, we apply model sharing and hidden representation merging techniques. The performance of DeepCCI was compared with a plain deep classifier and conventional machine learning methods. The proposed DeepCCI showed the best performance in all seven evaluation metrics used. In addition, the commutative property was experimentally validated. The automatically extracted features through end-to-end SMILES learning alleviates the significant efforts required for manual feature engineering. It is expected to improve prediction performance, in drug analyses.
ER  -


TY  - Preprint
T1  - On Improving Deep Reinforcement Learning for POMDPs
A1  - Pengfei Zhu
A1  - Xin Li
A1  - Pascal Poupart
A1  - Guanghui Miao
JO  - ArXiv e-prints
Y1  - 24 May, 2018
UR  - https://arxiv.org/abs/1704.07978
N2  - Deep Reinforcement Learning (RL) recently emerged as one of the most competitive approaches for learning in sequential decision making problems with fully observable environments, e.g., computer Go. However, very little work has been done in deep RL to handle partially observable environments. We propose a new architecture called Action-specific Deep Recurrent Q-Network (ADRQN) to enhance learning performance in partially observable domains. Actions are encoded by a fully connected layer and coupled with a convolutional observation to form an action-observation pair. The time series of action-observation pairs are then integrated by an LSTM layer that learns latent states based on which a fully connected layer computes Q-values as in conventional Deep Q-Networks (DQNs). We demonstrate the effectiveness of our new architecture in several partially observable domains, including flickering Atari games.
ER  -


TY  - Preprint
T1  - Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car
A1  - Mariusz Bojarski
A1  - Philip Yeres
A1  - Anna Choromanska
A1  - Krzysztof Choromanski
A1  - Bernhard Firner
A1  - Lawrence Jackel
A1  - Urs Muller
JO  - ArXiv e-prints
Y1  - 25 April, 2017
UR  - https://arxiv.org/abs/1704.07911
N2  - As part of a complete software stack for autonomous driving, NVIDIA has created a neural-network-based system, known as PilotNet, which outputs steering angles given images of the road ahead. PilotNet is trained using road images paired with the steering angles generated by a human driving a data-collection car. It derives the necessary domain knowledge by observing human drivers. This eliminates the need for human engineers to anticipate what is important in an image and foresee all the necessary rules for safe driving. Road tests demonstrated that PilotNet can successfully perform lane keeping in a wide variety of driving conditions, regardless of whether lane markings are present or not.
ER  -


TY  - Preprint
T1  - Molecular De Novo Design through Deep Reinforcement Learning
A1  - Marcus Olivecrona
A1  - Thomas Blaschke
A1  - Ola Engkvist
A1  - Hongming Chen
JO  - ArXiv e-prints
Y1  - 29 August, 2017
UR  - https://arxiv.org/abs/1704.07555
N2  - This work introduces a method to tune a sequence-based generative model for molecular de novo design that through augmented episodic likelihood can learn to generate structures with certain specified desirable properties. We demonstrate how this model can execute a range of tasks such as generating analogues to a query structure and generating compounds predicted to be active against a biological target. As a proof of principle, the model is first trained to generate molecules that do not contain sulphur. As a second example, the model is trained to generate analogues to the drug Celecoxib, a technique that could be used for scaffold hopping or library expansion starting from a single molecule. Finally, when tuning the model towards generating compounds predicted to be active against the dopamine receptor type 2, the model generates structures of which more than 95% are predicted to be active, including experimentally confirmed actives that have not been included in either the generative model nor the activity prediction model.
ER  -


TY  - Preprint
T1  - Learning of Human-like Algebraic Reasoning Using Deep Feedforward Neural Networks
A1  - Cheng-Hao Cai
A1  - Dengfeng Ke
A1  - Yanyan Xu
A1  - Kaile Su
JO  - ArXiv e-prints
Y1  - 24 April, 2017
UR  - https://arxiv.org/abs/1704.07503
N2  - There is a wide gap between symbolic reasoning and deep learning. In this research, we explore the possibility of using deep learning to improve symbolic reasoning. Briefly, in a reasoning system, a deep feedforward neural network is used to guide rewriting processes after learning from algebraic reasoning examples produced by humans. To enable the neural network to recognise patterns of algebraic expressions with non-deterministic sizes, reduced partial trees are used to represent the expressions. Also, to represent both top-down and bottom-up information of the expressions, a centralisation technique is used to improve the reduced partial trees. Besides, symbolic association vectors and rule application records are used to improve the rewriting processes. Experimental results reveal that the algebraic reasoning examples can be accurately learnt only if the feedforward neural network has enough hidden layers. Also, the centralisation technique, the symbolic association vectors and the rule application records can reduce error rates of reasoning. In particular, the above approaches have led to 4.6% error rate of reasoning on a dataset of linear equations, differentials and integrals.
ER  -


TY  - Preprint
T1  - A Review on Deep Learning Techniques Applied to Semantic Segmentation
A1  - Alberto Garcia-Garcia
A1  - Sergio Orts-Escolano
A1  - Sergiu Oprea
A1  - Victor Villena-Martinez
A1  - Jose Garcia-Rodriguez
JO  - ArXiv e-prints
Y1  - 22 April, 2017
UR  - https://arxiv.org/abs/1704.06857
N2  - Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.
ER  -


TY  - Preprint
T1  - Deep Multitask Learning for Semantic Dependency Parsing
A1  - Hao Peng
A1  - Sam Thomson
A1  - Noah A. Smith
JO  - ArXiv e-prints
Y1  - 25 April, 2017
UR  - https://arxiv.org/abs/1704.06855
N2  - We present a deep neural architecture that parses sentences into three semantic dependency graph formalisms. By using efficient, nearly arc-factored inference and a bidirectional-LSTM composed with a multi-layer perceptron, our base system is able to significantly improve the state of the art for semantic dependency parsing, without using hand-engineered features or syntax. We then explore two multitask learning approaches---one that shares parameters across formalisms, and one that uses higher-order structures to predict the graphs jointly. We find that both approaches improve performance across formalisms on average, achieving a new state of the art. Our code is open-source and available at https://github.com/Noahs-ARK/NeurboParser.
ER  -


TY  - Preprint
T1  - Deep Learning for Medical Image Processing: Overview, Challenges and Future
A1  - Muhammad Imran Razzak
A1  - Saeeda Naz
A1  - Ahmad Zaib
JO  - ArXiv e-prints
Y1  - 22 April, 2017
UR  - https://arxiv.org/abs/1704.06825
N2  - Healthcare sector is totally different from other industry. It is on high priority sector and people expect highest level of care and services regardless of cost. It did not achieve social expectation even though it consume huge percentage of budget. Mostly the interpretations of medical data is being done by medical expert. In terms of image interpretation by human expert, it is quite limited due to its subjectivity, the complexity of the image, extensive variations exist across different interpreters, and fatigue. After the success of deep learning in other real world application, it is also providing exciting solutions with good accuracy for medical imaging and is seen as a key method for future applications in health secotr. In this chapter, we discussed state of the art deep learning architecture and its optimization used for medical image segmentation and classification. In the last section, we have discussed the challenges deep learning based methods for medical imaging and open research issue.
ER  -


TY  - Preprint
T1  - Deep Learning based Isolated Arabic Scene Character Recognition
A1  - Saad Bin Ahmed
A1  - Saeeda Naz
A1  - Muhammad Imran Razzak
A1  - Rubiyah Yousaf
JO  - ArXiv e-prints
Y1  - 22 April, 2017
UR  - https://arxiv.org/abs/1704.06821
N2  - The technological advancement and sophistication in cameras and gadgets prompt researchers to have focus on image analysis and text understanding. The deep learning techniques demonstrated well to assess the potential for classifying text from natural scene images as reported in recent years. There are variety of deep learning approaches that prospects the detection and recognition of text, effectively from images. In this work, we presented Arabic scene text recognition using Convolutional Neural Networks (ConvNets) as a deep learning classifier. As the scene text data is slanted and skewed, thus to deal with maximum variations, we employ five orientations with respect to single occurrence of a character. The training is formulated by keeping filter size 3 x 3 and 5 x 5 with stride value as 1 and 2. During text classification phase, we trained network with distinct learning rates. Our approach reported encouraging results on recognition of Arabic characters from segmented Arabic scene images.
ER  -


TY  - Preprint
T1  - Modular Multi-Objective Deep Reinforcement Learning with Decision Values
A1  - Tomasz Tajmajer
JO  - ArXiv e-prints
Y1  - 22 February, 2018
UR  - https://arxiv.org/abs/1704.06676
N2  - In this work we present a method for using Deep Q-Networks (DQNs) in multi-objective environments. Deep Q-Networks provide remarkable performance in single objective problems learning from high-level visual state representations. However, in many scenarios (e.g in robotics, games), the agent needs to pursue multiple objectives simultaneously. We propose an architecture in which separate DQNs are used to control the agent&#39;s behaviour with respect to particular objectives. In this architecture we introduce decision values to improve the scalarization of multiple DQNs into a single action. Our architecture enables the decomposition of the agent&#39;s behaviour into controllable and replaceable sub-behaviours learned by distinct modules. Moreover, it allows to change the priorities of particular objectives post-learning, while preserving the overall performance of the agent. To evaluate our solution we used a game-like simulator in which an agent - provided with high-level visual input - pursues multiple objectives in a 2D world.
ER  -


TY  - Preprint
T1  - Using Mise-En-ScÃ¨ne Visual Features based on MPEG-7 and Deep Learning for Movie Recommendation
A1  - Yashar Deldjoo
A1  - Massimo Quadrana
A1  - Mehdi Elahi
A1  - Paolo Cremonesi
JO  - ArXiv e-prints
Y1  - 20 April, 2017
UR  - https://arxiv.org/abs/1704.06109
N2  - Item features play an important role in movie recommender systems, where recommendations can be generated by using explicit or implicit preferences of users on traditional features (attributes) such as tag, genre, and cast. Typically, movie features are human-generated, either editorially (e.g., genre and cast) or by leveraging the wisdom of the crowd (e.g., tag), and as such, they are prone to noise and are expensive to collect. Moreover, these features are often rare or absent for new items, making it difficult or even impossible to provide good quality recommendations.
ER  -


TY  - Preprint
T1  - Understanding the Mechanisms of Deep Transfer Learning for Medical Images
A1  - Hariharan Ravishankar
A1  - Prasad Sudhakar
A1  - Rahul Venkataramani
A1  - Sheshadri Thiruvenkadam
A1  - Pavan Annangi
A1  - Narayanan Babu
A1  - Vivek Vaidya
JO  - ArXiv e-prints
Y1  - 20 April, 2017
UR  - https://arxiv.org/abs/1704.06040
N2  - The ability to automatically learn task specific feature representations has led to a huge success of deep learning methods. When large training data is scarce, such as in medical imaging problems, transfer learning has been very effective. In this paper, we systematically investigate the process of transferring a Convolutional Neural Network, trained on ImageNet images to perform image classification, to kidney detection problem in ultrasound images. We study how the detection performance depends on the extent of transfer. We show that a transferred and tuned CNN can outperform a state-of-the-art feature engineered pipeline and a hybridization of these two techniques achieves 20\% higher performance. We also investigate how the evolution of intermediate response images from our network. Finally, we compare these responses to state-of-the-art image processing filters in order to gain greater insight into how transfer learning is able to effectively manage widely varying imaging regimes.
ER  -


TY  - Preprint
T1  - Predicting Cognitive Decline with Deep Learning of Brain Metabolism and Amyloid Imaging
A1  - Hongyoon Choi
A1  - Kyong Hwan Jin
JO  - ArXiv e-prints
Y1  - 20 April, 2017
UR  - https://arxiv.org/abs/1704.06033
N2  - For effective treatment of Alzheimer disease (AD), it is important to identify subjects who are most likely to exhibit rapid cognitive decline. Herein, we developed a novel framework based on a deep convolutional neural network which can predict future cognitive decline in mild cognitive impairment (MCI) patients using flurodeoxyglucose and florbetapir positron emission tomography (PET). The architecture of the network only relies on baseline PET studies of AD and normal subjects as the training dataset. Feature extraction and complicated image preprocessing including nonlinear warping are unnecessary for our approach. Accuracy of prediction (84.2%) for conversion to AD in MCI patients outperformed conventional feature-based quantification approaches. ROC analyses revealed that performance of CNN-based approach was significantly higher than that of the conventional quantification methods (p &lt; 0.05). Output scores of the network were strongly correlated with the longitudinal change in cognitive measurements. These results show the feasibility of deep learning as a tool for predicting disease outcome using brain images.
ER  -


TY  - Preprint
T1  - A Deep Learning Framework using Passive WiFi Sensing for Respiration Monitoring
A1  - U. M. Khan
A1  - Z. Kabir
A1  - S. A. Hassan
A1  - S. H. Ahmed
JO  - ArXiv e-prints
Y1  - 19 April, 2017
UR  - https://arxiv.org/abs/1704.05708
N2  - This paper presents an end-to-end deep learning framework using passive WiFi sensing to classify and estimate human respiration activity. A passive radar test-bed is used with two channels where the first channel provides the reference WiFi signal, whereas the other channel provides a surveillance signal that contains reflections from the human target. Adaptive filtering is performed to make the surveillance signal source-data invariant by eliminating the echoes of the direct transmitted signal. We propose a novel convolutional neural network to classify the complex time series data and determine if it corresponds to a breathing activity, followed by a random forest estimator to determine breathing rate. We collect an extensive dataset to train the learning models and develop reference benchmarks for the future studies in the field. Based on the results, we conclude that deep learning techniques coupled with passive radars offer great potential for end-to-end human activity recognition.
ER  -


TY  - Preprint
T1  - A Study of Deep Learning Robustness Against Computation Failures
A1  - Jean-Charles Vialatte
A1  - FranÃ§ois Leduc-Primeau
JO  - ArXiv e-prints
Y1  - 18 April, 2017
UR  - https://arxiv.org/abs/1704.05396
N2  - For many types of integrated circuits, accepting larger failure rates in computations can be used to improve energy efficiency. We study the performance of faulty implementations of certain deep neural networks based on pessimistic and optimistic models of the effect of hardware faults. After identifying the impact of hyperparameters such as the number of layers on robustness, we study the ability of the network to compensate for computational failures through an increase of the network size. We show that some networks can achieve equivalent performance under faulty implementations, and quantify the required increase in computational complexity.
ER  -


TY  - Preprint
T1  - Sentiment analysis based on rhetorical structure theory: Learning deep neural networks from discourse trees
A1  - Mathias Kraus
A1  - Stefan Feuerriegel
JO  - ArXiv e-prints
Y1  - 4 October, 2018
UR  - https://arxiv.org/abs/1704.05228
N2  - Prominent applications of sentiment analysis are countless, covering areas such as marketing, customer service and communication. The conventional bag-of-words approach for measuring sentiment merely counts term frequencies; however, it neglects the position of the terms within the discourse. As a remedy, we develop a discourse-aware method that builds upon the discourse structure of documents. For this purpose, we utilize rhetorical structure theory to label (sub-)clauses according to their hierarchical relationships and then assign polarity scores to individual leaves. To learn from the resulting rhetorical structure, we propose a tensor-based, tree-structured deep neural network (named Discourse-LSTM) in order to process the complete discourse tree. The underlying tensors infer the salient passages of narrative materials. In addition, we suggest two algorithms for data augmentation (node reordering and artificial leaf insertion) that increase our training set and reduce overfitting. Our benchmarks demonstrate the superior performance of our approach. Moreover, our tensor structure reveals the salient text passages and thereby provides explanatory insights.
ER  -


TY  - Preprint
T1  - Deep Self-Taught Learning for Weakly Supervised Object Localization
A1  - Zequn Jie
A1  - Yunchao Wei
A1  - Xiaojie Jin
A1  - Jiashi Feng
A1  - Wei Liu
JO  - ArXiv e-prints
Y1  - 30 April, 2017
UR  - https://arxiv.org/abs/1704.05188
N2  - Most existing weakly supervised localization (WSL) approaches learn detectors by finding positive bounding boxes based on features learned with image-level supervision. However, those features do not contain spatial location related information and usually provide poor-quality positive samples for training a detector. To overcome this issue, we propose a deep self-taught learning approach, which makes the detector learn the object-level features reliable for acquiring tight positive samples and afterwards re-train itself based on them. Consequently, the detector progressively improves its detection ability and localizes more informative positive samples. To implement such self-taught learning, we propose a seed sample acquisition method via image-to-object transferring and dense subgraph discovery to find reliable positive samples for initializing the detector. An online supportive sample harvesting scheme is further proposed to dynamically select the most confident tight positive samples and train the detector in a mutual boosting way. To prevent the detector from being trapped in poor optima due to overfitting, we propose a new relative improvement of predicted CNN scores for guiding the self-taught learning process. Extensive experiments on PASCAL 2007 and 2012 show that our approach outperforms the state-of-the-arts, strongly validating its effectiveness.
ER  -


TY  - Preprint
T1  - Deep Learning for Photoacoustic Tomography from Sparse Data
A1  - Stephan Antholzer
A1  - Markus Haltmeier
A1  - Johannes Schwab
JO  - ArXiv e-prints
Y1  - 30 August, 2018
UR  - https://arxiv.org/abs/1704.04587
N2  - The development of fast and accurate image reconstruction algorithms is a central aspect of computed tomography. In this paper, we investigate this issue for the sparse data problem in photoacoustic tomography (PAT). We develop a direct and highly efficient reconstruction algorithm based on deep learning. In our approach image reconstruction is performed with a deep convolutional neural network (CNN), whose weights are adjusted prior to the actual image reconstruction based on a set of training data. The proposed reconstruction approach can be interpreted as a network that uses the PAT filtered backprojection algorithm for the first layer, followed by the U-net architecture for the remaining layers. Actual image reconstruction with deep learning consists in one evaluation of the trained CNN, which does not require time consuming solution of the forward and adjoint problems. At the same time, our numerical results demonstrate that the proposed deep learning approach reconstructs images with a quality comparable to state of the art iterative approaches for PAT from sparse data.
ER  -


TY  - Preprint
T1  - Deep Structured Learning for Facial Action Unit Intensity Estimation
A1  - Robert Walecki
A1  -  Ognjen
A1  -  Rudovic
A1  - Vladimir Pavlovic
A1  - BjÃ¶rn Schuller
A1  - Maja Pantic
JO  - ArXiv e-prints
Y1  - 14 April, 2017
UR  - https://arxiv.org/abs/1704.04481
N2  - We consider the task of automated estimation of facial expression intensity. This involves estimation of multiple output variables (facial action units --- AUs) that are structurally dependent. Their structure arises from statistically induced co-occurrence patterns of AU intensity levels. Modeling this structure is critical for improving the estimation performance; however, this performance is bounded by the quality of the input features extracted from face images. The goal of this paper is to model these structures and estimate complex feature representations simultaneously by combining conditional random field (CRF) encoded AU dependencies with deep learning. To this end, we propose a novel Copula CNN deep learning approach for modeling multivariate ordinal variables. Our model accounts for $ordinal$ structure in output variables and their $non$-$linear$ dependencies via copula functions modeled as cliques of a CRF. These are jointly optimized with deep CNN feature encoding layers using a newly introduced balanced batch iterative training algorithm. We demonstrate the effectiveness of our approach on the task of AU intensity estimation on two benchmark datasets. We show that joint learning of the deep features and the target output structure results in significant performance gains compared to existing deep structured models for analysis of facial expressions.
ER  -


TY  - Preprint
T1  - Cross-media Similarity Metric Learning with Unified Deep Networks
A1  - Jinwei Qi
A1  - Xin Huang
A1  - Yuxin Peng
JO  - ArXiv e-prints
Y1  - 13 April, 2017
UR  - https://arxiv.org/abs/1704.04333
N2  - As a highlighting research topic in the multimedia area, cross-media retrieval aims to capture the complex correlations among multiple media types. Learning better shared representation and distance metric for multimedia data is important to boost the cross-media retrieval. Motivated by the strong ability of deep neural network in feature representation and comparison functions learning, we propose the Unified Network for Cross-media Similarity Metric (UNCSM) to associate cross-media shared representation learning with distance metric in a unified framework. First, we design a two-pathway deep network pretrained with contrastive loss, and employ double triplet similarity loss for fine-tuning to learn the shared representation for each media type by modeling the relative semantic similarity. Second, the metric network is designed for effectively calculating the cross-media similarity of the shared representation, by modeling the pairwise similar and dissimilar constraints. Compared to the existing methods which mostly ignore the dissimilar constraints and only use sample distance metric as Euclidean distance separately, our UNCSM approach unifies the representation learning and distance metric to preserve the relative similarity as well as embrace more complex similarity functions for further improving the cross-media retrieval accuracy. The experimental results show that our UNCSM approach outperforms 8 state-of-the-art methods on 4 widely-used cross-media datasets.
ER  -


TY  - Preprint
T1  - Deep API Programmer: Learning to Program with APIs
A1  - Surya Bhupatiraju
A1  - Rishabh Singh
A1  - Abdel-rahman Mohamed
A1  - Pushmeet Kohli
JO  - ArXiv e-prints
Y1  - 13 April, 2017
UR  - https://arxiv.org/abs/1704.04327
N2  - We present DAPIP, a Programming-By-Example system that learns to program with APIs to perform data transformation tasks. We design a domain-specific language (DSL) that allows for arbitrary concatenations of API outputs and constant strings. The DSL consists of three family of APIs: regular expression-based APIs, lookup APIs, and transformation APIs. We then present a novel neural synthesis algorithm to search for programs in the DSL that are consistent with a given set of examples. The search algorithm uses recently introduced neural architectures to encode input-output examples and to model the program search in the DSL. We show that synthesis algorithm outperforms baseline methods for synthesizing programs on both synthetic and real-world benchmarks.
ER  -


TY  - Preprint
T1  - Shape-independent Hardness Estimation Using Deep Learning and a GelSight Tactile Sensor
A1  - Wenzhen Yuan
A1  - Chenzhuo Zhu
A1  - Andrew Owens
A1  - Mandayam A. Srinivasan
A1  - Edward H. Adelson
JO  - ArXiv e-prints
Y1  - 12 April, 2017
UR  - https://arxiv.org/abs/1704.03955
N2  - Hardness is among the most important attributes of an object that humans learn about through touch. However, approaches for robots to estimate hardness are limited, due to the lack of information provided by current tactile sensors. In this work, we address these limitations by introducing a novel method for hardness estimation, based on the GelSight tactile sensor, and the method does not require accurate control of contact conditions or the shape of objects. A GelSight has a soft contact interface, and provides high resolution tactile images of contact geometry, as well as contact force and slip conditions. In this paper, we try to use the sensor to measure hardness of objects with multiple shapes, under a loosely controlled contact condition. The contact is made manually or by a robot hand, while the force and trajectory are unknown and uneven. We analyze the data using a deep constitutional (and recurrent) neural network. Experiments show that the neural net model can estimate the hardness of objects with different shapes and hardness ranging from 8 to 87 in Shore 00 scale.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning-based Image Captioning with Embedding Reward
A1  - Zhou Ren
A1  - Xiaoyu Wang
A1  - Ning Zhang
A1  - Xutao Lv
A1  - Li-Jia Li
JO  - ArXiv e-prints
Y1  - 12 April, 2017
UR  - https://arxiv.org/abs/1704.03899
N2  - Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model. However, in this paper, we introduce a novel decision-making framework for image captioning. We utilize a &#34;policy network&#34; and a &#34;value network&#34; to collaboratively generate captions. The policy network serves as a local guidance by providing the confidence of predicting the next word according to the current state. Additionally, the value network serves as a global and lookahead guidance by evaluating all possible extensions of the current state. In essence, it adjusts the goal of predicting the correct words towards the goal of generating captions similar to the ground truth captions. We train both networks using an actor-critic reinforcement learning model, with a novel reward defined by visual-semantic embedding. Extensive experiments and analyses on the Microsoft COCO dataset show that the proposed framework outperforms state-of-the-art approaches across different evaluation metrics.
ER  -


TY  - Preprint
T1  - Deep Q-learning from Demonstrations
A1  - Todd Hester
A1  - Matej Vecerik
A1  - Olivier Pietquin
A1  - Marc Lanctot
A1  - Tom Schaul
A1  - Bilal Piot
A1  - Dan Horgan
A1  - John Quan
A1  - Andrew Sendonaris
A1  - Gabriel Dulac-Arnold
A1  - Ian Osband
A1  - John Agapiou
A1  - Joel Z. Leibo
A1  - Audrunas Gruslys
JO  - ArXiv e-prints
Y1  - 22 November, 2017
UR  - https://arxiv.org/abs/1704.03732
N2  - Deep reinforcement learning (RL) has achieved several high profile successes in difficult decision-making problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages small sets of demonstration data to massively accelerate the learning process even from relatively small amounts of demonstration data and is able to automatically assess the necessary ratio of demonstration data while learning thanks to a prioritized replay mechanism. DQfD works by combining temporal difference updates with supervised classification of the demonstrator&#39;s actions. We show that DQfD has better initial performance than Prioritized Dueling Double Deep Q-Networks (PDD DQN) as it starts with better scores on the first million steps on 41 of 42 games and on average it takes PDD DQN 83 million steps to catch up to DQfD&#39;s performance. DQfD learns to out-perform the best demonstration given in 14 of 42 games. In addition, DQfD leverages human demonstrations to achieve state-of-the-art results for 11 games. Finally, we show that DQfD performs better than three related algorithms for incorporating demonstration data into DQN.
ER  -


TY  - Preprint
T1  - Deep Extreme Multi-label Learning
A1  - Wenjie Zhang
A1  - Junchi Yan
A1  - Xiangfeng Wang
A1  - Hongyuan Zha
JO  - ArXiv e-prints
Y1  - 8 June, 2018
UR  - https://arxiv.org/abs/1704.03718
N2  - Extreme multi-label learning (XML) or classification has been a practical and important problem since the boom of big data. The main challenge lies in the exponential label space which involves $2^L$ possible label sets especially when the label dimension $L$ is huge, e.g., in millions for Wikipedia labels. This paper is motivated to better explore the label space by originally establishing an explicit label graph. In the meanwhile, deep learning has been widely studied and used in various classification problems including multi-label classification, however it has not been properly introduced to XML, where the label space can be as large as in millions. In this paper, we propose a practical deep embedding method for extreme multi-label classification, which harvests the ideas of non-linear embedding and graph priors-based label space modeling simultaneously. Extensive experiments on public datasets for XML show that our method performs competitive against state-of-the-art result.
ER  -


TY  - Preprint
T1  - Feature Tracking Cardiac Magnetic Resonance via Deep Learning and Spline Optimization
A1  - Davis M. Vigneault
A1  - Weidi Xie
A1  - David A. Bluemke
A1  - J. Alison Noble
JO  - ArXiv e-prints
Y1  - 12 April, 2017
UR  - https://arxiv.org/abs/1704.03660
N2  - Feature tracking Cardiac Magnetic Resonance (CMR) has recently emerged as an area of interest for quantification of regional cardiac function from balanced, steady state free precession (SSFP) cine sequences. However, currently available techniques lack full automation, limiting reproducibility. We propose a fully automated technique whereby a CMR image sequence is first segmented with a deep, fully convolutional neural network (CNN) architecture, and quadratic basis splines are fitted simultaneously across all cardiac frames using least squares optimization. Experiments are performed using data from 42 patients with hypertrophic cardiomyopathy (HCM) and 21 healthy control subjects. In terms of segmentation, we compared state-of-the-art CNN frameworks, U-Net and dilated convolution architectures, with and without temporal context, using cross validation with three folds. Performance relative to expert manual segmentation was similar across all networks: pixel accuracy was ~97%, intersection-over-union (IoU) across all classes was ~87%, and IoU across foreground classes only was ~85%. Endocardial left ventricular circumferential strain calculated from the proposed pipeline was significantly different in control and disease subjects (-25.3% vs -29.1%, p = 0.006), in agreement with the current clinical literature.
ER  -


TY  - Preprint
T1  - Deep Learning for Multi-Task Medical Image Segmentation in Multiple Modalities
A1  - Pim Moeskops
A1  - Jelmer M. Wolterink
A1  - Bas H. M. van der Velden
A1  - Kenneth G. A. Gilhuijs
A1  - Tim Leiner
A1  - Max A. Viergever
A1  - Ivana IÅ¡gum
JO  - ArXiv e-prints
Y1  - 11 April, 2017
UR  - https://arxiv.org/abs/1704.03379
N2  - Automatic segmentation of medical images is an important task for many clinical applications. In practice, a wide range of anatomical structures are visualised using different imaging modalities. In this paper, we investigate whether a single convolutional neural network (CNN) can be trained to perform different segmentation tasks.
ER  -


TY  - Preprint
T1  - Learning Deep CNN Denoiser Prior for Image Restoration
A1  - Kai Zhang
A1  - Wangmeng Zuo
A1  - Shuhang Gu
A1  - Lei Zhang
JO  - ArXiv e-prints
Y1  - 11 April, 2017
UR  - https://arxiv.org/abs/1704.03264
N2  - Model-based optimization methods and discriminative learning methods have been the two dominant strategies for solving various inverse problems in low-level vision. Typically, those two kinds of methods have their respective merits and drawbacks, e.g., model-based optimization methods are flexible for handling different inverse problems but are usually time-consuming with sophisticated priors for the purpose of good performance; in the meanwhile, discriminative learning methods have fast testing speed but their application range is greatly restricted by the specialized task. Recent works have revealed that, with the aid of variable splitting techniques, denoiser prior can be plugged in as a modular part of model-based optimization methods to solve other inverse problems (e.g., deblurring). Such an integration induces considerable advantage when the denoiser is obtained via discriminative learning. However, the study of integration with fast discriminative denoiser prior is still lacking. To this end, this paper aims to train a set of fast and effective CNN (convolutional neural network) denoisers and integrate them into model-based optimization method to solve other inverse problems. Experimental results demonstrate that the learned set of denoisers not only achieve promising Gaussian denoising results but also can be used as prior to deliver good performance for various low-level vision applications.
ER  -


TY  - Preprint
T1  - Deep Multimodal Representation Learning from Temporal Data
A1  - Xitong Yang
A1  - Palghat Ramesh
A1  - Radha Chitta
A1  - Sriganesh Madhvanath
A1  - Edgar A. Bernal
A1  - Jiebo Luo
JO  - ArXiv e-prints
Y1  - 11 April, 2017
UR  - https://arxiv.org/abs/1704.03152
N2  - In recent years, Deep Learning has been successfully applied to multimodal learning problems, with the aim of learning useful joint representations in data fusion applications. When the available modalities consist of time series data such as video, audio and sensor signals, it becomes imperative to consider their temporal structure during the fusion process. In this paper, we propose the Correlational Recurrent Neural Network (CorrRNN), a novel temporal fusion model for fusing multiple input modalities that are inherently temporal in nature. Key features of our proposed model include: (i) simultaneous learning of the joint representation and temporal dependencies between modalities, (ii) use of multiple loss terms in the objective function, including a maximum correlation loss term to enhance learning of cross-modal information, and (iii) the use of an attention model to dynamically adjust the contribution of different input modalities to the joint representation. We validate our model via experimentation on two different tasks: video- and sensor-based activity classification, and audio-visual speech recognition. We empirically analyze the contributions of different components of the proposed CorrRNN model, and demonstrate its robustness, effectiveness and state-of-the-art performance on multiple datasets.
ER  -


TY  - Preprint
T1  - Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning
A1  - Baolin Peng
A1  - Xiujun Li
A1  - Lihong Li
A1  - Jianfeng Gao
A1  - Asli Celikyilmaz
A1  - Sungjin Lee
A1  - Kam-Fai Wong
JO  - ArXiv e-prints
Y1  - 22 July, 2017
UR  - https://arxiv.org/abs/1704.03084
N2  - Building a dialogue agent to fulfill complex tasks, such as travel planning, is challenging because the agent has to learn to collectively complete multiple subtasks. For example, the agent needs to reserve a hotel and book a flight so that there leaves enough time for commute between arrival and hotel check-in. This paper addresses this challenge by formulating the task in the mathematical framework of options over Markov Decision Processes (MDPs), and proposing a hierarchical deep reinforcement learning approach to learning a dialogue manager that operates at different temporal scales. The dialogue manager consists of: (1) a top-level dialogue policy that selects among subtasks or options, (2) a low-level dialogue policy that selects primitive actions to complete the subtask given by the top-level policy, and (3) a global state tracker that helps ensure all cross-subtask constraints be satisfied. Experiments on a travel planning task with simulated and real users show that our approach leads to significant improvements over three baselines, two based on handcrafted rules and the other based on flat deep reinforcement learning.
ER  -


TY  - Preprint
T1  - Data-efficient Deep Reinforcement Learning for Dexterous Manipulation
A1  - Ivaylo Popov
A1  - Nicolas Heess
A1  - Timothy Lillicrap
A1  - Roland Hafner
A1  - Gabriel Barth-Maron
A1  - Matej Vecerik
A1  - Thomas Lampe
A1  - Yuval Tassa
A1  - Tom Erez
A1  - Martin Riedmiller
JO  - ArXiv e-prints
Y1  - 10 April, 2017
UR  - https://arxiv.org/abs/1704.03073
N2  - Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.
ER  -


TY  - Preprint
T1  - Pyramid Vector Quantization for Deep Learning
A1  - Vincenzo Liguori
JO  - ArXiv e-prints
Y1  - 9 April, 2017
UR  - https://arxiv.org/abs/1704.02681
N2  - This paper explores the use of Pyramid Vector Quantization (PVQ) to reduce the computational cost for a variety of neural networks (NNs) while, at the same time, compressing the weights that describe them. This is based on the fact that the dot product between an N dimensional vector of real numbers and an N dimensional PVQ vector can be calculated with only additions and subtractions and one multiplication. This is advantageous since tensor products, commonly used in NNs, can be re-conduced to a dot product or a set of dot products. Finally, it is stressed that any NN architecture that is based on an operation that can be re-conduced to a dot product can benefit from the techniques described here.
ER  -


TY  - Preprint
T1  - Deep Multi-User Reinforcement Learning for Distributed Dynamic Spectrum Access
A1  - Oshri Naparstek
A1  - Kobi Cohen
JO  - ArXiv e-prints
Y1  - 23 November, 2017
UR  - https://arxiv.org/abs/1704.02613
N2  - We consider the problem of dynamic spectrum access for network utility maximization in multichannel wireless networks. The shared bandwidth is divided into K orthogonal channels. In the beginning of each time slot, each user selects a channel and transmits a packet with a certain attempt probability. After each time slot, each user that has transmitted a packet receives a local observation indicating whether its packet was successfully delivered or not (i.e., ACK signal). The objective is a multi-user strategy for accessing the spectrum that maximizes a certain network utility in a distributed manner without online coordination or message exchanges between users. Obtaining an optimal solution for the spectrum access problem is computationally expensive in general due to the large state space and partial observability of the states. To tackle this problem, we develop a novel distributed dynamic spectrum access algorithm based on deep multi-user reinforcement leaning. Specifically, at each time slot, each user maps its current state to spectrum access actions based on a trained deep-Q network used to maximize the objective function. Game theoretic analysis of the system dynamic is developed for establishing design principles for the implementation of the algorithm. Experimental results demonstrate strong performance of the algorithm.
ER  -


TY  - Preprint
T1  - Automatic Image Filtering on Social Networks Using Deep Learning and Perceptual Hashing During Crises
A1  - Dat Tien Nguyen
A1  - Firoj Alam
A1  - Ferda Ofli
A1  - Muhammad Imran
JO  - ArXiv e-prints
Y1  - 9 April, 2017
UR  - https://arxiv.org/abs/1704.02602
N2  - The extensive use of social media platforms, especially during disasters, creates unique opportunities for humanitarian organizations to gain situational awareness and launch relief operations accordingly. In addition to the textual content, people post overwhelming amounts of imagery data on social networks within minutes of a disaster hit. Studies point to the importance of this online imagery content for emergency response. Despite recent advances in the computer vision field, automatic processing of the crisis-related social media imagery data remains a challenging task. It is because a majority of which consists of redundant and irrelevant content. In this paper, we present an image processing pipeline that comprises de-duplication and relevancy filtering mechanisms to collect and filter social media image content in real-time during a crisis event. Results obtained from extensive experiments on real-world crisis datasets demonstrate the significance of the proposed pipeline for optimal utilization of both human and machine computing resources.
ER  -


TY  - Preprint
T1  - Coupled Deep Learning for Heterogeneous Face Recognition
A1  - Xiang Wu
A1  - Lingxiao Song
A1  - Ran He
A1  - Tieniu Tan
JO  - ArXiv e-prints
Y1  - 16 November, 2017
UR  - https://arxiv.org/abs/1704.02450
N2  - Heterogeneous face matching is a challenge issue in face recognition due to large domain difference as well as insufficient pairwise images in different modalities during training. This paper proposes a coupled deep learning (CDL) approach for the heterogeneous face matching. CDL seeks a shared feature space in which the heterogeneous face matching problem can be approximately treated as a homogeneous face matching problem. The objective function of CDL mainly includes two parts. The first part contains a trace norm and a block-diagonal prior as relevance constraints, which not only make unpaired images from multiple modalities be clustered and correlated, but also regularize the parameters to alleviate overfitting. An approximate variational formulation is introduced to deal with the difficulties of optimizing low-rank constraint directly. The second part contains a cross modal ranking among triplet domain specific images to maximize the margin for different identities and increase data for a small amount of training samples. Besides, an alternating minimization method is employed to iteratively update the parameters of CDL. Experimental results show that CDL achieves better performance on the challenging CASIA NIR-VIS 2.0 face recognition database, the IIIT-D Sketch database, the CUHK Face Sketch (CUFS), and the CUHK Face Sketch FERET (CUFSF), which significantly outperforms state-of-the-art heterogeneous face recognition methods.
ER  -


TY  - Preprint
T1  - Learning Cross-Modal Deep Representations for Robust Pedestrian Detection
A1  - Dan Xu
A1  - Wanli Ouyang
A1  - Elisa Ricci
A1  - Xiaogang Wang
A1  - Nicu Sebe
JO  - ArXiv e-prints
Y1  - 1 January, 2018
UR  - https://arxiv.org/abs/1704.02431
N2  - This paper presents a novel method for detecting pedestrians under adverse illumination conditions. Our approach relies on a novel cross-modality learning framework and it is based on two main phases. First, given a multimodal dataset, a deep convolutional network is employed to learn a non-linear mapping, modeling the relations between RGB and thermal data. Then, the learned feature representations are transferred to a second deep network, which receives as input an RGB image and outputs the detection results. In this way, features which are both discriminative and robust to bad illumination conditions are learned. Importantly, at test time, only the second pipeline is considered and no thermal data are required. Our extensive evaluation demonstrates that the proposed approach outperforms the state-of- the-art on the challenging KAIST multispectral pedestrian dataset and it is competitive with previous methods on the popular Caltech dataset.
ER  -


TY  - Preprint
T1  - Deep Unsupervised Similarity Learning using Partially Ordered Sets
A1  - Miguel A Bautista
A1  - Artsiom Sanakoyeu
A1  - BjÃ¶rn Ommer
JO  - ArXiv e-prints
Y1  - 11 April, 2017
UR  - https://arxiv.org/abs/1704.02268
N2  - Unsupervised learning of visual similarities is of paramount importance to computer vision, particularly due to lacking training data for fine-grained similarities. Deep learning of similarities is often based on relationships between pairs or triplets of samples. Many of these relations are unreliable and mutually contradicting, implying inconsistencies when trained without supervision information that relates different tuples or triplets to each other. To overcome this problem, we use local estimates of reliable (dis-)similarities to initially group samples into compact surrogate classes and use local partial orders of samples to classes to link classes to each other. Similarity learning is then formulated as a partial ordering task with soft correspondences of all samples to classes. Adopting a strategy of self-supervision, a CNN is trained to optimally represent samples in a mutually consistent manner while updating the classes. The similarity learning and grouping procedure are integrated in a single model and optimized jointly. The proposed unsupervised approach shows competitive performance on detailed pose estimation and object classification.
ER  -


TY  - Preprint
T1  - Best Practices for Applying Deep Learning to Novel Applications
A1  - Leslie N. Smith
JO  - ArXiv e-prints
Y1  - 5 April, 2017
UR  - https://arxiv.org/abs/1704.01568
N2  - This report is targeted to groups who are subject matter experts in their application but deep learning novices. It contains practical advice for those interested in testing the use of deep neural networks on applications that are novel for deep learning. We suggest making your project more manageable by dividing it into phases. For each phase this report contains numerous recommendations and insights to assist novice practitioners.
ER  -


TY  - Preprint
T1  - Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design
A1  - Yoav Levine
A1  - David Yakira
A1  - Nadav Cohen
A1  - Amnon Shashua
JO  - ArXiv e-prints
Y1  - 10 April, 2017
UR  - https://arxiv.org/abs/1704.01552
N2  - Deep convolutional networks have witnessed unprecedented success in various machine learning applications. Formal understanding on what makes these networks so successful is gradually unfolding, but for the most part there are still significant mysteries to unravel. The inductive bias, which reflects prior knowledge embedded in the network architecture, is one of them. In this work, we establish a fundamental connection between the fields of quantum physics and deep learning. We use this connection for asserting novel theoretical observations regarding the role that the number of channels in each layer of the convolutional network fulfills in the overall inductive bias. Specifically, we show an equivalence between the function realized by a deep convolutional arithmetic circuit (ConvAC) and a quantum many-body wave function, which relies on their common underlying tensorial structure. This facilitates the use of quantum entanglement measures as well-defined quantifiers of a deep network&#39;s expressive ability to model intricate correlation structures of its inputs. Most importantly, the construction of a deep ConvAC in terms of a Tensor Network is made available. This description enables us to carry a graph-theoretic analysis of a convolutional network, with which we demonstrate a direct control over the inductive bias of the deep network via its channel numbers, that are related to the min-cut in the underlying graph. This result is relevant to any practitioner designing a network for a specific task. We theoretically analyze ConvACs, and empirically validate our findings on more common ConvNets which involve ReLU activations and max pooling. Beyond the results described above, the description of a deep convolutional network in well-defined graph-theoretic tools and the formal connection to quantum entanglement, are two interdisciplinary bridges that are brought forth by this work.
ER  -


TY  - Preprint
T1  - Smart Mining for Deep Metric Learning
A1  - Ben Harwood
A1  - Vijay Kumar B G
A1  - Gustavo Carneiro
A1  - Ian Reid
A1  - Tom Drummond
JO  - ArXiv e-prints
Y1  - 27 July, 2017
UR  - https://arxiv.org/abs/1704.01285
N2  - To solve deep metric learning problems and producing feature embeddings, current methodologies will commonly use a triplet model to minimise the relative distance between samples from the same class and maximise the relative distance between samples from different classes. Though successful, the training convergence of this triplet model can be compromised by the fact that the vast majority of the training samples will produce gradients with magnitudes that are close to zero. This issue has motivated the development of methods that explore the global structure of the embedding and other methods that explore hard negative/positive mining. The effectiveness of such mining methods is often associated with intractable computational requirements. In this paper, we propose a novel deep metric learning method that combines the triplet model and the global structure of the embedding space. We rely on a smart mining procedure that produces effective training samples for a low computational cost. In addition, we propose an adaptive controller that automatically adjusts the smart mining hyper-parameters and speeds up the convergence of the training process. We show empirically that our proposed method allows for fast and more accurate training of triplet ConvNets than other competing mining methods. Additionally, we show that our method achieves new state-of-the-art embedding results for CUB-200-2011 and Cars196 datasets.
ER  -


TY  - Preprint
T1  - Geometric Loss Functions for Camera Pose Regression with Deep Learning
A1  - Alex Kendall
A1  - Roberto Cipolla
JO  - ArXiv e-prints
Y1  - 23 May, 2017
UR  - https://arxiv.org/abs/1704.00390
N2  - Deep learning has shown to be effective for robust and real-time monocular image relocalisation. In particular, PoseNet is a deep convolutional neural network which learns to regress the 6-DOF camera pose from a single image. It learns to localize using high level features and is robust to difficult lighting, motion blur and unknown camera intrinsics, where point based SIFT registration fails. However, it was trained using a naive loss function, with hyper-parameters which require expensive tuning. In this paper, we give the problem a more fundamental theoretical treatment. We explore a number of novel loss functions for learning camera pose which are based on geometry and scene reprojection error. Additionally we show how to automatically learn an optimal weighting to simultaneously regress position and orientation. By leveraging geometry, we demonstrate that our technique significantly improves PoseNet&#39;s performance across datasets ranging from indoor rooms to a small city.
ER  -


TY  - Preprint
T1  - Learning Visual Servoing with Deep Features and Fitted Q-Iteration
A1  - Alex X. Lee
A1  - Sergey Levine
A1  - Pieter Abbeel
JO  - ArXiv e-prints
Y1  - 10 July, 2017
UR  - https://arxiv.org/abs/1703.11000
N2  - Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at http://rll.berkeley.edu/visual_servoing .
ER  -


TY  - Preprint
T1  - Sentence Simplification with Deep Reinforcement Learning
A1  - Xingxing Zhang
A1  - Mirella Lapata
JO  - ArXiv e-prints
Y1  - 15 July, 2017
UR  - https://arxiv.org/abs/1703.10931
N2  - Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call {\sc Dress} (as shorthand for {\bf D}eep {\bf RE}inforcement {\bf S}entence {\bf S}implification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.
ER  -


TY  - Preprint
T1  - Quicksilver: Fast Predictive Image Registration - a Deep Learning Approach
A1  - Xiao Yang
A1  - Roland Kwitt
A1  - Martin Styner
A1  - Marc Niethammer
JO  - ArXiv e-prints
Y1  - 19 July, 2017
UR  - https://arxiv.org/abs/1703.10908
N2  - This paper introduces Quicksilver, a fast deformable image registration method. Quicksilver registration for image-pairs works by patch-wise prediction of a deformation model based directly on image appearance. A deep encoder-decoder network is used as the prediction model. While the prediction strategy is general, we focus on predictions for the Large Deformation Diffeomorphic Metric Mapping (LDDMM) model. Specifically, we predict the momentum-parameterization of LDDMM, which facilitates a patch-wise prediction strategy while maintaining the theoretical properties of LDDMM, such as guaranteed diffeomorphic mappings for sufficiently strong regularization. We also provide a probabilistic version of our prediction network which can be sampled during the testing time to calculate uncertainties in the predicted deformations. Finally, we introduce a new correction network which greatly increases the prediction accuracy of an already existing prediction network. We show experimental results for uni-modal atlas-to-image as well as uni- / multi- modal image-to-image registrations. These experiments demonstrate that our method accurately predicts registrations obtained by numerical optimization, is very fast, achieves state-of-the-art registration results on four standard validation datasets, and can jointly learn an image similarity measure. Quicksilver is freely available as an open-source software.
ER  -


TY  - Preprint
T1  - A deep learning classification scheme based on augmented-enhanced features to segment organs at risk on the optic region in brain cancer patients
A1  - Jose Dolz
A1  - Nicolas Reyns
A1  - Nacim Betrouni
A1  - Dris Kharroubi
A1  - Mathilde Quidet
A1  - Laurent Massoptier
A1  - Maximilien Vermandel
JO  - ArXiv e-prints
Y1  - 5 April, 2017
UR  - https://arxiv.org/abs/1703.10480
N2  - Radiation therapy has emerged as one of the preferred techniques to treat brain cancer patients. During treatment, a very high dose of radiation is delivered to a very narrow area. Prescribed radiation therapy for brain cancer requires precisely defining the target treatment area, as well as delineating vital brain structures which must be spared from radiotoxicity. Nevertheless, delineation task is usually still manually performed, which is inefficient and operator-dependent. Several attempts of automatizing this process have reported. however, marginal results when analyzing organs in the optic region. In this work we present a deep learning classification scheme based on augmented-enhanced features to automatically segment organs at risk (OARs) in the optic region -optic nerves, optic chiasm, pituitary gland and pituitary stalk-. Fifteen MR images with various types of brain tumors were retrospectively collected to undergo manual and automatic segmentation. Mean Dice Similarity coefficients around 0.80 were reported. Incorporation of proposed features yielded to improvements on the segmentation. Compared with support vector machines, our method achieved better performance with less variation on the results, as well as a considerably reduction on the classification time. Performance of the proposed approach was also evaluated with respect to manual contours. In this case, results obtained from the automatic contours mostly lie on the variability of the observers, showing no significant differences with respect to them. These results suggest therefore that the proposed system is more accurate than other presented approaches, up to date, to segment these structures. The speed, reproducibility, and robustness of the process make the proposed deep learning-based classification system a valuable tool for assisting in the delineation task of small OARs in brain cancer.
ER  -


TY  - Preprint
T1  - Semantic Instance Segmentation via Deep Metric Learning
A1  - Alireza Fathi
A1  - Zbigniew Wojna
A1  - Vivek Rathod
A1  - Peng Wang
A1  - Hyun Oh Song
A1  - Sergio Guadarrama
A1  - Kevin P. Murphy
JO  - ArXiv e-prints
Y1  - 29 March, 2017
UR  - https://arxiv.org/abs/1703.10277
N2  - We propose a new method for semantic instance segmentation, by first computing how likely two pixels are to belong to the same object, and then by grouping similar pixels together. Our similarity metric is based on a deep, fully convolutional embedding model. Our grouping method is based on selecting all points that are sufficiently similar to a set of &#34;seed points&#34;, chosen from a deep, fully convolutional scoring model. We show competitive results on the Pascal VOC instance segmentation benchmark.
ER  -


TY  - Preprint
T1  - Theory II: Landscape of the Empirical Risk in Deep Learning
A1  - Qianli Liao
A1  - Tomaso Poggio
JO  - ArXiv e-prints
Y1  - 22 June, 2017
UR  - https://arxiv.org/abs/1703.09833
N2  - Previous theoretical work on deep learning and neural network optimization tend to focus on avoiding saddle points and local minima. However, the practical observation is that, at least in the case of the most successful Deep Convolutional Neural Networks (DCNNs), practitioners can always increase the network size to fit the training data (an extreme example would be [1]). The most successful DCNNs such as VGG and ResNets are best used with a degree of &#34;overparametrization&#34;. In this work, we characterize with a mix of theory and experiments, the landscape of the empirical risk of overparametrized DCNNs. We first prove in the regression framework the existence of a large number of degenerate global minimizers with zero empirical error (modulo inconsistent equations). The argument that relies on the use of Bezout theorem is rigorous when the RELUs are replaced by a polynomial nonlinearity (which empirically works as well). As described in our Theory III [2] paper, the same minimizers are degenerate and thus very likely to be found by SGD that will furthermore select with higher probability the most robust zero-minimizer. We further experimentally explored and visualized the landscape of empirical risk of a DCNN on CIFAR-10 during the entire training process and especially the global minima. Finally, based on our theoretical and experimental results, we propose an intuitive model of the landscape of DCNN&#39;s empirical loss surface, which might not be as complicated as people commonly believe.
ER  -


TY  - Preprint
T1  - Feature Analysis and Selection for Training an End-to-End Autonomous Vehicle Controller Using the Deep Learning Approach
A1  - Shun Yang
A1  - Wenshuo Wang
A1  - Chang Liu
A1  - Kevin Deng
A1  - J. Karl Hedrick
JO  - ArXiv e-prints
Y1  - 28 March, 2017
UR  - https://arxiv.org/abs/1703.09744
N2  - Deep learning-based approaches have been widely used for training controllers for autonomous vehicles due to their powerful ability to approximate nonlinear functions or policies. However, the training process usually requires large labeled data sets and takes a lot of time. In this paper, we analyze the influences of features on the performance of controllers trained using the convolutional neural networks (CNNs), which gives a guideline of feature selection to reduce computation cost. We collect a large set of data using The Open Racing Car Simulator (TORCS) and classify the image features into three categories (sky-related, roadside-related, and road-related features).We then design two experimental frameworks to investigate the importance of each single feature for training a CNN controller.The first framework uses the training data with all three features included to train a controller, which is then tested with data that has one feature removed to evaluate the feature&#39;s effects. The second framework is trained with the data that has one feature excluded, while all three features are included in the test data. Different driving scenarios are selected to test and analyze the trained controllers using the two experimental frameworks. The experiment results show that (1) the road-related features are indispensable for training the controller, (2) the roadside-related features are useful to improve the generalizability of the controller to scenarios with complicated roadside information, and (3) the sky-related features have limited contribution to train an end-to-end autonomous vehicle controller.
ER  -


TY  - Preprint
T1  - Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics
A1  - Jeffrey Mahler
A1  - Jacky Liang
A1  - Sherdil Niyaz
A1  - Michael Laskey
A1  - Richard Doan
A1  - Xinyu Liu
A1  - Juan Aparicio Ojea
A1  - Ken Goldberg
JO  - ArXiv e-prints
Y1  - 8 August, 2017
UR  - https://arxiv.org/abs/1703.09312
N2  - To reduce data collection time for deep learning of robust robotic grasp plans, we explore training from a synthetic dataset of 6.7 million point clouds, grasps, and analytic grasp metrics generated from thousands of 3D models from Dex-Net 1.0 in randomized poses on a table. We use the resulting dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network (GQ-CNN) model that rapidly predicts the probability of success of grasps from depth images, where grasps are specified as the planar position, angle, and depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000 trials on an ABB YuMi comparing grasp planning methods on singulated objects suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be used to plan grasps in 0.8sec with a success rate of 93% on eight known objects with adversarial geometry and is 3x faster than registering point clouds to a precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp planner also has the highest success rate on a dataset of 10 novel rigid objects and achieves 99% precision (one false positive out of 69 grasps classified as robust) on a dataset of 40 novel household objects, some of which are articulated or deformable. Code, datasets, videos, and supplementary material are available at http://berkeleyautomation.github.io/dex-net .
ER  -


TY  - Preprint
T1  - Multimodal deep learning approach for joint EEG-EMG data compression and classification
A1  - Ahmed Ben Said
A1  - Amr Mohamed
A1  - Tarek Elfouly
A1  - Khaled Harras
A1  - Z. Jane Wang
JO  - ArXiv e-prints
Y1  - 27 March, 2017
UR  - https://arxiv.org/abs/1703.08970
N2  - In this paper, we present a joint compression and classification approach of EEG and EMG signals using a deep learning approach. Specifically, we build our system based on the deep autoencoder architecture which is designed not only to extract discriminant features in the multimodal data representation but also to reconstruct the data from the latent representation using encoder-decoder layers. Since autoencoder can be seen as a compression approach, we extend it to handle multimodal data at the encoder layer, reconstructed and retrieved at the decoder layer. We show through experimental results, that exploiting both multimodal data intercorellation and intracorellation 1) Significantly reduces signal distortion particularly for high compression levels 2) Achieves better accuracy in classifying EEG and EMG signals recorded and labeled according to the sentiments of the volunteer.
ER  -


TY  - Preprint
T1  - Multi-View Deep Learning for Consistent Semantic Mapping with RGB-D Cameras
A1  - Lingni Ma
A1  - JÃ¶rg StÃ¼ckler
A1  - Christian Kerl
A1  - Daniel Cremers
JO  - ArXiv e-prints
Y1  - 4 December, 2017
UR  - https://arxiv.org/abs/1703.08866
N2  - Visual scene understanding is an important capability that enables robots to purposefully act in their environment. In this paper, we propose a novel approach to object-class segmentation from multiple RGB-D views using deep learning. We train a deep neural network to predict object-class semantics that is consistent from several view points in a semi-supervised way. At test time, the semantics predictions of our network can be fused more consistently in semantic keyframe maps than predictions of a network trained on individual views. We base our network architecture on a recent single-view deep learning approach to RGB and depth fusion for semantic object-class segmentation and enhance it with multi-scale loss minimization. We obtain the camera trajectory using RGB-D SLAM and warp the predictions of RGB-D images into ground-truth annotated frames in order to enforce multi-view consistency during training. At test time, predictions from multiple views are fused into keyframes. We propose and analyze several methods for enforcing multi-view consistency during training and testing. We evaluate the benefit of multi-view consistency training and demonstrate that pooling of deep features and fusion over multiple views outperforms single-view baselines on the NYUDv2 benchmark for semantic segmentation. Our end-to-end trained network achieves state-of-the-art performance on the NYUDv2 dataset in single-view segmentation as well as multi-view semantic fusion.
ER  -


TY  - Preprint
T1  - Socially Aware Motion Planning with Deep Reinforcement Learning
A1  - Yu Fan Chen
A1  - Michael Everett
A1  - Miao Liu
A1  - Jonathan P. How
JO  - ArXiv e-prints
Y1  - 4 May, 2018
UR  - https://arxiv.org/abs/1703.08862
N2  - For robotic vehicles to navigate safely and efficiently in pedestrian-rich environments, it is important to model subtle human behaviors and navigation rules (e.g., passing on the right). However, while instinctive to humans, socially compliant navigation is still difficult to quantify due to the stochasticity in people&#39;s behaviors. Existing works are mostly focused on using feature-matching techniques to describe and imitate human paths, but often do not generalize well since the feature values can vary from person to person, and even run to run. This work notes that while it is challenging to directly specify the details of what to do (precise mechanisms of human navigation), it is straightforward to specify what not to do (violations of social norms). Specifically, using deep reinforcement learning, this work develops a time-efficient navigation policy that respects common social norms. The proposed method is shown to enable fully autonomous navigation of a robotic vehicle moving at human walking speed in an environment with many pedestrians.
ER  -


TY  - Preprint
T1  - Comparing Rule-Based and Deep Learning Models for Patient Phenotyping
A1  - Sebastian Gehrmann
A1  - Franck Dernoncourt
A1  - Yeran Li
A1  - Eric T. Carlson
A1  - Joy T. Wu
A1  - Jonathan Welt
A1  - John Foote Jr.
A1  - Edward T. Moseley
A1  - David W. Grant
A1  - Patrick D. Tyler
A1  - Leo Anthony Celi
JO  - ArXiv e-prints
Y1  - 25 March, 2017
UR  - https://arxiv.org/abs/1703.08705
N2  - Objective: We investigate whether deep learning techniques for natural language processing (NLP) can be used efficiently for patient phenotyping. Patient phenotyping is a classification task for determining whether a patient has a medical condition, and is a crucial part of secondary analysis of healthcare data. We assess the performance of deep learning algorithms and compare them with classical NLP approaches.
ER  -


TY  - Preprint
T1  - Deep Residual Learning for Instrument Segmentation in Robotic Surgery
A1  - Daniil Pakhomov
A1  - Vittal Premachandran
A1  - Max Allan
A1  - Mahdi Azizian
A1  - Nassir Navab
JO  - ArXiv e-prints
Y1  - 24 March, 2017
UR  - https://arxiv.org/abs/1703.08580
N2  - Detection, tracking, and pose estimation of surgical instruments are crucial tasks for computer assistance during minimally invasive robotic surgery. In the majority of cases, the first step is the automatic segmentation of surgical tools. Prior work has focused on binary segmentation, where the objective is to label every pixel in an image as tool or background. We improve upon previous work in two major ways. First, we leverage recent techniques such as deep residual learning and dilated convolutions to advance binary-segmentation performance. Second, we extend the approach to multi-class segmentation, which lets us segment different parts of the tool, in addition to background. We demonstrate the performance of this method on the MICCAI Endoscopic Vision Challenge Robotic Instruments dataset.
ER  -


TY  - Preprint
T1  - A Hybrid Deep Learning Approach for Texture Analysis
A1  - Hussein Adly
A1  - Mohamed Moustafa
JO  - ArXiv e-prints
Y1  - 24 March, 2017
UR  - https://arxiv.org/abs/1703.08366
N2  - Texture classification is a problem that has various applications such as remote sensing and forest species recognition. Solutions tend to be custom fit to the dataset used but fails to generalize. The Convolutional Neural Network (CNN) in combination with Support Vector Machine (SVM) form a robust selection between powerful invariant feature extractor and accurate classifier. The fusion of experts provides stability in classification rates among different datasets.
ER  -


TY  - Preprint
T1  - Failures of Gradient-Based Deep Learning
A1  - Shai Shalev-Shwartz
A1  - Ohad Shamir
A1  - Shaked Shammah
JO  - ArXiv e-prints
Y1  - 26 April, 2017
UR  - https://arxiv.org/abs/1703.07950
N2  - In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four types of simple problems, for which the gradient-based algorithms commonly used in deep learning either fail or suffer from significant difficulties. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.
ER  -


TY  - Preprint
T1  - An End-to-End Approach to Natural Language Object Retrieval via Context-Aware Deep Reinforcement Learning
A1  - Fan Wu
A1  - Zhongwen Xu
A1  - Yi Yang
JO  - ArXiv e-prints
Y1  - 22 March, 2017
UR  - https://arxiv.org/abs/1703.07579
N2  - We propose an end-to-end approach to the natural language object retrieval task, which localizes an object within an image according to a natural language description, i.e., referring expression. Previous works divide this problem into two independent stages: first, compute region proposals from the image without the exploration of the language description; second, score the object proposals with regard to the referring expression and choose the top-ranked proposals. The object proposals are generated independently from the referring expression, which makes the proposal generation redundant and even irrelevant to the referred object. In this work, we train an agent with deep reinforcement learning, which learns to move and reshape a bounding box to localize the object according to the referring expression. We incorporate both the spatial and temporal context information into the training procedure. By simultaneously exploiting local visual information, the spatial and temporal context and the referring language a priori, the agent selects an appropriate action to take at each time. A special action is defined to indicate when the agent finds the referred object, and terminate the procedure. We evaluate our model on various datasets, and our algorithm significantly outperforms the compared algorithms. Notably, the accuracy improvement of our method over the recent method GroundeR and SCRC on the ReferItGame dataset are 7.67% and 18.25%, respectively.
ER  -


TY  - Preprint
T1  - Knowledge Transfer for Melanoma Screening with Deep Learning
A1  - Afonso Menegola
A1  - Michel Fornaciali
A1  - Ramon Pires
A1  - FlÃ¡via Vasques Bittencourt
A1  - Sandra Avila
A1  - Eduardo Valle
JO  - ArXiv e-prints
Y1  - 21 March, 2017
UR  - https://arxiv.org/abs/1703.07479
N2  - Knowledge transfer impacts the performance of deep learning -- the state of the art for image classification tasks, including automated melanoma screening. Deep learning&#39;s greed for large amounts of training data poses a challenge for medical tasks, which we can alleviate by recycling knowledge from models trained on different tasks, in a scheme called transfer learning. Although much of the best art on automated melanoma screening employs some form of transfer learning, a systematic evaluation was missing. Here we investigate the presence of transfer, from which task the transfer is sourced, and the application of fine tuning (i.e., retraining of the deep learning model after transfer). We also test the impact of picking deeper (and more expensive) models. Our results favor deeper models, pre-trained over ImageNet, with fine-tuning, reaching an AUC of 80.7% and 84.5% for the two skin-lesion datasets evaluated.
ER  -


TY  - Preprint
T1  - Deep Learning for Explicitly Modeling Optimization Landscapes
A1  - Shumeet Baluja
JO  - ArXiv e-prints
Y1  - 21 March, 2017
UR  - https://arxiv.org/abs/1703.07394
N2  - In all but the most trivial optimization problems, the structure of the solutions exhibit complex interdependencies between the input parameters. Decades of research with stochastic search techniques has shown the benefit of explicitly modeling the interactions between sets of parameters and the overall quality of the solutions discovered. We demonstrate a novel method, based on learning deep networks, to model the global landscapes of optimization problems. To represent the search space concisely and accurately, the deep networks must encode information about the underlying parameter interactions and their contributions to the quality of the solution. Once the networks are trained, the networks are probed to reveal parameter combinations with high expected performance with respect to the optimization task. These estimates are used to initialize fast, randomized, local search algorithms, which in turn expose more information about the search space that is subsequently used to refine the models. We demonstrate the technique on multiple optimization problems that have arisen in a variety of real-world domains, including: packing, graphics, job scheduling, layout and compression. The problems include combinatoric search spaces, discontinuous and highly non-linear spaces, and span binary, higher-cardinality discrete, as well as continuous parameters. Strengths, limitations, and extensions of the approach are extensively discussed and demonstrated.
ER  -


TY  - Preprint
T1  - Multi-Objective Learning and Mask-Based Post-Processing for Deep Neural Network Based Speech Enhancement
A1  - Yong Xu
A1  - Jun Du
A1  - Zhen Huang
A1  - Li-Rong Dai
A1  - Chin-Hui Lee
JO  - ArXiv e-prints
Y1  - 21 March, 2017
UR  - https://arxiv.org/abs/1703.07172
N2  - We propose a multi-objective framework to learn both secondary targets not directly related to the intended task of speech enhancement (SE) and the primary target of the clean log-power spectra (LPS) features to be used directly for constructing the enhanced speech signals. In deep neural network (DNN) based SE we introduce an auxiliary structure to learn secondary continuous features, such as mel-frequency cepstral coefficients (MFCCs), and categorical information, such as the ideal binary mask (IBM), and integrate it into the original DNN architecture for joint optimization of all the parameters. This joint estimation scheme imposes additional constraints not available in the direct prediction of LPS, and potentially improves the learning of the primary target. Furthermore, the learned secondary information as a byproduct can be used for other purposes, e.g., the IBM-based post-processing in this work. A series of experiments show that joint LPS and MFCC learning improves the SE performance, and IBM-based post-processing further enhances listening quality of the reconstructed speech.
ER  -


TY  - Preprint
T1  - Cross-modal Deep Metric Learning with Multi-task Regularization
A1  - Xin Huang
A1  - Yuxin Peng
JO  - ArXiv e-prints
Y1  - 5 April, 2017
UR  - https://arxiv.org/abs/1703.07026
N2  - DNN-based cross-modal retrieval has become a research hotspot, by which users can search results across various modalities like image and text. However, existing methods mainly focus on the pairwise correlation and reconstruction error of labeled data. They ignore the semantically similar and dissimilar constraints between different modalities, and cannot take advantage of unlabeled data. This paper proposes Cross-modal Deep Metric Learning with Multi-task Regularization (CDMLMR), which integrates quadruplet ranking loss and semi-supervised contrastive loss for modeling cross-modal semantic similarity in a unified multi-task learning architecture. The quadruplet ranking loss can model the semantically similar and dissimilar constraints to preserve cross-modal relative similarity ranking information. The semi-supervised contrastive loss is able to maximize the semantic similarity on both labeled and unlabeled data. Compared to the existing methods, CDMLMR exploits not only the similarity ranking information but also unlabeled cross-modal data, and thus boosts cross-modal retrieval accuracy.
ER  -


TY  - Preprint
T1  - Applying Deep Machine Learning for psycho-demographic profiling of Internet users using O.C.E.A.N. model of personality
A1  - Iaroslav Omelianenko
JO  - ArXiv e-prints
Y1  - 5 July, 2017
UR  - https://arxiv.org/abs/1703.06914
N2  - In the modern era, each Internet user leaves enormous amounts of auxiliary digital residuals (footprints) by using a variety of on-line services. All this data is already collected and stored for many years. In recent works, it was demonstrated that it&#39;s possible to apply simple machine learning methods to analyze collected digital footprints and to create psycho-demographic profiles of individuals. However, while these works clearly demonstrated the applicability of machine learning methods for such an analysis, created simple prediction models still lacks accuracy necessary to be successfully applied for practical needs. We have assumed that using advanced deep machine learning methods may considerably increase the accuracy of predictions. We started with simple machine learning methods to estimate basic prediction performance and moved further by applying advanced methods based on shallow and deep neural networks. Then we compared prediction power of studied models and made conclusions about its performance. Finally, we made hypotheses how prediction accuracy can be further improved. As result of this work, we provide full source code used in the experiments for all interested researchers and practitioners in corresponding GitHub repository. We believe that applying deep machine learning for psycho-demographic profiling may have an enormous impact on the society (for good or worse) and provides means for Artificial Intelligence (AI) systems to better understand humans by creating their psychological profiles. Thus AI agents may achieve the human-like ability to participate in conversation (communication) flow by anticipating human opponents&#39; reactions, expectations, and behavior.
ER  -


TY  - Preprint
T1  - A Comparison of deep learning methods for environmental sound
A1  - Juncheng Li
A1  - Wei Dai
A1  - Florian Metze
A1  - Shuhui Qu
A1  - Samarjit Das
JO  - ArXiv e-prints
Y1  - 20 March, 2017
UR  - https://arxiv.org/abs/1703.06902
N2  - Environmental sound detection is a challenging application of machine learning because of the noisy nature of the signal, and the small amount of (labeled) data that is typically available. This work thus presents a comparison of several state-of-the-art Deep Learning models on the IEEE challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2016 challenge task and data, classifying sounds into one of fifteen common indoor and outdoor acoustic scenes, such as bus, cafe, car, city center, forest path, library, train, etc. In total, 13 hours of stereo audio recordings are available, making this one of the largest datasets available. We perform experiments on six sets of features, including standard Mel-frequency cepstral coefficients (MFCC), Binaural MFCC, log Mel-spectrum and two different large- scale temporal pooling features extracted using OpenSMILE. On these features, we apply five models: Gaussian Mixture Model (GMM), Deep Neural Network (DNN), Recurrent Neural Network (RNN), Convolutional Deep Neural Net- work (CNN) and i-vector. Using the late-fusion approach, we improve the performance of the baseline 72.5% by 15.6% in 4-fold Cross Validation (CV) avg. accuracy and 11% in test accuracy, which matches the best result of the DCASE 2016 challenge. With large feature sets, deep neural network models out- perform traditional methods and achieve the best performance among all the studied methods. Consistent with other work, the best performing single model is the non-temporal DNN model, which we take as evidence that sounds in the DCASE challenge do not exhibit strong temporal dynamics.
ER  -


TY  - Preprint
T1  - Tactics of Adversarial Attack on Deep Reinforcement Learning Agents
A1  - Yen-Chen Lin
A1  - Zhang-Wei Hong
A1  - Yuan-Hong Liao
A1  - Meng-Li Shih
A1  - Ming-Yu Liu
A1  - Min Sun
JO  - ArXiv e-prints
Y1  - 22 May, 2017
UR  - https://arxiv.org/abs/1703.06748
N2  - We introduce two tactics to attack agents trained by deep reinforcement learning algorithms using adversarial examples, namely the strategically-timed attack and the enchanting attack. In the strategically-timed attack, the adversary aims at minimizing the agent&#39;s reward by only attacking the agent at a small subset of time steps in an episode. Limiting the attack activity to this subset helps prevent detection of the attack by the agent. We propose a novel method to determine when an adversarial example should be crafted and applied. In the enchanting attack, the adversary aims at luring the agent to a designated target state. This is achieved by combining a generative model and a planning algorithm: while the generative model predicts the future states, the planning algorithm generates a preferred sequence of actions for luring the agent. A sequence of adversarial examples is then crafted to lure the agent to take the preferred sequence of actions. We apply the two tactics to the agents trained by the state-of-the-art deep reinforcement learning algorithm including DQN and A3C. In 5 Atari games, our strategically timed attack reduces as much reward as the uniform attack (i.e., attacking at every time step) does by attacking the agent 4 times less often. Our enchanting attack lures the agent toward designated target states with a more than 70% success rate. Videos are available at http://yclin.me/adversarial_attack_RL/
ER  -


TY  - Preprint
T1  - QMDP-Net: Deep Learning for Planning under Partial Observability
A1  - Peter Karkus
A1  - David Hsu
A1  - Wee Sun Lee
JO  - ArXiv e-prints
Y1  - 2 November, 2017
UR  - https://arxiv.org/abs/1703.06692
N2  - This paper introduces the QMDP-net, a neural network architecture for planning under partial observability. The QMDP-net combines the strengths of model-free learning and model-based planning. It is a recurrent policy network, but it represents a policy for a parameterized set of tasks by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in a network learning architecture. The QMDP-net is fully differentiable and allows for end-to-end training. We train a QMDP-net on different tasks so that it can generalize to new ones in the parameterized task set and &#34;transfer&#34; to other similar tasks beyond the set. In preliminary experiments, QMDP-net showed strong performance on several robotic tasks in simulation. Interestingly, while QMDP-net encodes the QMDP algorithm, it sometimes outperforms the QMDP algorithm in the experiments, as a result of end-to-end learning.
ER  -


TY  - Preprint
T1  - Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning
A1  - Abhishek Das
A1  - Satwik Kottur
A1  - JosÃ© M. F. Moura
A1  - Stefan Lee
A1  - Dhruv Batra
JO  - ArXiv e-prints
Y1  - 21 March, 2017
UR  - https://arxiv.org/abs/1703.06585
N2  - We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative &#39;image guessing&#39; game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward.
ER  -


TY  - Preprint
T1  - Algorithms for Semantic Segmentation of Multispectral Remote Sensing Imagery using Deep Learning
A1  - Ronald Kemker
A1  - Carl Salvaggio
A1  - Christopher Kanan
JO  - ArXiv e-prints
Y1  - 1 May, 2018
UR  - https://arxiv.org/abs/1703.06452
N2  - Deep convolutional neural networks (DCNNs) have been used to achieve state-of-the-art performance on many computer vision tasks (e.g., object recognition, object detection, semantic segmentation) thanks to a large repository of annotated image data. Large labeled datasets for other sensor modalities, e.g., multispectral imagery (MSI), are not available due to the large cost and manpower required. In this paper, we adapt state-of-the-art DCNN frameworks in computer vision for semantic segmentation for MSI imagery. To overcome label scarcity for MSI data, we substitute real MSI for generated synthetic MSI in order to initialize a DCNN framework. We evaluate our network initialization scheme on the new RIT-18 dataset that we present in this paper. This dataset contains very-high resolution MSI collected by an unmanned aircraft system. The models initialized with synthetic imagery were less prone to over-fitting and provide a state-of-the-art baseline for future work.
ER  -


TY  - Preprint
T1  - Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability
A1  - Shayegan Omidshafiei
A1  - Jason Pazis
A1  - Christopher Amato
A1  - Jonathan P. How
A1  - John Vian
JO  - ArXiv e-prints
Y1  - 13 July, 2017
UR  - https://arxiv.org/abs/1703.06182
N2  - Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrently-exploring teammates. Approaches that learn specialized policies for individual tasks face problems when applied to the real world: not only do agents have to learn and store distinct policies for each task, but in practice identities of tasks are often non-observable, making these approaches inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.
ER  -


TY  - Preprint
T1  - Semi-Supervised Deep Learning for Fully Convolutional Networks
A1  - Christoph Baur
A1  - Shadi Albarqouni
A1  - Nassir Navab
JO  - ArXiv e-prints
Y1  - 25 July, 2017
UR  - https://arxiv.org/abs/1703.06000
N2  - Deep learning usually requires large amounts of labeled training data, but annotating data is costly and tedious. The framework of semi-supervised learning provides the means to use both labeled data and arbitrary amounts of unlabeled data for training. Recently, semi-supervised deep learning has been intensively studied for standard CNN architectures. However, Fully Convolutional Networks (FCNs) set the state-of-the-art for many image segmentation tasks. To the best of our knowledge, there is no existing semi-supervised learning method for such FCNs yet. We lift the concept of auxiliary manifold embedding for semi-supervised learning to FCNs with the help of Random Feature Embedding. In our experiments on the challenging task of MS Lesion Segmentation, we leverage the proposed framework for the purpose of domain adaptation and report substantial improvements over the baseline model.
ER  -


TY  - Preprint
T1  - Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning
A1  - Mohammed Sadegh Norouzzadeh
A1  - Anh Nguyen
A1  - Margaret Kosmala
A1  - Ali Swanson
A1  - Meredith Palmer
A1  - Craig Packer
A1  - Jeff Clune
JO  - ArXiv e-prints
Y1  - 15 November, 2017
UR  - https://arxiv.org/abs/1703.05830
N2  - Having accurate, detailed, and up-to-date information about the location and behavior of animals in the wild would revolutionize our ability to study and conserve ecosystems. We investigate the ability to automatically, accurately, and inexpensively collect such data, which could transform many fields of biology, ecology, and zoology into &#34;big data&#34; sciences. Motion sensor &#34;camera traps&#34; enable collecting wildlife pictures inexpensively, unobtrusively, and frequently. However, extracting information from these pictures remains an expensive, time-consuming, manual task. We demonstrate that such information can be automatically extracted by deep learning, a cutting-edge type of artificial intelligence. We train deep convolutional neural networks to identify, count, and describe the behaviors of 48 species in the 3.2-million-image Snapshot Serengeti dataset. Our deep neural networks automatically identify animals with over 93.8% accuracy, and we expect that number to improve rapidly in years to come. More importantly, if our system classifies only images it is confident about, our system can automate animal identification for 99.3% of the data while still performing at the same 96.6% accuracy as that of crowdsourced teams of human volunteers, saving more than 8.4 years (at 40 hours per week) of human labeling effort (i.e. over 17,000 hours) on this 3.2-million-image dataset. Those efficiency gains immediately highlight the importance of using deep neural networks to automate data extraction from camera-trap images. Our results suggest that this technology could enable the inexpensive, unobtrusive, high-volume, and even real-time collection of a wealth of information about vast numbers of animals in the wild.
ER  -


TY  - Preprint
T1  - A Study of Complex Deep Learning Networks on High Performance, Neuromorphic, and Quantum Computers
A1  - Thomas E. Potok
A1  - Catherine Schuman
A1  - Steven R. Young
A1  - Robert M. Patton
A1  - Federico Spedalieri
A1  - Jeremy Liu
A1  - Ke-Thia Yao
A1  - Garrett Rose
A1  - Gangotree Chakma
JO  - ArXiv e-prints
Y1  - 13 July, 2017
UR  - https://arxiv.org/abs/1703.05364
N2  - Current Deep Learning approaches have been very successful using convolutional neural networks (CNN) trained on large graphical processing units (GPU)-based computers. Three limitations of this approach are: 1) they are based on a simple layered network topology, i.e., highly connected layers, without intra-layer connections; 2) the networks are manually configured to achieve optimal results, and 3) the implementation of neuron model is expensive in both cost and power. In this paper, we evaluate deep learning models using three different computing architectures to address these problems: quantum computing to train complex topologies, high performance computing (HPC) to automatically determine network topology, and neuromorphic computing for a low-power hardware implementation. We use the MNIST dataset for our experiment, due to input size limitations of current quantum computers. Our results show the feasibility of using the three architectures in tandem to address the above deep learning limitations. We show a quantum computer can find high quality values of intra-layer connections weights, in a tractable time as the complexity of the network increases; a high performance computer can find optimal layer-based topologies; and a neuromorphic computer can represent the complex topology and weights derived from the other architectures in low power memristive hardware.
ER  -


TY  - Preprint
T1  - Deep learning with convolutional neural networks for EEG decoding and visualization
A1  - Robin Tibor Schirrmeister
A1  - Jost Tobias Springenberg
A1  - Lukas Dominique Josef Fiederer
A1  - Martin Glasstetter
A1  - Katharina Eggensperger
A1  - Michael Tangermann
A1  - Frank Hutter
A1  - Wolfram Burgard
A1  - Tonio Ball
JO  - ArXiv e-prints
Y1  - 8 June, 2018
UR  - https://arxiv.org/abs/1703.05051
N2  - PLEASE READ AND CITE THE REVISED VERSION at Human Brain Mapping: http://onlinelibrary.wiley.com/doi/10.1002/hbm.23730/full
ER  -


TY  - Preprint
T1  - What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?
A1  - Alex Kendall
A1  - Yarin Gal
JO  - ArXiv e-prints
Y1  - 5 October, 2017
UR  - https://arxiv.org/abs/1703.04977
N2  - There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model -- uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.
ER  -


TY  - Preprint
T1  - Comparison of the Deep-Learning-Based Automated Segmentation Methods for the Head Sectioned Images of the Virtual Korean Human Project
A1  - Mohammad Eshghi
A1  - Holger R. Roth
A1  - Masahiro Oda
A1  - Min Suk Chung
A1  - Kensaku Mori
JO  - ArXiv e-prints
Y1  - 15 March, 2017
UR  - https://arxiv.org/abs/1703.04967
N2  - This paper presents an end-to-end pixelwise fully automated segmentation of the head sectioned images of the Visible Korean Human (VKH) project based on Deep Convolutional Neural Networks (DCNNs). By converting classification networks into Fully Convolutional Networks (FCNs), a coarse prediction map, with smaller size than the original input image, can be created for segmentation purposes. To refine this map and to obtain a dense pixel-wise output, standard FCNs use deconvolution layers to upsample the coarse map. However, upsampling based on deconvolution increases the number of network parameters and causes loss of detail because of interpolation. On the other hand, dilated convolution is a new technique introduced recently that attempts to capture multi-scale contextual information without increasing the network parameters while keeping the resolution of the prediction maps high. We used both a standard FCN and a dilated convolution based FCN for semantic segmentation of the head sectioned images of the VKH dataset. Quantitative results showed approximately 20% improvement in the segmentation accuracy when using FCNs with dilated convolutions.
ER  -


TY  - Preprint
T1  - A fully end-to-end deep learning approach for real-time simultaneous 3D reconstruction and material recognition
A1  - Cheng Zhao
A1  - Li Sun
A1  - Rustam Stolkin
JO  - ArXiv e-prints
Y1  - 14 March, 2017
UR  - https://arxiv.org/abs/1703.04699
N2  - This paper addresses the problem of simultaneous 3D reconstruction and material recognition and segmentation. Enabling robots to recognise different materials (concrete, metal etc.) in a scene is important for many tasks, e.g. robotic interventions in nuclear decommissioning. Previous work on 3D semantic reconstruction has predominantly focused on recognition of everyday domestic objects (tables, chairs etc.), whereas previous work on material recognition has largely been confined to single 2D images without any 3D reconstruction. Meanwhile, most 3D semantic reconstruction methods rely on computationally expensive post-processing, using Fully-Connected Conditional Random Fields (CRFs), to achieve consistent segmentations. In contrast, we propose a deep learning method which performs 3D reconstruction while simultaneously recognising different types of materials and labelling them at the pixel level. Unlike previous methods, we propose a fully end-to-end approach, which does not require hand-crafted features or CRF post-processing. Instead, we use only learned features, and the CRF segmentation constraints are incorporated inside the fully end-to-end learned system. We present the results of experiments, in which we trained our system to perform real-time 3D semantic reconstruction for 23 different materials in a real-world application. The run-time performance of the system can be boosted to around 10Hz, using a conventional GPU, which is enough to achieve real-time semantic reconstruction using a 30fps RGB-D camera. To the best of our knowledge, this work is the first real-time end-to-end system for simultaneous 3D reconstruction and material recognition.
ER  -


TY  - Preprint
T1  - Sensor Fusion for Robot Control through Deep Reinforcement Learning
A1  - Steven Bohez
A1  - Tim Verbelen
A1  - Elias De Coninck
A1  - Bert Vankeirsbilck
A1  - Pieter Simoens
A1  - Bart Dhoedt
JO  - ArXiv e-prints
Y1  - 13 March, 2017
UR  - https://arxiv.org/abs/1703.04550
N2  - Deep reinforcement learning is becoming increasingly popular for robot control algorithms, with the aim for a robot to self-learn useful feature representations from unstructured sensory input leading to the optimal actuation policy. In addition to sensors mounted on the robot, sensors might also be deployed in the environment, although these might need to be accessed via an unreliable wireless connection. In this paper, we demonstrate deep neural network architectures that are able to fuse information coming from multiple sensors and are robust to sensor failures at runtime. We evaluate our method on a search and pick task for a robot both in simulation and the real world.
ER  -


TY  - Preprint
T1  - Deep Learning for Skin Lesion Classification
A1  - P. Mirunalini
A1  - Aravindan Chandrabose
A1  - Vignesh Gokul
A1  - S. M. Jaisakthi
JO  - ArXiv e-prints
Y1  - 13 March, 2017
UR  - https://arxiv.org/abs/1703.04364
N2  - Melanoma, a malignant form of skin cancer is very threatening to life. Diagnosis of melanoma at an earlier stage is highly needed as it has a very high cure rate. Benign and malignant forms of skin cancer can be detected by analyzing the lesions present on the surface of the skin using dermoscopic images. In this work, an automated skin lesion detection system has been developed which learns the representation of the image using Google&#39;s pretrained CNN model known as Inception-v3 \cite{cnn}. After obtaining the representation vector for our input dermoscopic images we have trained two layer feed forward neural network to classify the images as malignant or benign. The system also classifies the images based on the cause of the cancer either due to melanocytic or non-melanocytic cells using a different neural network. These classification tasks are part of the challenge organized by International Skin Imaging Collaboration (ISIC) 2017. Our system learns to classify the images based on the model built using the training images given in the challenge and the experimental results were evaluated using validation and test sets. Our system has achieved an overall accuracy of 65.8\% for the validation set.
ER  -


TY  - Preprint
T1  - Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs
A1  - Michael Gygli
A1  - Mohammad Norouzi
A1  - Anelia Angelova
JO  - ArXiv e-prints
Y1  - 8 August, 2017
UR  - https://arxiv.org/abs/1703.04363
N2  - We approach structured output prediction by optimizing a deep value network (DVN) to precisely estimate the task loss on different output configurations for a given input. Once the model is trained, we perform inference by gradient descent on the continuous relaxations of the output variables to find outputs with promising scores from the value network. When applied to image segmentation, the value network takes an image and a segmentation mask as inputs and predicts a scalar estimating the intersection over union between the input and ground truth masks. For multi-label classification, the DVN&#39;s objective is to correctly predict the F1 score for any potential label configuration. The DVN framework achieves the state-of-the-art results on multi-label prediction and image segmentation benchmarks.
ER  -


TY  - Preprint
T1  - End-to-End Learning of Geometry and Context for Deep Stereo Regression
A1  - Alex Kendall
A1  - Hayk Martirosyan
A1  - Saumitro Dasgupta
A1  - Peter Henry
A1  - Ryan Kennedy
A1  - Abraham Bachrach
A1  - Adam Bry
JO  - ArXiv e-prints
Y1  - 13 March, 2017
UR  - https://arxiv.org/abs/1703.04309
N2  - We propose a novel deep learning architecture for regressing disparity from a rectified pair of stereo images. We leverage knowledge of the problem&#39;s geometry to form a cost volume using deep feature representations. We learn to incorporate contextual information using 3-D convolutions over this volume. Disparity values are regressed from the cost volume using a proposed differentiable soft argmin operation, which allows us to train our method end-to-end to sub-pixel accuracy without any additional post-processing or regularization. We evaluate our method on the Scene Flow and KITTI datasets and on KITTI we set a new state-of-the-art benchmark, while being significantly faster than competing approaches.
ER  -


TY  - Preprint
T1  - A Hierarchical Framework of Cloud Resource Allocation and Power Management Using Deep Reinforcement Learning
A1  - Ning Liu
A1  - Zhe Li
A1  - Zhiyuan Xu
A1  - Jielong Xu
A1  - Sheng Lin
A1  - Qinru Qiu
A1  - Jian Tang
A1  - Yanzhi Wang
JO  - ArXiv e-prints
Y1  - 11 August, 2017
UR  - https://arxiv.org/abs/1703.04221
N2  - Automatic decision-making approaches, such as reinforcement learning (RL), have been applied to (partially) solve the resource allocation problem adaptively in the cloud computing system. However, a complete cloud resource allocation framework exhibits high dimensions in state and action spaces, which prohibit the usefulness of traditional RL techniques. In addition, high power consumption has become one of the critical concerns in design and control of cloud computing systems, which degrades system reliability and increases cooling cost. An effective dynamic power management (DPM) policy should minimize power consumption while maintaining performance degradation within an acceptable level. Thus, a joint virtual machine (VM) resource allocation and power management framework is critical to the overall cloud computing system. Moreover, novel solution framework is necessary to address the even higher dimensions in state and action spaces. In this paper, we propose a novel hierarchical framework for solving the overall resource allocation and power management problem in cloud computing systems. The proposed hierarchical framework comprises a global tier for VM resource allocation to the servers and a local tier for distributed power management of local servers. The emerging deep reinforcement learning (DRL) technique, which can deal with complicated control problems with large state space, is adopted to solve the global tier problem. Furthermore, an autoencoder and a novel weight sharing structure are adopted to handle the high-dimensional state space and accelerate the convergence speed. On the other hand, the local tier of distributed server power managements comprises an LSTM based workload predictor and a model-free RL based power manager, operating in a distributed manner.
ER  -


TY  - Preprint
T1  - Prostate Cancer Diagnosis using Deep Learning with 3D Multiparametric MRI
A1  - Saifeng Liu
A1  - Huaixiu Zheng
A1  - Yesu Feng
A1  - Wei Li
JO  - ArXiv e-prints
Y1  - 12 March, 2017
UR  - https://arxiv.org/abs/1703.04078
N2  - A novel deep learning architecture (XmasNet) based on convolutional neural networks was developed for the classification of prostate cancer lesions, using the 3D multiparametric MRI data provided by the PROSTATEx challenge. End-to-end training was performed for XmasNet, with data augmentation done through 3D rotation and slicing, in order to incorporate the 3D information of the lesion. XmasNet outperformed traditional machine learning models based on engineered features, for both train and test data. For the test data, XmasNet outperformed 69 methods from 33 participating groups and achieved the second highest AUC (0.84) in the PROSTATEx challenge. This study shows the great potential of deep learning for cancer imaging.
ER  -


TY  - Preprint
T1  - Micro-Objective Learning : Accelerating Deep Reinforcement Learning through the Discovery of Continuous Subgoals
A1  - Sungtae Lee
A1  - Sang-Woo Lee
A1  - Jinyoung Choi
A1  - Dong-Hyun Kwak
A1  - Byoung-Tak Zhang
JO  - ArXiv e-prints
Y1  - 11 March, 2017
UR  - https://arxiv.org/abs/1703.03933
N2  - Recently, reinforcement learning has been successfully applied to the logical game of Go, various Atari games, and even a 3D game, Labyrinth, though it continues to have problems in sparse reward settings. It is difficult to explore, but also difficult to exploit, a small number of successes when learning policy. To solve this issue, the subgoal and option framework have been proposed. However, discovering subgoals online is too expensive to be used to learn options in large state spaces. We propose Micro-objective learning (MOL) to solve this problem. The main idea is to estimate how important a state is while training and to give an additional reward proportional to its importance. We evaluated our algorithm in two Atari games: Montezuma&#39;s Revenge and Seaquest. With three experiments to each game, MOL significantly improved the baseline scores. Especially in Montezuma&#39;s Revenge, MOL achieved two times better results than the previous state-of-the-art model.
ER  -


TY  - Preprint
T1  - Deep Learning in Customer Churn Prediction: Unsupervised Feature Learning on Abstract Company Independent Feature Vectors
A1  - Philip Spanoudes
A1  - Thomson Nguyen
JO  - ArXiv e-prints
Y1  - 10 March, 2017
UR  - https://arxiv.org/abs/1703.03869
N2  - As companies increase their efforts in retaining customers, being able to predict accurately ahead of time, whether a customer will churn in the foreseeable future is an extremely powerful tool for any marketing team. The paper describes in depth the application of Deep Learning in the problem of churn prediction. Using abstract feature vectors, that can generated on any subscription based company&#39;s user event logs, the paper proves that through the use of the intrinsic property of Deep Neural Networks (learning secondary features in an unsupervised manner), the complete pipeline can be applied to any subscription based company with extremely good churn predictive performance. Furthermore the research documented in the paper was performed for Framed Data (a company that sells churn prediction as a service for other companies) in conjunction with the Data Science Institute at Lancaster University, UK. This paper is the intellectual property of Framed Data.
ER  -


TY  - Preprint
T1  - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks
A1  - Chelsea Finn
A1  - Pieter Abbeel
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 18 July, 2017
UR  - https://arxiv.org/abs/1703.03400
N2  - We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.
ER  -


TY  - Preprint
T1  - Deep Learning applied to NLP
A1  - Marc Moreno Lopez
A1  - Jugal Kalita
JO  - ArXiv e-prints
Y1  - 8 March, 2017
UR  - https://arxiv.org/abs/1703.03091
N2  - Convolutional Neural Network (CNNs) are typically associated with Computer Vision. CNNs are responsible for major breakthroughs in Image Classification and are the core of most Computer Vision systems today. More recently CNNs have been applied to problems in Natural Language Processing and gotten some interesting results. In this paper, we will try to explain the basics of CNNs, its different variations and how they have been applied to NLP.
ER  -


TY  - Preprint
T1  - Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection
A1  - Xiaodan Liang
A1  - Lisa Lee
A1  - Eric P. Xing
JO  - ArXiv e-prints
Y1  - 8 March, 2017
UR  - https://arxiv.org/abs/1703.03054
N2  - Despite progress in visual perception tasks such as image classification and detection, computers still struggle to understand the interdependency of objects in the scene as a whole, e.g., relations between objects or their attributes. Existing methods often ignore global context cues capturing the interactions among different object instances, and can only recognize a handful of types by exhaustively training individual detectors for all possible relationships. To capture such global interdependency, we propose a deep Variation-structured Reinforcement Learning (VRL) framework to sequentially discover object relationships and attributes in the whole image. First, a directed semantic action graph is built using language priors to provide a rich and compact representation of semantic correlations between object categories, predicates, and attributes. Next, we use a variation-structured traversal over the action graph to construct a small, adaptive action set for each step based on the current state and historical actions. In particular, an ambiguity-aware object mining scheme is used to resolve semantic ambiguity among object categories that the object detector fails to distinguish. We then make sequential predictions using a deep RL framework, incorporating global context cues and semantic embeddings of previously extracted phrases in the state vector. Our experiments on the Visual Relationship Detection (VRD) dataset and the large-scale Visual Genome dataset validate the superiority of VRL, which can achieve significantly better detection results on datasets involving thousands of relationship and attribute types. We also demonstrate that VRL is able to predict unseen types embedded in our action graph by learning correlations on shared graph nodes.
ER  -


TY  - Preprint
T1  - A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile Analytics
A1  - Seyed Ali Osia
A1  - Ali Shahin Shamsabadi
A1  - Ali Taheri
A1  - Kleomenis Katevas
A1  - Sina Sajadmanesh
A1  - Hamid R. Rabiee
A1  - Nicholas D. Lane
A1  - Hamed Haddadi
JO  - ArXiv e-prints
Y1  - 18 April, 2018
UR  - https://arxiv.org/abs/1703.02952
N2  - Deep Neural Networks are increasingly being used in a variety of machine learning applications applied to user data on the cloud. However, this approach introduces a number of privacy and efficiency challenges, as the cloud operator can perform secondary inferences on the available data. Recently, advances in edge processing have paved the way for more efficient, and private, data processing at the source for simple tasks and lighter models, though they remain a challenge for larger, and more complicated models. In this paper, we present a hybrid approach for breaking down large, complex deep models for cooperative, privacy-preserving analytics. We do this by breaking down the popular deep architectures and fine-tune them in a suitable way. We then evaluate the privacy benefits of this approach based on the information exposed to the cloud service. We also assess the local inference cost of different layers on a modern handset for mobile applications. Our evaluations show that by using certain kind of fine-tuning and embedding techniques and at a small processing cost, we can greatly reduce the level of information available to unintended tasks applied to the data features on the cloud, and hence achieving the desired tradeoff between privacy and performance.
ER  -


TY  - Preprint
T1  - Deep Bayesian Active Learning with Image Data
A1  - Yarin Gal
A1  - Riashat Islam
A1  - Zoubin Ghahramani
JO  - ArXiv e-prints
Y1  - 8 March, 2017
UR  - https://arxiv.org/abs/1703.02910
N2  - Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).
ER  -


TY  - Preprint
T1  - Deep Learning for Automated Quality Assessment of Color Fundus Images in Diabetic Retinopathy Screening
A1  - Sajib Kumar Saha
A1  - Basura Fernando
A1  - Jorge Cuadros
A1  - Di Xiao
A1  - Yogesan Kanagasingam
JO  - ArXiv e-prints
Y1  - 7 March, 2017
UR  - https://arxiv.org/abs/1703.02511
N2  - Purpose To develop a computer based method for the automated assessment of image quality in the context of diabetic retinopathy (DR) to guide the photographer.
ER  -


TY  - Preprint
T1  - Object classification in images of Neoclassical furniture using Deep Learning
A1  - Bernhard Bermeitinger
A1  - AndrÃ© Freitas
A1  - Simon Donig
A1  - Siegfried Handschuh
JO  - ArXiv e-prints
Y1  - 7 March, 2017
UR  - https://arxiv.org/abs/1703.02445
N2  - This short paper outlines research results on object classification in images of Neoclassical furniture. The motivation was to provide an object recognition framework which is able to support the alignment of furniture images with a symbolic level model. A data-driven bottom-up research routine in the Neoclassica research framework is the main use-case. It strives to deliver tools for analyzing the spread of aesthetic forms which are considered as a cultural transfer process.
ER  -


TY  - Preprint
T1  - Deep Learning based Large Scale Visual Recommendation and Search for E-Commerce
A1  - Devashish Shankar
A1  - Sujay Narumanchi
A1  - H A Ananya
A1  - Pramod Kompalli
A1  - Krishnendu Chaudhury
JO  - ArXiv e-prints
Y1  - 7 March, 2017
UR  - https://arxiv.org/abs/1703.02344
N2  - In this paper, we present a unified end-to-end approach to build a large scale Visual Search and Recommendation system for e-commerce. Previous works have targeted these problems in isolation. We believe a more effective and elegant solution could be obtained by tackling them together. We propose a unified Deep Convolutional Neural Network architecture, called VisNet, to learn embeddings to capture the notion of visual similarity, across several semantic granularities. We demonstrate the superiority of our approach for the task of image retrieval, by comparing against the state-of-the-art on the Exact Street2Shop dataset. We then share the design decisions and trade-offs made while deploying the model to power Visual Recommendations across a catalog of 50M products, supporting 2K queries a second at Flipkart, India&#39;s largest e-commerce company. The deployment of our solution has yielded a significant business impact, as measured by the conversion-rate.
ER  -


TY  - Preprint
T1  - Using Deep Learning Method for Classification: A Proposed Algorithm for the ISIC 2017 Skin Lesion Classification Challenge
A1  - Wenhao Zhang
A1  - Liangcai Gao
A1  - Runtao Liu
JO  - ArXiv e-prints
Y1  - 10 March, 2017
UR  - https://arxiv.org/abs/1703.02182
N2  - Skin cancer, the most common human malignancy, is primarily diagnosed visually by physicians [1]. Classification with an automated method like CNN [2, 3] shows potential for challenging tasks [1]. By now, the deep convolutional neural networks are on par with human dermatologist [1]. This abstract is dedicated on developing a Deep Learning method for ISIC [5] 2017 Skin Lesion Detection Competition hosted at [6] to classify the dermatology pictures, which is aimed at improving the diagnostic accuracy rate and general level of the human health. The challenge falls into three sub-challenges, including Lesion Segmentation, Lesion Dermoscopic Feature Extraction and Lesion Classification. This project only participates in the Lesion Classification part. This algorithm is comprised of three steps: (1) original images preprocessing, (2) modelling the processed images using CNN [2, 3] in Caffe [4] framework, (3) predicting the test images and calculating the scores that represent the likelihood of corresponding classification. The models are built on the source images are using the Caffe [4] framework. The scores in prediction step are obtained by two different models from the source images.
ER  -


TY  - Preprint
T1  - On the Expressive Power of Overlapping Architectures of Deep Learning
A1  - Or Sharir
A1  - Amnon Shashua
JO  - ArXiv e-prints
Y1  - 24 February, 2018
UR  - https://arxiv.org/abs/1703.02065
N2  - Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger. For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size. In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of &#34;overlaps&#34; in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field). To theoretically analyze this aspect of network&#39;s design, we focus on a well-established surrogate for ConvNets called Convolutional Arithmetic Circuits (ConvACs), and then demonstrate empirically that our results hold for standard ConvNets as well. Specifically, our analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks. Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers.
ER  -


TY  - Preprint
T1  - Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results
A1  - Antti Tarvainen
A1  - Harri Valpola
JO  - ArXiv e-prints
Y1  - 16 April, 2018
UR  - https://arxiv.org/abs/1703.01780
N2  - The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels from 35.24% to 9.11%.
ER  -


TY  - Preprint
T1  - Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning
A1  - Joshua Achiam
A1  - Shankar Sastry
JO  - ArXiv e-prints
Y1  - 6 March, 2017
UR  - https://arxiv.org/abs/1703.01732
N2  - Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as $Îµ$-greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent&#39;s surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the $k$-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.
ER  -


TY  - Preprint
T1  - Automatic Classification of Cancerous Tissue in Laserendomicroscopy Images of the Oral Cavity using Deep Learning
A1  - Marc Aubreville
A1  - Christian Knipfer
A1  - Nicolai Oetter
A1  - Christian Jaremenko
A1  - Erik Rodner
A1  - Joachim Denzler
A1  - Christopher Bohr
A1  - Helmut Neumann
A1  - Florian Stelzle
A1  - Andreas Maier
JO  - ArXiv e-prints
Y1  - 10 March, 2017
UR  - https://arxiv.org/abs/1703.01622
N2  - Oral Squamous Cell Carcinoma (OSCC) is a common type of cancer of the oral epithelium. Despite their high impact on mortality, sufficient screening methods for early diagnosis of OSCC often lack accuracy and thus OSCCs are mostly diagnosed at a late stage. Early detection and accurate outline estimation of OSCCs would lead to a better curative outcome and an reduction in recurrence rates after surgical treatment.
ER  -


TY  - Preprint
T1  - Deep-Learning for Classification of Colorectal Polyps on Whole-Slide Images
A1  - Bruno Korbar
A1  - Andrea M. Olofson
A1  - Allen P. Miraflor
A1  - Katherine M. Nicka
A1  - Matthew A. Suriawinata
A1  - Lorenzo Torresani
A1  - Arief A. Suriawinata
A1  - Saeed Hassanpour
JO  - ArXiv e-prints
Y1  - 12 April, 2017
UR  - https://arxiv.org/abs/1703.01550
N2  - Histopathological characterization of colorectal polyps is an important principle for determining the risk of colorectal cancer and future rates of surveillance for patients. This characterization is time-intensive, requires years of specialized training, and suffers from significant inter-observer and intra-observer variability. In this work, we built an automatic image-understanding method that can accurately classify different types of colorectal polyps in whole-slide histology images to help pathologists with histopathological characterization and diagnosis of colorectal polyps. The proposed image-understanding method is based on deep-learning techniques, which rely on numerous levels of abstraction for data representation and have shown state-of-the-art results for various image analysis tasks. Our image-understanding method covers all five polyp types (hyperplastic polyp, sessile serrated polyp, traditional serrated adenoma, tubular adenoma, and tubulovillous/villous adenoma) that are included in the US multi-society task force guidelines for colorectal cancer risk assessment and surveillance, and encompasses the most common occurrences of colorectal polyps. Our evaluation on 239 independent test samples shows our proposed method can identify the types of colorectal polyps in whole-slide images with a high efficacy (accuracy: 93.0%, precision: 89.7%, recall: 88.3%, F1 score: 88.8%). The presented method in this paper can reduce the cognitive burden on pathologists and improve their accuracy and efficiency in histopathological characterization of colorectal polyps, and in subsequent risk assessment and follow-up recommendations.
ER  -


TY  - Preprint
T1  - Learning Deep Matrix Representations
A1  - Kien Do
A1  - Truyen Tran
A1  - Svetha Venkatesh
JO  - ArXiv e-prints
Y1  - 4 February, 2018
UR  - https://arxiv.org/abs/1703.01454
N2  - We present a new distributed representation in deep neural nets wherein the information is represented in native form as a matrix. This differs from current neural architectures that rely on vector representations. We consider matrices as central to the architecture and they compose the input, hidden and output layers. The model representation is more compact and elegant -- the number of parameters grows only with the largest dimension of the incoming layer rather than the number of hidden units. We derive several new deep networks: (i) feed-forward nets that map an input matrix into an output matrix, (ii) recurrent nets which map a sequence of input matrices into a sequence of output matrices. We also reinterpret existing models for (iii) memory-augmented networks and (iv) graphs using matrix notations. For graphs we demonstrate how the new notations lead to simple but effective extensions with multiple attentions. Extensive experiments on handwritten digits recognition, face reconstruction, sequence to sequence learning, EEG classification, and graph-based node classification demonstrate the efficacy and compactness of the matrix architectures.
ER  -


TY  - Preprint
T1  - EX2: Exploration with Exemplar Models for Deep Reinforcement Learning
A1  - Justin Fu
A1  - John D. Co-Reyes
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 27 May, 2017
UR  - https://arxiv.org/abs/1703.01260
N2  - Deep reinforcement learning algorithms have been shown to learn complex tasks using highly general policy classes. However, sparse reward problems remain a significant challenge. Exploration methods based on novelty detection have been particularly successful in such settings but typically require generative or predictive models of the observations, which can be difficult to train when the observations are very high-dimensional and complex, as in the case of raw images. We propose a novelty detection algorithm for exploration that is based entirely on discriminatively trained exemplar models, where classifiers are trained to discriminate each visited state against all others. Intuitively, novel states are easier to distinguish against other states seen during training. We show that this kind of discriminative modeling corresponds to implicit density estimation, and that it can be combined with count-based exploration to produce competitive results on a range of popular benchmark tasks, including state-of-the-art results on challenging egocentric observations in the vizDoom benchmark.
ER  -


TY  - Preprint
T1  - Deep Collaborative Learning for Visual Recognition
A1  - Yan Wang
A1  - Lingxi Xie
A1  - Ya Zhang
A1  - Wenjun Zhang
A1  - Alan Yuille
JO  - ArXiv e-prints
Y1  - 3 March, 2017
UR  - https://arxiv.org/abs/1703.01229
N2  - Deep neural networks are playing an important role in state-of-the-art visual recognition. To represent high-level visual concepts, modern networks are equipped with large convolutional layers, which use a large number of filters and contribute significantly to model complexity. For example, more than half of the weights of AlexNet are stored in the first fully-connected layer (4,096 filters).
ER  -


TY  - Preprint
T1  - Deep Learning with Domain Adaptation for Accelerated Projection-Reconstruction MR
A1  - Yo Seob Han
A1  - Jaejun Yoo
A1  - Jong Chul Ye
JO  - ArXiv e-prints
Y1  - 8 January, 2018
UR  - https://arxiv.org/abs/1703.01135
N2  - Purpose: The radial k-space trajectory is a well-established sampling trajectory used in conjunction with magnetic resonance imaging. However, the radial k-space trajectory requires a large number of radial lines for high-resolution reconstruction. Increasing the number of radial lines causes longer acquisition time, making it more difficult for routine clinical use. On the other hand, if we reduce the number of radial lines, streaking artifact patterns are unavoidable. To solve this problem, we propose a novel deep learning approach with domain adaptation to restore high-resolution MR images from under-sampled k-space data.
ER  -


TY  - Preprint
T1  - Deep artifact learning for compressed sensing and parallel MRI
A1  - Dongwook Lee
A1  - Jaejun Yoo
A1  - Jong Chul Ye
JO  - ArXiv e-prints
Y1  - 3 March, 2017
UR  - https://arxiv.org/abs/1703.01120
N2  - Purpose: Compressed sensing MRI (CS-MRI) from single and parallel coils is one of the powerful ways to reduce the scan time of MR imaging with performance guarantee. However, the computational costs are usually expensive. This paper aims to propose a computationally fast and accurate deep learning algorithm for the reconstruction of MR images from highly down-sampled k-space data.
ER  -


TY  - Preprint
T1  - A Novel Multi-task Deep Learning Model for Skin Lesion Segmentation and Classification
A1  - Xulei Yang
A1  - Zeng Zeng
A1  - Si Yong Yeo
A1  - Colin Tan
A1  - Hong Liang Tey
A1  - Yi Su
JO  - ArXiv e-prints
Y1  - 2 March, 2017
UR  - https://arxiv.org/abs/1703.01025
N2  - In this study, a multi-task deep neural network is proposed for skin lesion analysis. The proposed multi-task learning model solves different tasks (e.g., lesion segmentation and two independent binary lesion classifications) at the same time by exploiting commonalities and differences across tasks. This results in improved learning efficiency and potential prediction accuracy for the task-specific models, when compared to training the individual models separately. The proposed multi-task deep learning model is trained and evaluated on the dermoscopic image sets from the International Skin Imaging Collaboration (ISIC) 2017 Challenge - Skin Lesion Analysis towards Melanoma Detection, which consists of 2000 training samples and 150 evaluation samples. The experimental results show that the proposed multi-task deep learning model achieves promising performances on skin lesion segmentation and classification. The average value of Jaccard index for lesion segmentation is 0.724, while the average values of area under the receiver operating characteristic curve (AUC) on two individual lesion classifications are 0.880 and 0.972, respectively.
ER  -


TY  - Preprint
T1  - Autonomous Skill-centric Testing using Deep Learning
A1  - Simon Hangl
A1  - Sebastian Stabinger
A1  - Justus Piater
JO  - ArXiv e-prints
Y1  - 13 August, 2017
UR  - https://arxiv.org/abs/1703.00835
N2  - Software testing is an important tool to ensure software quality. This is a hard task in robotics due to dynamic environments and the expensive development and time-consuming execution of test cases. Most testing approaches use model-based and / or simulation-based testing to overcome these problems. We propose model-free skill-centric testing in which a robot autonomously executes skills in the real world and compares it to previous experiences. The skills are selected by maximising the expected information gain on the distribution of erroneous software functions. We use deep learning to model the sensor data observed during previous successful skill executions and to detect irregularities. Sensor data is connected to function call profiles such that certain misbehaviour can be related to specific functions. We evaluate our approach in simulation and in experiments with a KUKA LWR 4+ robot by purposefully introducing bugs to the software. We demonstrate that these bugs can be detected with high accuracy and without the need for the implementation of specific tests or task-specific models.
ER  -


TY  - Preprint
T1  - A Robust Adaptive Stochastic Gradient Method for Deep Learning
A1  - Caglar Gulcehre
A1  - Jose Sotelo
A1  - Marcin Moczulski
A1  - Yoshua Bengio
JO  - ArXiv e-prints
Y1  - 2 March, 2017
UR  - https://arxiv.org/abs/1703.00788
N2  - Stochastic gradient algorithms are the main focus of large-scale optimization problems and led to important successes in the recent advancement of the deep learning algorithms. The convergence of SGD depends on the careful choice of learning rate and the amount of the noise in stochastic estimates of the gradients. In this paper, we propose an adaptive learning rate algorithm, which utilizes stochastic curvature information of the loss function for automatically tuning the learning rates. The information about the element-wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. We further propose a new variance reduction technique to speed up the convergence. In our experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms.
ER  -


TY  - Preprint
T1  - Deep Predictive Policy Training using Reinforcement Learning
A1  - Ali Ghadirzadeh
A1  - Atsuto Maki
A1  - Danica Kragic
A1  - MÃ¥rten BjÃ¶rkman
JO  - ArXiv e-prints
Y1  - 2 March, 2017
UR  - https://arxiv.org/abs/1703.00727
N2  - Skilled robot task learning is best implemented by predictive action policies due to the inherent latency of sensorimotor processes. However, training such predictive policies is challenging as it involves finding a trajectory of motor activations for the full duration of the action. We propose a data-efficient deep predictive policy training (DPPT) framework with a deep neural network policy architecture which maps an image observation to a sequence of motor activations. The architecture consists of three sub-networks referred to as the perception, policy and behavior super-layers. The perception and behavior super-layers force an abstraction of visual and motor data trained with synthetic and simulated training samples, respectively. The policy super-layer is a small sub-network with fewer parameters that maps data in-between the abstracted manifolds. It is trained for each task using methods for policy search reinforcement learning. We demonstrate the suitability of the proposed architecture and learning framework by training predictive policies for skilled object grasping and ball throwing on a PR2 robot. The effectiveness of the method is illustrated by the fact that these tasks are trained using only about 180 real robot attempts with qualitative terminal rewards.
ER  -


TY  - Preprint
T1  - Skin Lesion Analysis Towards Melanoma Detection Using Deep Learning Network
A1  - Yuexiang Li
A1  - Linlin Shen
JO  - ArXiv e-prints
Y1  - 22 November, 2017
UR  - https://arxiv.org/abs/1703.00577
N2  - Skin lesion is a severe disease in world-wide extent. Early detection of melanoma in dermoscopy images significantly increases the survival rate. However, the accurate recognition of melanoma is extremely challenging due to the following reasons, e.g. low contrast between lesions and skin, visual similarity between melanoma and non-melanoma lesions, etc. Hence, reliable automatic detection of skin tumors is very useful to increase the accuracy and efficiency of pathologists. International Skin Imaging Collaboration (ISIC) is a challenge focusing on the automatic analysis of skin lesion. In this paper, we proposed two deep learning methods to address all the three tasks announced in ISIC 2017, i.e. lesion segmentation (task 1), lesion dermoscopic feature extraction (task 2) and lesion classification (task 3). A deep learning framework consisting of two fully-convolutional residual networks (FCRN) is proposed to simultaneously produce the segmentation result and the coarse classification result. A lesion index calculation unit (LICU) is developed to refine the coarse classification results by calculating the distance heat-map. A straight-forward CNN is proposed for the dermoscopic feature extraction task. To our best knowledges, we are not aware of any previous work proposed for this task. The proposed deep learning frameworks were evaluated on the ISIC 2017 testing set. Experimental results show the promising accuracies of our frameworks, i.e. 0.718 for task 1, 0.833 for task 2 and 0.823 for task 3 were achieved.
ER  -


TY  - Preprint
T1  - Virtual-to-real Deep Reinforcement Learning: Continuous Control of Mobile Robots for Mapless Navigation
A1  - Lei Tai
A1  - Giuseppe Paolo
A1  - Ming Liu
JO  - ArXiv e-prints
Y1  - 21 July, 2017
UR  - https://arxiv.org/abs/1703.00420
N2  - We present a learning-based mapless motion planner by taking the sparse 10-dimensional range findings and the target position with respect to the mobile robot coordinate frame as input and the continuous steering commands as output. Traditional motion planners for mobile ground robots with a laser range sensor mostly depend on the obstacle map of the navigation environment where both the highly precise laser sensor and the obstacle map building work of the environment are indispensable. We show that, through an asynchronous deep reinforcement learning method, a mapless motion planner can be trained end-to-end without any manually designed features and prior demonstrations. The trained planner can be directly applied in unseen virtual and real environments. The experiments show that the proposed mapless motion planner can navigate the nonholonomic mobile robot to the desired targets without colliding with any obstacles.
ER  -


TY  - Preprint
T1  - Easy over Hard: A Case Study on Deep Learning
A1  - Wei Fu
A1  - Tim Menzies
JO  - ArXiv e-prints
Y1  - 24 June, 2017
UR  - https://arxiv.org/abs/1703.00133
N2  - While deep learning is an exciting new technique, the benefits of this method need to be assessed with respect to its computational cost. This is particularly important for deep learning since these learners need hours (to weeks) to train the model. Such long training time limits the ability of (a)~a researcher to test the stability of their conclusion via repeated runs with different random seeds; and (b)~other researchers to repeat, improve, or even refute that original work.
ER  -


TY  - Preprint
T1  - Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning
A1  - Jakob Foerster
A1  - Nantas Nardelli
A1  - Gregory Farquhar
A1  - Triantafyllos Afouras
A1  - Philip H. S. Torr
A1  - Pushmeet Kohli
A1  - Shimon Whiteson
JO  - ArXiv e-prints
Y1  - 21 May, 2018
UR  - https://arxiv.org/abs/1702.08887
N2  - Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on single-agent RL to the multi-agent setting. A major stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep Q-learning relies. This paper proposes two methods that address this problem: 1) using a multi-agent variant of importance sampling to naturally decay obsolete data and 2) conditioning each agent&#39;s value function on a fingerprint that disambiguates the age of the data sampled from the replay memory. Results on a challenging decentralised variant of StarCraft unit micromanagement confirm that these methods enable the successful combination of experience replay with multi-agent RL.
ER  -


TY  - Preprint
T1  - Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees
A1  - Daniel Zoran
A1  - Balaji Lakshminarayanan
A1  - Charles Blundell
JO  - ArXiv e-prints
Y1  - 28 February, 2017
UR  - https://arxiv.org/abs/1702.08833
N2  - Nearest neighbor (kNN) methods have been gaining popularity in recent years in light of advances in hardware and efficiency of algorithms. There is a plethora of methods to choose from today, each with their own advantages and disadvantages. One requirement shared between all kNN based methods is the need for a good representation and distance measure between samples.
ER  -


TY  - Preprint
T1  - Borrowing Treasures from the Wealthy: Deep Transfer Learning through Selective Joint Fine-tuning
A1  - Weifeng Ge
A1  - Yizhou Yu
JO  - ArXiv e-prints
Y1  - 6 June, 2017
UR  - https://arxiv.org/abs/1702.08690
N2  - Deep neural networks require a large amount of labeled training data during supervised learning. However, collecting and labeling so much data might be infeasible in many cases. In this paper, we introduce a source-target selective joint fine-tuning scheme for improving the performance of deep learning tasks with insufficient training data. In this scheme, a target learning task with insufficient training data is carried out simultaneously with another source learning task with abundant training data. However, the source learning task does not use all existing training data. Our core idea is to identify and use a subset of training images from the original source learning task whose low-level characteristics are similar to those from the target learning task, and jointly fine-tune shared convolutional layers for both tasks. Specifically, we compute descriptors from linear or nonlinear filter bank responses on training images from both tasks, and use such descriptors to search for a desired subset of training samples for the source learning task.
ER  -


TY  - Preprint
T1  - On architectural choices in deep learning: From network structure to gradient convergence and parameter estimation
A1  - Vamsi K Ithapu
A1  - Sathya N Ravi
A1  - Vikas Singh
JO  - ArXiv e-prints
Y1  - 28 February, 2017
UR  - https://arxiv.org/abs/1702.08670
N2  - We study mechanisms to characterize how the asymptotic convergence of backpropagation in deep architectures, in general, is related to the network structure, and how it may be influenced by other design choices including activation type, denoising and dropout rate. We seek to analyze whether network architecture and input data statistics may guide the choices of learning parameters and vice versa. Given the broad applicability of deep architectures, this issue is interesting both from theoretical and a practical standpoint. Using properties of general nonconvex objectives (with first-order information), we first build the association between structural, distributional and learnability aspects of the network vis-Ã -vis their interaction with parameter convergence rates. We identify a nice relationship between feature denoising and dropout, and construct families of networks that achieve the same level of convergence. We then derive a workflow that provides systematic guidance regarding the choice of network sizes and learning parameters often mediated4 by input statistics. Our technical results are corroborated by an extensive set of evaluations, presented in this paper as well as independent empirical observations reported by other groups. We also perform experiments showing the practical implications of our framework for choosing the best fully-connected design for a given problem.
ER  -


TY  - Preprint
T1  - Lensless computational imaging through deep learning
A1  - Ayan Sinha
A1  - Justin Lee
A1  - Shuai Li
A1  - George Barbastathis
JO  - ArXiv e-prints
Y1  - 26 June, 2017
UR  - https://arxiv.org/abs/1702.08516
N2  - Deep learning has been proven to yield reliably generalizable answers to numerous classification and decision tasks. Here, we demonstrate for the first time, to our knowledge, that deep neural networks (DNNs) can be trained to solve inverse problems in computational imaging. We experimentally demonstrate a lens-less imaging system where a DNN was trained to recover a phase object given a raw intensity image recorded some distance away.
ER  -


TY  - Preprint
T1  - Learning Deep Visual Object Models From Noisy Web Data: How to Make it Work
A1  - Nizar Massouh
A1  - Francesca Babiloni
A1  - Tatiana Tommasi
A1  - Jay Young
A1  - Nick Hawes
A1  - Barbara Caputo
JO  - ArXiv e-prints
Y1  - 28 February, 2017
UR  - https://arxiv.org/abs/1702.08513
N2  - Deep networks thrive when trained on large scale data collections. This has given ImageNet a central role in the development of deep architectures for visual object classification. However, ImageNet was created during a specific period in time, and as such it is prone to aging, as well as dataset bias issues. Moving beyond fixed training datasets will lead to more robust visual systems, especially when deployed on robots in new environments which must train on the objects they encounter there. To make this possible, it is important to break free from the need for manual annotators. Recent work has begun to investigate how to use the massive amount of images available on the Web in place of manual image annotations. We contribute to this research thread with two findings: (1) a study correlating a given level of noisily labels to the expected drop in accuracy, for two deep architectures, on two different types of noise, that clearly identifies GoogLeNet as a suitable architecture for learning from Web data; (2) a recipe for the creation of Web datasets with minimal noise and maximum visual variability, based on a visual and natural language processing concept expansion strategy. By combining these two results, we obtain a method for learning powerful deep object models automatically from the Web. We confirm the effectiveness of our approach through object categorization experiments using our Web-derived version of ImageNet on a popular robot vision benchmark database, and on a lifelong object discovery task on a mobile robot.
ER  -


TY  - Preprint
T1  - Neural Map: Structured Memory for Deep Reinforcement Learning
A1  - Emilio Parisotto
A1  - Ruslan Salakhutdinov
JO  - ArXiv e-prints
Y1  - 27 February, 2017
UR  - https://arxiv.org/abs/1702.08360
N2  - A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training.
ER  -


TY  - Preprint
T1  - Identifying beneficial task relations for multi-task learning in deep neural networks
A1  - Joachim Bingel
A1  - Anders SÃ¸gaard
JO  - ArXiv e-prints
Y1  - 27 February, 2017
UR  - https://arxiv.org/abs/1702.08303
N2  - Multi-task learning (MTL) in deep neural networks for NLP has recently received increasing interest due to some compelling benefits, including its potential to efficiently regularize models and to reduce the need for labeled data. While it has brought significant improvements in a number of NLP tasks, mixed results have been reported, and little is known about the conditions under which MTL leads to gains in NLP. This paper sheds light on the specific task relations that can lead to gains from MTL models over single-task setups.
ER  -


TY  - Preprint
T1  - Reinforcement Learning with Deep Energy-Based Policies
A1  - Tuomas Haarnoja
A1  - Haoran Tang
A1  - Pieter Abbeel
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 21 July, 2017
UR  - https://arxiv.org/abs/1702.08165
N2  - We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.
ER  -


TY  - Preprint
T1  - Learning Control for Air Hockey Striking using Deep Reinforcement Learning
A1  - Ayal Taitler
A1  - Nahum Shimkin
JO  - ArXiv e-prints
Y1  - 25 April, 2017
UR  - https://arxiv.org/abs/1702.08074
N2  - We consider the task of learning control policies for a robotic mechanism striking a puck in an air hockey game. The control signal is a direct command to the robot&#39;s motors. We employ a model free deep reinforcement learning framework to learn the motoric skills of striking the puck accurately in order to score. We propose certain improvements to the standard learning scheme which make the deep Q-learning algorithm feasible when it might otherwise fail. Our improvements include integrating prior knowledge into the learning scheme, and accounting for the changing distribution of samples in the experience replay buffer. Finally we present our simulation results for aimed striking which demonstrate the successful learning of this task, and the improvement in algorithm stability due to the proposed modifications.
ER  -


TY  - Preprint
T1  - Criticality &amp; Deep Learning I: Generally Weighted Nets
A1  - Dan Oprisa
A1  - Peter Toth
JO  - ArXiv e-prints
Y1  - 31 May, 2017
UR  - https://arxiv.org/abs/1702.08039
N2  - Motivated by the idea that criticality and universality of phase transitions might play a crucial role in achieving and sustaining learning and intelligent behaviour in biological and artificial networks, we analyse a theoretical and a pragmatic experimental set up for critical phenomena in deep learning. On the theoretical side, we use results from statistical physics to carry out critical point calculations in feed-forward/fully connected networks, while on the experimental side we set out to find traces of criticality in deep neural networks. This is our first step in a series of upcoming investigations to map out the relationship between criticality and learning in deep networks.
ER  -


TY  - Preprint
T1  - Spatially Aware Melanoma Segmentation Using Hybrid Deep Learning Techniques
A1  - M. Attia
A1  - M. Hossny
A1  - S. Nahavandi
A1  - A. Yazdabadi
JO  - ArXiv e-prints
Y1  - 25 February, 2017
UR  - https://arxiv.org/abs/1702.07963
N2  - In this paper, we proposed using a hybrid method that utilises deep convolutional and recurrent neural networks for accurate delineation of skin lesion of images supplied with ISBI 2017 lesion segmentation challenge. The proposed method was trained using 1800 images and tested on 150 images from ISBI 2017 challenge.
ER  -


TY  - Preprint
T1  - Learning Deep NBNN Representations for Robust Place Categorization
A1  - Massimiliano Mancini
A1  - Samuel Rota BulÃ²
A1  - Elisa Ricci
A1  - Barbara Caputo
JO  - ArXiv e-prints
Y1  - 4 May, 2017
UR  - https://arxiv.org/abs/1702.07898
N2  - This paper presents an approach for semantic place categorization using data obtained from RGB cameras. Previous studies on visual place recognition and classification have shown that, by considering features derived from pre-trained Convolutional Neural Networks (CNNs) in combination with part-based classification models, high recognition accuracy can be achieved, even in presence of occlusions and severe viewpoint changes. Inspired by these works, we propose to exploit local deep representations, representing images as set of regions applying a NaÃ¯ve Bayes Nearest Neighbor (NBNN) model for image classification. As opposed to previous methods where CNNs are merely used as feature extractors, our approach seamlessly integrates the NBNN model into a fully-convolutional neural network. Experimental results show that the proposed algorithm outperforms previous methods based on pre-trained CNN models and that, when employed in challenging robot place recognition tasks, it is robust to occlusions, environmental and sensor changes.
ER  -


TY  - Preprint
T1  - On the Origin of Deep Learning
A1  - Haohan Wang
A1  - Bhiksha Raj
JO  - ArXiv e-prints
Y1  - 2 March, 2017
UR  - https://arxiv.org/abs/1702.07800
N2  - This paper is a review of the evolutionary history of deep learning models. It covers from the genesis of neural networks when associationism modeling of the brain is studied, to the models that dominate the last decade of research in deep learning like convolutional neural networks, deep belief networks, and recurrent neural networks. In addition to a review of these models, this paper primarily focuses on the precedents of the models above, examining how the initial ideas are assembled to construct the early models and how these preliminary models are developed into their current forms. Many of these evolutionary paths last more than half a century and have a diversity of directions. For example, CNN is built on prior knowledge of biological vision system; DBN is evolved from a trade-off of modeling power and computation complexity of graphical models and many nowadays models are neural counterparts of ancient linear models. This paper reviews these evolutionary paths and offers a concise thought flow of how these models are developed, and aims to provide a thorough background for deep learning. More importantly, along with the path, this paper summarizes the gist behind these milestones and proposes many directions to guide the future research of deep learning.
ER  -


TY  - Preprint
T1  - Robot gains Social Intelligence through Multimodal Deep Reinforcement Learning
A1  - Ahmed Hussain Qureshi
A1  - Yutaka Nakamura
A1  - Yuichiro Yoshikawa
A1  - Hiroshi Ishiguro
JO  - ArXiv e-prints
Y1  - 24 February, 2017
UR  - https://arxiv.org/abs/1702.07492
N2  - For robots to coexist with humans in a social world like ours, it is crucial that they possess human-like social interaction skills. Programming a robot to possess such skills is a challenging task. In this paper, we propose a Multimodal Deep Q-Network (MDQN) to enable a robot to learn human-like interaction skills through a trial and error method. This paper aims to develop a robot that gathers data during its interaction with a human and learns human interaction behaviour from the high-dimensional sensory information using end-to-end reinforcement learning. This paper demonstrates that the robot was able to learn basic interaction skills successfully, after 14 days of interacting with people.
ER  -


TY  - Preprint
T1  - Deep representation learning for human motion prediction and classification
A1  - Judith BÃ¼tepage
A1  - Michael Black
A1  - Danica Kragic
A1  - Hedvig KjellstrÃ¶m
JO  - ArXiv e-prints
Y1  - 13 April, 2017
UR  - https://arxiv.org/abs/1702.07486
N2  - Generative models of 3D human motion are often restricted to a small number of activities and can therefore not generalize well to novel movements or applications. In this work we propose a deep learning framework for human motion capture data that learns a generic representation from a large corpus of motion capture data and generalizes well to new, unseen, motions. Using an encoding-decoding network that learns to predict future 3D poses from the most recent past, we extract a feature representation of human motion. Most work on deep learning for sequence prediction focuses on video and speech. Since skeletal data has a different structure, we present and evaluate different network architectures that make different assumptions about time dependencies and limb correlations. To quantify the learned features, we use the output of different layers for action classification and visualize the receptive fields of the network units. Our method outperforms the recent state of the art in skeletal motion prediction even though these use action specific training data. Our results show that deep feedforward networks, trained from a generic mocap database, can successfully be used for feature extraction from human motion data and that this representation can be used as a foundation for classification and prediction.
ER  -


TY  - Preprint
T1  - Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning
A1  - Briland Hitaj
A1  - Giuseppe Ateniese
A1  - Fernando Perez-Cruz
JO  - ArXiv e-prints
Y1  - 14 September, 2017
UR  - https://arxiv.org/abs/1702.07464
N2  - Deep Learning has recently become hugely popular in machine learning, providing significant improvements in classification accuracy in the presence of highly-structured and large databases.
ER  -


TY  - Preprint
T1  - Data-Driven Fuzzy Modeling Using Deep Learning
A1  - Erick de la Rosa
A1  - Wen Yu
JO  - ArXiv e-prints
Y1  - 22 February, 2017
UR  - https://arxiv.org/abs/1702.07076
N2  - Fuzzy modeling has many advantages over the non-fuzzy methods, such as robustness against uncertainties and less sensitivity to the varying dynamics of nonlinear systems. Data-driven fuzzy modeling needs to extract fuzzy rules from the input/output data, and train the fuzzy parameters. This paper takes advantages from deep learning, probability theory, fuzzy modeling, and extreme learning machines. We use the restricted Boltzmann machine (RBM) and probability theory to overcome some common problems in data based modeling methods. The RBM is modified such that it can be trained with continuous values. A probability based clustering method is proposed to partition the hidden features from the RBM, and extract fuzzy rules with probability measurement. An extreme learning machine and an optimization method are applied to train the consequent part of the fuzzy rules and the probability parameters. The proposed method is validated with two benchmark problems.
ER  -


TY  - Preprint
T1  - Learning Chained Deep Features and Classifiers for Cascade in Object Detection
A1  - Wanli Ouyang
A1  - Ku Wang
A1  - Xin Zhu
A1  - Xiaogang Wang
JO  - ArXiv e-prints
Y1  - 22 February, 2017
UR  - https://arxiv.org/abs/1702.07054
N2  - Cascade is a widely used approach that rejects obvious negative samples at early stages for learning better classifier and faster inference. This paper presents chained cascade network (CC-Net). In this CC-Net, the cascaded classifier at a stage is aided by the classification scores in previous stages. Feature chaining is further proposed so that the feature learning for the current cascade stage uses the features in previous stages as the prior information. The chained ConvNet features and classifiers of multiple stages are jointly learned in an end-to-end network. In this way, features and classifiers at latter stages handle more difficult samples with the help of features and classifiers in previous stages. It yields consistent boost in detection performance on benchmarks like PASCAL VOC 2007 and ImageNet. Combined with better region proposal, CC-Net leads to state-of-the-art result of 81.1% mAP on PASCAL VOC 2007.
ER  -


TY  - Preprint
T1  - Proactive Resource Management in LTE-U Systems: A Deep Learning Perspective
A1  - Ursula Challita
A1  - Li Dong
A1  - Walid Saad
JO  - ArXiv e-prints
Y1  - 22 February, 2017
UR  - https://arxiv.org/abs/1702.07031
N2  - LTE in unlicensed spectrum (LTE-U) is a promising approach to overcome the wireless spectrum scarcity. However, to reap the benefits of LTE-U, a fair coexistence mechanism with other incumbent WiFi deployments is required. In this paper, a novel deep learning approach is proposed for modeling the resource allocation problem of LTE-U small base stations (SBSs). The proposed approach enables multiple SBSs to proactively perform dynamic channel selection, carrier aggregation, and fractional spectrum access while guaranteeing fairness with existing WiFi networks and other LTE-U operators. Adopting a proactive coexistence mechanism enables future delay-intolerant LTE-U data demands to be served within a given prediction window ahead of their actual arrival time thus avoiding the underutilization of the unlicensed spectrum during off-peak hours while maximizing the total served LTE-U traffic load. To this end, a noncooperative game model is formulated in which SBSs are modeled as Homo Egualis agents that aim at predicting a sequence of future actions and thus achieving long-term equal weighted fairness with WLAN and other LTE-U operators over a given time horizon. The proposed deep learning algorithm is then shown to reach a mixed-strategy Nash equilibrium (NE), when it converges. Simulation results using real data traces show that the proposed scheme can yield up to 28% and 11% gains over a conventional reactive approach and a proportional fair coexistence mechanism, respectively. The results also show that the proposed framework prevents WiFi performance degradation for a densely deployed LTE-U network.
ER  -


TY  - Preprint
T1  - Scaling Deep Learning-based Decoding of Polar Codes via Partitioning
A1  - Sebastian Cammerer
A1  - Tobias Gruber
A1  - Jakob Hoydis
A1  - Stephan ten Brink
JO  - ArXiv e-prints
Y1  - 22 February, 2017
UR  - https://arxiv.org/abs/1702.06901
N2  - The training complexity of deep learning-based channel decoders scales exponentially with the codebook size and therefore with the number of information bits. Thus, neural network decoding (NND) is currently only feasible for very short block lengths. In this work, we show that the conventional iterative decoding algorithm for polar codes can be enhanced when sub-blocks of the decoder are replaced by neural network (NN) based components. Thus, we partition the encoding graph into smaller sub-blocks and train them individually, closely approaching maximum a posteriori (MAP) performance per sub-block. These blocks are then connected via the remaining conventional belief propagation decoding stage(s). The resulting decoding algorithm is non-iterative and inherently enables a high-level of parallelization, while showing a competitive bit error rate (BER) performance. We examine the degradation through partitioning and compare the resulting decoder to state-of-the-art polar decoders such as successive cancellation list and belief propagation decoding.
ER  -


TY  - Preprint
T1  - Learning Deep Features via Congenerous Cosine Loss for Person Recognition
A1  - Yu Liu
A1  - Hongyang Li
A1  - Xiaogang Wang
JO  - ArXiv e-prints
Y1  - 31 March, 2017
UR  - https://arxiv.org/abs/1702.06890
N2  - Person recognition aims at recognizing the same identity across time and space with complicated scenes and similar appearance. In this paper, we propose a novel method to address this task by training a network to obtain robust and representative features. The intuition is that we directly compare and optimize the cosine distance between two features - enlarging inter-class distinction as well as alleviating inner-class variance. We propose a congenerous cosine loss by minimizing the cosine distance between samples and their cluster centroid in a cooperative way. Such a design reduces the complexity and could be implemented via softmax with normalized inputs. Our method also differs from previous work in person recognition that we do not conduct a second training on the test subset. The identity of a person is determined by measuring the similarity from several body regions in the reference set. Experimental results show that the proposed approach achieves better classification accuracy against previous state-of-the-arts.
ER  -


TY  - Preprint
T1  - Using Deep Learning and Google Street View to Estimate the Demographic Makeup of the US
A1  - Timnit Gebru
A1  - Jonathan Krause
A1  - Yilun Wang
A1  - Duyun Chen
A1  - Jia Deng
A1  - Erez Lieberman Aiden
A1  - Li Fei-Fei
JO  - ArXiv e-prints
Y1  - 2 March, 2017
UR  - https://arxiv.org/abs/1702.06683
N2  - The United States spends more than $1B each year on initiatives such as the American Community Survey (ACS), a labor-intensive door-to-door study that measures statistics relating to race, gender, education, occupation, unemployment, and other demographic factors. Although a comprehensive source of data, the lag between demographic changes and their appearance in the ACS can exceed half a decade. As digital imagery becomes ubiquitous and machine vision techniques improve, automated data analysis may provide a cheaper and faster alternative. Here, we present a method that determines socioeconomic trends from 50 million images of street scenes, gathered in 200 American cities by Google Street View cars. Using deep learning-based computer vision techniques, we determined the make, model, and year of all motor vehicles encountered in particular neighborhoods. Data from this census of motor vehicles, which enumerated 22M automobiles in total (8% of all automobiles in the US), was used to accurately estimate income, race, education, and voting patterns, with single-precinct resolution. (The average US precinct contains approximately 1000 people.) The resulting associations are surprisingly simple and powerful. For instance, if the number of sedans encountered during a 15-minute drive through a city is higher than the number of pickup trucks, the city is likely to vote for a Democrat during the next Presidential election (88% chance); otherwise, it is likely to vote Republican (82%). Our results suggest that automated systems for monitoring demographic trends may effectively complement labor-intensive approaches, with the potential to detect trends with fine spatial resolution, in close to real time.
ER  -


TY  - Preprint
T1  - Mimicking Ensemble Learning with Deep Branched Networks
A1  - Byungju Kim
A1  - Youngsoo Kim
A1  - Yeakang Lee
A1  - Junmo Kim
JO  - ArXiv e-prints
Y1  - 21 February, 2017
UR  - https://arxiv.org/abs/1702.06376
N2  - This paper proposes a branched residual network for image classification. It is known that high-level features of deep neural network are more representative than lower-level features. By sharing the low-level features, the network can allocate more memory to high-level features. The upper layers of our proposed network are branched, so that it mimics the ensemble learning. By mimicking ensemble learning with single network, we have achieved better performance on ImageNet classification task.
ER  -


TY  - Preprint
T1  - Beating the World&#39;s Best at Super Smash Bros. with Deep Reinforcement Learning
A1  - Vlad Firoiu
A1  - William F. Whitney
A1  - Joshua B. Tenenbaum
JO  - ArXiv e-prints
Y1  - 8 May, 2017
UR  - https://arxiv.org/abs/1702.06230
N2  - There has been a recent explosion in the capabilities of game-playing artificial intelligence. Many classes of RL tasks, from Atari games to motor control to board games, are now solvable by fairly generic algorithms, based on deep learning, that learn to play from experience with minimal knowledge of the specific domain of interest. In this work, we will investigate the performance of these methods on Super Smash Bros. Melee (SSBM), a popular console fighting game. The SSBM environment has complex dynamics and partial observability, making it challenging for human and machine alike. The multi-player aspect poses an additional challenge, as the vast majority of recent advances in RL have focused on single-agent environments. Nonetheless, we will show that it is possible to train agents that are competitive against and even surpass human professionals, a new result for the multi-player video game setting.
ER  -


TY  - Preprint
T1  - An Attention-Based Deep Net for Learning to Rank
A1  - Baiyang Wang
A1  - Diego Klabjan
JO  - ArXiv e-prints
Y1  - 10 December, 2017
UR  - https://arxiv.org/abs/1702.06106
N2  - In information retrieval, learning to rank constructs a machine-based ranking model which given a query, sorts the search results by their degree of relevance or importance to the query. Neural networks have been successfully applied to this problem, and in this paper, we propose an attention-based deep neural network which better incorporates different embeddings of the queries and search results with an attention-based mechanism. This model also applies a decoder mechanism to learn the ranks of the search results in a listwise fashion. The embeddings are trained with convolutional neural networks or the word2vec model. We demonstrate the performance of this model with image retrieval and text querying data sets.
ER  -


TY  - Preprint
T1  - Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning
A1  - Sahil Sharma
A1  - Aravind S. Lakshminarayanan
A1  - Balaraman Ravindran
JO  - ArXiv e-prints
Y1  - 20 February, 2017
UR  - https://arxiv.org/abs/1702.06054
N2  - Reinforcement Learning algorithms can learn complex behavioral patterns for sequential decision making tasks wherein an agent interacts with an environment and acquires feedback in the form of rewards sampled from it. Traditionally, such algorithms make decisions, i.e., select actions to execute, at every single time step of the agent-environment interactions. In this paper, we propose a novel framework, Fine Grained Action Repetition (FiGAR), which enables the agent to decide the action as well as the time scale of repeating it. FiGAR can be used for improving any Deep Reinforcement Learning algorithm which maintains an explicit policy estimate by enabling temporal abstractions in the action space. We empirically demonstrate the efficacy of our framework by showing performance improvements on top of three policy search algorithms in different domains: Asynchronous Advantage Actor Critic in the Atari 2600 domain, Trust Region Policy Optimization in Mujoco domain and Deep Deterministic Policy Gradients in the TORCS car racing domain.
ER  -


TY  - Preprint
T1  - Deep learning-based assessment of tumor-associated stroma for diagnosing breast cancer in histopathology images
A1  - Babak Ehteshami Bejnordi
A1  - Jimmy Linz
A1  - Ben Glass
A1  - Maeve Mullooly
A1  - Gretchen L Gierach
A1  - Mark E Sherman
A1  - Nico Karssemeijer
A1  - Jeroen van der Laak
A1  - Andrew H Beck
JO  - ArXiv e-prints
Y1  - 19 February, 2017
UR  - https://arxiv.org/abs/1702.05803
N2  - Diagnosis of breast carcinomas has so far been limited to the morphological interpretation of epithelial cells and the assessment of epithelial tissue architecture. Consequently, most of the automated systems have focused on characterizing the epithelial regions of the breast to detect cancer. In this paper, we propose a system for classification of hematoxylin and eosin (H&amp;E) stained breast specimens based on convolutional neural networks that primarily targets the assessment of tumor-associated stroma to diagnose breast cancer patients. We evaluate the performance of our proposed system using a large cohort containing 646 breast tissue biopsies. Our evaluations show that the proposed system achieves an area under ROC of 0.92, demonstrating the discriminative power of previously neglected tumor-associated stroma as a diagnostic biomarker.
ER  -


TY  - Preprint
T1  - Collaborative Deep Reinforcement Learning
A1  - Kaixiang Lin
A1  - Shu Wang
A1  - Jiayu Zhou
JO  - ArXiv e-prints
Y1  - 19 February, 2017
UR  - https://arxiv.org/abs/1702.05796
N2  - Besides independent learning, human learning process is highly improved by summarizing what has been learned, communicating it with peers, and subsequently fusing knowledge from different sources to assist the current learning goal. This collaborative learning procedure ensures that the knowledge is shared, continuously refined, and concluded from different perspectives to construct a more profound understanding. The idea of knowledge transfer has led to many advances in machine learning and data mining, but significant challenges remain, especially when it comes to reinforcement learning, heterogeneous model structures, and different learning tasks. Motivated by human collaborative learning, in this paper we propose a collaborative deep reinforcement learning (CDRL) framework that performs adaptive knowledge transfer among heterogeneous learning agents. Specifically, the proposed CDRL conducts a novel deep knowledge distillation method to address the heterogeneity among different learning tasks with a deep alignment network. Furthermore, we present an efficient collaborative Asynchronous Advantage Actor-Critic (cA3C) algorithm to incorporate deep knowledge distillation into the online training of agents, and demonstrate the effectiveness of the CDRL framework using extensive empirical evaluation on OpenAI gym.
ER  -


TY  - Preprint
T1  - A Survey on Deep Learning in Medical Image Analysis
A1  - Geert Litjens
A1  - Thijs Kooi
A1  - Babak Ehteshami Bejnordi
A1  - Arnaud Arindra Adiyoso Setio
A1  - Francesco Ciompi
A1  - Mohsen Ghafoorian
A1  - Jeroen A. W. M. van der Laak
A1  - Bram van Ginneken
A1  - Clara I. SÃ¡nchez
JO  - ArXiv e-prints
Y1  - 4 June, 2017
UR  - https://arxiv.org/abs/1702.05747
N2  - Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks and provide concise overviews of studies per application area. Open challenges and directions for future research are discussed.
ER  -


TY  - Preprint
T1  - Collaborative Deep Reinforcement Learning for Joint Object Search
A1  - Xiangyu Kong
A1  - Bo Xin
A1  - Yizhou Wang
A1  - Gang Hua
JO  - ArXiv e-prints
Y1  - 18 February, 2017
UR  - https://arxiv.org/abs/1702.05573
N2  - We examine the problem of joint top-down active search of multiple objects under interaction, e.g., person riding a bicycle, cups held by the table, etc.. Such objects under interaction often can provide contextual cues to each other to facilitate more efficient search. By treating each detector as an agent, we present the first collaborative multi-agent deep reinforcement learning algorithm to learn the optimal policy for joint active object localization, which effectively exploits such beneficial contextual information. We learn inter-agent communication through cross connections with gates between the Q-networks, which is facilitated by a novel multi-agent deep Q-learning algorithm with joint exploitation sampling. We verify our proposed method on multiple object detection benchmarks. Not only does our model help to improve the performance of state-of-the-art active localization models, it also reveals interesting co-detection patterns that are intuitively interpretable.
ER  -


TY  - Preprint
T1  - Cloud-based Deep Learning of Big EEG Data for Epileptic Seizure Prediction
A1  - Mohammad-Parsa Hosseini
A1  - Hamid Soltanian-Zadeh
A1  - Kost Elisevich
A1  - Dario Pompili
JO  - ArXiv e-prints
Y1  - 16 February, 2017
UR  - https://arxiv.org/abs/1702.05192
N2  - Developing a Brain-Computer Interface~(BCI) for seizure prediction can help epileptic patients have a better quality of life. However, there are many difficulties and challenges in developing such a system as a real-life support for patients. Because of the nonstationary nature of EEG signals, normal and seizure patterns vary across different patients. Thus, finding a group of manually extracted features for the prediction task is not practical. Moreover, when using implanted electrodes for brain recording massive amounts of data are produced. This big data calls for the need for safe storage and high computational resources for real-time processing. To address these challenges, a cloud-based BCI system for the analysis of this big EEG data is presented. First, a dimensionality-reduction technique is developed to increase classification accuracy as well as to decrease the communication bandwidth and computation time. Second, following a deep-learning approach, a stacked autoencoder is trained in two steps for unsupervised feature extraction and classification. Third, a cloud-computing solution is proposed for real-time analysis of big EEG data. The results on a benchmark clinical dataset illustrate the superiority of the proposed patient-specific BCI as an alternative method and its expected usefulness in real-life support of epilepsy patients.
ER  -


TY  - Preprint
T1  - Automatic Handgun Detection Alarm in Videos Using Deep Learning
A1  - Roberto Olmos
A1  - Siham Tabik
A1  - Francisco Herrera
JO  - ArXiv e-prints
Y1  - 16 February, 2017
UR  - https://arxiv.org/abs/1702.05147
N2  - Current surveillance and control systems still require human supervision and intervention. This work presents a novel automatic handgun detection system in videos appropriate for both, surveillance and control purposes. We reformulate this detection problem into the problem of minimizing false positives and solve it by building the key training data-set guided by the results of a deep Convolutional Neural Networks (CNN) classifier, then assessing the best classification model under two approaches, the sliding window approach and region proposal approach. The most promising results are obtained by Faster R-CNN based model trained on our new database. The best detector show a high potential even in low quality youtube videos and provides satisfactory results as automatic alarm system. Among 30 scenes, it successfully activates the alarm after five successive true positives in less than 0.2 seconds, in 27 scenes. We also define a new metric, Alarm Activation per Interval (AApI), to assess the performance of a detection model as an automatic detection system in videos.
ER  -


TY  - Preprint
T1  - Deep Hybrid Similarity Learning for Person Re-identification
A1  - Jianqing Zhu
A1  - Huanqiang Zeng
A1  - Shengcai Liao
A1  - Zhen Lei
A1  - Canhui Cai
A1  - LiXin Zheng
JO  - ArXiv e-prints
Y1  - 17 February, 2017
UR  - https://arxiv.org/abs/1702.04858
N2  - Person Re-IDentification (Re-ID) aims to match person images captured from two non-overlapping cameras. In this paper, a deep hybrid similarity learning (DHSL) method for person Re-ID based on a convolution neural network (CNN) is proposed. In our approach, a CNN learning feature pair for the input image pair is simultaneously extracted. Then, both the element-wise absolute difference and multiplication of the CNN learning feature pair are calculated. Finally, a hybrid similarity function is designed to measure the similarity between the feature pair, which is realized by learning a group of weight coefficients to project the element-wise absolute difference and multiplication into a similarity score. Consequently, the proposed DHSL method is able to reasonably assign parameters of feature learning and metric learning in a CNN so that the performance of person Re-ID is improved. Experiments on three challenging person Re-ID databases, QMUL GRID, VIPeR and CUHK03, illustrate that the proposed DHSL method is superior to multiple state-of-the-art person Re-ID methods.
ER  -


TY  - Preprint
T1  - Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study
A1  - John P. Lalor
A1  - Hao Wu
A1  - Tsendsuren Munkhdalai
A1  - Hong Yu
JO  - ArXiv e-prints
Y1  - 7 September, 2018
UR  - https://arxiv.org/abs/1702.04811
N2  - Interpreting the performance of deep learning models beyond test set accuracy is challenging. Characteristics of individual data points are often not considered during evaluation, and each data point is treated equally. We examine the impact of a test set question&#39;s difficulty to determine if there is a relationship between difficulty and performance. We model difficulty using well-studied psychometric methods on human response patterns. Experiments on Natural Language Inference (NLI) and Sentiment Analysis (SA) show that the likelihood of answering a question correctly is impacted by the question&#39;s difficulty. As DNNs are trained with more data, easy examples are learned more quickly than hard examples.
ER  -


TY  - Preprint
T1  - Distributed deep learning on edge-devices: feasibility via adaptive compression
A1  - Corentin Hardy
A1  - Erwan Le Merrer
A1  - Bruno Sericola
JO  - ArXiv e-prints
Y1  - 6 November, 2017
UR  - https://arxiv.org/abs/1702.04683
N2  - A large portion of data mining and analytic services use modern machine learning techniques, such as deep learning. The state-of-the-art results by deep learning come at the price of an intensive use of computing resources. The leading frameworks (e.g., TensorFlow) are executed on GPUs or on high-end servers in datacenters. On the other end, there is a proliferation of personal devices with possibly free CPU cycles; this can enable services to run in users&#39; homes, embedding machine learning operations. In this paper, we ask the following question: Is distributed deep learning computation on WAN connected devices feasible, in spite of the traffic caused by learning tasks? We show that such a setup rises some important challenges, most notably the ingress traffic that the servers hosting the up-to-date model have to sustain.
ER  -


TY  - Preprint
T1  - Handwritten Arabic Numeral Recognition using Deep Learning Neural Networks
A1  - Akm Ashiquzzaman
A1  - Abdul Kawsar Tushar
JO  - ArXiv e-prints
Y1  - 15 February, 2017
UR  - https://arxiv.org/abs/1702.04663
N2  - Handwritten character recognition is an active area of research with applications in numerous fields. Past and recent works in this field have concentrated on various languages. Arabic is one language where the scope of research is still widespread, with it being one of the most popular languages in the world and being syntactically different from other major languages. Das et al. \cite{DBLP:journals/corr/abs-1003-1891} has pioneered the research for handwritten digit recognition in Arabic. In this paper, we propose a novel algorithm based on deep learning neural networks using appropriate activation function and regularization layer, which shows significantly improved accuracy compared to the existing Arabic numeral recognition methods. The proposed model gives 97.4 percent accuracy, which is the recorded highest accuracy of the dataset used in the experiment. We also propose a modification of the method described in \cite{DBLP:journals/corr/abs-1003-1891}, where our method scores identical accuracy as that of \cite{DBLP:journals/corr/abs-1003-1891}, with the value of 93.8 percent.
ER  -


TY  - Preprint
T1  - A deep learning model integrating FCNNs and CRFs for brain tumor segmentation
A1  - Xiaomei Zhao
A1  - Yihong Wu
A1  - Guidong Song
A1  - Zhenye Li
A1  - Yazhuo Zhang
A1  - Yong Fan
JO  - ArXiv e-prints
Y1  - 9 November, 2017
UR  - https://arxiv.org/abs/1702.04528
N2  - Accurate and reliable brain tumor segmentation is a critical component in cancer diagnosis, treatment planning, and treatment outcome evaluation. Build upon successful deep learning techniques, a novel brain tumor segmentation method is developed by integrating fully convolutional neural networks (FCNNs) and Conditional Random Fields (CRFs) in a unified framework to obtain segmentation results with appearance and spatial consistency. We train a deep learning based segmentation model using 2D image patches and image slices in following steps: 1) training FCNNs using image patches; 2) training CRFs as Recurrent Neural Networks (CRF-RNN) using image slices with parameters of FCNNs fixed; and 3) fine-tuning the FCNNs and the CRF-RNN using image slices. Particularly, we train 3 segmentation models using 2D image patches and slices obtained in axial, coronal and sagittal views respectively, and combine them to segment brain tumors using a voting based fusion strategy. Our method could segment brain images slice-by-slice, much faster than those based on image patches. We have evaluated our method based on imaging data provided by the Multimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013, BRATS 2015 and BRATS 2016. The experimental results have demonstrated that our method could build a segmentation model with Flair, T1c, and T2 scans and achieve competitive performance as those built with Flair, T1, T1c, and T2 scans.
ER  -


TY  - Preprint
T1  - Transfer Deep Learning for Low-Resource Chinese Word Segmentation with a Novel Neural Network
A1  - Jingjing Xu
A1  - Xu Sun
JO  - ArXiv e-prints
Y1  - 14 September, 2017
UR  - https://arxiv.org/abs/1702.04488
N2  - Recent studies have shown effectiveness in using neural networks for Chinese word segmentation. However, these models rely on large-scale data and are less effective for low-resource datasets because of insufficient training data. We propose a transfer learning method to improve low-resource word segmentation by leveraging high-resource corpora. First, we train a teacher model on high-resource corpora and then use the learned knowledge to initialize a student model. Second, a weighted data similarity method is proposed to train the student model on low-resource data. Experiment results show that our work significantly improves the performance on low-resource datasets: 2.3% and 1.5% F-score on PKU and CTB datasets. Furthermore, this paper achieves state-of-the-art results: 96.1%, and 96.2% F-score on PKU and CTB datasets.
ER  -


TY  - Preprint
T1  - Small Boxes Big Data: A Deep Learning Approach to Optimize Variable Sized Bin Packing
A1  - Feng Mao
A1  - Edgar Blanco
A1  - Mingang Fu
A1  - Rohit Jain
A1  - Anurag Gupta
A1  - Sebastien Mancel
A1  - Rong Yuan
A1  - Stephen Guo
A1  - Sai Kumar
A1  - Yayang Tian
JO  - ArXiv e-prints
Y1  - 14 February, 2017
UR  - https://arxiv.org/abs/1702.04415
N2  - Bin Packing problems have been widely studied because of their broad applications in different domains. Known as a set of NP-hard problems, they have different vari- ations and many heuristics have been proposed for obtaining approximate solutions. Specifically, for the 1D variable sized bin packing problem, the two key sets of optimization heuristics are the bin assignment and the bin allocation. Usually the performance of a single static optimization heuristic can not beat that of a dynamic one which is tailored for each bin packing instance. Building such an adaptive system requires modeling the relationship between bin features and packing perform profiles. The primary drawbacks of traditional AI machine learnings for this task are the natural limitations of feature engineering, such as the curse of dimensionality and feature selection quality. We introduce a deep learning approach to overcome the drawbacks by applying a large training data set, auto feature selection and fast, accurate labeling. We show in this paper how to build such a system by both theoretical formulation and engineering practices. Our prediction system achieves up to 89% training accuracy and 72% validation accuracy to select the best heuristic that can generate a better quality bin packing solution.
ER  -


TY  - Preprint
T1  - On the Relevance of Auditory-Based Gabor Features for Deep Learning in Automatic Speech Recognition
A1  - Angel Mario Castro Martinez
A1  - Sri Harish Mallidi
A1  - Bernd T. Meyer
JO  - ArXiv e-prints
Y1  - 14 February, 2017
UR  - https://arxiv.org/abs/1702.04333
N2  - Previous studies support the idea of merging auditory-based Gabor features with deep learning architectures to achieve robust automatic speech recognition, however, the cause behind the gain of such combination is still unknown. We believe these representations provide the deep learning decoder with more discriminable cues. Our aim with this paper is to validate this hypothesis by performing experiments with three different recognition tasks (Aurora 4, CHiME 2 and CHiME 3) and assess the discriminability of the information encoded by Gabor filterbank features. Additionally, to identify the contribution of low, medium and high temporal modulation frequencies subsets of the Gabor filterbank were used as features (dubbed LTM, MTM and HTM respectively). With temporal modulation frequencies between 16 and 25 Hz, HTM consistently outperformed the remaining ones in every condition, highlighting the robustness of these representations against channel distortions, low signal-to-noise ratios and acoustically challenging real-life scenarios with relative improvements from 11 to 56% against a Mel-filterbank-DNN baseline. To explain the results, a measure of similarity between phoneme classes from DNN activations is proposed and linked to their acoustic properties. We find this measure to be consistent with the observed error rates and highlight specific differences on phoneme level to pinpoint the benefit of the proposed features.
ER  -


TY  - Preprint
T1  - Multitask Learning with Deep Neural Networks for Community Question Answering
A1  - Daniele Bonadiman
A1  - Antonio Uva
A1  - Alessandro Moschitti
JO  - ArXiv e-prints
Y1  - 13 February, 2017
UR  - https://arxiv.org/abs/1702.03706
N2  - In this paper, we developed a deep neural network (DNN) that learns to solve simultaneously the three tasks of the cQA challenge proposed by the SemEval-2016 Task 3, i.e., question-comment similarity, question-question similarity and new question-comment similarity. The latter is the main task, which can exploit the previous two for achieving better results. Our DNN is trained jointly on all the three cQA tasks and learns to encode questions and comments into a single vector representation shared across the multiple tasks. The results on the official challenge test set show that our approach produces higher accuracy and faster convergence rates than the individual neural networks. Additionally, our method, which does not use any manual feature engineering, approaches the state of the art established with methods that make heavy use of it.
ER  -


TY  - Preprint
T1  - Supervised Learning Based Algorithm Selection for Deep Neural Networks
A1  - Shaohuai Shi
A1  - Pengfei Xu
A1  - Xiaowen Chu
JO  - ArXiv e-prints
Y1  - 16 March, 2017
UR  - https://arxiv.org/abs/1702.03192
N2  - Many recent deep learning platforms rely on third-party libraries (such as cuBLAS) to utilize the computing power of modern hardware accelerators (such as GPUs). However, we observe that they may achieve suboptimal performance because the library functions are not used appropriately. In this paper, we target at optimizing the operations of multiplying a matrix with the transpose of another matrix (referred to as NT operation hereafter), which contribute about half of the training time of fully connected deep neural networks. Rather than directly calling the library function, we propose a supervised learning based algorithm selection approach named MTNN, which uses a gradient boosted decision tree to select one from two alternative NT implementations intelligently: (1) calling the cuBLAS library function; (2) calling our proposed algorithm TNN that uses an efficient out-of-place matrix transpose. We evaluate the performance of MTNN on two modern GPUs: NVIDIA GTX 1080 and NVIDIA Titan X Pascal. MTNN can achieve 96\% of prediction accuracy with very low computational overhead, which results in an average of 54\% performance improvement on a range of NT operations. To further evaluate the impact of MTNN on the training process of deep neural networks, we have integrated MTNN into a popular deep learning platform Caffe. Our experimental results show that the revised Caffe can outperform the original one by an average of 28\%. Both MTNN and the revised Caffe are open-source.
ER  -


TY  - Preprint
T1  - Semi-Supervised Deep Learning for Monocular Depth Map Prediction
A1  - Yevhen Kuznietsov
A1  - JÃ¶rg StÃ¼ckler
A1  - Bastian Leibe
JO  - ArXiv e-prints
Y1  - 9 May, 2017
UR  - https://arxiv.org/abs/1702.02706
N2  - Supervised deep learning often suffers from the lack of sufficient training data. Specifically in the context of monocular depth map prediction, it is barely possible to determine dense ground truth depth images in realistic dynamic outdoor environments. When using LiDAR sensors, for instance, noise is present in the distance measurements, the calibration between sensors cannot be perfect, and the measurements are typically much sparser than the camera images. In this paper, we propose a novel approach to depth map prediction from monocular images that learns in a semi-supervised way. While we use sparse ground-truth depth for supervised learning, we also enforce our deep network to produce photoconsistent dense depth maps in a stereo setup using a direct image alignment loss. In experiments we demonstrate superior performance in depth map prediction from single images compared to the state-of-the-art methods.
ER  -


TY  - Preprint
T1  - Autonomous Braking System via Deep Reinforcement Learning
A1  - Hyunmin Chae
A1  - Chang Mook Kang
A1  - ByeoungDo Kim
A1  - Jaekyum Kim
A1  - Chung Choo Chung
A1  - Jun Won Choi
JO  - ArXiv e-prints
Y1  - 24 April, 2017
UR  - https://arxiv.org/abs/1702.02302
N2  - In this paper, we propose a new autonomous braking system based on deep reinforcement learning. The proposed autonomous braking system automatically decides whether to apply the brake at each time step when confronting the risk of collision using the information on the obstacle obtained by the sensors. The problem of designing brake control is formulated as searching for the optimal policy in Markov decision process (MDP) model where the state is given by the relative position of the obstacle and the vehicle&#39;s speed, and the action space is defined as whether brake is stepped or not. The policy used for brake control is learned through computer simulations using the deep reinforcement learning method called deep Q-network (DQN). In order to derive desirable braking policy, we propose the reward function which balances the damage imposed to the obstacle in case of accident and the reward achieved when the vehicle runs out of risk as soon as possible. DQN is trained for the scenario where a vehicle is encountered with a pedestrian crossing the urban road. Experiments show that the control agent exhibits desirable control behavior and avoids collision without any mistake in various uncertain environments.
ER  -


TY  - Preprint
T1  - Deep Learning with Dynamic Computation Graphs
A1  - Moshe Looks
A1  - Marcello Herreshoff
A1  - DeLesley Hutchins
A1  - Peter Norvig
JO  - ArXiv e-prints
Y1  - 21 February, 2017
UR  - https://arxiv.org/abs/1702.02181
N2  - Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.
ER  -


TY  - Preprint
T1  - An Integrated Simulator and Dataset that Combines Grasping and Vision for Deep Learning
A1  - Matthew Veres
A1  - Medhat Moussa
A1  - Graham W. Taylor
JO  - ArXiv e-prints
Y1  - 17 April, 2017
UR  - https://arxiv.org/abs/1702.02103
N2  - Deep learning is an established framework for learning hierarchical data representations. While compute power is in abundance, one of the main challenges in applying this framework to robotic grasping has been obtaining the amount of data needed to learn these representations, and structuring the data to the task at hand. Among contemporary approaches in the literature, we highlight key properties that have encouraged the use of deep learning techniques, and in this paper, detail our experience in developing a simulator for collecting cylindrical precision grasps of a multi-fingered dexterous robotic hand.
ER  -


TY  - Preprint
T1  - Development of JavaScript-based deep learning platform and application to distributed training
A1  - Masatoshi Hidaka
A1  - Ken Miura
A1  - Tatsuya Harada
JO  - ArXiv e-prints
Y1  - 27 March, 2017
UR  - https://arxiv.org/abs/1702.01846
N2  - Deep learning is increasingly attracting attention for processing big data. Existing frameworks for deep learning must be set up to specialized computer systems. Gaining sufficient computing resources therefore entails high costs of deployment and maintenance. In this work, we implement a matrix library and deep learning framework that uses JavaScript. It can run on web browsers operating on ordinary personal computers and smartphones. Using JavaScript, deep learning can be accomplished in widely diverse environments without the necessity for software installation. Using GPGPU from WebCL framework, our framework can train large scale convolutional neural networks such as VGGNet and ResNet. In the experiments, we demonstrate their practicality by training VGGNet in a distributed manner using web browsers as the client.
ER  -


TY  - Preprint
T1  - Search Intelligence: Deep Learning For Dominant Category Prediction
A1  - Zeeshan Khawar Malik
A1  - Mo Kobrosli
A1  - Peter Maas
JO  - ArXiv e-prints
Y1  - 6 February, 2017
UR  - https://arxiv.org/abs/1702.01717
N2  - Deep Neural Networks, and specifically fully-connected convolutional neural networks are achieving remarkable results across a wide variety of domains. They have been trained to achieve state-of-the-art performance when applied to problems such as speech recognition, image classification, natural language processing and bioinformatics. Most of these deep learning models when applied to classification employ the softmax activation function for prediction and aim to minimize cross-entropy loss. In this paper, we have proposed a supervised model for dominant category prediction to improve search recall across all eBay classifieds platforms. The dominant category label for each query in the last 90 days is first calculated by summing the total number of collaborative clicks among all categories. The category having the highest number of collaborative clicks for the given query will be considered its dominant category. Second, each query is transformed to a numeric vector by mapping each unique word in the query document to a unique integer value; all padded to equal length based on the maximum document length within the pre-defined vocabulary size. A fully-connected deep convolutional neural network (CNN) is then applied for classification. The proposed model achieves very high classification accuracy compared to other state-of-the-art machine learning techniques.
ER  -


TY  - Preprint
T1  - Deep Learning with Low Precision by Half-wave Gaussian Quantization
A1  - Zhaowei Cai
A1  - Xiaodong He
A1  - Jian Sun
A1  - Nuno Vasconcelos
JO  - ArXiv e-prints
Y1  - 3 February, 2017
UR  - https://arxiv.org/abs/1702.00953
N2  - The problem of quantizing the activations of a deep neural network is considered. An examination of the popular binary quantization approach shows that this consists of approximating a classical non-linearity, the hyperbolic tangent, by two functions: a piecewise constant sign function, which is used in feedforward network computations, and a piecewise linear hard tanh function, used in the backpropagation step during network learning. The problem of approximating the ReLU non-linearity, widely used in the recent deep learning literature, is then considered. An half-wave Gaussian quantizer (HWGQ) is proposed for forward approximation and shown to have efficient implementation, by exploiting the statistics of of network activations and batch normalization operations commonly used in the literature. To overcome the problem of gradient mismatch, due to the use of different forward and backward approximations, several piece-wise backward approximators are then investigated. The implementation of the resulting quantized network, denoted as HWGQ-Net, is shown to achieve much closer performance to full precision networks, such as AlexNet, ResNet, GoogLeNet and VGG-Net, than previously available low-precision networks, with 1-bit binary weights and 2-bit quantized activations.
ER  -


TY  - Preprint
T1  - An Introduction to Deep Learning for the Physical Layer
A1  - Timothy J. O&#39;Shea
A1  - Jakob Hoydis
JO  - ArXiv e-prints
Y1  - 11 July, 2017
UR  - https://arxiv.org/abs/1702.00832
N2  - We present and discuss several novel applications of deep learning for the physical layer. By interpreting a communications system as an autoencoder, we develop a fundamental new way to think about communications system design as an end-to-end reconstruction task that seeks to jointly optimize transmitter and receiver components in a single process. We show how this idea can be extended to networks of multiple transmitters and receivers and present the concept of radio transformer networks as a means to incorporate expert domain knowledge in the machine learning model. Lastly, we demonstrate the application of convolutional neural networks on raw IQ samples for modulation classification which achieves competitive accuracy with respect to traditional schemes relying on expert features. The paper is concluded with a discussion of open challenges and areas for future investigation.
ER  -


TY  - Preprint
T1  - Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a Survey
A1  - Lorenzo Ferrone
A1  - Fabio Massimo Zanzotto
JO  - ArXiv e-prints
Y1  - 2 February, 2017
UR  - https://arxiv.org/abs/1702.00764
N2  - Natural language and symbols are intimately correlated. Recent advances in machine learning (ML) and in natural language processing (NLP) seem to contradict the above intuition: symbols are fading away, erased by vectors or tensors called distributed and distributional representations. However, there is a strict link between distributed/distributional representations and symbols, being the first an approximation of the second. A clearer understanding of the strict link between distributed/distributional representations and symbols will certainly lead to radically new deep learning networks. In this paper we make a survey that aims to draw the link between symbolic representations and distributed/distributional representations. This is the right time to revitalize the area of interpreting how symbols are represented inside neural networks.
ER  -


TY  - Preprint
T1  - HashNet: Deep Learning to Hash by Continuation
A1  - Zhangjie Cao
A1  - Mingsheng Long
A1  - Jianmin Wang
A1  - Philip S. Yu
JO  - ArXiv e-prints
Y1  - 29 July, 2017
UR  - https://arxiv.org/abs/1702.00758
N2  - Learning to hash has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval, due to its computation efficiency and retrieval quality. Deep learning to hash, which improves retrieval quality by end-to-end representation learning and hash encoding, has received increasing attention recently. Subject to the ill-posed gradient difficulty in the optimization with sign activations, existing deep learning to hash methods need to first learn continuous representations and then generate binary hash codes in a separated binarization step, which suffer from substantial loss of retrieval quality. This work presents HashNet, a novel deep architecture for deep learning to hash by continuation method with convergence guarantees, which learns exactly binary hash codes from imbalanced similarity data. The key idea is to attack the ill-posed gradient problem in optimizing deep networks with non-smooth binary activations by continuation method, in which we begin from learning an easier network with smoothed activation function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, deep network with the sign activation function. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art multimedia retrieval performance on standard benchmarks.
ER  -


TY  - Preprint
T1  - Deep Learning the Indus Script
A1  - Satish Palaniappan
A1  - Ronojoy Adhikari
JO  - ArXiv e-prints
Y1  - 1 February, 2017
UR  - https://arxiv.org/abs/1702.00523
N2  - Standardized corpora of undeciphered scripts, a necessary starting point for computational epigraphy, requires laborious human effort for their preparation from raw archaeological records. Automating this process through machine learning algorithms can be of significant aid to epigraphical research. Here, we take the first steps in this direction and present a deep learning pipeline that takes as input images of the undeciphered Indus script, as found in archaeological artifacts, and returns as output a string of graphemes, suitable for inclusion in a standard corpus. The image is first decomposed into regions using Selective Search and these regions are classified as containing textual and/or graphical information using a convolutional neural network. Regions classified as potentially containing text are hierarchically merged and trimmed to remove non-textual information. The remaining textual part of the image is segmented using standard image processing techniques to isolate individual graphemes. This set is finally passed to a second convolutional neural network to classify the graphemes, based on a standard corpus. The classifier can identify the presence or absence of the most frequent Indus grapheme, the &#34;jar&#34; sign, with an accuracy of 92%. Our results demonstrate the great potential of deep learning approaches in computational epigraphy and, more generally, in the digital humanities.
ER  -


TY  - Preprint
T1  - Mixed Low-precision Deep Learning Inference using Dynamic Fixed Point
A1  - Naveen Mellempudi
A1  - Abhisek Kundu
A1  - Dipankar Das
A1  - Dheevatsa Mudigere
A1  - Bharat Kaul
JO  - ArXiv e-prints
Y1  - 31 January, 2017
UR  - https://arxiv.org/abs/1701.08978
N2  - We propose a cluster-based quantization method to convert pre-trained full precision weights into ternary weights with minimal impact on the accuracy. In addition, we also constrain the activations to 8-bits thus enabling sub 8-bit full integer inference pipeline. Our method uses smaller clusters of N filters with a common scaling factor to minimize the quantization loss, while also maximizing the number of ternary operations. We show that with a cluster size of N=4 on Resnet-101, can achieve 71.8% TOP-1 accuracy, within 6% of the best full precision results while replacing ~85% of all multiplications with 8-bit accumulations. Using the same method with 4-bit weights achieves 76.3% TOP-1 accuracy which within 2% of the full precision result. We also study the impact of the size of the cluster on both performance and accuracy, larger cluster sizes N=64 can replace ~98% of the multiplications with ternary operations but introduces significant drop in accuracy which necessitates fine tuning the parameters with retraining the network at lower precision. To address this we have also trained low-precision Resnet-50 with 8-bit activations and ternary weights by pre-initializing the network with full precision weights and achieve 68.9% TOP-1 accuracy within 4 additional epochs. Our final quantized model can run on a full 8-bit compute pipeline, with a potential 16x improvement in performance compared to baseline full-precision models.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Visual Object Tracking in Videos
A1  - Da Zhang
A1  - Hamid Maei
A1  - Xin Wang
A1  - Yuan-Fang Wang
JO  - ArXiv e-prints
Y1  - 10 April, 2017
UR  - https://arxiv.org/abs/1701.08936
N2  - In this paper we introduce a fully end-to-end approach for visual tracking in videos that learns to predict the bounding box locations of a target object at every frame. An important insight is that the tracking problem can be considered as a sequential decision-making process and historical semantics encode highly relevant information for future decisions. Based on this intuition, we formulate our model as a recurrent convolutional neural network agent that interacts with a video overtime, and our model can be trained with reinforcement learning (RL) algorithms to learn good tracking policies that pay attention to continuous, inter-frame correlation and maximize tracking performance in the long run. The proposed tracking algorithm achieves state-of-the-art performance in an existing tracking benchmark and operates at frame-rates faster than real-time. To the best of our knowledge, our tracker is the first neural-network tracker that combines convolutional and recurrent networks with RL algorithms.
ER  -


TY  - Preprint
T1  - SenseGen: A Deep Learning Architecture for Synthetic Sensor Data Generation
A1  - Moustafa Alzantot
A1  - Supriyo Chakraborty
A1  - Mani B. Srivastava
JO  - ArXiv e-prints
Y1  - 30 January, 2017
UR  - https://arxiv.org/abs/1701.08886
N2  - Our ability to synthesize sensory data that preserves specific statistical properties of the real data has had tremendous implications on data privacy and big data analytics. The synthetic data can be used as a substitute for selective real data segments,that are sensitive to the user, thus protecting privacy and resulting in improved analytics.However, increasingly adversarial roles taken by data recipients such as mobile apps, or other cloud-based analytics services, mandate that the synthetic data, in addition to preserving statistical properties, should also be difficult to distinguish from the real data. Typically, visual inspection has been used as a test to distinguish between datasets. But more recently, sophisticated classifier models (discriminators), corresponding to a set of events, have also been employed to distinguish between synthesized and real data. The model operates on both datasets and the respective event outputs are compared for consistency. In this paper, we take a step towards generating sensory data that can pass a deep learning based discriminator model test, and make two specific contributions: first, we present a deep learning based architecture for synthesizing sensory data. This architecture comprises of a generator model, which is a stack of multiple Long-Short-Term-Memory (LSTM) networks and a Mixture Density Network. second, we use another LSTM network based discriminator model for distinguishing between the true and the synthesized data. Using a dataset of accelerometer traces, collected using smartphones of users doing their daily activities, we show that the deep learning based discriminator model can only distinguish between the real and synthesized traces with an accuracy in the neighborhood of 50%.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Robotic Manipulation-The state of the art
A1  - Smruti Amarjyoti
JO  - ArXiv e-prints
Y1  - 30 January, 2017
UR  - https://arxiv.org/abs/1701.08878
N2  - The focus of this work is to enumerate the various approaches and algorithms that center around application of reinforcement learning in robotic ma- ]]nipulation tasks. Earlier methods utilized specialized policy representations and human demonstrations to constrict the policy. Such methods worked well with continuous state and policy space of robots but failed to come up with generalized policies. Subsequently, high dimensional non-linear function approximators like neural networks have been used to learn policies from scratch. Several novel and recent approaches have also embedded control policy with efficient perceptual representation using deep learning. This has led to the emergence of a new branch of dynamic robot control system called deep r inforcement learning(DRL). This work embodies a survey of the most recent algorithms, architectures and their implementations in simulations and real world robotic platforms. The gamut of DRL architectures are partitioned into two different branches namely, discrete action space algorithms(DAS) and continuous action space algorithms(CAS). Further, the CAS algorithms are divided into stochastic continuous action space(SCAS) and deterministic continuous action space(DCAS) algorithms. Along with elucidating an organ- isation of the DRL algorithms this work also manifests some of the state of the art applications of these approaches in robotic manipulation tasks.
ER  -


TY  - Preprint
T1  - Expert Level control of Ramp Metering based on Multi-task Deep Reinforcement Learning
A1  - Francois Belletti
A1  - Daniel Haziza
A1  - Gabriel Gomes
A1  - Alexandre M. Bayen
JO  - ArXiv e-prints
Y1  - 30 January, 2017
UR  - https://arxiv.org/abs/1701.08832
N2  - This article shows how the recent breakthroughs in Reinforcement Learning (RL) that have enabled robots to learn to play arcade video games, walk or assemble colored bricks, can be used to perform other tasks that are currently at the core of engineering cyberphysical systems. We present the first use of RL for the control of systems modeled by discretized non-linear Partial Differential Equations (PDEs) and devise a novel algorithm to use non-parametric control techniques for large multi-agent systems. We show how neural network based RL enables the control of discretized PDEs whose parameters are unknown, random, and time-varying. We introduce an algorithm of Mutual Weight Regularization (MWR) which alleviates the curse of dimensionality of multi-agent control schemes by sharing experience between agents while giving each agent the opportunity to specialize its action policy so as to tailor it to the local parameters of the part of the system it is located in.
ER  -


TY  - Preprint
T1  - Face Detection using Deep Learning: An Improved Faster RCNN Approach
A1  - Xudong Sun
A1  - Pengcheng Wu
A1  - Steven C. H. Hoi
JO  - ArXiv e-prints
Y1  - 28 January, 2017
UR  - https://arxiv.org/abs/1701.08289
N2  - In this report, we present a new face detection scheme using deep learning and achieve the state-of-the-art detection performance on the well-known FDDB face detetion benchmark evaluation. In particular, we improve the state-of-the-art faster RCNN framework by combining a number of strategies, including feature concatenation, hard negative mining, multi-scale training, model pretraining, and proper calibration of key parameters. As a consequence, the proposed scheme obtained the state-of-the-art face detection performance, making it the best model in terms of ROC curves among all the published methods on the FDDB benchmark.
ER  -


TY  - Preprint
T1  - Reinforced stochastic gradient descent for deep neural network learning
A1  - Haiping Huang
A1  - Taro Toyoizumi
JO  - ArXiv e-prints
Y1  - 22 November, 2017
UR  - https://arxiv.org/abs/1701.07974
N2  - Stochastic gradient descent (SGD) is a standard optimization method to minimize a training error with respect to network parameters in modern neural network learning. However, it typically suffers from proliferation of saddle points in the high-dimensional parameter space. Therefore, it is highly desirable to design an efficient algorithm to escape from these saddle points and reach a parameter region of better generalization capabilities. Here, we propose a simple extension of SGD, namely reinforced SGD, which simply adds previous first-order gradients in a stochastic manner with a probability that increases with learning time. As verified in a simple synthetic dataset, this method significantly accelerates learning compared with the original SGD. Surprisingly, it dramatically reduces over-fitting effects, even compared with state-of-the-art adaptive learning algorithm---Adam. For a benchmark handwritten digits dataset, the learning performance is comparable to Adam, yet with an extra advantage of requiring one-fold less computer memory. The reinforced SGD is also compared with SGD with fixed or adaptive momentum parameter and Nesterov&#39;s momentum, which shows that the proposed framework is able to reach a similar generalization accuracy with less computational costs. Overall, our method introduces stochastic memory into gradients, which plays an important role in understanding how gradient-based training algorithms can work and its relationship with generalization abilities of deep networks.
ER  -


TY  - Preprint
T1  - On Deep Learning-Based Channel Decoding
A1  - Tobias Gruber
A1  - Sebastian Cammerer
A1  - Jakob Hoydis
A1  - Stephan ten Brink
JO  - ArXiv e-prints
Y1  - 26 January, 2017
UR  - https://arxiv.org/abs/1701.07738
N2  - We revisit the idea of using deep neural networks for one-shot decoding of random and structured codes, such as polar codes. Although it is possible to achieve maximum a posteriori (MAP) bit error rate (BER) performance for both code families and for short codeword lengths, we observe that (i) structured codes are easier to learn and (ii) the neural network is able to generalize to codewords that it has never seen during training for structured, but not for random codes. These results provide some evidence that neural networks can learn a form of decoding algorithm, rather than only a simple classifier. We introduce the metric normalized validation error (NVE) in order to further investigate the potential and limitations of deep learning-based decoding with respect to performance and complexity.
ER  -


TY  - Preprint
T1  - FPGA Architecture for Deep Learning and its application to Planetary Robotics
A1  - Pranay Gankidi
A1  - Jekan Thangavelautham
JO  - ArXiv e-prints
Y1  - 25 January, 2017
UR  - https://arxiv.org/abs/1701.07543
N2  - Autonomous control systems onboard planetary rovers and spacecraft benefit from having cognitive capabilities like learning so that they can adapt to unexpected situations in-situ. Q-learning is a form of reinforcement learning and it has been efficient in solving certain class of learning problems. However, embedded systems onboard planetary rovers and spacecraft rarely implement learning algorithms due to the constraints faced in the field, like processing power, chip size, convergence rate and costs due to the need for radiation hardening. These challenges present a compelling need for a portable, low-power, area efficient hardware accelerator to make learning algorithms practical onboard space hardware. This paper presents a FPGA implementation of Q-learning with Artificial Neural Networks (ANN). This method matches the massive parallelism inherent in neural network software with the fine-grain parallelism of an FPGA hardware thereby dramatically reducing processing time. Mars Science Laboratory currently uses Xilinx-Space-grade Virtex FPGA devices for image processing, pyrotechnic operation control and obstacle avoidance. We simulate and program our architecture on a Xilinx Virtex 7 FPGA. The architectural implementation for a single neuron Q-learning and a more complex Multilayer Perception (MLP) Q-learning accelerator has been demonstrated. The results show up to a 43-fold speed up by Virtex 7 FPGAs compared to a conventional Intel i5 2.3 GHz CPU. Finally, we simulate the proposed architecture using the Symphony simulator and compiler from Xilinx, and evaluate the performance and power consumption.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning: An Overview
A1  - Yuxi Li
JO  - ArXiv e-prints
Y1  - 15 September, 2017
UR  - https://arxiv.org/abs/1701.07274
N2  - We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.
ER  -


TY  - Preprint
T1  - Neurostream: Scalable and Energy Efficient Deep Learning with Smart Memory Cubes
A1  - Erfan Azarkhish
A1  - Davide Rossi
A1  - Igor Loi
A1  - Luca Benini
JO  - ArXiv e-prints
Y1  - 24 September, 2017
UR  - https://arxiv.org/abs/1701.06420
N2  - High-performance computing systems are moving towards 2.5D and 3D memory hierarchies, based on High Bandwidth Memory (HBM) and Hybrid Memory Cube (HMC) to mitigate the main memory bottlenecks. This trend is also creating new opportunities to revisit near-memory computation. In this paper, we propose a flexible processor-in-memory (PIM) solution for scalable and energy-efficient execution of deep convolutional networks (ConvNets), one of the fastest-growing workloads for servers and high-end embedded systems. Our codesign approach consists of a network of Smart Memory Cubes (modular extensions to the standard HMC) each augmented with a many-core PIM platform called NeuroCluster. NeuroClusters have a modular design based on NeuroStream coprocessors (for Convolution-intensive computations) and general-purpose RISCV cores. In addition, a DRAM-friendly tiling mechanism and a scalable computation paradigm are presented to efficiently harness this computational capability with a very low programming effort. NeuroCluster occupies only 8% of the total logic-base (LoB) die area in a standard HMC and achieves an average performance of 240 GFLOPS for complete execution of full-featured state-of-the-art (SoA) ConvNets within a power budget of 2.5W. Overall 11 W is consumed in a single SMC device, with 22.5 GFLOPS/W energy-efficiency which is 3.5X better than the best GPU implementations in similar technologies. The minor increase in system-level power and the negligible area increase make our PIM system a cost-effective and energy efficient solution, easily scalable to 955 GFLOPS with a small network of just four SMCs.
ER  -


TY  - Preprint
T1  - Holistic Interstitial Lung Disease Detection using Deep Convolutional Neural Networks: Multi-label Learning and Unordered Pooling
A1  - Mingchen Gao
A1  - Ziyue Xu
A1  - Le Lu
A1  - Adam P. Harrison
A1  - Ronald M. Summers
A1  - Daniel J. Mollura
JO  - ArXiv e-prints
Y1  - 19 January, 2017
UR  - https://arxiv.org/abs/1701.05616
N2  - Accurately predicting and detecting interstitial lung disease (ILD) patterns given any computed tomography (CT) slice without any pre-processing prerequisites, such as manually delineated regions of interest (ROIs), is a clinically desirable, yet challenging goal. The majority of existing work relies on manually-provided ILD ROIs to extract sampled 2D image patches from CT slices and, from there, performs patch-based ILD categorization. Acquiring manual ROIs is labor intensive and serves as a bottleneck towards fully-automated CT imaging ILD screening over large-scale populations. Furthermore, despite the considerable high frequency of more than one ILD pattern on a single CT slice, previous works are only designed to detect one ILD pattern per slice or patch.
ER  -


TY  - Preprint
T1  - Deep Learning Features at Scale for Visual Place Recognition
A1  - Zetao Chen
A1  - Adam Jacobson
A1  - Niko Sunderhauf
A1  - Ben Upcroft
A1  - Lingqiao Liu
A1  - Chunhua Shen
A1  - Ian Reid
A1  - Michael Milford
JO  - ArXiv e-prints
Y1  - 18 January, 2017
UR  - https://arxiv.org/abs/1701.05105
N2  - The success of deep learning techniques in the computer vision domain has triggered a range of initial investigations into their utility for visual place recognition, all using generic features from networks that were trained for other types of recognition tasks. In this paper, we train, at large scale, two CNN architectures for the specific place recognition task and employ a multi-scale feature encoding method to generate condition- and viewpoint-invariant features. To enable this training to occur, we have developed a massive Specific PlacEs Dataset (SPED) with hundreds of examples of place appearance change at thousands of different places, as opposed to the semantic place type datasets currently available. This new dataset enables us to set up a training regime that interprets place recognition as a classification problem. We comprehensively evaluate our trained networks on several challenging benchmark place recognition datasets and demonstrate that they achieve an average 10% increase in performance over other place recognition algorithms and pre-trained CNNs. By analyzing the network responses and their differences from pre-trained networks, we provide insights into what a network learns when training for place recognition, and what these results signify for future research in this area.
ER  -


TY  - Preprint
T1  - Fusing Deep Learned and Hand-Crafted Features of Appearance, Shape, and Dynamics for Automatic Pain Estimation
A1  - Joy Egede
A1  - Michel Valstar
A1  - Brais Martinez
JO  - ArXiv e-prints
Y1  - 17 January, 2017
UR  - https://arxiv.org/abs/1701.04540
N2  - Automatic continuous time, continuous value assessment of a patient&#39;s pain from face video is highly sought after by the medical profession. Despite the recent advances in deep learning that attain impressive results in many domains, pain estimation risks not being able to benefit from this due to the difficulty in obtaining data sets of considerable size. In this work we propose a combination of hand-crafted and deep-learned features that makes the most of deep learning techniques in small sample settings. Encoding shape, appearance, and dynamics, our method significantly outperforms the current state of the art, attaining a RMSE error of less than 1 point on a 16-level pain scale, whilst simultaneously scoring a 67.3% Pearson correlation coefficient between our predicted pain level time series and the ground truth.
ER  -


TY  - Preprint
T1  - Classification of MRI data using Deep Learning and Gaussian Process-based Model Selection
A1  - Hadrien Bertrand
A1  - Matthieu Perrot
A1  - Roberto Ardon
A1  - Isabelle Bloch
JO  - ArXiv e-prints
Y1  - 16 January, 2017
UR  - https://arxiv.org/abs/1701.04355
N2  - The classification of MRI images according to the anatomical field of view is a necessary task to solve when faced with the increasing quantity of medical images. In parallel, advances in deep learning makes it a suitable tool for computer vision problems. Using a common architecture (such as AlexNet) provides quite good results, but not sufficient for clinical use. Improving the model is not an easy task, due to the large number of hyper-parameters governing both the architecture and the training of the network, and to the limited understanding of their relevance. Since an exhaustive search is not tractable, we propose to optimize the network first by random search, and then by an adaptive search based on Gaussian Processes and Probability of Improvement. Applying this method on a large and varied MRI dataset, we show a substantial improvement between the baseline network and the final one (up to 20\% for the most difficult classes).
ER  -


TY  - Preprint
T1  - Learning Traffic as Images: A Deep Convolutional Neural Network for Large-Scale Transportation Network Speed Prediction
A1  - Xiaolei Ma
A1  - Zhuang Dai
A1  - Zhengbing He
A1  - Jihui Na
A1  - Yong Wang
A1  - Yunpeng Wang
JO  - ArXiv e-prints
Y1  - 10 April, 2017
UR  - https://arxiv.org/abs/1701.04245
N2  - This paper proposes a convolutional neural network (CNN)-based method that learns traffic as images and predicts large-scale, network-wide traffic speed with a high accuracy. Spatiotemporal traffic dynamics are converted to images describing the time and space relations of traffic flow via a two-dimensional time-space matrix. A CNN is applied to the image following two consecutive steps: abstract traffic feature extraction and network-wide traffic speed prediction. The effectiveness of the proposed method is evaluated by taking two real-world transportation networks, the second ring road and north-east transportation network in Beijing, as examples, and comparing the method with four prevailing algorithms, namely, ordinary least squares, k-nearest neighbors, artificial neural network, and random forest, and three deep learning architectures, namely, stacked autoencoder, recurrent neural network, and long-short-term memory network. The results show that the proposed method outperforms other algorithms by an average accuracy improvement of 42.91% within an acceptable execution time. The CNN can train the model in a reasonable time and, thus, is suitable for large-scale transportation networks.
ER  -


TY  - Preprint
T1  - Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks
A1  - Vahid Behzadan
A1  - Arslan Munir
JO  - ArXiv e-prints
Y1  - 15 January, 2017
UR  - https://arxiv.org/abs/1701.04143
N2  - Deep learning classifiers are known to be inherently vulnerable to manipulation by intentionally perturbed inputs, named adversarial examples. In this work, we establish that reinforcement learning techniques based on Deep Q-Networks (DQNs) are also vulnerable to adversarial input perturbations, and verify the transferability of adversarial examples across different DQN models. Furthermore, we present a novel class of attacks based on this vulnerability that enable policy manipulation and induction in the learning process of DQNs. We propose an attack mechanism that exploits the transferability of adversarial examples to implement policy induction attacks on DQNs, and demonstrate its efficacy and impact through experimental study of a game-learning scenario.
ER  -


TY  - Preprint
T1  - Cost-Effective Active Learning for Deep Image Classification
A1  - Keze Wang
A1  - Dongyu Zhang
A1  - Ya Li
A1  - Ruimao Zhang
A1  - Liang Lin
JO  - ArXiv e-prints
Y1  - 12 January, 2017
UR  - https://arxiv.org/abs/1701.03551
N2  - Recent successes in learning-based image classification, however, heavily rely on the large number of annotated training samples, which may require considerable human efforts. In this paper, we propose a novel active learning framework, which is capable of building a competitive classifier with optimal feature representation via a limited amount of labeled training instances in an incremental learning manner. Our approach advances the existing active learning methods in two aspects. First, we incorporate deep convolutional neural networks into active learning. Through the properly designed framework, the feature representation and the classifier can be simultaneously updated with progressively annotated informative samples. Second, we present a cost-effective sample selection strategy to improve the classification performance with less manual annotations. Unlike traditional methods focusing on only the uncertain samples of low prediction confidence, we especially discover the large amount of high confidence samples from the unlabeled set for feature learning. Specifically, these high confidence samples are automatically selected and iteratively assigned pseudo-labels. We thus call our framework &#34;Cost-Effective Active Learning&#34; (CEAL) standing for the two advantages.Extensive experiments demonstrate that the proposed CEAL framework can achieve promising results on two challenging image classification datasets, i.e., face recognition on CACD database [1] and object categorization on Caltech-256 [2].
ER  -


TY  - Preprint
T1  - An OpenCL(TM) Deep Learning Accelerator on Arria 10
A1  - Utku Aydonat
A1  - Shane O&#39;Connell
A1  - Davor Capalija
A1  - Andrew C. Ling
A1  - Gordon R. Chiu
JO  - ArXiv e-prints
Y1  - 12 January, 2017
UR  - https://arxiv.org/abs/1701.03534
N2  - Convolutional neural nets (CNNs) have become a practical means to perform vision tasks, particularly in the area of image classification. FPGAs are well known to be able to perform convolutions efficiently, however, most recent efforts to run CNNs on FPGAs have shown limited advantages over other devices such as GPUs. Previous approaches on FPGAs have often been memory bound due to the limited external memory bandwidth on the FPGA device. We show a novel architecture written in OpenCL(TM), which we refer to as a Deep Learning Accelerator (DLA), that maximizes data reuse and minimizes external memory bandwidth. Furthermore, we show how we can use the Winograd transform to significantly boost the performance of the FPGA. As a result, when running our DLA on Intel&#39;s Arria 10 device we can achieve a performance of 1020 img/s, or 23 img/s/W when running the AlexNet CNN benchmark. This comes to 1382 GFLOPs and is 10x faster with 8.4x more GFLOPS and 5.8x better efficiency than the state-of-the-art on FPGAs. Additionally, 23 img/s/W is competitive against the best publicly known implementation of AlexNet on nVidia&#39;s TitanX GPU.
ER  -


TY  - Preprint
T1  - Deep Learning for Logo Recognition
A1  - Simone Bianco
A1  - Marco Buzzelli
A1  - Davide Mazzini
A1  - Raimondo Schettini
JO  - ArXiv e-prints
Y1  - 3 May, 2017
UR  - https://arxiv.org/abs/1701.02620
N2  - In this paper we propose a method for logo recognition using deep learning. Our recognition pipeline is composed of a logo region proposal followed by a Convolutional Neural Network (CNN) specifically trained for logo classification, even if they are not precisely localized. Experiments are carried out on the FlickrLogos-32 database, and we evaluate the effect on recognition performance of synthetic versus real data augmentation, and image pre-processing. Moreover, we systematically investigate the benefits of different training choices such as class-balancing, sample-weighting and explicit modeling the background class (i.e. no-logo regions). Experimental results confirm the feasibility of the proposed method, that outperforms the methods in the state of the art.
ER  -


TY  - Preprint
T1  - Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic Speech Recognition
A1  - Abhinav Thanda
A1  - Shankar M Venkatesan
JO  - ArXiv e-prints
Y1  - 10 January, 2017
UR  - https://arxiv.org/abs/1701.02477
N2  - Multi-task learning (MTL) involves the simultaneous training of two or more related tasks over shared representations. In this work, we apply MTL to audio-visual automatic speech recognition(AV-ASR). Our primary task is to learn a mapping between audio-visual fused features and frame labels obtained from acoustic GMM/HMM model. This is combined with an auxiliary task which maps visual features to frame labels obtained from a separate visual GMM/HMM model. The MTL model is tested at various levels of babble noise and the results are compared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate that MTL is especially useful at higher level of noise. Compared to base-line, upto 7\% relative improvement in WER is reported at -3 SNR dB
ER  -


TY  - Preprint
T1  - DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning
A1  - Tian Zhao
A1  - Xiaobing Huang
A1  - Yu Cao
JO  - ArXiv e-prints
Y1  - 9 January, 2017
UR  - https://arxiv.org/abs/1701.02284
N2  - In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.
ER  -


TY  - Preprint
T1  - Deep Learning for Time-Series Analysis
A1  - John Cristian Borges Gamboa
JO  - ArXiv e-prints
Y1  - 7 January, 2017
UR  - https://arxiv.org/abs/1701.01887
N2  - In many real-world application, e.g., speech recognition or sleep stage classification, data are captured over the course of time, constituting a Time-Series. Time-Series often contain temporal dependencies that cause two otherwise identical points of time to belong to different classes or predict different behavior. This characteristic generally increases the difficulty of analysing them. Existing techniques often depended on hand-crafted features that were expensive to create and required expert knowledge of the field. With the advent of Deep Learning new models of unsupervised learning of features for Time-series analysis and forecast have been developed. Such new developments are the topic of this paper: a review of the main Deep Learning techniques is presented, and some applications on Time-Series analysis are summaried. The results make it clear that Deep Learning has a lot to contribute to the field.
ER  -


TY  - Preprint
T1  - DeepFace: Face Generation using Deep Learning
A1  - Hardie Cate
A1  - Fahim Dalvi
A1  - Zeshan Hussain
JO  - ArXiv e-prints
Y1  - 7 January, 2017
UR  - https://arxiv.org/abs/1701.01876
N2  - We use CNNs to build a system that both classifies images of faces based on a variety of different facial attributes and generates new faces given a set of desired facial characteristics. After introducing the problem and providing context in the first section, we discuss recent work related to image generation in Section 2. In Section 3, we describe the methods used to fine-tune our CNN and generate new images using a novel approach inspired by a Gaussian mixture model. In Section 4, we discuss our working dataset and describe our preprocessing steps and handling of facial attributes. Finally, in Sections 5, 6 and 7, we explain our experiments and results and conclude in the following section. Our classification system has 82\% test accuracy. Furthermore, our generation pipeline successfully creates well-formed faces.
ER  -


TY  - Preprint
T1  - Transforming Sensor Data to the Image Domain for Deep Learning - an Application to Footstep Detection
A1  - Monit Shah Singh
A1  - Vinaychandran Pondenkandath
A1  - Bo Zhou
A1  - Paul Lukowicz
A1  - Marcus Liwicki
JO  - ArXiv e-prints
Y1  - 14 July, 2017
UR  - https://arxiv.org/abs/1701.01077
N2  - Convolutional Neural Networks (CNNs) have become the state-of-the-art in various computer vision tasks, but they are still premature for most sensor data, especially in pervasive and wearable computing. A major reason for this is the limited amount of annotated training data. In this paper, we propose the idea of leveraging the discriminative power of pre-trained deep CNNs on 2-dimensional sensor data by transforming the sensor modality to the visual domain. By three proposed strategies, 2D sensor output is converted into pressure distribution imageries. Then we utilize a pre-trained CNN for transfer learning on the converted imagery data. We evaluate our method on a gait dataset of floor surface pressure mapping. We obtain a classification accuracy of 87.66%, which outperforms the conventional machine learning methods by over 10%.
ER  -


TY  - Preprint
T1  - Learning a Mixture of Deep Networks for Single Image Super-Resolution
A1  - Ding Liu
A1  - Zhaowen Wang
A1  - Nasser Nasrabadi
A1  - Thomas Huang
JO  - ArXiv e-prints
Y1  - 3 January, 2017
UR  - https://arxiv.org/abs/1701.00823
N2  - Single image super-resolution (SR) is an ill-posed problem which aims to recover high-resolution (HR) images from their low-resolution (LR) observations. The crux of this problem lies in learning the complex mapping between low-resolution patches and the corresponding high-resolution patches. Prior arts have used either a mixture of simple regression models or a single non-linear neural network for this propose. This paper proposes the method of learning a mixture of SR inference modules in a unified framework to tackle this problem. Specifically, a number of SR inference modules specialized in different image local patterns are first independently applied on the LR image to obtain various HR estimates, and the resultant HR estimates are adaptively aggregated to form the final HR image. By selecting neural networks as the SR inference module, the whole procedure can be incorporated into a unified network and be optimized jointly. Extensive experiments are conducted to investigate the relation between restoration performance and different network architectures. Compared with other current image SR approaches, our proposed method achieves state-of-the-arts restoration results on a wide range of images consistently while allowing more flexible design choices. The source codes are available in http://www.ifp.illinois.edu/~dingliu2/accv2016.
ER  -


TY  - Preprint
T1  - AENet: Learning Deep Audio Features for Video Analysis
A1  - Naoya Takahashi
A1  - Michael Gygli
A1  - Luc Van Gool
JO  - ArXiv e-prints
Y1  - 3 January, 2017
UR  - https://arxiv.org/abs/1701.00599
N2  - We propose a new deep network for audio event recognition, called AENet. In contrast to speech, sounds coming from audio events may be produced by a wide variety of sources. Furthermore, distinguishing them often requires analyzing an extended time period due to the lack of clear sub-word units that are present in speech. In order to incorporate this long-time frequency structure of audio events, we introduce a convolutional neural network (CNN) operating on a large temporal input. In contrast to previous works this allows us to train an audio event detection system end-to-end. The combination of our network architecture and a novel data augmentation outperforms previous methods for audio event detection by 16%. Furthermore, we perform transfer learning and show that our model learnt generic audio features, similar to the way CNNs learn generic features on vision tasks. In video analysis, combining visual features and traditional audio features such as MFCC typically only leads to marginal improvements. Instead, combining visual features with our AENet features, which can be computed efficiently on a GPU, leads to significant performance improvements on action recognition and video highlight detection. In video highlight detection, our audio features improve the performance by more than 8% over visual features alone.
ER  -


TY  - Preprint
T1  - Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices
A1  - Wenjia Meng
A1  - Zonghua Gu
A1  - Ming Zhang
A1  - Zhaohui Wu
JO  - ArXiv e-prints
Y1  - 4 January, 2017
UR  - https://arxiv.org/abs/1701.00485
N2  - With the rapid proliferation of Internet of Things and intelligent edge devices, there is an increasing need for implementing machine learning algorithms, including deep learning, on resource-constrained mobile embedded devices with limited memory and computation power. Typical large Convolutional Neural Networks (CNNs) need large amounts of memory and computational power, and cannot be deployed on embedded devices efficiently. We present Two-Bit Networks (TBNs) for model compression of CNNs with edge weights constrained to (-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the memory usage and improve computational efficiency significantly while achieving good performance in terms of classification accuracy, thus representing a reasonable tradeoff between model size and performance.
ER  -


TY  - Preprint
T1  - Deep Learning Logo Detection with Data Expansion by Synthesising Context
A1  - Hang Su
A1  - Xiatian Zhu
A1  - Shaogang Gong
JO  - ArXiv e-prints
Y1  - 16 March, 2018
UR  - https://arxiv.org/abs/1612.09322
N2  - Logo detection in unconstrained images is challenging, particularly when only very sparse labelled training images are accessible due to high labelling costs. In this work, we describe a model training image synthesising method capable of improving significantly logo detection performance when only a handful of (e.g., 10) labelled training images captured in realistic context are available, avoiding extensive manual labelling costs. Specifically, we design a novel algorithm for generating Synthetic Context Logo (SCL) training images to increase model robustness against unknown background clutters, resulting in superior logo detection performance. For benchmarking model performance, we introduce a new logo detection dataset TopLogo-10 collected from top 10 most popular clothing/wearable brandname logos captured in rich visual context. Extensive comparisons show the advantages of our proposed SCL model over the state-of-the-art alternatives for logo detection using two real-world logo benchmark datasets: FlickrLogo-32 and our new TopLogo-10.
ER  -


TY  - Preprint
T1  - Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling Task Hierarchies
A1  - Jonathan Godwin
A1  - Pontus Stenetorp
A1  - Sebastian Riedel
JO  - ArXiv e-prints
Y1  - 29 December, 2016
UR  - https://arxiv.org/abs/1612.09113
N2  - In this paper we present a novel Neural Network algorithm for conducting semi-supervised learning for sequence labeling tasks arranged in a linguistically motivated hierarchy. This relationship is exploited to regularise the representations of supervised tasks by backpropagating the error of the unsupervised task through the supervised tasks. We introduce a neural network where lower layers are supervised by junior downstream tasks and the final layer task is an auxiliary unsupervised task. The architecture shows improvements of up to two percentage points F1 for Chunking compared to a plausible baseline.
ER  -


TY  - Preprint
T1  - Deep Learning and Hierarchal Generative Models
A1  - Elchanan Mossel
JO  - ArXiv e-prints
Y1  - 4 September, 2018
UR  - https://arxiv.org/abs/1612.09057
N2  - It is argued that deep learning is efficient for data that is generated from hierarchal generative models. Examples of such generative models include wavelet scattering networks, functions of compositional structure, and deep rendering models. Unfortunately so far, for all such models, it is either not rigorously known that they can be learned efficiently, or it is not known that &#34;deep algorithms&#34; are required in order to learn them.
ER  -


TY  - Preprint
T1  - Text Summarization using Deep Learning and Ridge Regression
A1  - Karthik Bangalore Mani
JO  - ArXiv e-prints
Y1  - 14 June, 2017
UR  - https://arxiv.org/abs/1612.08333
N2  - We develop models and extract relevant features for automatic text summarization and investigate the performance of different models on the DUC 2001 dataset. Two different models were developed, one being a ridge regressor and the other one was a multi-layer perceptron. The hyperparameters were varied and their performance were noted. We segregated the summarization task into 2 main steps, the first being sentence ranking and the second step being sentence selection. In the first step, given a document, we sort the sentences based on their Importance, and in the second step, in order to obtain non-redundant sentences, we weed out the sentences that are have high similarity with the previously selected sentences.
ER  -


TY  - Preprint
T1  - Deep Learning and Its Applications to Machine Health Monitoring: A Survey
A1  - Rui Zhao
A1  - Ruqiang Yan
A1  - Zhenghua Chen
A1  - Kezhi Mao
A1  - Peng Wang
A1  - Robert X. Gao
JO  - ArXiv e-prints
Y1  - 15 December, 2016
UR  - https://arxiv.org/abs/1612.07640
N2  - Since 2006, deep learning (DL) has become a rapidly growing research direction, redefining state-of-the-art performances in a wide range of areas such as object recognition, image segmentation, speech recognition and machine translation. In modern manufacturing systems, data-driven machine health monitoring is gaining in popularity due to the widespread deployment of low-cost sensors and their connection to the Internet. Meanwhile, deep learning provides useful tools for processing and analyzing these big machinery data. The main purpose of this paper is to review and summarize the emerging research work of deep learning on machine health monitoring. After the brief introduction of deep learning techniques, the applications of deep learning in machine health monitoring systems are reviewed mainly from the following aspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and its variants including Deep Belief Network (DBN) and Deep Boltzmann Machines (DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). Finally, some new trends of DL-based machine health monitoring methods are discussed.
ER  -


TY  - Preprint
T1  - How to Train Your Deep Neural Network with Dictionary Learning
A1  - Vanika Singhal
A1  - Shikha Singh
A1  - Angshul Majumdar
JO  - ArXiv e-prints
Y1  - 22 December, 2016
UR  - https://arxiv.org/abs/1612.07454
N2  - Currently there are two predominant ways to train deep neural networks. The first one uses restricted Boltzmann machine (RBM) and the second one autoencoders. RBMs are stacked in layers to form deep belief network (DBN); the final representation layer is attached to the target to complete the deep neural network. Autoencoders are nested one inside the other to form stacked autoencoders; once the stcaked autoencoder is learnt the decoder portion is detached and the target attached to the deepest layer of the encoder to form the deep neural network. This work proposes a new approach to train deep neural networks using dictionary learning as the basic building block; the idea is to use the features from the shallower layer as inputs for training the next deeper layer. One can use any type of dictionary learning (unsupervised, supervised, discriminative etc.) as basic units till the pre-final layer. In the final layer one needs to use the label consistent dictionary learning formulation for classification. We compare our proposed framework with existing state-of-the-art deep learning techniques on benchmark problems; we are always within the top 10 results. In actual problems of age and gender classification, we are better than the best known techniques.
ER  -


TY  - Preprint
T1  - A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation
A1  - Lei Tai
A1  - Jingwei Zhang
A1  - Ming Liu
A1  - Joschka Boedecker
A1  - Wolfram Burgard
JO  - ArXiv e-prints
Y1  - 8 April, 2018
UR  - https://arxiv.org/abs/1612.07139
N2  - Deep learning techniques have been widely applied, achieving state-of-the-art results in various fields of study. This survey focuses on deep learning solutions that target learning control policies for robotics applications. We carry out our discussions on the two main paradigms for learning control with deep networks: deep reinforcement learning and imitation learning. For deep reinforcement learning (DRL), we begin from traditional reinforcement learning algorithms, showing how they are extended to the deep context and effective mechanisms that could be added on top of the DRL algorithms. We then introduce representative works that utilize DRL to solve navigation and manipulation tasks in robotics. We continue our discussion on methods addressing the challenge of the reality gap for transferring DRL policies trained in simulation to real-world scenarios, and summarize robotics simulation platforms for conducting DRL research. For imitation leaning, we go through its three main categories, behavior cloning, inverse reinforcement learning and generative adversarial imitation learning, by introducing their formulations and their corresponding robotics applications. Finally, we discuss the open challenges and research frontiers.
ER  -


TY  - Preprint
T1  - A deep learning approach for predicting the quality of online health expert question-answering services
A1  - Ze Hu
A1  - Zhan Zhang
A1  - Qing Chen
A1  - Haiqin Yang
A1  - Decheng Zuo
JO  - ArXiv e-prints
Y1  - 21 December, 2016
UR  - https://arxiv.org/abs/1612.07040
N2  - Currently, a growing number of health consumers are asking health-related questions online, at any time and from anywhere, which effectively lowers the cost of health care. The most common approach is using online health expert question-answering (HQA) services, as health consumers are more willing to trust answers from professional physicians. However, these answers can be of varying quality depending on circumstance. In addition, as the available HQA services grow, how to predict the answer quality of HQA services via machine learning becomes increasingly important and challenging. In an HQA service, answers are normally short texts, which are severely affected by the data sparsity problem. Furthermore, HQA services lack community features such as best answer and user votes. Therefore, the wisdom of the crowd is not available to rate answer quality. To address these problems, in this paper, the prediction of HQA answer quality is defined as a classification task. First, based on the characteristics of HQA services and feedback from medical experts, a standard for HQA service answer quality evaluation is defined. Next, based on the characteristics of HQA services, several novel non-textual features are proposed, including surface linguistic features and social features. Finally, a deep belief network (DBN)-based HQA answer quality prediction framework is proposed to predict the quality of answers by learning the high-level hidden semantic representation from the physicians&#39; answers. Our results prove that the proposed framework overcomes the problem of overly sparse textual features in short text answers and effectively identifies high-quality answers.
ER  -


TY  - Preprint
T1  - Detecting Unexpected Obstacles for Self-Driving Cars: Fusing Deep Learning and Geometric Modeling
A1  - Sebastian Ramos
A1  - Stefan Gehrig
A1  - Peter Pinggera
A1  - Uwe Franke
A1  - Carsten Rother
JO  - ArXiv e-prints
Y1  - 20 December, 2016
UR  - https://arxiv.org/abs/1612.06573
N2  - The detection of small road hazards, such as lost cargo, is a vital capability for self-driving cars. We tackle this challenging and rarely addressed problem with a vision system that leverages appearance, contextual as well as geometric cues. To utilize the appearance and contextual cues, we propose a new deep learning-based obstacle detection framework. Here a variant of a fully convolutional network is used to predict a pixel-wise semantic labeling of (i) free-space, (ii) on-road unexpected obstacles, and (iii) background. The geometric cues are exploited using a state-of-the-art detection approach that predicts obstacles from stereo input images via model-based statistical hypothesis tests. We present a principled Bayesian framework to fuse the semantic and stereo-based detection results. The mid-level Stixel representation is used to describe obstacles in a flexible, compact and robust manner. We evaluate our new obstacle detection system on the Lost and Found dataset, which includes very challenging scenes with obstacles of only 5 cm height. Overall, we report a major improvement over the state-of-the-art, with relative performance gains of up to 50%. In particular, we achieve a detection rate of over 90% for distances of up to 50 m. Our system operates at 22 Hz on our self-driving platform.
ER  -


TY  - Preprint
T1  - Sample-efficient Deep Reinforcement Learning for Dialog Control
A1  - Kavosh Asadi
A1  - Jason D. Williams
JO  - ArXiv e-prints
Y1  - 18 December, 2016
UR  - https://arxiv.org/abs/1612.06000
N2  - Representing a dialog policy as a recurrent neural network (RNN) is attractive because it handles partial observability, infers a latent representation of state, and can be optimized with supervised learning (SL) or reinforcement learning (RL). For RL, a policy gradient approach is natural, but is sample inefficient. In this paper, we present 3 methods for reducing the number of dialogs required to optimize an RNN-based dialog policy with RL. The key idea is to maintain a second RNN which predicts the value of the current policy, and to apply experience replay to both networks. On two tasks, these methods reduce the number of dialogs/episodes required by about a third, vs. standard policy gradient methods.
ER  -


TY  - Preprint
T1  - Deep Learning on Lie Groups for Skeleton-based Action Recognition
A1  - Zhiwu Huang
A1  - Chengde Wan
A1  - Thomas Probst
A1  - Luc Van Gool
JO  - ArXiv e-prints
Y1  - 11 April, 2017
UR  - https://arxiv.org/abs/1612.05877
N2  - In recent years, skeleton-based action recognition has become a popular 3D classification problem. State-of-the-art methods typically first represent each motion sequence as a high-dimensional trajectory on a Lie group with an additional dynamic time warping, and then shallowly learn favorable Lie group features. In this paper we incorporate the Lie group structure into a deep network architecture to learn more appropriate Lie group features for 3D action recognition. Within the network structure, we design rotation mapping layers to transform the input Lie group features into desirable ones, which are aligned better in the temporal domain. To reduce the high feature dimensionality, the architecture is equipped with rotation pooling layers for the elements on the Lie group. Furthermore, we propose a logarithm mapping layer to map the resulting manifold data into a tangent space that facilitates the application of regular output layers for the final classification. Evaluations of the proposed network for standard 3D human action recognition datasets clearly demonstrate its superiority over existing shallow Lie group feature learning methods as well as most conventional deep learning methods.
ER  -


TY  - Preprint
T1  - Learning to predict where to look in interactive environments using deep recurrent q-learning
A1  - Sajad Mousavi
A1  - Michael Schukat
A1  - Enda Howley
A1  - Ali Borji
A1  - Nasser Mozayani
JO  - ArXiv e-prints
Y1  - 18 February, 2017
UR  - https://arxiv.org/abs/1612.05753
N2  - Bottom-Up (BU) saliency models do not perform well in complex interactive environments where humans are actively engaged in tasks (e.g., sandwich making and playing the video games). In this paper, we leverage Reinforcement Learning (RL) to highlight task-relevant locations of input frames. We propose a soft attention mechanism combined with the Deep Q-Network (DQN) model to teach an RL agent how to play a game and where to look by focusing on the most pertinent parts of its visual input. Our evaluations on several Atari 2600 games show that the soft attention based model could predict fixation locations significantly better than bottom-up models such as Itti-Kochs saliency and Graph-Based Visual Saliency (GBVS) models.
ER  -


TY  - Preprint
T1  - Neuromorphic Deep Learning Machines
A1  - Emre Neftci
A1  - Charles Augustine
A1  - Somnath Paul
A1  - Georgios Detorakis
JO  - ArXiv e-prints
Y1  - 21 January, 2017
UR  - https://arxiv.org/abs/1612.05596
N2  - An ongoing challenge in neuromorphic computing is to devise general and computationally efficient models of inference and learning which are compatible with the spatial and temporal constraints of the brain. One increasingly popular and successful approach is to take inspiration from inference and learning algorithms used in deep neural networks. However, the workhorse of deep learning, the gradient descent Back Propagation (BP) rule, often relies on the immediate availability of network-wide information stored with high-precision memory, and precise operations that are difficult to realize in neuromorphic hardware. Remarkably, recent work showed that exact backpropagated weights are not essential for learning deep representations. Random BP replaces feedback weights with random ones and encourages the network to adjust its feed-forward weights to learn pseudo-inverses of the (random) feedback weights. Building on these results, we demonstrate an event-driven random BP (eRBP) rule that uses an error-modulated synaptic plasticity for learning deep representations in neuromorphic computing hardware. The rule requires only one addition and two comparisons for each synaptic weight using a two-compartment leaky Integrate &amp; Fire (I&amp;F) neuron, making it very suitable for implementation in digital or mixed-signal neuromorphic hardware. Our results show that using eRBP, deep representations are rapidly learned, achieving nearly identical classification accuracies compared to artificial neural network simulations on GPUs, while being robust to neural and synaptic state quantizations during learning.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning with Successor Features for Navigation across Similar Environments
A1  - Jingwei Zhang
A1  - Jost Tobias Springenberg
A1  - Joschka Boedecker
A1  - Wolfram Burgard
JO  - ArXiv e-prints
Y1  - 23 July, 2017
UR  - https://arxiv.org/abs/1612.05533
N2  - In this paper we consider the problem of robot navigation in simple maze-like environments where the robot has to rely on its onboard sensors to perform the navigation task. In particular, we are interested in solutions to this problem that do not require localization, mapping or planning. Additionally, we require that our solution can quickly adapt to new situations (e.g., changing navigation goals and environments). To meet these criteria we frame this problem as a sequence of related reinforcement learning tasks. We propose a successor feature based deep reinforcement learning algorithm that can learn to transfer knowledge from previously mastered navigation tasks to new problem instances. Our algorithm substantially decreases the required learning time after the first task instance has been solved, which makes it easily adaptable to changing environments. We validate our method in both simulated and real robot experiments with a Robotino and compare it to a set of baseline methods including classical planning-based navigation.
ER  -


TY  - Preprint
T1  - Towards a Deep Learning Framework for Unconstrained Face Detection
A1  - Yutong Zheng
A1  - Chenchen Zhu
A1  - Khoa Luu
A1  - Chandrasekhar Bhagavatula
A1  - T. Hoang Ngan Le
A1  - Marios Savvides
JO  - ArXiv e-prints
Y1  - 2 January, 2017
UR  - https://arxiv.org/abs/1612.05322
N2  - Robust face detection is one of the most important pre-processing steps to support facial expression analysis, facial landmarking, face recognition, pose estimation, building of 3D facial models, etc. Although this topic has been intensely studied for decades, it is still challenging due to numerous variants of face images in real-world scenarios. In this paper, we present a novel approach named Multiple Scale Faster Region-based Convolutional Neural Network (MS-FRCNN) to robustly detect human facial regions from images collected under various challenging conditions, e.g. large occlusions, extremely low resolutions, facial expressions, strong illumination variations, etc. The proposed approach is benchmarked on two challenging face detection databases, i.e. the Wider Face database and the Face Detection Dataset and Benchmark (FDDB), and compared against recent other face detection methods, e.g. Two-stage CNN, Multi-scale Cascade CNN, Faceness, Aggregate Chanel Features, HeadHunter, Multi-view Face Detection, Cascade CNN, etc. The experimental results show that our proposed approach consistently achieves highly competitive results with the state-of-the-art performance against other recent face detection methods.
ER  -


TY  - Preprint
T1  - Feature Learning for Chord Recognition: The Deep Chroma Extractor
A1  - Filip Korzeniowski
A1  - Gerhard Widmer
JO  - ArXiv e-prints
Y1  - 15 December, 2016
UR  - https://arxiv.org/abs/1612.05065
N2  - We explore frame-level audio feature learning for chord recognition using artificial neural networks. We present the argument that chroma vectors potentially hold enough information to model harmonic content of audio for chord recognition, but that standard chroma extractors compute too noisy features. This leads us to propose a learned chroma feature extractor based on artificial neural networks. It is trained to compute chroma features that encode harmonic information important for chord recognition, while being robust to irrelevant interferences. We achieve this by feeding the network an audio spectrum with context instead of a single frame as input. This way, the network can learn to selectively compensate noise and resolve harmonic ambiguities.
ER  -


TY  - Preprint
T1  - Music Generation with Deep Learning
A1  - Vasanth Kalingeri
A1  - Srikanth Grandhe
JO  - ArXiv e-prints
Y1  - 15 December, 2016
UR  - https://arxiv.org/abs/1612.04928
N2  - The use of deep learning to solve problems in literary arts has been a recent trend that has gained a lot of attention and automated generation of music has been an active area. This project deals with the generation of music using raw audio files in the frequency domain relying on various LSTM architectures. Fully connected and convolutional layers are used along with LSTM&#39;s to capture rich features in the frequency domain and increase the quality of music generated. The work is focused on unconstrained music generation and uses no information about musical structure(notes or chords) to aid learning.The music generated from various architectures are compared using blind fold tests. Using the raw audio to train models is the direction to tapping the enormous amount of mp3 files that exist over the internet without requiring the manual effort to make structured MIDI files. Moreover, not all audio files can be represented with MIDI files making the study of these models an interesting prospect to the future of such models.
ER  -


TY  - Preprint
T1  - Beam Search for Learning a Deep Convolutional Neural Network of 3D Shapes
A1  - Xu Xu
A1  - Sinisa Todorovic
JO  - ArXiv e-prints
Y1  - 14 December, 2016
UR  - https://arxiv.org/abs/1612.04774
N2  - This paper addresses 3D shape recognition. Recent work typically represents a 3D shape as a set of binary variables corresponding to 3D voxels of a uniform 3D grid centered on the shape, and resorts to deep convolutional neural networks(CNNs) for modeling these binary variables. Robust learning of such CNNs is currently limited by the small datasets of 3D shapes available, an order of magnitude smaller than other common datasets in computer vision. Related work typically deals with the small training datasets using a number of ad hoc, hand-tuning strategies. To address this issue, we formulate CNN learning as a beam search aimed at identifying an optimal CNN architecture, namely, the number of layers, nodes, and their connectivity in the network, as well as estimating parameters of such an optimal CNN. Each state of the beam search corresponds to a candidate CNN. Two types of actions are defined to add new convolutional filters or new convolutional layers to a parent CNN, and thus transition to children states. The utility function of each action is efficiently computed by transferring parameter values of the parent CNN to its children, thereby enabling an efficient beam search. Our experimental evaluation on the 3D ModelNet dataset demonstrates that our model pursuit using the beam search yields a CNN with superior performance on 3D shape classification than the state of the art.
ER  -


TY  - Preprint
T1  - Predicting Process Behaviour using Deep Learning
A1  - Joerg Evermann
A1  - Jana-Rebecca Rehse
A1  - Peter Fettke
JO  - ArXiv e-prints
Y1  - 22 March, 2017
UR  - https://arxiv.org/abs/1612.04600
N2  - Predicting business process behaviour is an important aspect of business process management. Motivated by research in natural language processing, this paper describes an application of deep learning with recurrent neural networks to the problem of predicting the next event in a business process. This is both a novel method in process prediction, which has largely relied on explicit process models, and also a novel application of deep learning methods. The approach is evaluated on two real datasets and our results surpass the state-of-the-art in prediction precision.
ER  -


TY  - Preprint
T1  - Deep Active Learning for Dialogue Generation
A1  - Nabiha Asghar
A1  - Pascal Poupart
A1  - Xin Jiang
A1  - Hang Li
JO  - ArXiv e-prints
Y1  - 16 June, 2017
UR  - https://arxiv.org/abs/1612.03929
N2  - We propose an online, end-to-end, neural generative conversational model for open-domain dialogue. It is trained using a unique combination of offline two-phase supervised learning and online human-in-the-loop active learning. While most existing research proposes offline supervision or hand-crafted reward functions for online reinforcement, we devise a novel interactive learning mechanism based on hamming-diverse beam search for response generation and one-character user-feedback at each step. Experiments show that our model inherently promotes the generation of semantically relevant and interesting responses, and can be used to train agents with customized personas, moods and conversational styles.
ER  -


TY  - Preprint
T1  - Neurogenesis Deep Learning
A1  - Timothy J. Draelos
A1  - Nadine E. Miner
A1  - Christopher C. Lamb
A1  - Jonathan A. Cox
A1  - Craig M. Vineyard
A1  - Kristofor D. Carlson
A1  - William M. Severa
A1  - Conrad D. James
A1  - James B. Aimone
JO  - ArXiv e-prints
Y1  - 28 March, 2017
UR  - https://arxiv.org/abs/1612.03770
N2  - Neural machine learning methods, such as deep neural networks (DNN), have achieved remarkable success in a number of complex data processing tasks. These methods have arguably had their strongest impact on tasks such as image and audio processing - data processing domains in which humans have long held clear advantages over conventional algorithms. In contrast to biological neural systems, which are capable of learning continuously, deep artificial networks have a limited ability for incorporating new information in an already trained network. As a result, methods for continuous learning are potentially highly impactful in enabling the application of deep networks to dynamic data sets. Here, inspired by the process of adult neurogenesis in the hippocampus, we explore the potential for adding new neurons to deep layers of artificial neural networks in order to facilitate their acquisition of novel information while preserving previously trained data representations. Our results on the MNIST handwritten digit dataset and the NIST SD 19 dataset, which includes lower and upper case letters and digits, demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms.
ER  -


TY  - Preprint
T1  - Learning to Drive using Inverse Reinforcement Learning and Deep Q-Networks
A1  - Sahand Sharifzadeh
A1  - Ioannis Chiotellis
A1  - Rudolph Triebel
A1  - Daniel Cremers
JO  - ArXiv e-prints
Y1  - 21 September, 2017
UR  - https://arxiv.org/abs/1612.03653
N2  - We propose an inverse reinforcement learning (IRL) approach using Deep Q-Networks to extract the rewards in problems with large state spaces. We evaluate the performance of this approach in a simulation-based autonomous driving scenario. Our results resemble the intuitive relation between the reward function and readings of distance sensors mounted at different poses on the car. We also show that, after a few learning rounds, our simulated agent generates collision-free motions and performs human-like lane change behaviour.
ER  -


TY  - Preprint
T1  - Towards deep learning with spiking neurons in energy based models with contrastive Hebbian plasticity
A1  - Thomas Mesnard
A1  - Wulfram Gerstner
A1  - Johanni Brea
JO  - ArXiv e-prints
Y1  - 9 December, 2016
UR  - https://arxiv.org/abs/1612.03214
N2  - In machine learning, error back-propagation in multi-layer neural networks (deep learning) has been impressively successful in supervised and reinforcement learning tasks. As a model for learning in the brain, however, deep learning has long been regarded as implausible, since it relies in its basic form on a non-local plasticity rule. To overcome this problem, energy-based models with local contrastive Hebbian learning were proposed and tested on a classification task with networks of rate neurons. We extended this work by implementing and testing such a model with networks of leaky integrate-and-fire neurons. Preliminary results indicate that it is possible to learn a non-linear regression task with hidden layers, spiking neurons and a local synaptic plasticity rule.
ER  -


TY  - Preprint
T1  - DeepCancer: Detecting Cancer through Gene Expressions via Deep Generative Learning
A1  - Rajendra Rana Bhat
A1  - Vivek Viswanath
A1  - Xiaolin Li
JO  - ArXiv e-prints
Y1  - 13 December, 2016
UR  - https://arxiv.org/abs/1612.03211
N2  - Transcriptional profiling on microarrays to obtain gene expressions has been used to facilitate cancer diagnosis. We propose a deep generative machine learning architecture (called DeepCancer) that learn features from unlabeled microarray data. These models have been used in conjunction with conventional classifiers that perform classification of the tissue samples as either being cancerous or non-cancerous. The proposed model has been tested on two different clinical datasets. The evaluation demonstrates that DeepCancer model achieves a very high precision score, while significantly controlling the false positive and false negative scores.
ER  -


TY  - Preprint
T1  - Learning in the Machine: Random Backpropagation and the Deep Learning Channel
A1  - Pierre Baldi
A1  - Peter Sadowski
A1  - Zhiqin Lu
JO  - ArXiv e-prints
Y1  - 22 December, 2017
UR  - https://arxiv.org/abs/1612.02734
N2  - Random backpropagation (RBP) is a variant of the backpropagation algorithm for training neural networks, where the transpose of the forward matrices are replaced by fixed random matrices in the calculation of the weight updates. It is remarkable both because of its effectiveness, in spite of using random matrices to communicate error information, and because it completely removes the taxing requirement of maintaining symmetric weights in a physical neural system. To better understand random backpropagation, we first connect it to the notions of local learning and learning channels. Through this connection, we derive several alternatives to RBP, including skipped RBP (SRPB), adaptive RBP (ARBP), sparse RBP, and their combinations (e.g. ASRBP) and analyze their computational complexity. We then study their behavior through simulations using the MNIST and CIFAR-10 bechnmark datasets. These simulations show that most of these variants work robustly, almost as well as backpropagation, and that multiplication by the derivatives of the activation functions is important. As a follow-up, we study also the low-end of the number of bits required to communicate error information over the learning channel. We then provide partial intuitive explanations for some of the remarkable properties of RBP and its variations. Finally, we prove several mathematical results, including the convergence to fixed points of linear chains of arbitrary length, the convergence to fixed points of linear autoencoders with decorrelated data, the long-term existence of solutions for linear systems with a single hidden layer and convergence in special cases, and the convergence to fixed points of non-linear chains, when the derivative of the activation functions is included.
ER  -


TY  - Preprint
T1  - From Motion Blur to Motion Flow: a Deep Learning Solution for Removing Heterogeneous Motion Blur
A1  - Dong Gong
A1  - Jie Yang
A1  - Lingqiao Liu
A1  - Yanning Zhang
A1  - Ian Reid
A1  - Chunhua Shen
A1  - Anton van den Hengel
A1  - Qinfeng Shi
JO  - ArXiv e-prints
Y1  - 8 December, 2016
UR  - https://arxiv.org/abs/1612.02583
N2  - Removing pixel-wise heterogeneous motion blur is challenging due to the ill-posed nature of the problem. The predominant solution is to estimate the blur kernel by adding a prior, but the extensive literature on the subject indicates the difficulty in identifying a prior which is suitably informative, and general. Rather than imposing a prior based on theory, we propose instead to learn one from the data. Learning a prior over the latent image would require modeling all possible image content. The critical observation underpinning our approach is thus that learning the motion flow instead allows the model to focus on the cause of the blur, irrespective of the image content. This is a much easier learning task, but it also avoids the iterative process through which latent image priors are typically applied. Our approach directly estimates the motion flow from the blurred image through a fully-convolutional deep neural network (FCN) and recovers the unblurred image from the estimated motion flow. Our FCN is the first universal end-to-end mapping from the blurred image to the dense motion flow. To train the FCN, we simulate motion flows to generate synthetic blurred-image-motion-flow pairs thus avoiding the need for human labeling. Extensive experiments on challenging realistic blurred images demonstrate that the proposed method outperforms the state-of-the-art.
ER  -


TY  - Preprint
T1  - Learning Adversary-Resistant Deep Neural Networks
A1  - Qinglong Wang
A1  - Wenbo Guo
A1  - Kaixuan Zhang
A1  - Alexander G. Ororbia II
A1  - Xinyu Xing
A1  - Xue Liu
A1  - C. Lee Giles
JO  - ArXiv e-prints
Y1  - 18 August, 2017
UR  - https://arxiv.org/abs/1612.01401
N2  - Deep neural networks (DNNs) have proven to be quite effective in a vast array of machine learning tasks, with recent examples in cyber security and autonomous vehicles. Despite the superior performance of DNNs in these applications, it has been recently shown that these models are susceptible to a particular type of attack that exploits a fundamental flaw in their design. This attack consists of generating particular synthetic examples referred to as adversarial samples. These samples are constructed by slightly manipulating real data-points in order to &#34;fool&#34; the original DNN model, forcing it to mis-classify previously correctly classified samples with high confidence. Addressing this flaw in the model is essential if DNNs are to be used in critical applications such as those in cyber security.
ER  -


TY  - Preprint
T1  - On-Demand Learning for Deep Image Restoration
A1  - Ruohan Gao
A1  - Kristen Grauman
JO  - ArXiv e-prints
Y1  - 2 August, 2017
UR  - https://arxiv.org/abs/1612.01380
N2  - While machine learning approaches to image restoration offer great promise, current methods risk training models fixated on performing well only for image corruption of a particular level of difficulty---such as a certain level of noise or blur. First, we examine the weakness of conventional &#34;fixated&#34; models and demonstrate that training general models to handle arbitrary levels of corruption is indeed non-trivial. Then, we propose an on-demand learning algorithm for training image restoration models with deep convolutional neural networks. The main idea is to exploit a feedback mechanism to self-generate training instances where they are needed most, thereby learning models that can generalize across difficulty levels. On four restoration tasks---image inpainting, pixel interpolation, image deblurring, and image denoising---and three diverse datasets, our approach consistently outperforms both the status quo training procedure and curriculum learning alternatives.
ER  -


TY  - Preprint
T1  - Cryptocurrency Portfolio Management with Deep Reinforcement Learning
A1  - Zhengyao Jiang
A1  - Jinjun Liang
JO  - ArXiv e-prints
Y1  - 11 May, 2017
UR  - https://arxiv.org/abs/1612.01277
N2  - Portfolio management is the decision-making process of allocating an amount of fund into different financial investment products. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. This paper presents a model-less convolutional neural network with historic prices of a set of financial assets as its input, outputting portfolio weights of the set. The network is trained with 0.7 years&#39; price data from a cryptocurrency exchange. The training is done in a reinforcement manner, maximizing the accumulative return, which is regarded as the reward function of the network. Backtest trading experiments with trading period of 30 minutes is conducted in the same market, achieving 10-fold returns in 1.8 months&#39; periods. Some recently published portfolio selection strategies are also used to perform the same back-tests, whose results are compared with the neural network. The network is not limited to cryptocurrency, but can be applied to any other financial markets.
ER  -


TY  - Preprint
T1  - Deep Symbolic Representation Learning for Heterogeneous Time-series Classification
A1  - Shengdong Zhang
A1  - Soheil Bahrampour
A1  - Naveen Ramakrishnan
A1  - Mohak Shah
JO  - ArXiv e-prints
Y1  - 5 December, 2016
UR  - https://arxiv.org/abs/1612.01254
N2  - In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.
ER  -


TY  - Preprint
T1  - Deep Multi-Modal Image Correspondence Learning
A1  - Chen Liu
A1  - Jiajun Wu
A1  - Pushmeet Kohli
A1  - Yasutaka Furukawa
JO  - ArXiv e-prints
Y1  - 4 December, 2016
UR  - https://arxiv.org/abs/1612.01225
N2  - Inference of correspondences between images from different modalities is an extremely important perceptual ability that enables humans to understand and recognize cross-modal concepts. In this paper, we consider an instance of this problem that involves matching photographs of building interiors with their corresponding floorplan. This is a particularly challenging problem because a floorplan, as a stylized architectural drawing, is very different in appearance from a color photograph. Furthermore, individual photographs by themselves depict only a part of a floorplan (e.g., kitchen, bathroom, and living room). We propose the use of a number of different neural network architectures for this task, which are trained and evaluated on a novel large-scale dataset of 5 million floorplan images and 80 million associated photographs. Experimental evaluation reveals that our neural network architectures are able to identify visual cues that result in reliable matches across these two quite different modalities. In fact, the trained networks are able to even outperform human subjects in several challenging image matching problems. Our result implies that neural networks are effective at perceptual tasks that require long periods of reasoning even for humans to solve.
ER  -


TY  - Preprint
T1  - Deep Metric Learning via Facility Location
A1  - Hyun Oh Song
A1  - Stefanie Jegelka
A1  - Vivek Rathod
A1  - Kevin Murphy
JO  - ArXiv e-prints
Y1  - 11 April, 2017
UR  - https://arxiv.org/abs/1612.01213
N2  - Learning the representation and the similarity metric in an end-to-end fashion with deep networks have demonstrated outstanding results for clustering and retrieval. However, these recent approaches still suffer from the performance degradation stemming from the local metric training procedure which is unaware of the global structure of the embedding space.
ER  -


TY  - Preprint
T1  - Deep Learning of Robotic Tasks without a Simulator using Strong and Weak Human Supervision
A1  - Bar Hilleli
A1  - Ran El-Yaniv
JO  - ArXiv e-prints
Y1  - 26 March, 2017
UR  - https://arxiv.org/abs/1612.01086
N2  - We propose a scheme for training a computerized agent to perform complex human tasks such as highway steering. The scheme is designed to follow a natural learning process whereby a human instructor teaches a computerized trainee. The learning process consists of five elements: (i) unsupervised feature learning; (ii) supervised imitation learning; (iii) supervised reward induction; (iv) supervised safety module construction; and (v) reinforcement learning. We implemented the last four elements of the scheme using deep convolutional networks and applied it to successfully create a computerized agent capable of autonomous highway steering over the well-known racing game Assetto Corsa. We demonstrate that the use of the last four elements is essential to effectively carry out the steering task using vision alone, without access to a driving simulator internals, and operating in wall-clock time. This is made possible also through the introduction of a safety network, a novel way for preventing the agent from performing catastrophic mistakes during the reinforcement learning stage.
ER  -


TY  - Preprint
T1  - Joint Visual Denoising and Classification using Deep Learning
A1  - Gang Chen
A1  - Yawei Li
A1  - Sargur N. Srihari
JO  - ArXiv e-prints
Y1  - 4 December, 2016
UR  - https://arxiv.org/abs/1612.01075
N2  - Visual restoration and recognition are traditionally addressed in pipeline fashion, i.e. denoising followed by classification. Instead, observing correlations between the two tasks, for example clearer image will lead to better categorization and vice visa, we propose a joint framework for visual restoration and recognition for handwritten images, inspired by advances in deep autoencoder and multi-modality learning. Our model is a 3-pathway deep architecture with a hidden-layer representation which is shared by multi-inputs and outputs, and each branch can be composed of a multi-layer deep model. Thus, visual restoration and classification can be unified using shared representation via non-linear mapping, and model parameters can be learnt via backpropagation. Using MNIST and USPS data corrupted with structured noise, the proposed framework performs at least 20\% better in classification than separate pipelines, as well as clearer recovered images. The noise model and the reproducible source code is available at {\url{https://github.com/ganggit/jointmodel}}.
ER  -


TY  - Preprint
T1  - Skin Cancer Detection and Tracking using Data Synthesis and Deep Learning
A1  - Yunzhu Li
A1  - Andre Esteva
A1  - Brett Kuprel
A1  - Rob Novoa
A1  - Justin Ko
A1  - Sebastian Thrun
JO  - ArXiv e-prints
Y1  - 4 December, 2016
UR  - https://arxiv.org/abs/1612.01074
N2  - Dense object detection and temporal tracking are needed across applications domains ranging from people-tracking to analysis of satellite imagery over time. The detection and tracking of malignant skin cancers and benign moles poses a particularly challenging problem due to the general uniformity of large skin patches, the fact that skin lesions vary little in their appearance, and the relatively small amount of data available. Here we introduce a novel data synthesis technique that merges images of individual skin lesions with full-body images and heavily augments them to generate significant amounts of data. We build a convolutional neural network (CNN) based system, trained on this synthetic data, and demonstrate superior performance to traditional detection and tracking techniques. Additionally, we compare our system to humans trained with simple criteria. Our system is intended for potential clinical use to augment the capabilities of healthcare providers. While domain-specific, we believe the methods invoked in this work will be useful in applying CNNs across domains that suffer from limited data availability.
ER  -


TY  - Preprint
T1  - Short-term traffic flow forecasting with spatial-temporal correlation in a hybrid deep learning framework
A1  - Yuankai Wu
A1  - Huachun Tan
JO  - ArXiv e-prints
Y1  - 3 December, 2016
UR  - https://arxiv.org/abs/1612.01022
N2  - Deep learning approaches have reached a celebrity status in artificial intelligence field, its success have mostly relied on Convolutional Networks (CNN) and Recurrent Networks. By exploiting fundamental spatial properties of images and videos, the CNN always achieves dominant performance on visual tasks. And the Recurrent Networks (RNN) especially long short-term memory methods (LSTM) can successfully characterize the temporal correlation, thus exhibits superior capability for time series tasks. Traffic flow data have plentiful characteristics on both time and space domain. However, applications of CNN and LSTM approaches on traffic flow are limited. In this paper, we propose a novel deep architecture combined CNN and LSTM to forecast future traffic flow (CLTFP). An 1-dimension CNN is exploited to capture spatial features of traffic flow, and two LSTMs are utilized to mine the short-term variability and periodicities of traffic flow. Given those meaningful features, the feature-level fusion is performed to achieve short-term forecasting. The proposed CLTFP is compared with other popular forecasting methods on an open datasets. Experimental results indicate that the CLTFP has considerable advantages in traffic flow forecasting. in additional, the proposed CLTFP is analyzed from the view of Granger Causality, and several interesting properties of CLTFP are discovered and discussed .
ER  -


TY  - Preprint
T1  - Deep Learning with Energy-efficient Binary Gradient Cameras
A1  - Suren Jayasuriya
A1  - Orazio Gallo
A1  - Jinwei Gu
A1  - Jan Kautz
JO  - ArXiv e-prints
Y1  - 3 December, 2016
UR  - https://arxiv.org/abs/1612.00986
N2  - Power consumption is a critical factor for the deployment of embedded computer vision systems. We explore the use of computational cameras that directly output binary gradient images to reduce the portion of the power consumption allocated to image sensing. We survey the accuracy of binary gradient cameras on a number of computer vision tasks using deep learning. These include object recognition, head pose regression, face detection, and gesture recognition. We show that, for certain applications, accuracy can be on par or even better than what can be achieved on traditional images. We are also the first to recover intensity information from binary spatial gradient images--useful for applications with a human observer in the loop, such as surveillance. Our results, which we validate with a prototype binary gradient camera, point to the potential of gradient-based computer vision systems.
ER  -


TY  - Preprint
T1  - Semi-supervised learning of deep metrics for stereo reconstruction
A1  - Stepan Tulyakov
A1  - Anton Ivanov
A1  - Francois Fleuret
JO  - ArXiv e-prints
Y1  - 3 December, 2016
UR  - https://arxiv.org/abs/1612.00979
N2  - Deep-learning metrics have recently demonstrated extremely good performance to match image patches for stereo reconstruction. However, training such metrics requires large amount of labeled stereo images, which can be difficult or costly to collect for certain applications. The main contribution of our work is a new semi-supervised method for learning deep metrics from unlabeled stereo images, given coarse information about the scenes and the optical system. Our method alternatively optimizes the metric with a standard stochastic gradient descent, and applies stereo constraints to regularize its prediction. Experiments on reference data-sets show that, for a given network architecture, training with this new method without ground-truth produces a metric with performance as good as state-of-the-art baselines trained with the said ground-truth. This work has three practical implications. Firstly, it helps to overcome limitations of training sets, in particular noisy ground truth. Secondly it allows to use much more training data during learning. Thirdly, it allows to tune deep metric for a particular stereo system, even if ground truth is not available.
ER  -


TY  - Preprint
T1  - PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation
A1  - Charles R. Qi
A1  - Hao Su
A1  - Kaichun Mo
A1  - Leonidas J. Guibas
JO  - ArXiv e-prints
Y1  - 10 April, 2017
UR  - https://arxiv.org/abs/1612.00593
N2  - Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.
ER  -


TY  - Preprint
T1  - 3D Bounding Box Estimation Using Deep Learning and Geometry
A1  - Arsalan Mousavian
A1  - Dragomir Anguelov
A1  - John Flynn
A1  - Jana Kosecka
JO  - ArXiv e-prints
Y1  - 10 April, 2017
UR  - https://arxiv.org/abs/1612.00496
N2  - We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors and sub-category detection. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset.
ER  -


TY  - Preprint
T1  - Playing Doom with SLAM-Augmented Deep Reinforcement Learning
A1  - Shehroze Bhatti
A1  - Alban Desmaison
A1  - Ondrej Miksik
A1  - Nantas Nardelli
A1  - N. Siddharth
A1  - Philip H. S. Torr
JO  - ArXiv e-prints
Y1  - 1 December, 2016
UR  - https://arxiv.org/abs/1612.00380
N2  - A number of recent approaches to policy learning in 2D game domains have been successful going directly from raw input images to actions. However when employed in complex 3D environments, they typically suffer from challenges related to partial observability, combinatorial exploration spaces, path planning, and a scarcity of rewarding scenarios. Inspired from prior work in human cognition that indicates how humans employ a variety of semantic concepts and abstractions (object categories, localisation, etc.) to reason about the world, we build an agent-model that incorporates such abstractions into its policy-learning framework. We augment the raw image input to a Deep Q-Learning Network (DQN), by adding details of objects and structural elements encountered, along with the agent&#39;s localisation. The different components are automatically extracted and composed into a topological representation using on-the-fly object detection and 3D-scene reconstruction.We evaluate the efficacy of our approach in Doom, a 3D first-person combat game that exhibits a number of challenges discussed, and show that our augmented framework consistently learns better, more effective policies.
ER  -


TY  - Preprint
T1  - Combining Deep Reinforcement Learning and Safety Based Control for Autonomous Driving
A1  - Xi Xiong
A1  - Jianqiang Wang
A1  - Fang Zhang
A1  - Keqiang Li
JO  - ArXiv e-prints
Y1  - 1 December, 2016
UR  - https://arxiv.org/abs/1612.00147
N2  - With the development of state-of-art deep reinforcement learning, we can efficiently tackle continuous control problems. But the deep reinforcement learning method for continuous control is based on historical data, which would make unpredicted decisions in unfamiliar scenarios. Combining deep reinforcement learning and safety based control can get good performance for self-driving and collision avoidance. In this passage, we use the Deep Deterministic Policy Gradient algorithm to implement autonomous driving without vehicles around. The vehicle can learn the driving policy in a stable and familiar environment, which is efficient and reliable. Then we use the artificial potential field to design collision avoidance algorithm with vehicles around. The path tracking method is also taken into consideration. The combination of deep reinforcement learning and safety based control performs well in most scenarios.
ER  -


TY  - Preprint
T1  - The observer-assisted method for adjusting hyper-parameters in deep learning algorithms
A1  - Maciej Wielgosz
JO  - ArXiv e-prints
Y1  - 30 November, 2016
UR  - https://arxiv.org/abs/1611.10328
N2  - This paper presents a concept of a novel method for adjusting hyper-parameters in Deep Learning (DL) algorithms. An external agent-observer monitors a performance of a selected Deep Learning algorithm. The observer learns to model the DL algorithm using a series of random experiments. Consequently, it may be used for predicting a response of the DL algorithm in terms of a selected quality measurement to a set of hyper-parameters. This allows to construct an ensemble composed of a series of evaluators which constitute an observer-assisted architecture. The architecture may be used to gradually iterate towards to the best achievable quality score in tiny steps governed by a unit of progress. The algorithm is stopped when the maximum number of steps is reached or no further progress is made.
ER  -


TY  - Preprint
T1  - Active Deep Learning for Classification of Hyperspectral Images
A1  - Peng Liu
A1  - Hui Zhang
A1  - Kie B. Eom
JO  - ArXiv e-prints
Y1  - 30 November, 2016
UR  - https://arxiv.org/abs/1611.10031
N2  - Active deep learning classification of hyperspectral images is considered in this paper. Deep learning has achieved success in many applications, but good-quality labeled samples are needed to construct a deep learning network. It is expensive getting good labeled samples in hyperspectral images for remote sensing applications. An active learning algorithm based on a weighted incremental dictionary learning is proposed for such applications. The proposed algorithm selects training samples that maximize two selection criteria, namely representative and uncertainty. This algorithm trains a deep network efficiently by actively selecting training samples at each iteration. The proposed algorithm is applied for the classification of hyperspectral images, and compared with other classification algorithms employing active learning. It is shown that the proposed algorithm is efficient and effective in classifying hyperspectral images.
ER  -


TY  - Preprint
T1  - Attend in groups: a weakly-supervised deep learning framework for learning from web data
A1  - Bohan Zhuang
A1  - Lingqiao Liu
A1  - Yao Li
A1  - Chunhua Shen
A1  - Ian Reid
JO  - ArXiv e-prints
Y1  - 29 November, 2016
UR  - https://arxiv.org/abs/1611.09960
N2  - Large-scale datasets have driven the rapid development of deep neural networks for visual recognition. However, annotating a massive dataset is expensive and time-consuming. Web images and their labels are, in comparison, much easier to obtain, but direct training on such automatically harvested images can lead to unsatisfactory performance, because the noisy labels of Web images adversely affect the learned recognition models. To address this drawback we propose an end-to-end weakly-supervised deep learning framework which is robust to the label noise in Web images. The proposed framework relies on two unified strategies -- random grouping and attention -- to effectively reduce the negative impact of noisy web image annotations. Specifically, random grouping stacks multiple images into a single training instance and thus increases the labeling accuracy at the instance level. Attention, on the other hand, suppresses the noisy signals from both incorrectly labeled images and less discriminative image regions. By conducting intensive experiments on two challenging datasets, including a newly collected fine-grained dataset with Web images of different car models, the superior performance of the proposed methods over competitive baselines is clearly demonstrated.
ER  -


TY  - Preprint
T1  - Exploration for Multi-task Reinforcement Learning with Deep Generative Models
A1  - Sai Praveen Bangaru
A1  - JS Suhas
A1  - Balaraman Ravindran
JO  - ArXiv e-prints
Y1  - 29 November, 2016
UR  - https://arxiv.org/abs/1611.09894
N2  - Exploration in multi-task reinforcement learning is critical in training agents to deduce the underlying MDP. Many of the existing exploration frameworks such as $E^3$, $R_{max}$, Thompson sampling assume a single stationary MDP and are not suitable for system identification in the multi-task setting. We present a novel method to facilitate exploration in multi-task reinforcement learning using deep generative models. We supplement our method with a low dimensional energy model to learn the underlying MDP distribution and provide a resilient and adaptive exploration signal to the agent. We evaluate our method on a new set of environments and provide intuitive interpretation of our results.
ER  -


TY  - Preprint
T1  - Gossip training for deep learning
A1  - Michael Blot
A1  - David Picard
A1  - Matthieu Cord
A1  - Nicolas Thome
JO  - ArXiv e-prints
Y1  - 29 November, 2016
UR  - https://arxiv.org/abs/1611.09726
N2  - We address the issue of speeding up the training of convolutional networks. Here we study a distributed method adapted to stochastic gradient descent (SGD). The parallel optimization setup uses several threads, each applying individual gradient descents on a local variable. We propose a new way to share information between different threads inspired by gossip algorithms and showing good consensus convergence properties. Our method called GoSGD has the advantage to be fully asynchronous and decentralized. We compared our method to the recent EASGD in \cite{elastic} on CIFAR-10 show encouraging results.
ER  -


TY  - Preprint
T1  - Learning Filter Banks Using Deep Learning For Acoustic Signals
A1  - Shuhui Qu
A1  - Juncheng Li
A1  - Wei Dai
A1  - Samarjit Das
JO  - ArXiv e-prints
Y1  - 29 November, 2016
UR  - https://arxiv.org/abs/1611.09526
N2  - Designing appropriate features for acoustic event recognition tasks is an active field of research. Expressive features should both improve the performance of the tasks and also be interpret-able. Currently, heuristically designed features based on the domain knowledge requires tremendous effort in hand-crafting, while features extracted through deep network are difficult for human to interpret. In this work, we explore the experience guided learning method for designing acoustic features. This is a novel hybrid approach combining both domain knowledge and purely data driven feature designing. Based on the procedure of log Mel-filter banks, we design a filter bank learning layer. We concatenate this layer with a convolutional neural network (CNN) model. After training the network, the weight of the filter bank learning layer is extracted to facilitate the design of acoustic features. We smooth the trained weight of the learning layer and re-initialize it in filter bank learning layer as audio feature extractor. For the environmental sound recognition task based on the Urban- sound8K dataset, the experience guided learning leads to a 2% accuracy improvement compared with the fixed feature extractors (the log Mel-filter bank). The shape of the new filter banks are visualized and explained to prove the effectiveness of the feature design process.
ER  -


TY  - Preprint
T1  - Learning Deep Representations Using Convolutional Auto-encoders with Symmetric Skip Connections
A1  - Jianfeng Dong
A1  - Xiao-Jiao Mao
A1  - Chunhua Shen
A1  - Yu-Bin Yang
JO  - ArXiv e-prints
Y1  - 28 March, 2017
UR  - https://arxiv.org/abs/1611.09119
N2  - Unsupervised pre-training was a critical technique for training deep neural networks years ago. With sufficient labeled data and modern training techniques, it is possible to train very deep neural networks from scratch in a purely supervised manner nowadays. However, unlabeled data is easier to obtain and usually of very large scale. How to make use of them better to help supervised learning is still a well-valued topic. In this paper, we investigate convolutional denoising auto-encoders to show that unsupervised pre-training can still improve the performance of high-level image related tasks such as image classification and semantic segmentation. The architecture we use is a convolutional auto-encoder network with symmetric shortcut connections. We empirically show that symmetric shortcut connections are very important for learning abstract representations via image reconstruction. When no extra unlabeled data are available, unsupervised pre-training with our network can regularize the supervised training and therefore lead to better generalization performance. With the help of unsupervised pre-training, our method achieves very competitive results in image classification using very simple all-convolution networks. When labeled data are limited but extra unlabeled data are available, our method achieves good results in several semi-supervised learning tasks.
ER  -


TY  - Preprint
T1  - Voronoi-based compact image descriptors: Efficient Region-of-Interest retrieval with VLAD and deep-learning-based descriptors
A1  - Aaron Chadha
A1  - Yiannis Andreopoulos
JO  - ArXiv e-prints
Y1  - 20 March, 2017
UR  - https://arxiv.org/abs/1611.08906
N2  - We investigate the problem of image retrieval based on visual queries when the latter comprise arbitrary regions-of-interest (ROI) rather than entire images. Our proposal is a compact image descriptor that combines the state-of-the-art in content-based descriptor extraction with a multi-level, Voronoi-based spatial partitioning of each dataset image. The proposed multi-level Voronoi-based encoding uses a spatial hierarchical K-means over interest-point locations, and computes a content-based descriptor over each cell. In order to reduce the matching complexity with minimal or no sacrifice in retrieval performance: (i) we utilize the tree structure of the spatial hierarchical K-means to perform a top-to-bottom pruning for local similarity maxima; (ii) we propose a new image similarity score that combines relevant information from all partition levels into a single measure for similarity; (iii) we combine our proposal with a novel and efficient approach for optimal bit allocation within quantized descriptor representations. By deriving both a Voronoi-based VLAD descriptor (termed as Fast-VVLAD) and a Voronoi-based deep convolutional neural network (CNN) descriptor (termed as Fast-VDCNN), we demonstrate that our Voronoi-based framework is agnostic to the descriptor basis, and can easily be slotted into existing frameworks. Via a range of ROI queries in two standard datasets, it is shown that the Voronoi-based descriptors achieve comparable or higher mean Average Precision against conventional grid-based spatial search, while offering more than two-fold reduction in complexity. Finally, beyond ROI queries, we show that Voronoi partitioning improves the geometric invariance of compact CNN descriptors, thereby resulting in competitive performance to the current state-of-the-art on whole image retrieval.
ER  -


TY  - Preprint
T1  - Did Evolution get it right? An evaluation of Near-Infrared imaging in semantic scene segmentation using deep learning
A1  - J. Rafid Siddiqui
JO  - ArXiv e-prints
Y1  - 27 November, 2016
UR  - https://arxiv.org/abs/1611.08815
N2  - Animals have evolved to restrict their sensing capabilities to certain region of electromagnetic spectrum. This is surprisingly a very narrow band on a vast scale which makes one think if there is a systematic bias underlying such selective filtration. The situation becomes even more intriguing when we find a sharp cutoff point at Near-infrared point whereby almost all animal vision systems seem to have a lower bound. This brings us to an interesting question: did evolution &#34;intentionally&#34; performed such a restriction in order to evolve higher visual cognition? In this work this question is addressed by experimenting with Near-infrared images for their potential applicability in higher visual processing such as semantic segmentation. A modified version of Fully Convolutional Networks are trained on NIR images and RGB images respectively and compared for their respective effectiveness in the wake of semantic segmentation. The results from the experiments show that visible part of the spectrum alone is sufficient for the robust semantic segmentation of the indoor as well as outdoor scenes.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Multi-Domain Dialogue Systems
A1  - Heriberto CuayÃ¡huitl
A1  - Seunghak Yu
A1  - Ashley Williamson
A1  - Jacob Carse
JO  - ArXiv e-prints
Y1  - 26 November, 2016
UR  - https://arxiv.org/abs/1611.08675
N2  - Standard deep reinforcement learning methods such as Deep Q-Networks (DQN) for multiple tasks (domains) face scalability problems. We propose a method for multi-domain dialogue policy learning---termed NDQN, and apply it to an information-seeking spoken dialogue system in the domains of restaurants and hotels. Experimental results comparing DQN (baseline) versus NDQN (proposed) using simulations report that our proposed method exhibits better scalability and is promising for optimising the behaviour of multi-domain dialogue systems.
ER  -


TY  - Preprint
T1  - Training an Interactive Humanoid Robot Using Multimodal Deep Reinforcement Learning
A1  - Heriberto CuayÃ¡huitl
A1  - Guillaume Couly
A1  - ClÃ©ment Olalainty
JO  - ArXiv e-prints
Y1  - 26 November, 2016
UR  - https://arxiv.org/abs/1611.08666
N2  - Training robots to perceive, act and communicate using multiple modalities still represents a challenging problem, particularly if robots are expected to learn efficiently from small sets of example interactions. We describe a learning approach as a step in this direction, where we teach a humanoid robot how to play the game of noughts and crosses. Given that multiple multimodal skills can be trained to play this game, we focus our attention to training the robot to perceive the game, and to interact in this game. Our multimodal deep reinforcement learning agent perceives multimodal features and exhibits verbal and non-verbal actions while playing. Experimental results using simulations show that the robot can learn to win or draw up to 98% of the games. A pilot test of the proposed multimodal system for the targeted game---integrating speech, vision and gestures---reports that reasonable and fluent interactions can be achieved using the proposed approach.
ER  -


TY  - Preprint
T1  - Geometric deep learning on graphs and manifolds using mixture model CNNs
A1  - Federico Monti
A1  - Davide Boscaini
A1  - Jonathan Masci
A1  - Emanuele RodolÃ 
A1  - Jan Svoboda
A1  - Michael M. Bronstein
JO  - ArXiv e-prints
Y1  - 6 December, 2016
UR  - https://arxiv.org/abs/1611.08402
N2  - Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph- and 3D shape analysis and show that it consistently outperforms previous approaches.
ER  -


TY  - Preprint
T1  - An Overview on Data Representation Learning: From Traditional Feature Learning to Recent Deep Learning
A1  - Guoqiang Zhong
A1  - Li-Na Wang
A1  - Junyu Dong
JO  - ArXiv e-prints
Y1  - 24 November, 2016
UR  - https://arxiv.org/abs/1611.08331
N2  - Since about 100 years ago, to learn the intrinsic structure of data, many representation learning approaches have been proposed, including both linear ones and nonlinear ones, supervised ones and unsupervised ones. Particularly, deep architectures are widely applied for representation learning in recent years, and have delivered top results in many tasks, such as image classification, object detection and speech recognition. In this paper, we review the development of data representation learning methods. Specifically, we investigate both traditional feature learning algorithms and state-of-the-art deep learning models. The history of data representation learning is introduced, while available resources (e.g. online course, tutorial and book information) and toolboxes are provided. Finally, we conclude this paper with remarks and some interesting research directions on data representation learning.
ER  -


TY  - Preprint
T1  - Geometric deep learning: going beyond Euclidean data
A1  - Michael M. Bronstein
A1  - Joan Bruna
A1  - Yann LeCun
A1  - Arthur Szlam
A1  - Pierre Vandergheynst
JO  - ArXiv e-prints
Y1  - 3 May, 2017
UR  - https://arxiv.org/abs/1611.08097
N2  - Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.
ER  -


TY  - Preprint
T1  - User Personalized Satisfaction Prediction via Multiple Instance Deep Learning
A1  - Zheqian Chen
A1  - Ben Gao
A1  - Huimin Zhang
A1  - Zhou Zhao
A1  - Deng Cai
JO  - ArXiv e-prints
Y1  - 24 November, 2016
UR  - https://arxiv.org/abs/1611.08096
N2  - Community based question answering services have arisen as a popular knowledge sharing pattern for netizens. With abundant interactions among users, individuals are capable of obtaining satisfactory information. However, it is not effective for users to attain answers within minutes. Users have to check the progress over time until the satisfying answers submitted. We address this problem as a user personalized satisfaction prediction task. Existing methods usually exploit manual feature selection. It is not desirable as it requires careful design and is labor intensive. In this paper, we settle this issue by developing a new multiple instance deep learning framework. Specifically, in our settings, each question follows a weakly supervised learning multiple instance learning assumption, where its obtained answers can be regarded as instance sets and we define the question resolved with at least one satisfactory answer. We thus design an efficient framework exploiting multiple instance learning property with deep learning to model the question answer pairs. Extensive experiments on large scale datasets from Stack Exchange demonstrate the feasibility of our proposed framework in predicting askers personalized satisfaction. Our framework can be extended to numerous applications such as UI satisfaction Prediction, multi armed bandit problem, expert finding and so on.
ER  -


TY  - Preprint
T1  - Relaxed Earth Mover&#39;s Distances for Chain- and Tree-connected Spaces and their use as a Loss Function in Deep Learning
A1  - Manuel Martinez
A1  - Monica Haurilet
A1  - Ziad Al-Halah
A1  - Makarand Tapaswi
A1  - Rainer Stiefelhagen
JO  - ArXiv e-prints
Y1  - 22 November, 2016
UR  - https://arxiv.org/abs/1611.07573
N2  - The Earth Mover&#39;s Distance (EMD) computes the optimal cost of transforming one distribution into another, given a known transport metric between them. In deep learning, the EMD loss allows us to embed information during training about the output space structure like hierarchical or semantic relations. This helps in achieving better output smoothness and generalization. However EMD is computationally expensive.Moreover, solving EMD optimization problems usually require complex techniques like lasso. These properties limit the applicability of EMD-based approaches in large scale machine learning.
ER  -


TY  - Preprint
T1  - Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond
A1  - Levent Sagun
A1  - Leon Bottou
A1  - Yann LeCun
JO  - ArXiv e-prints
Y1  - 5 October, 2017
UR  - https://arxiv.org/abs/1611.07476
N2  - We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how over-parametrized the system is, and for the edges that depend on the input data.
ER  -


TY  - Preprint
T1  - Deep Learning Approximation for Stochastic Control Problems
A1  - Jiequn Han
A1  - Weinan E
JO  - ArXiv e-prints
Y1  - 1 November, 2016
UR  - https://arxiv.org/abs/1611.07422
N2  - Many real world stochastic control problems suffer from the &#34;curse of dimensionality&#34;. To overcome this difficulty, we develop a deep learning approach that directly solves high-dimensional stochastic control problems based on Monte-Carlo sampling. We approximate the time-dependent controls as feedforward neural networks and stack these networks together through model dynamics. The objective function for the control problem plays the role of the loss function for the deep neural network. We test this approach using examples from the areas of optimal trading and energy storage. Our results suggest that the algorithm presented here achieves satisfactory accuracy and at the same time, can handle rather high dimensional problems.
ER  -


TY  - Preprint
T1  - A Deep Learning Based DDoS Detection System in Software-Defined Networking (SDN)
A1  - Quamar Niyaz
A1  - Weiqing Sun
A1  - Ahmad Y Javaid
JO  - ArXiv e-prints
Y1  - 22 November, 2016
UR  - https://arxiv.org/abs/1611.07400
N2  - Distributed Denial of Service (DDoS) is one of the most prevalent attacks that an organizational network infrastructure comes across nowadays. We propose a deep learning based multi-vector DDoS detection system in a software-defined network (SDN) environment. SDN provides flexibility to program network devices for different objectives and eliminates the need for third-party vendor-specific hardware. We implement our system as a network application on top of an SDN controller. We use deep learning for feature reduction of a large set of features derived from network traffic headers. We evaluate our system based on different performance metrics by applying it on traffic traces collected from different scenarios. We observe high accuracy with a low false-positive for attack detection in our proposed system.
ER  -


TY  - Preprint
T1  - Learning Multi-level Deep Representations for Image Emotion Classification
A1  - Tianrong Rao
A1  - Min Xu
A1  - Dong Xu
JO  - ArXiv e-prints
Y1  - 25 September, 2018
UR  - https://arxiv.org/abs/1611.07145
N2  - In this paper, we propose a new deep network that learns multi-level deep representations for image emotion classification (MldrNet). Image emotion can be recognized through image semantics, image aesthetics and low-level visual features from both global and local views. Existing image emotion classification works using hand-crafted features or deep features mainly focus on either low-level visual features or semantic-level image representations without taking all factors into consideration. The proposed MldrNet combines deep representations of different levels, i.e. image semantics, image aesthetics, and low-level visual features to effectively classify the emotion types of different kinds of images, such as abstract paintings and web images. Extensive experiments on both Internet images and abstract paintings demonstrate the proposed method outperforms the state-of-the-art methods using deep features or hand-crafted features. The proposed approach also outperforms the state-of-the-art methods with at least 6% performance improvement in terms of overall classification accuracy.
ER  -


TY  - Preprint
T1  - Max-Margin Deep Generative Models for (Semi-)Supervised Learning
A1  - Chongxuan Li
A1  - Jun Zhu
A1  - Bo Zhang
JO  - ArXiv e-prints
Y1  - 21 November, 2016
UR  - https://arxiv.org/abs/1611.07119
N2  - Deep generative models (DGMs) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability. However, it is relatively insufficient to empower the discriminative ability of DGMs on making accurate predictions. This paper presents max-margin deep generative models (mmDGMs) and a class-conditional variant (mmDCGMs), which explore the strongly discriminative principle of max-margin learning to improve the predictive performance of DGMs in both supervised and semi-supervised learning, while retaining the generative capability. In semi-supervised learning, we use the predictions of a max-margin classifier as the missing labels instead of performing full posterior inference for efficiency; we also introduce additional max-margin and label-balance regularization terms of unlabeled data for effectiveness. We develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objectives in different settings. Empirical results on various datasets demonstrate that: (1) max-margin learning can significantly improve the prediction performance of DGMs and meanwhile retain the generative ability; (2) in supervised learning, mmDGMs are competitive to the best fully discriminative networks when employing convolutional neural networks as the generative and recognition models; and (3) in semi-supervised learning, mmDCGMs can perform efficient inference and achieve state-of-the-art classification results on several benchmarks.
ER  -


TY  - Preprint
T1  - A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games
A1  - Felix Leibfried
A1  - Nate Kushman
A1  - Katja Hofmann
JO  - ArXiv e-prints
Y1  - 17 August, 2017
UR  - https://arxiv.org/abs/1611.07078
N2  - Reinforcement learning is concerned with identifying reward-maximizing behaviour policies in environments that are initially unknown. State-of-the-art reinforcement learning approaches, such as deep Q-networks, are model-free and learn to act effectively across a wide range of environments such as Atari games, but require huge amounts of data. Model-based techniques are more data-efficient, but need to acquire explicit knowledge about the environment.
ER  -


TY  - Preprint
T1  - A Metaprogramming and Autotuning Framework for Deploying Deep Learning Applications
A1  - Matthew W. Moskewicz
A1  - Ali Jannesari
A1  - Kurt Keutzer
JO  - ArXiv e-prints
Y1  - 21 November, 2016
UR  - https://arxiv.org/abs/1611.06945
N2  - In recent years, deep neural networks (DNNs), have yielded strong results on a wide range of applications. Graphics Processing Units (GPUs) have been one key enabling factor leading to the current popularity of DNNs. However, despite increasing hardware flexibility and software programming toolchain maturity, high efficiency GPU programming remains difficult: it suffers from high complexity, low productivity, and low portability. GPU vendors such as NVIDIA have spent enormous effort to write special-purpose DNN libraries. However, on other hardware targets, especially mobile GPUs, such vendor libraries are not generally available. Thus, the development of portable, open, high-performance, energy-efficient GPU code for DNN operations would enable broader deployment of DNN-based algorithms. Toward this end, this work presents a framework to enable productive, high-efficiency GPU programming for DNN computations across hardware platforms and programming models. In particular, the framework provides specific support for metaprogramming, autotuning, and DNN-tailored data types. Using our framework, we explore implementing DNN operations on three different hardware targets: NVIDIA, AMD, and Qualcomm GPUs. On NVIDIA GPUs, we show both portability between OpenCL and CUDA as well competitive performance compared to the vendor library. On Qualcomm GPUs, we show that our framework enables productive development of target-specific optimizations, and achieves reasonable absolute performance. Finally, On AMD GPUs, we show initial results that indicate our framework can yield reasonable performance on a new platform with minimal effort.
ER  -


TY  - Preprint
T1  - Predicting 1p19q Chromosomal Deletion of Low-Grade Gliomas from MR Images using Deep Learning
A1  - Zeynettin Akkus
A1  - Issa Ali
A1  - Jiri Sedlar
A1  - Timothy L. Kline
A1  - Jay P. Agrawal
A1  - Ian F. Parney
A1  - Caterina Giannini
A1  - Bradley J. Erickson
JO  - ArXiv e-prints
Y1  - 21 November, 2016
UR  - https://arxiv.org/abs/1611.06939
N2  - Objective: Several studies have associated codeletion of chromosome arms 1p/19q in low-grade gliomas (LGG) with positive response to treatment and longer progression free survival. Therefore, predicting 1p/19q status is crucial for effective treatment planning of LGG. In this study, we predict the 1p/19q status from MR images using convolutional neural networks (CNN), which could be a noninvasive alternative to surgical biopsy and histopathological analysis. Method: Our method consists of three main steps: image registration, tumor segmentation, and classification of 1p/19q status using CNN. We included a total of 159 LGG with 3 image slices each who had biopsy-proven 1p/19q status (57 nondeleted and 102 codeleted) and preoperative postcontrast-T1 (T1C) and T2 images. We divided our data into training, validation, and test sets. The training data was balanced for equal class probability and then augmented with iterations of random translational shift, rotation, and horizontal and vertical flips to increase the size of the training set. We shuffled and augmented the training data to counter overfitting in each epoch. Finally, we evaluated several configurations of a multi-scale CNN architecture until training and validation accuracies became consistent. Results: The results of the best performing configuration on the unseen test set were 93.3% (sensitivity), 82.22% (specificity), and 87.7% (accuracy). Conclusion: Multi-scale CNN with their self-learning capability provides promising results for predicting 1p/19q status noninvasively based on T1C and T2 images. Significance: Predicting 1p/19q status noninvasively from MR images would allow selecting effective treatment strategies for LGG patients without the need for surgical biopsy.
ER  -


TY  - Preprint
T1  - Deep Residual Learning for Compressed Sensing CT Reconstruction via Persistent Homology Analysis
A1  - Yo Seob Han
A1  - Jaejun Yoo
A1  - Jong Chul Ye
JO  - ArXiv e-prints
Y1  - 25 November, 2016
UR  - https://arxiv.org/abs/1611.06391
N2  - Recently, compressed sensing (CS) computed tomography (CT) using sparse projection views has been extensively investigated to reduce the potential risk of radiation to patient. However, due to the insufficient number of projection views, an analytic reconstruction approach results in severe streaking artifacts and CS-based iterative approach is computationally very expensive. To address this issue, here we propose a novel deep residual learning approach for sparse view CT reconstruction. Specifically, based on a novel persistent homology analysis showing that the manifold of streaking artifacts is topologically simpler than original ones, a deep residual learning architecture that estimates the streaking artifacts is developed. Once a streaking artifact image is estimated, an artifact-free image can be obtained by subtracting the streaking artifacts from the input image. Using extensive experiments with real patient data set, we confirm that the proposed residual learning provides significantly better image reconstruction performance with several orders of magnitude faster computational speed.
ER  -


TY  - Preprint
T1  - Beyond Deep Residual Learning for Image Restoration: Persistent Homology-Guided Manifold Simplification
A1  - Woong Bae
A1  - Jaejun Yoo
A1  - Jong Chul Ye
JO  - ArXiv e-prints
Y1  - 8 June, 2017
UR  - https://arxiv.org/abs/1611.06345
N2  - The latest deep learning approaches perform better than the state-of-the-art signal processing approaches in various image restoration tasks. However, if an image contains many patterns and structures, the performance of these CNNs is still inferior. To address this issue, here we propose a novel feature space deep residual learning algorithm that outperforms the existing residual learning. The main idea is originated from the observation that the performance of a learning algorithm can be improved if the input and/or label manifolds can be made topologically simpler by an analytic mapping to a feature space. Our extensive numerical studies using denoising experiments and NTIRE single-image super-resolution (SISR) competition demonstrate that the proposed feature space residual learning outperforms the existing state-of-the-art approaches. Moreover, our algorithm was ranked third in NTIRE competition with 5-10 times faster computational time compared to the top ranked teams. The source code is available on page : https://github.com/iorism/CNN.git
ER  -


TY  - Preprint
T1  - Learning the Number of Neurons in Deep Networks
A1  - Jose M Alvarez
A1  - Mathieu Salzmann
JO  - ArXiv e-prints
Y1  - 13 January, 2017
UR  - https://arxiv.org/abs/1611.06321
N2  - Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80\% while retaining or even improving the network accuracy.
ER  -


TY  - Preprint
T1  - ModelHub: Towards Unified Data and Lifecycle Management for Deep Learning
A1  - Hui Miao
A1  - Ang Li
A1  - Larry S. Davis
A1  - Amol Deshpande
JO  - ArXiv e-prints
Y1  - 18 November, 2016
UR  - https://arxiv.org/abs/1611.06224
N2  - Deep learning has improved state-of-the-art results in many important fields, and has been the subject of much research in recent years, leading to the development of several systems for facilitating deep learning. Current systems, however, mainly focus on model building and training phases, while the issues of data management, model sharing, and lifecycle management are largely ignored. Deep learning modeling lifecycle generates a rich set of data artifacts, such as learned parameters and training logs, and comprises of several frequently conducted tasks, e.g., to understand the model behaviors and to try out new models. Dealing with such artifacts and tasks is cumbersome and largely left to the users. This paper describes our vision and implementation of a data and lifecycle management system for deep learning. First, we generalize model exploration and model enumeration queries from commonly conducted tasks by deep learning modelers, and propose a high-level domain specific language (DSL), inspired by SQL, to raise the abstraction level and accelerate the modeling process. To manage the data artifacts, especially the large amount of checkpointed float parameters, we design a novel model versioning system (dlv), and a read-optimized parameter archival storage system (PAS) that minimizes storage footprint and accelerates query workloads without losing accuracy. PAS archives versioned models using deltas in a multi-resolution fashion by separately storing the less significant bits, and features a novel progressive query (inference) evaluation algorithm. Third, we show that archiving versioned models using deltas poses a new dataset versioning problem and we develop efficient algorithms for solving it. We conduct extensive experiments over several real datasets from computer vision domain to show the efficiency of the proposed techniques.
ER  -


TY  - Preprint
T1  - DeepVO: A Deep Learning approach for Monocular Visual Odometry
A1  - Vikram Mohanty
A1  - Shubh Agrawal
A1  - Shaswat Datta
A1  - Arna Ghosh
A1  - Vishnu Dutt Sharma
A1  - Debashish Chakravarty
JO  - ArXiv e-prints
Y1  - 18 November, 2016
UR  - https://arxiv.org/abs/1611.06069
N2  - Deep Learning based techniques have been adopted with precision to solve a lot of standard computer vision problems, some of which are image classification, object detection and segmentation. Despite the widespread success of these approaches, they have not yet been exploited largely for solving the standard perception related problems encountered in autonomous navigation such as Visual Odometry (VO), Structure from Motion (SfM) and Simultaneous Localization and Mapping (SLAM). This paper analyzes the problem of Monocular Visual Odometry using a Deep Learning-based framework, instead of the regular &#39;feature detection and tracking&#39; pipeline approaches. Several experiments were performed to understand the influence of a known/unknown environment, a conventional trackable feature and pre-trained activations tuned for object classification on the network&#39;s ability to accurately estimate the motion trajectory of the camera (or the vehicle). Based on these observations, we propose a Convolutional Neural Network architecture, best suited for estimating the object&#39;s pose under known environment conditions, and displays promising results when it comes to inferring the actual scale using just a single camera in real-time.
ER  -


TY  - Preprint
T1  - Deep Action- and Context-Aware Sequence Learning for Activity Recognition and Anticipation
A1  - Mohammad Sadegh Aliakbarian
A1  - Fatemehsadat Saleh
A1  - Basura Fernando
A1  - Mathieu Salzmann
A1  - Lars Petersson
A1  - Lars Andersson
JO  - ArXiv e-prints
Y1  - 17 November, 2016
UR  - https://arxiv.org/abs/1611.05520
N2  - Action recognition and anticipation are key to the success of many computer vision applications. Existing methods can roughly be grouped into those that extract global, context-aware representations of the entire image or sequence, and those that aim at focusing on the regions where the action occurs. While the former may suffer from the fact that context is not always reliable, the latter completely ignore this source of information, which can nonetheless be helpful in many situations. In this paper, we aim at making the best of both worlds by developing an approach that leverages both context-aware and action-aware features. At the core of our method lies a novel multi-stage recurrent architecture that allows us to effectively combine these two sources of information throughout a video. This architecture first exploits the global, context-aware features, and merges the resulting representation with the localized, action-aware ones. Our experiments on standard datasets evidence the benefits of our approach over methods that use each information type separately. We outperform the state-of-the-art methods that, as us, rely only on RGB frames as input for both action recognition and anticipation.
ER  -


TY  - Preprint
T1  - Solving Cold-Start Problem in Large-scale Recommendation Engines: A Deep Learning Approach
A1  - Jianbo Yuan
A1  - Walid Shalaby
A1  - Mohammed Korayem
A1  - David Lin
A1  - Khalifeh AlJadda
A1  - Jiebo Luo
JO  - ArXiv e-prints
Y1  - 16 November, 2016
UR  - https://arxiv.org/abs/1611.05480
N2  - Collaborative Filtering (CF) is widely used in large-scale recommendation engines because of its efficiency, accuracy and scalability. However, in practice, the fact that recommendation engines based on CF require interactions between users and items before making recommendations, make it inappropriate for new items which haven&#39;t been exposed to the end users to interact with. This is known as the cold-start problem. In this paper we introduce a novel approach which employs deep learning to tackle this problem in any CF based recommendation engine. One of the most important features of the proposed technique is the fact that it can be applied on top of any existing CF based recommendation engine without changing the CF core. We successfully applied this technique to overcome the item cold-start problem in Careerbuilder&#39;s CF based recommendation engine. Our experiments show that the proposed technique is very efficient to resolve the cold-start problem while maintaining high accuracy of the CF recommendations.
ER  -


TY  - Preprint
T1  - The ZipML Framework for Training Models with End-to-End Low Precision: The Cans, the Cannots, and a Little Bit of Deep Learning
A1  - Hantian Zhang
A1  - Jerry Li
A1  - Kaan Kara
A1  - Dan Alistarh
A1  - Ji Liu
A1  - Ce Zhang
JO  - ArXiv e-prints
Y1  - 19 June, 2017
UR  - https://arxiv.org/abs/1611.05402
N2  - Recently there has been significant interest in training machine-learning models at low precision: by reducing precision, one can reduce computation and communication by one order of magnitude. We examine training at reduced precision, both from a theoretical and practical perspective, and ask: is it possible to train models at end-to-end low precision with provable guarantees? Can this lead to consistent order-of-magnitude speedups? We present a framework called ZipML to answer these questions. For linear models, the answer is yes. We develop a simple framework based on one simple but novel strategy called double sampling. Our framework is able to execute training at low precision with no bias, guaranteeing convergence, whereas naive quantization would introduce significant bias. We validate our framework across a range of applications, and show that it enables an FPGA prototype that is up to 6.5x faster than an implementation using full 32-bit precision. We further develop a variance-optimal stochastic quantization strategy and show that it can make a significant difference in a variety of settings. When applied to linear models together with double sampling, we save up to another 1.7x in data movement compared with uniform quantization. When training deep networks with quantized models, we achieve higher accuracy than the state-of-the-art XNOR-Net. Finally, we extend our framework through approximation to non-linear models, such as SVM. We show that, although using low-precision data induces bias, we can appropriately bound and control the bias. We find in practice 8-bit precision is often sufficient to converge to the correct solution. Interestingly, however, in practice we notice that our framework does not always outperform the naive rounding approach. We discuss this negative result in detail.
ER  -


TY  - Preprint
T1  - Deep Transfer Learning for Person Re-identification
A1  - Mengyue Geng
A1  - Yaowei Wang
A1  - Tao Xiang
A1  - Yonghong Tian
JO  - ArXiv e-prints
Y1  - 22 November, 2016
UR  - https://arxiv.org/abs/1611.05244
N2  - Person re-identification (Re-ID) poses a unique challenge to deep learning: how to learn a deep model with millions of parameters on a small training set of few or no labels. In this paper, a number of deep transfer learning models are proposed to address the data sparsity problem. First, a deep network architecture is designed which differs from existing deep Re-ID models in that (a) it is more suitable for transferring representations learned from large image classification datasets, and (b) classification loss and verification loss are combined, each of which adopts a different dropout strategy. Second, a two-stepped fine-tuning strategy is developed to transfer knowledge from auxiliary datasets. Third, given an unlabelled Re-ID dataset, a novel unsupervised deep transfer learning model is developed based on co-training. The proposed models outperform the state-of-the-art deep Re-ID models by large margins: we achieve Rank-1 accuracy of 85.4\%, 83.7\% and 56.3\% on CUHK03, Market1501, and VIPeR respectively, whilst on VIPeR, our unsupervised model (45.1\%) beats most supervised models.
ER  -


TY  - Preprint
T1  - Learning long-term dependencies for action recognition with a biologically-inspired deep network
A1  - Yemin Shi
A1  - Yonghong Tian
A1  - Yaowei Wang
A1  - Tiejun Huang
JO  - ArXiv e-prints
Y1  - 19 March, 2017
UR  - https://arxiv.org/abs/1611.05216
N2  - Despite a lot of research efforts devoted in recent years, how to efficiently learn long-term dependencies from sequences still remains a pretty challenging task. As one of the key models for sequence learning, recurrent neural network (RNN) and its variants such as long short term memory (LSTM) and gated recurrent unit (GRU) are still not powerful enough in practice. One possible reason is that they have only feedforward connections, which is different from the biological neural system that is typically composed of both feedforward and feedback connections. To address this problem, this paper proposes a biologically-inspired deep network, called shuttleNet\footnote{Our code is available at \url{https://github.com/shiyemin/shuttlenet}}. Technologically, the shuttleNet consists of several processors, each of which is a GRU while associated with multiple groups of cells and states. Unlike traditional RNNs, all processors inside shuttleNet are loop connected to mimic the brain&#39;s feedforward and feedback connections, in which they are shared across multiple pathways in the loop connection. Attention mechanism is then employed to select the best information flow pathway. Extensive experiments conducted on two benchmark datasets (i.e UCF101 and HMDB51) show that we can beat state-of-the-art methods by simply embedding shuttleNet into a CNN-RNN framework.
ER  -


TY  - Preprint
T1  - Cost-Sensitive Deep Learning with Layer-Wise Cost Estimation
A1  - Yu-An Chung
A1  - Hsuan-Tien Lin
JO  - ArXiv e-prints
Y1  - 15 November, 2016
UR  - https://arxiv.org/abs/1611.05134
N2  - While deep neural networks have succeeded in several visual applications, such as object recognition, detection, and localization, by reaching very high classification accuracies, it is important to note that many real-world applications demand vary- ing costs for different types of misclassification errors, thus requiring cost-sensitive classification algorithms. Current models of deep neural networks for cost-sensitive classification are restricted to some specific network structures and limited depth. In this paper, we propose a novel framework that can be applied to deep neural networks with any structure to facilitate their learning of meaningful representations for cost-sensitive classification problems. Furthermore, the framework allows end- to-end training of deeper networks directly. The framework is designed by augmenting auxiliary neurons to the output of each hidden layer for layer-wise cost estimation, and including the total estimation loss within the optimization objective. Experimental results on public benchmark visual data sets with two cost information settings demonstrate that the proposed frame- work outperforms state-of-the-art cost-sensitive deep learning models.
ER  -


TY  - Preprint
T1  - Learning a Deep Embedding Model for Zero-Shot Learning
A1  - Li Zhang
A1  - Tao Xiang
A1  - Shaogang Gong
JO  - ArXiv e-prints
Y1  - 12 April, 2017
UR  - https://arxiv.org/abs/1611.05088
N2  - Zero-shot learning (ZSL) models rely on learning a joint embedding space where both textual/semantic description of object classes and visual representation of object images can be projected to for nearest neighbour search. Despite the success of deep neural networks that learn an end-to-end model between text and images in other vision problems such as image captioning, very few deep ZSL model exists and they show little advantage over ZSL models that utilise deep feature representations but do not learn an end-to-end embedding. In this paper we argue that the key to make deep ZSL models succeed is to choose the right embedding space. Instead of embedding into a semantic space or an intermediate space, we propose to use the visual space as the embedding space. This is because that in this space, the subsequent nearest neighbour search would suffer much less from the hubness problem and thus become more effective. This model design also provides a natural mechanism for multiple semantic modalities (e.g., attributes and sentence descriptions) to be fused and optimised jointly in an end-to-end manner. Extensive experiments on four benchmarks show that our model significantly outperforms the existing models.
ER  -


TY  - Preprint
T1  - OctNet: Learning Deep 3D Representations at High Resolutions
A1  - Gernot Riegler
A1  - Ali Osman Ulusoy
A1  - Andreas Geiger
JO  - ArXiv e-prints
Y1  - 10 April, 2017
UR  - https://arxiv.org/abs/1611.05009
N2  - We present OctNet, a representation for deep learning with sparse 3D data. In contrast to existing models, our representation enables 3D convolutional networks which are both deep and high resolution. Towards this goal, we exploit the sparsity in the input data to hierarchically partition the space using a set of unbalanced octrees where each leaf node stores a pooled feature representation. This allows to focus memory allocation and computation to the relevant dense regions and enables deeper networks without compromising resolution. We demonstrate the utility of our OctNet representation by analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling.
ER  -


TY  - Preprint
T1  - #Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning
A1  - Haoran Tang
A1  - Rein Houthooft
A1  - Davis Foote
A1  - Adam Stooke
A1  - Xi Chen
A1  - Yan Duan
A1  - John Schulman
A1  - Filip De Turck
A1  - Pieter Abbeel
JO  - ArXiv e-prints
Y1  - 5 December, 2017
UR  - https://arxiv.org/abs/1611.04717
N2  - Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.
ER  -


TY  - Preprint
T1  - How to scale distributed deep learning?
A1  - Peter H. Jin
A1  - Qiaochu Yuan
A1  - Forrest Iandola
A1  - Kurt Keutzer
JO  - ArXiv e-prints
Y1  - 14 November, 2016
UR  - https://arxiv.org/abs/1611.04581
N2  - Training time on large datasets for deep neural networks is the principal workflow bottleneck in a number of important applications of deep learning, such as object classification and detection in automatic driver assistance systems (ADAS). To minimize training time, the training of a deep neural network must be scaled beyond a single machine to as many machines as possible by distributing the optimization method used for training. While a number of approaches have been proposed for distributed stochastic gradient descent (SGD), at the current time synchronous approaches to distributed SGD appear to be showing the greatest performance at large scale. Synchronous scaling of SGD suffers from the need to synchronize all processors on each gradient step and is not resilient in the face of failing or lagging processors. In asynchronous approaches using parameter servers, training is slowed by contention to the parameter server. In this paper we compare the convergence of synchronous and asynchronous SGD for training a modern ResNet network architecture on the ImageNet classification problem. We also propose an asynchronous method, gossiping SGD, that aims to retain the positive features of both systems by replacing the all-reduce collective operation of synchronous training with a gossip aggregation algorithm. We find, perhaps counterintuitively, that asynchronous SGD, including both elastic averaging and gossiping, converges faster at fewer nodes (up to about 32 nodes), whereas synchronous SGD scales better to more nodes (up to about 100 nodes).
ER  -


TY  - Preprint
T1  - Identity Matters in Deep Learning
A1  - Moritz Hardt
A1  - Tengyu Ma
JO  - ArXiv e-prints
Y1  - 20 July, 2018
UR  - https://arxiv.org/abs/1611.04231
N2  - An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as \emph{batch normalization}, but was also key to the immense success of \emph{residual networks}.
ER  -


TY  - Preprint
T1  - Tricks from Deep Learning
A1  - AtÄ±lÄ±m GÃ¼neÅ Baydin
A1  - Barak A. Pearlmutter
A1  - Jeffrey Mark Siskind
JO  - ArXiv e-prints
Y1  - 10 November, 2016
UR  - https://arxiv.org/abs/1611.03777
N2  - The deep learning community has devised a diverse set of methods to make gradient optimization, using large datasets, of large and highly complex models with deeply cascaded nonlinearities, practical. Taken as a whole, these methods constitute a breakthrough, allowing computational structures which are quite wide, very deep, and with an enormous number and variety of free parameters to be effectively optimized. The result now dominates much of practical machine learning, with applications in machine translation, computer vision, and speech recognition. Many of these methods, viewed through the lens of algorithmic differentiation (AD), can be seen as either addressing issues with the gradient itself, or finding ways of achieving increased efficiency using tricks that are AD-related, but not provided by current AD systems.
ER  -


TY  - Preprint
T1  - Hierarchical Object Detection with Deep Reinforcement Learning
A1  - Miriam Bellver
A1  - Xavier Giro-i-Nieto
A1  - Ferran Marques
A1  - Jordi Torres
JO  - ArXiv e-prints
Y1  - 25 November, 2016
UR  - https://arxiv.org/abs/1611.03718
N2  - We present a method for performing hierarchical object detection in images guided by a deep reinforcement learning agent. The key idea is to focus on those parts of the image that contain richer information and zoom on them. We train an intelligent agent that, given an image window, is capable of deciding where to focus the attention among five different predefined region candidates (smaller windows). This procedure is iterated providing a hierarchical image analysis.We compare two different candidate proposal strategies to guide the object search: with and without overlap. Moreover, our work compares two different strategies to extract features from a convolutional neural network for each region proposal: a first one that computes new feature maps for each region proposal, and a second one that computes the feature maps for the whole image to later generate crops for each region proposal. Experiments indicate better results for the overlapping candidate proposal strategy and a loss of performance for the cropped image features due to the loss of spatial resolution. We argue that, while this loss seems unavoidable when working with large amounts of object candidates, the much more reduced amount of region proposals generated by our reinforcement learning agent allows considering to extract features for each location without sharing convolutional computation among regions.
ER  -


TY  - Preprint
T1  - UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text
A1  - Wei-Fan Chen
A1  - Lun-Wei Ku
JO  - ArXiv e-prints
Y1  - 11 November, 2016
UR  - https://arxiv.org/abs/1611.03599
N2  - Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.
ER  -


TY  - Preprint
T1  - Learning Multi-Scale Deep Features for High-Resolution Satellite Image Classification
A1  - Qingshan Liu
A1  - Renlong Hang
A1  - Huihui Song
A1  - Zhi Li
JO  - ArXiv e-prints
Y1  - 11 November, 2016
UR  - https://arxiv.org/abs/1611.03591
N2  - In this paper, we propose a multi-scale deep feature learning method for high-resolution satellite image classification. Specifically, we firstly warp the original satellite image into multiple different scales. The images in each scale are employed to train a deep convolutional neural network (DCNN). However, simultaneously training multiple DCNNs is time-consuming. To address this issue, we explore DCNN with spatial pyramid pooling (SPP-net). Since different SPP-nets have the same number of parameters, which share the identical initial values, and only fine-tuning the parameters in fully-connected layers ensures the effectiveness of each network, thereby greatly accelerating the training process. Then, the multi-scale satellite images are fed into their corresponding SPP-nets respectively to extract multi-scale deep features. Finally, a multiple kernel learning method is developed to automatically learn the optimal combination of such features. Experiments on two difficult datasets show that the proposed method achieves favorable performance compared to other state-of-the-art methods.
ER  -


TY  - Preprint
T1  - Understanding deep learning requires rethinking generalization
A1  - Chiyuan Zhang
A1  - Samy Bengio
A1  - Moritz Hardt
A1  - Benjamin Recht
A1  - Oriol Vinyals
JO  - ArXiv e-prints
Y1  - 26 February, 2017
UR  - https://arxiv.org/abs/1611.03530
N2  - Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.
ER  -


TY  - Preprint
T1  - X-ray Scattering Image Classification Using Deep Learning
A1  - Boyu Wang
A1  - Kevin Yager
A1  - Dantong Yu
A1  - Minh Hoai
JO  - ArXiv e-prints
Y1  - 10 November, 2016
UR  - https://arxiv.org/abs/1611.03313
N2  - Visual inspection of x-ray scattering images is a powerful technique for probing the physical structure of materials at the molecular scale. In this paper, we explore the use of deep learning to develop methods for automatically analyzing x-ray scattering images. In particular, we apply Convolutional Neural Networks and Convolutional Autoencoders for x-ray scattering image classification. To acquire enough training data for deep learning, we use simulation software to generate synthetic x-ray scattering images. Experiments show that deep learning methods outperform previously published methods by 10\% on synthetic and real datasets.
ER  -


TY  - Preprint
T1  - Large-scale JPEG steganalysis using hybrid deep-learning framework
A1  - Jishen Zeng
A1  - Shunquan Tan
A1  - Bin Li
A1  - Jiwu Huang
JO  - ArXiv e-prints
Y1  - 24 November, 2017
UR  - https://arxiv.org/abs/1611.03233
N2  - Adoption of deep learning in image steganalysis is still in its initial stage. In this paper we propose a generic hybrid deep-learning framework for JPEG steganalysis incorporating the domain knowledge behind rich steganalytic models. Our proposed framework involves two main stages. The first stage is hand-crafted, corresponding to the convolution phase and the quantization &amp; truncation phase of the rich models. The second stage is a compound deep neural network containing multiple deep subnets in which the model parameters are learned in the training procedure. We provided experimental evidences and theoretical reflections to argue that the introduction of threshold quantizers, though disable the gradient-descent-based learning of the bottom convolution phase, is indeed cost-effective. We have conducted extensive experiments on a large-scale dataset extracted from ImageNet. The primary dataset used in our experiments contains 500,000 cover images, while our largest dataset contains five million cover images. Our experiments show that the integration of quantization and truncation into deep-learning steganalyzers do boost the detection performance by a clear margin. Furthermore, we demonstrate that our framework is insensitive to JPEG blocking artifact alterations, and the learned model can be easily transferred to a different attacking target and even a different dataset. These properties are of critical importance in practical applications.
ER  -


TY  - Preprint
T1  - Low-effort place recognition with WiFi fingerprints using deep learning
A1  - MichaÅ Nowicki
A1  - Jan Wietrzykowski
JO  - ArXiv e-prints
Y1  - 28 April, 2017
UR  - https://arxiv.org/abs/1611.02049
N2  - Using WiFi signals for indoor localization is the main localization modality of the existing personal indoor localization systems operating on mobile devices. WiFi fingerprinting is also used for mobile robots, as WiFi signals are usually available indoors and can provide rough initial position estimate or can be used together with other positioning systems. Currently, the best solutions rely on filtering, manual data analysis, and time-consuming parameter tuning to achieve reliable and accurate localization. In this work, we propose to use deep neural networks to significantly lower the work-force burden of the localization system design, while still achieving satisfactory results. Assuming the state-of-the-art hierarchical approach, we employ the DNN system for building/floor classification. We show that stacked autoencoders allow to efficiently reduce the feature space in order to achieve robust and precise classification. The proposed architecture is verified on the publicly available UJIIndoorLoc dataset and the results are compared with other solutions.
ER  -


TY  - Preprint
T1  - DeepSense: A Unified Deep Learning Framework for Time-Series Mobile Sensing Data Processing
A1  - Shuochao Yao
A1  - Shaohan Hu
A1  - Yiran Zhao
A1  - Aston Zhang
A1  - Tarek Abdelzaher
JO  - ArXiv e-prints
Y1  - 2 July, 2017
UR  - https://arxiv.org/abs/1611.01942
N2  - Mobile sensing applications usually require time-series inputs from sensors. Some applications, such as tracking, can use sensed acceleration and rate of rotation to calculate displacement based on physical system models. Other applications, such as activity recognition, extract manually designed features from sensor inputs for classification. Such applications face two challenges. On one hand, on-device sensor measurements are noisy. For many mobile applications, it is hard to find a distribution that exactly describes the noise in practice. Unfortunately, calculating target quantities based on physical system and noise models is only as accurate as the noise assumptions. Similarly, in classification applications, although manually designed features have proven to be effective, it is not always straightforward to find the most robust features to accommodate diverse sensor noise patterns and user behaviors. To this end, we propose DeepSense, a deep learning framework that directly addresses the aforementioned noise and feature customization challenges in a unified manner. DeepSense integrates convolutional and recurrent neural networks to exploit local interactions among similar mobile sensors, merge local interactions of different sensory modalities into global interactions, and extract temporal relationships to model signal dynamics. DeepSense thus provides a general signal estimation and classification framework that accommodates a wide range of applications. We demonstrate the effectiveness of DeepSense using three representative and challenging tasks: car tracking with motion sensors, heterogeneous human activity recognition, and user identification with biometric motion analysis. DeepSense significantly outperforms the state-of-the-art methods for all three tasks. In addition, DeepSense is feasible to implement on smartphones due to its moderate energy consumption and low latency
ER  -


TY  - Preprint
T1  - Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning
A1  - Oron Anschel
A1  - Nir Baram
A1  - Nahum Shimkin
JO  - ArXiv e-prints
Y1  - 10 March, 2017
UR  - https://arxiv.org/abs/1611.01929
N2  - Instability and variability of Deep Reinforcement Learning (DRL) algorithms tend to adversely affect their performance. Averaged-DQN is a simple extension to the DQN algorithm, based on averaging previously learned Q-values estimates, which leads to a more stable training procedure and improved performance by reducing approximation error variance in the target values. To understand the effect of the algorithm, we examine the source of value function estimation errors and provide an analytical comparison within a simplified model. We further present experiments on the Arcade Learning Environment benchmark that demonstrate significantly improved stability and performance due to the proposed extension.
ER  -


TY  - Preprint
T1  - Domain Adaptation For Formant Estimation Using Deep Learning
A1  - Yehoshua Dissen
A1  - Joseph Keshet
A1  - Jacob Goldberger
A1  - Cynthia Clopper
JO  - ArXiv e-prints
Y1  - 6 November, 2016
UR  - https://arxiv.org/abs/1611.01783
N2  - In this paper we present a domain adaptation technique for formant estimation using a deep network. We first train a deep learning network on a small read speech dataset. We then freeze the parameters of the trained network and use several different datasets to train an adaptation layer that makes the obtained network universal in the sense that it works well for a variety of speakers and speech domains with very different characteristics. We evaluated our adapted network on three datasets, each of which has different speaker characteristics and speech styles. The performance of our method compares favorably with alternative methods for formant estimation.
ER  -


TY  - Preprint
T1  - Deep Label Distribution Learning with Label Ambiguity
A1  - Bin-Bin Gao
A1  - Chao Xing
A1  - Chen-Wei Xie
A1  - Jianxin Wu
A1  - Xin Geng
JO  - ArXiv e-prints
Y1  - 10 May, 2017
UR  - https://arxiv.org/abs/1611.01731
N2  - Convolutional Neural Networks (ConvNets) have achieved excellent recognition performance in various visual recognition tasks. A large labeled training set is one of the most important factors for its success. However, it is difficult to collect sufficient training images with precise labels in some domains such as apparent age estimation, head pose estimation, multi-label classification and semantic segmentation. Fortunately, there is ambiguous information among labels, which makes these tasks different from traditional classification. Based on this observation, we convert the label of each image into a discrete label distribution, and learn the label distribution by minimizing a Kullback-Leibler divergence between the predicted and ground-truth label distributions using deep ConvNets. The proposed DLDL (Deep Label Distribution Learning) method effectively utilizes the label ambiguity in both feature learning and classifier learning, which help prevent the network from over-fitting even when the training set is small. Experimental results show that the proposed approach produces significantly better results than state-of-the-art methods for age estimation and head pose estimation. At the same time, it also improves recognition performance for multi-label classification and semantic segmentation tasks.
ER  -


TY  - Preprint
T1  - A Differentiable Physics Engine for Deep Learning in Robotics
A1  - Jonas Degrave
A1  - Michiel Hermans
A1  - Joni Dambre
A1  - Francis wyffels
JO  - ArXiv e-prints
Y1  - 5 November, 2016
UR  - https://arxiv.org/abs/1611.01652
N2  - One of the most important fields in robotics is the optimization of controllers. Currently, robots are treated as a black box in this optimization process, which is the reason why derivative-free optimization methods such as evolutionary algorithms or reinforcement learning are omnipresent. We propose an implementation of a modern physics engine, which has the ability to differentiate control parameters. This has been implemented on both CPU and GPU. We show how this speeds up the optimization process, even for small problems, and why it will scale to bigger problems. We explain why this is an alternative approach to deep Q-learning, for using deep learning in robotics. Lastly, we argue that this is a big step for deep learning in robotics, as it opens up new possibilities to optimize robots, both in hardware and software.
ER  -


TY  - Preprint
T1  - Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening
A1  - Frank S. He
A1  - Yang Liu
A1  - Alexander G. Schwing
A1  - Jian Peng
JO  - ArXiv e-prints
Y1  - 5 November, 2016
UR  - https://arxiv.org/abs/1611.01606
N2  - We propose a novel training algorithm for reinforcement learning which combines the strength of deep Q-learning with a constrained optimization approach to tighten optimality and encourage faster reward propagation. Our novel technique makes deep reinforcement learning more practical by drastically reducing the training time. We evaluate the performance of our approach on the 49 games of the challenging Arcade Learning Environment, and report significant improvements in both training time and accuracy.
ER  -


TY  - Preprint
T1  - Multi-task learning with deep model based reinforcement learning
A1  - Asier Mujika
JO  - ArXiv e-prints
Y1  - 23 May, 2017
UR  - https://arxiv.org/abs/1611.01457
N2  - In recent years, model-free methods that use deep learning have achieved great success in many different reinforcement learning environments. Most successful approaches focus on solving a single task, while multi-task reinforcement learning remains an open problem. In this paper, we present a model based approach to deep reinforcement learning which we use to solve different tasks simultaneously. We show that our approach not only does not degrade but actually benefits from learning multiple tasks. For our model, we also present a new kind of recurrent neural network inspired by residual networks that decouples memory from computation allowing to model complex environments that do not require lots of memory.
ER  -


TY  - Preprint
T1  - Semi-supervised deep learning by metric embedding
A1  - Elad Hoffer
A1  - Nir Ailon
JO  - ArXiv e-prints
Y1  - 4 November, 2016
UR  - https://arxiv.org/abs/1611.01449
N2  - Deep networks are successfully used as classification models yielding state-of-the-art results when trained on a large number of labeled samples. These models, however, are usually much less suited for semi-supervised problems because of their tendency to overfit easily when trained on small amounts of data. In this work we will explore a new training objective that is targeting a semi-supervised regime with only a small subset of labeled data. This criterion is based on a deep metric embedding over distance relations within the set of labeled samples, together with constraints over the embeddings of the unlabeled set. The final learned representations are discriminative in euclidean space, and hence can be used with subsequent nearest-neighbor classification using the labeled samples.
ER  -


TY  - Preprint
T1  - Using a Deep Reinforcement Learning Agent for Traffic Signal Control
A1  - Wade Genders
A1  - Saiedeh Razavi
JO  - ArXiv e-prints
Y1  - 3 November, 2016
UR  - https://arxiv.org/abs/1611.01142
N2  - Ensuring transportation systems are efficient is a priority for modern society. Technological advances have made it possible for transportation systems to collect large volumes of varied data on an unprecedented scale. We propose a traffic signal control system which takes advantage of this new, high quality data, with minimal abstraction compared to other proposed systems. We apply modern deep reinforcement learning methods to build a truly adaptive traffic signal control agent in the traffic microsimulator SUMO. We propose a new state space, the discrete traffic state encoding, which is information dense. The discrete traffic state encoding is used as input to a deep convolutional neural network, trained using Q-learning with experience replay. Our agent was compared against a one hidden layer neural network traffic signal control agent and reduces average cumulative delay by 82%, average queue length by 66% and average travel time by 20%.
ER  -


TY  - Preprint
T1  - Learning Deep Embeddings with Histogram Loss
A1  - Evgeniya Ustinova
A1  - Victor Lempitsky
JO  - ArXiv e-prints
Y1  - 2 November, 2016
UR  - https://arxiv.org/abs/1611.00822
N2  - We suggest a loss for learning deep embeddings. The new loss does not introduce parameters that need to be tuned and results in very good embeddings across a range of datasets and problems. The loss is computed by estimating two distribution of similarities for positive (matching) and negative (non-matching) sample pairs, and then computing the probability of a positive pair to have a lower similarity score than a negative pair based on the estimated similarity distributions. We show that such operations can be performed in a simple and piecewise-differentiable manner using 1D histograms with soft assignment operations. This makes the proposed loss suitable for learning deep embeddings using stochastic optimization. In the experiments, the new loss performs favourably compared to recently proposed alternatives.
ER  -


TY  - Preprint
T1  - Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics
A1  - Jay M. Wong
JO  - ArXiv e-prints
Y1  - 1 November, 2016
UR  - https://arxiv.org/abs/1611.00201
N2  - Despite outstanding success in vision amongst other domains, many of the recent deep learning approaches have evident drawbacks for robots. This manuscript surveys recent work in the literature that pertain to applying deep learning systems to the robotics domain, either as means of estimation or as a tool to resolve motor commands directly from raw percepts. These recent advances are only a piece to the puzzle. We suggest that deep learning as a tool alone is insufficient in building a unified framework to acquire general intelligence. For this reason, we complement our survey with insights from cognitive development and refer to ideas from classical control theory, producing an integrated direction for a lifelong learning architecture.
ER  -


TY  - Preprint
T1  - Towards Deep Learning in Hindi NER: An approach to tackle the Labelled Data Scarcity
A1  - Vinayak Athavale
A1  - Shreenivas Bharadwaj
A1  - Monik Pamecha
A1  - Ameya Prabhu
A1  - Manish Shrivastava
JO  - ArXiv e-prints
Y1  - 16 November, 2016
UR  - https://arxiv.org/abs/1610.09756
N2  - In this paper we describe an end to end Neural Model for Named Entity Recognition NER) which is based on Bi-Directional RNN-LSTM. Almost all NER systems for Hindi use Language Specific features and handcrafted rules with gazetteers. Our model is language independent and uses no domain specific features or any handcrafted rules. Our models rely on semantic information in the form of word vectors which are learnt by an unsupervised learning algorithm on an unannotated corpus. Our model attained state of the art performance in both English and Hindi without the use of any morphological analysis or without using gazetteers of any sort.
ER  -


TY  - Preprint
T1  - Compressed Learning: A Deep Neural Network Approach
A1  - Amir Adler
A1  - Michael Elad
A1  - Michael Zibulevsky
JO  - ArXiv e-prints
Y1  - 30 October, 2016
UR  - https://arxiv.org/abs/1610.09615
N2  - Compressed Learning (CL) is a joint signal processing and machine learning framework for inference from a signal, using a small number of measurements obtained by linear projections of the signal. In this paper we present an end-to-end deep learning approach for CL, in which a network composed of fully-connected layers followed by convolutional layers perform the linear sensing and non-linear inference stages. During the training phase, the sensing matrix and the non-linear inference operator are jointly optimized, and the proposed approach outperforms state-of-the-art for the task of image classification. For example, at a sensing rate of 1% (only 8 measurements of 28 X 28 pixels images), the classification error for the MNIST handwritten digits dataset is 6.46% compared to 41.06% with state-of-the-art.
ER  -


TY  - Preprint
T1  - Towards automatic pulmonary nodule management in lung cancer screening with deep learning
A1  - Francesco Ciompi
A1  - Kaman Chung
A1  - Sarah J. van Riel
A1  - Arnaud Arindra Adiyoso Setio
A1  - Paul K. Gerke
A1  - Colin Jacobs
A1  - Ernst Th. Scholten
A1  - Cornelia Schaefer-Prokop
A1  - Mathilde M. W. Wille
A1  - Alfonso Marchiano
A1  - Ugo Pastorino
A1  - Mathias Prokop
A1  - Bram van Ginneken
JO  - ArXiv e-prints
Y1  - 23 May, 2017
UR  - https://arxiv.org/abs/1610.09157
N2  - The introduction of lung cancer screening programs will produce an unprecedented amount of chest CT scans in the near future, which radiologists will have to read in order to decide on a patient follow-up strategy. According to the current guidelines, the workup of screen-detected nodules strongly relies on nodule size and nodule type. In this paper, we present a deep learning system based on multi-stream multi-scale convolutional networks, which automatically classifies all nodule types relevant for nodule workup. The system processes raw CT data containing a nodule without the need for any additional information such as nodule segmentation or nodule size and learns a representation of 3D data by analyzing an arbitrary number of 2D views of a given nodule. The deep learning system was trained with data from the Italian MILD screening trial and validated on an independent set of data from the Danish DLCST screening trial. We analyze the advantage of processing nodules at multiple scales with a multi-stream convolutional network architecture, and we show that the proposed deep learning system achieves performance at classifying nodule type that surpasses the one of classical machine learning approaches and is within the inter-observer variability among four experienced human observers.
ER  -


TY  - Preprint
T1  - Learning Scalable Deep Kernels with Recurrent Structure
A1  - Maruan Al-Shedivat
A1  - Andrew Gordon Wilson
A1  - Yunus Saatchi
A1  - Zhiting Hu
A1  - Eric P. Xing
JO  - ArXiv e-prints
Y1  - 4 October, 2017
UR  - https://arxiv.org/abs/1610.08936
N2  - Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure and exploit the structure of these kernels for scalable training and prediction. This approach provides a practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable.
ER  -


TY  - Preprint
T1  - Predicting First Impressions with Deep Learning
A1  - Mel McCurrie
A1  - Fernando Beletti
A1  - Lucas Parzianello
A1  - Allen Westendorp
A1  - Samuel Anthony
A1  - Walter Scheirer
JO  - ArXiv e-prints
Y1  - 10 May, 2017
UR  - https://arxiv.org/abs/1610.08119
N2  - Describable visual facial attributes are now commonplace in human biometrics and affective computing, with existing algorithms even reaching a sufficient point of maturity for placement into commercial products. These algorithms model objective facets of facial appearance, such as hair and eye color, expression, and aspects of the geometry of the face. A natural extension, which has not been studied to any great extent thus far, is the ability to model subjective attributes that are assigned to a face based purely on visual judgements. For instance, with just a glance, our first impression of a face may lead us to believe that a person is smart, worthy of our trust, and perhaps even our admiration - regardless of the underlying truth behind such attributes. Psychologists believe that these judgements are based on a variety of factors such as emotional states, personality traits, and other physiognomic cues. But work in this direction leads to an interesting question: how do we create models for problems where there is no ground truth, only measurable behavior? In this paper, we introduce a new convolutional neural network-based regression framework that allows us to train predictive models of crowd behavior for social attribute assignment. Over images from the AFLW face database, these models demonstrate strong correlations with human crowd ratings.
ER  -


TY  - Preprint
T1  - End-to-end Learning of Deep Visual Representations for Image Retrieval
A1  - Albert Gordo
A1  - Jon Almazan
A1  - Jerome Revaud
A1  - Diane Larlus
JO  - ArXiv e-prints
Y1  - 5 May, 2017
UR  - https://arxiv.org/abs/1610.07940
N2  - While deep learning has become a key ingredient in the top performing methods for many computer vision tasks, it has failed so far to bring similar improvements to instance-level image retrieval. In this article, we argue that reasons for the underwhelming results of deep methods on image retrieval are threefold: i) noisy training data, ii) inappropriate deep architecture, and iii) suboptimal training procedure. We address all three issues.
ER  -


TY  - Preprint
T1  - A data augmentation methodology for training machine/deep learning gait recognition algorithms
A1  - Christoforos C. Charalambous
A1  - Anil A. Bharath
JO  - ArXiv e-prints
Y1  - 24 October, 2016
UR  - https://arxiv.org/abs/1610.07570
N2  - There are several confounding factors that can reduce the accuracy of gait recognition systems. These factors can reduce the distinctiveness, or alter the features used to characterise gait, they include variations in clothing, lighting, pose and environment, such as the walking surface. Full invariance to all confounding factors is challenging in the absence of high-quality labelled training data. We introduce a simulation-based methodology and a subject-specific dataset which can be used for generating synthetic video frames and sequences for data augmentation. With this methodology, we generated a multi-modal dataset. In addition, we supply simulation files that provide the ability to simultaneously sample from several confounding variables. The basis of the data is real motion capture data of subjects walking and running on a treadmill at different speeds. Results from gait recognition experiments suggest that information about the identity of subjects is retained within synthetically generated examples. The dataset and methodology allow studies into fully-invariant identity recognition spanning a far greater number of observation conditions than would otherwise be possible.
ER  -


TY  - Preprint
T1  - DeepSpace: An Online Deep Learning Framework for Mobile Big Data to Understand Human Mobility Patterns
A1  - Xi Ouyang
A1  - Chaoyun Zhang
A1  - Pan Zhou
A1  - Hao Jiang
JO  - ArXiv e-prints
Y1  - 22 October, 2016
UR  - https://arxiv.org/abs/1610.07009
N2  - In the recent years, the rapid spread of mobile device has create the vast amount of mobile data. However, some shallow-structure models such as support vector machine (SVM) have difficulty dealing with high dimensional data with the development of mobile network. In this paper, we analyze mobile data to predict human trajectories in order to understand human mobility pattern via a deep-structure model called &#34;DeepSpace&#34;. To the best of out knowledge, it is the first time that the deep learning approach is applied to predicting human trajectories. Furthermore, we develop the vanilla convolutional neural network (CNN) to be an online learning system, which can deal with the continuous mobile data stream. In general, &#34;DeepSpace&#34; consists of two different prediction models corresponding to different scales in space (the coarse prediction model and fine prediction models). This two models constitute a hierarchical structure, which enable the whole architecture to be run in parallel. Finally, we test our model based on the data usage detail records (UDRs) from the mobile cellular network in a city of southeastern China, instead of the call detail records (CDRs) which are widely used by others as usual. The experiment results show that &#34;DeepSpace&#34; is promising in human trajectories prediction.
ER  -


TY  - Preprint
T1  - Utilization of Deep Reinforcement Learning for saccadic-based object visual search
A1  - Tomasz Kornuta
A1  - Kamil Rocki
JO  - ArXiv e-prints
Y1  - 20 October, 2016
UR  - https://arxiv.org/abs/1610.06492
N2  - The paper focuses on the problem of learning saccades enabling visual object search. The developed system combines reinforcement learning with a neural network for learning to predict the possible outcomes of its actions. We validated the solution in three types of environment consisting of (pseudo)-randomly generated matrices of digits. The experimental verification is followed by the discussion regarding elements required by systems mimicking the fovea movement and possible further research directions.
ER  -


TY  - Preprint
T1  - Master&#39;s Thesis : Deep Learning for Visual Recognition
A1  - RÃ©mi CadÃ¨ne
A1  - Nicolas Thome
A1  - Matthieu Cord
JO  - ArXiv e-prints
Y1  - 18 October, 2016
UR  - https://arxiv.org/abs/1610.05567
N2  - The goal of our research is to develop methods advancing automatic visual recognition. In order to predict the unique or multiple labels associated to an image, we study different kind of Deep Neural Networks architectures and methods for supervised features learning. We first draw up a state-of-the-art review of the Convolutional Neural Networks aiming to understand the history behind this family of statistical models, the limit of modern architectures and the novel techniques currently used to train deep CNNs. The originality of our work lies in our approach focusing on tasks with a low amount of data. We introduce different models and techniques to achieve the best accuracy on several kind of datasets, such as a medium dataset of food recipes (100k images) for building a web API, or a small dataset of satellite images (6,000) for the DSG online challenge that we&#39;ve won. We also draw up the state-of-the-art in Weakly Supervised Learning, introducing different kind of CNNs able to localize regions of interest. Our last contribution is a framework, build on top of Torch7, for training and testing deep models on any visual recognition tasks and on datasets of any scale.
ER  -


TY  - Preprint
T1  - Deep Learning Prototype Domains for Person Re-Identification
A1  - Arne Schumann
A1  - Shaogang Gong
A1  - Tobias Schuchert
JO  - ArXiv e-prints
Y1  - 19 September, 2017
UR  - https://arxiv.org/abs/1610.05047
N2  - Person re-identification (re-id) is the task of matching multiple occurrences of the same person from different cameras, poses, lighting conditions, and a multitude of other factors which alter the visual appearance. Typically, this is achieved by learning either optimal features or matching metrics which are adapted to specific pairs of camera views dictated by the pairwise labelled training datasets. In this work, we formulate a deep learning based novel approach to automatic prototype-domain discovery for domain perceptive (adaptive) person re-id (rather than camera pair specific learning) for any camera views scalable to new unseen scenes without training data. We learn a separate re-id model for each of the discovered prototype-domains and during model deployment, use the person probe image to select automatically the model of the closest prototype domain. Our approach requires neither supervised nor unsupervised domain adaptation learning, i.e. no data available from the target domains. We evaluate extensively our model under realistic re-id conditions using automatically detected bounding boxes with low-resolution and partial occlusion. We show that our approach outperforms most of the state-of-the-art supervised and unsupervised methods on the latest CUHK-SYSU and PRW benchmarks.
ER  -


TY  - Preprint
T1  - Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering
A1  - Bo Yang
A1  - Xiao Fu
A1  - Nicholas D. Sidiropoulos
A1  - Mingyi Hong
JO  - ArXiv e-prints
Y1  - 13 June, 2017
UR  - https://arxiv.org/abs/1610.04794
N2  - Most learning approaches treat dimensionality reduction (DR) and clustering separately (i.e., sequentially), but recent research has shown that optimizing the two tasks jointly can substantially improve the performance of both. The premise behind the latter genre is that the data samples are obtained via linear transformation of latent representations that are easy to cluster; but in practice, the transformation from the latent space to the data can be more complicated. In this work, we assume that this transformation is an unknown and possibly nonlinear function. To recover the `clustering-friendly&#39; latent representations and to better cluster the data, we propose a joint DR and K-means clustering approach in which DR is accomplished via learning a deep neural network (DNN). The motivation is to keep the advantages of jointly optimizing the two tasks, while exploiting the deep neural network&#39;s ability to approximate any nonlinear function. This way, the proposed approach can work well for a broad class of generative models. Towards this end, we carefully design the DNN structure and the associated joint optimization criterion, and propose an effective and scalable algorithm to handle the formulated optimization problem. Experiments using different real datasets are employed to showcase the effectiveness of the proposed approach.
ER  -


TY  - Preprint
T1  - Deep Learning Ensembles for Melanoma Recognition in Dermoscopy Images
A1  - Noel Codella
A1  - Quoc-Bao Nguyen
A1  - Sharath Pankanti
A1  - David Gutman
A1  - Brian Helba
A1  - Allan Halpern
A1  - John R. Smith
JO  - ArXiv e-prints
Y1  - 17 October, 2016
UR  - https://arxiv.org/abs/1610.04662
N2  - Melanoma is the deadliest form of skin cancer. While curable with early detection, only highly trained specialists are capable of accurately recognizing the disease. As expertise is in limited supply, automated systems capable of identifying disease could save lives, reduce unnecessary biopsies, and reduce costs. Toward this goal, we propose a system that combines recent developments in deep learning with established machine learning approaches, creating ensembles of methods that are capable of segmenting skin lesions, as well as analyzing the detected area and surrounding tissue for melanoma detection. The system is evaluated using the largest publicly available benchmark dataset of dermoscopic images, containing 900 training and 379 testing images. New state-of-the-art performance levels are demonstrated, leading to an improvement in the area under receiver operating characteristic curve of 7.5% (0.843 vs. 0.783), in average precision of 4% (0.649 vs. 0.624), and in specificity measured at the clinically relevant 95% sensitivity operating point 2.9 times higher than the previous state-of-the-art (36.8% specificity compared to 12.5%). Compared to the average of 8 expert dermatologists on a subset of 100 test images, the proposed system produces a higher accuracy (76% vs. 70.5%), and specificity (62% vs. 59%) evaluated at an equivalent sensitivity (82%).
ER  -


TY  - Preprint
T1  - Multi-Task Curriculum Transfer Deep Learning of Clothing Attributes
A1  - Qi Dong
A1  - Shaogang Gong
A1  - Xiatian Zhu
JO  - ArXiv e-prints
Y1  - 25 December, 2016
UR  - https://arxiv.org/abs/1610.03670
N2  - Recognising detailed clothing characteristics (fine-grained attributes) in unconstrained images of people in-the-wild is a challenging task for computer vision, especially when there is only limited training data from the wild whilst most data available for model learning are captured in well-controlled environments using fashion models (well lit, no background clutter, frontal view, high-resolution). In this work, we develop a deep learning framework capable of model transfer learning from well-controlled shop clothing images collected from web retailers to in-the-wild images from the street. Specifically, we formulate a novel Multi-Task Curriculum Transfer (MTCT) deep learning method to explore multiple sources of different types of web annotations with multi-labelled fine-grained attributes. Our multi-task loss function is designed to extract more discriminative representations in training by jointly learning all attributes, and our curriculum strategy exploits the staged easy-to-complex transfer learning motivated by cognitive studies. We demonstrate the advantages of the MTCT model over the state-of-the-art methods on the X-Domain benchmark, a large scale clothing attribute dataset. Moreover, we show that the MTCT model has a notable advantage over contemporary models when the training data size is small.
ER  -


TY  - Preprint
T1  - Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model
A1  - Paul Christiano
A1  - Zain Shah
A1  - Igor Mordatch
A1  - Jonas Schneider
A1  - Trevor Blackwell
A1  - Joshua Tobin
A1  - Pieter Abbeel
A1  - Wojciech Zaremba
JO  - ArXiv e-prints
Y1  - 11 October, 2016
UR  - https://arxiv.org/abs/1610.03518
N2  - Developing control policies in simulation is often more practical and safer than directly running experiments in the real world. This applies to policies obtained from planning and optimization, and even more so to policies obtained from reinforcement learning, which is often very data demanding. However, a policy that succeeds in simulation often doesn&#39;t work when deployed on a real robot. Nevertheless, often the overall gist of what the policy does in simulation remains valid in the real world. In this paper we investigate such settings, where the sequence of states traversed in simulation remains reasonable for the real world, even if the details of the controls are not, as could be the case when the key differences lie in detailed friction, contact, mass and geometry properties. During execution, at each time step our approach computes what the simulation-based control policy would do, but then, rather than executing these controls on the real robot, our approach computes what the simulation expects the resulting next state(s) will be, and then relies on a learned deep inverse dynamics model to decide which real-world action is most suitable to achieve those next states. Deep models are only as good as their training data, and we also propose an approach for data collection to (incrementally) learn the deep inverse dynamics model. Our experiments shows our approach compares favorably with various baselines that have been developed for dealing with simulation to real world model discrepancy, including output error control and Gaussian dynamics adaptation.
ER  -


TY  - Preprint
T1  - Deep Learning Assessment of Tumor Proliferation in Breast Cancer Histological Images
A1  - Manan Shah
A1  - Christopher Rubadue
A1  - David Suster
A1  - Dayong Wang
JO  - ArXiv e-prints
Y1  - 11 October, 2016
UR  - https://arxiv.org/abs/1610.03467
N2  - Current analysis of tumor proliferation, the most salient prognostic biomarker for invasive breast cancer, is limited to subjective mitosis counting by pathologists in localized regions of tissue images. This study presents the first data-driven integrative approach to characterize the severity of tumor growth and spread on a categorical and molecular level, utilizing multiple biologically salient deep learning classifiers to develop a comprehensive prognostic model. Our approach achieves pathologist-level performance on three-class categorical tumor severity prediction. It additionally pioneers prediction of molecular expression data from a tissue image, obtaining a Spearman&#39;s rank correlation coefficient of 0.60 with ex vivo mean calculated RNA expression. Furthermore, our framework is applied to identify over two hundred unprecedented biomarkers critical to the accurate assessment of tumor proliferation, validating our proposed integrative pipeline as the first to holistically and objectively analyze histopathological images.
ER  -


TY  - Preprint
T1  - Multi-Objective Deep Reinforcement Learning
A1  - Hossam Mossalam
A1  - Yannis M. Assael
A1  - Diederik M. Roijers
A1  - Shimon Whiteson
JO  - ArXiv e-prints
Y1  - 9 October, 2016
UR  - https://arxiv.org/abs/1610.02707
N2  - We propose Deep Optimistic Linear Support Learning (DOL) to solve high-dimensional multi-objective decision problems where the relative importances of the objectives are not known a priori. Using features from the high-dimensional inputs, DOL computes the convex coverage set containing all potential optimal solutions of the convex combinations of the objectives. To our knowledge, this is the first time that deep reinforcement learning has succeeded in learning multi-objective policies. In addition, we provide a testbed with two experiments to be used as a benchmark for deep multi-objective reinforcement learning.
ER  -


TY  - Preprint
T1  - Learning Deep Generative Spatial Models for Mobile Robots
A1  - Andrzej Pronobis
A1  - Rajesh P. N. Rao
JO  - ArXiv e-prints
Y1  - 28 December, 2017
UR  - https://arxiv.org/abs/1610.02627
N2  - We propose a new probabilistic framework that allows mobile robots to autonomously learn deep, generative models of their environments that span multiple levels of abstraction. Unlike traditional approaches that combine engineered models for low-level features, geometry, and semantics, our approach leverages recent advances in Sum-Product Networks (SPNs) and deep learning to learn a single, universal model of the robot&#39;s spatial environment. Our model is fully probabilistic and generative, and represents a joint distribution over spatial information ranging from low-level geometry to semantic interpretations. Once learned, it is capable of solving a wide range of tasks: from semantic classification of places, uncertainty estimation, and novelty detection, to generation of place appearances based on semantic information and prediction of missing data in partial observations. Experiments on laser-range data from a mobile robot show that the proposed universal model obtains performance superior to state-of-the-art models fine-tuned to one specific task, such as Generative Adversarial Networks (GANs) or SVMs.
ER  -


TY  - Preprint
T1  - Xception: Deep Learning with Depthwise Separable Convolutions
A1  - FranÃ§ois Chollet
JO  - ArXiv e-prints
Y1  - 4 April, 2017
UR  - https://arxiv.org/abs/1610.02357
N2  - We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.
ER  -


TY  - Preprint
T1  - Morphology Generation for Statistical Machine Translation using Deep Learning Techniques
A1  - Marta R. Costa-jussÃ 
A1  - Carlos Escolano
JO  - ArXiv e-prints
Y1  - 6 February, 2017
UR  - https://arxiv.org/abs/1610.02209
N2  - Morphology in unbalanced languages remains a big challenge in the context of machine translation. In this paper, we propose to de-couple machine translation from morphology generation in order to better deal with the problem. We investigate the morphology simplification with a reasonable trade-off between expected gain and generation complexity. For the Chinese-Spanish task, optimum morphological simplification is in gender and number. For this purpose, we design a new classification architecture which, compared to other standard machine learning techniques, obtains the best results. This proposed neural-based architecture consists of several layers: an embedding, a convolutional followed by a recurrent neural network and, finally, ends with sigmoid and softmax layers. We obtain classification results over 98% accuracy in gender classification, over 93% in number classification, and an overall translation improvement of 0.7 METEOR.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning From Raw Pixels in Doom
A1  - Danijar Hafner
JO  - ArXiv e-prints
Y1  - 7 October, 2016
UR  - https://arxiv.org/abs/1610.02164
N2  - Using current reinforcement learning methods, it has recently become possible to learn to play unknown 3D games from raw pixels. In this work, we study the challenges that arise in such complex environments, and summarize current methods to approach these. We choose a task within the Doom game, that has not been approached yet. The goal for the agent is to fight enemies in a 3D world consisting of five rooms. We train the DQN and LSTM-A3C algorithms on this task. Results show that both algorithms learn sensible policies, but fail to achieve high scores given the amount of training. We provide insights into the learned behavior, which can serve as a valuable starting point for further research in the Doom domain.
ER  -


TY  - Preprint
T1  - Do They All Look the Same? Deciphering Chinese, Japanese and Koreans by Fine-Grained Deep Learning
A1  - Yu Wang
A1  - Haofu Liao
A1  - Yang Feng
A1  - Xiangyang Xu
A1  - Jiebo Luo
JO  - ArXiv e-prints
Y1  - 22 October, 2016
UR  - https://arxiv.org/abs/1610.01854
N2  - We study to what extend Chinese, Japanese and Korean faces can be classified and which facial attributes offer the most important cues. First, we propose a novel way of obtaining large numbers of facial images with nationality labels. Then we train state-of-the-art neural networks with these labeled images. We are able to achieve an accuracy of 75.03% in the classification task, with chances being 33.33% and human accuracy 38.89% . Further, we train multiple facial attribute classifiers to identify the most distinctive features for each group. We find that Chinese, Japanese and Koreans do exhibit substantial differences in certain attributes, such as bangs, smiling, and bushy eyebrows. Along the way, we uncover several gender-related cross-country patterns as well. Our work, which complements existing APIs such as Microsoft Cognitive Services and Face++, could find potential applications in tourism, e-commerce, social media marketing, criminal justice and even counter-terrorism.
ER  -


TY  - Preprint
T1  - Multiple Regularizations Deep Learning for Paddy Growth Stages Classification from LANDSAT-8
A1  - Ines Heidieni Ikasari
A1  - Vina Ayumi
A1  - Mohamad Ivan Fanany
A1  - Sidik Mulyono
JO  - ArXiv e-prints
Y1  - 6 October, 2016
UR  - https://arxiv.org/abs/1610.01795
N2  - This study uses remote sensing technology that can provide information about the condition of the earth&#39;s surface area, fast, and spatially. The study area was in Karawang District, lying in the Northern part of West Java-Indonesia. We address a paddy growth stages classification using LANDSAT 8 image data obtained from multi-sensor remote sensing image taken in October 2015 to August 2016. This study pursues a fast and accurate classification of paddy growth stages by employing multiple regularizations learning on some deep learning methods such as DNN (Deep Neural Networks) and 1-D CNN (1-D Convolutional Neural Networks). The used regularizations are Fast Dropout, Dropout, and Batch Normalization. To evaluate the effectiveness, we also compared our method with other machine learning methods such as (Logistic Regression, SVM, Random Forest, and XGBoost). The data used are seven bands of LANDSAT-8 spectral data samples that correspond to paddy growth stages data obtained from i-Sky (eye in the sky) Innovation system. The growth stages are determined based on paddy crop phenology profile from time series of LANDSAT-8 images. The classification results show that MLP using multiple regularization Dropout and Batch Normalization achieves the highest accuracy for this dataset.
ER  -


TY  - Preprint
T1  - Towards Cognitive Exploration through Deep Reinforcement Learning for Mobile Robots
A1  - Lei Tai
A1  - Ming Liu
JO  - ArXiv e-prints
Y1  - 6 October, 2016
UR  - https://arxiv.org/abs/1610.01733
N2  - Exploration in an unknown environment is the core functionality for mobile robots. Learning-based exploration methods, including convolutional neural networks, provide excellent strategies without human-designed logic for the feature extraction. But the conventional supervised learning algorithms cost lots of efforts on the labeling work of datasets inevitably. Scenes not included in the training set are mostly unrecognized either. We propose a deep reinforcement learning method for the exploration of mobile robots in an indoor environment with the depth information from an RGB-D sensor only. Based on the Deep Q-Network framework, the raw depth image is taken as the only input to estimate the Q values corresponding to all moving commands. The training of the network weights is end-to-end. In arbitrarily constructed simulation environments, we show that the robot can be quickly adapted to unfamiliar scenes without any man-made labeling. Besides, through analysis of receptive fields of feature representations, deep reinforcement learning motivates the convolutional networks to estimate the traversability of the scenes. The test results are compared with the exploration strategies separately based on deep learning or reinforcement learning. Even trained only in the simulated environment, experimental results in real-world environment demonstrate that the cognitive ability of robot controller is dramatically improved compared with the supervised method. We believe it is the first time that raw sensor information is used to build cognitive exploration strategy for mobile robots through end-to-end deep reinforcement learning.
ER  -


TY  - Preprint
T1  - Domain Adaptation with Soft-margin multiple feature-kernel learning beats Deep Learning for surveillance face recognition
A1  - Samik Banerjee
A1  - Sukhendu Das
JO  - ArXiv e-prints
Y1  - 27 October, 2016
UR  - https://arxiv.org/abs/1610.01374
N2  - Face recognition (FR) is the most preferred mode for biometric-based surveillance, due to its passive nature of detecting subjects, amongst all different types of biometric traits. FR under surveillance scenario does not give satisfactory performance due to low contrast, noise and poor illumination conditions on probes, as compared to the training samples. A state-of-the-art technology, Deep Learning, even fails to perform well in these scenarios. We propose a novel soft-margin based learning method for multiple feature-kernel combinations, followed by feature transformed using Domain Adaptation, which outperforms many recent state-of-the-art techniques, when tested using three real-world surveillance face datasets.
ER  -


TY  - Preprint
T1  - Multi-View Representation Learning: A Survey from Shallow Methods to Deep Methods
A1  - Yingming Li
A1  - Ming Yang
A1  - Zhongfei Zhang
JO  - ArXiv e-prints
Y1  - 1 September, 2017
UR  - https://arxiv.org/abs/1610.01206
N2  - Recently, multi-view representation learning has become a rapidly growing direction in machine learning and data mining areas. This paper introduces several principles for multi-view representation learning: correlation, consensus, and complementarity principles. Consequently, we first review the representative methods and theories of multi-view representation learning based on correlation principle, especially on canonical correlation analysis (CCA) and its several extensions. Then from the viewpoint of consensus and complementarity principles we investigate the advancement of multi-view representation learning that ranges from shallow methods including multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, to deep methods including multi-modal restricted Boltzmann machines, multi-modal autoencoders, and multi-modal recurrent neural networks. Further, we also provide an important perspective from manifold alignment for multi-view representation learning. Overall, this survey aims to provide an insightful overview of theoretical basis and state-of-the-art developments in the field of multi-view representation learning and to help researchers find the most appropriate tools for particular applications.
ER  -


TY  - Preprint
T1  - Reset-Free Guided Policy Search: Efficient Deep Reinforcement Learning with Stochastic Initial States
A1  - William Montgomery
A1  - Anurag Ajay
A1  - Chelsea Finn
A1  - Pieter Abbeel
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 6 October, 2016
UR  - https://arxiv.org/abs/1610.01112
N2  - Autonomous learning of robotic skills can allow general-purpose robots to learn wide behavioral repertoires without requiring extensive manual engineering. However, robotic skill learning methods typically make one of several trade-offs to enable practical real-world learning, such as requiring manually designed policy or value function representations, initialization from human-provided demonstrations, instrumentation of the training environment, or extremely long training times. In this paper, we propose a new reinforcement learning algorithm for learning manipulation skills that can train general-purpose neural network policies with minimal human engineering, while still allowing for fast, efficient learning in stochastic environments. Our approach builds on the guided policy search (GPS) algorithm, which transforms the reinforcement learning problem into supervised learning from a computational teacher (without human demonstrations). In contrast to prior GPS methods, which require a consistent set of initial states to which the system must be reset after each episode, our approach can handle randomized initial states, allowing it to be used in environments where deterministic resets are impossible. We compare our method to existing policy search techniques in simulation, showing that it can train high-dimensional neural network policies with the same sample efficiency as prior GPS methods, and present real-world results on a PR2 robotic manipulator.
ER  -


TY  - Preprint
T1  - Tutorial on Answering Questions about Images with Deep Learning
A1  - Mateusz Malinowski
A1  - Mario Fritz
JO  - ArXiv e-prints
Y1  - 4 October, 2016
UR  - https://arxiv.org/abs/1610.01076
N2  - Together with the development of more accurate methods in Computer Vision and Natural Language Understanding, holistic architectures that answer on questions about the content of real-world images have emerged. In this tutorial, we build a neural-based approach to answer questions about images. We base our tutorial on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the models that we present here can achieve a competitive performance on both datasets, in fact, they are among the best methods that use a combination of LSTM with a global, full frame CNN representation of an image. We hope that after reading this tutorial, the reader will be able to use Deep Learning frameworks, such as Keras and introduced Kraino, to build various architectures that will lead to a further performance improvement on this challenging task.
ER  -


TY  - Preprint
T1  - Applications of Online Deep Learning for Crisis Response Using Social Media Information
A1  - Dat Tien Nguyen
A1  - Shafiq Joty
A1  - Muhammad Imran
A1  - Hassan Sajjad
A1  - Prasenjit Mitra
JO  - ArXiv e-prints
Y1  - 5 October, 2016
UR  - https://arxiv.org/abs/1610.01030
N2  - During natural or man-made disasters, humanitarian response organizations look for useful information to support their decision-making processes. Social media platforms such as Twitter have been considered as a vital source of useful information for disaster response and management. Despite advances in natural language processing techniques, processing short and informal Twitter messages is a challenging task. In this paper, we propose to use Deep Neural Network (DNN) to address two types of information needs of response organizations: 1) identifying informative tweets and 2) classifying them into topical classes. DNNs use distributed representation of words and learn the representation as well as higher level features automatically for the classification task. We propose a new online algorithm based on stochastic gradient descent to train DNNs in an online fashion during disaster situations. We test our models using a crisis-related real-world Twitter dataset.
ER  -


TY  - Preprint
T1  - Comparing Human-Centric and Robot-Centric Sampling for Robot Deep Learning from Demonstrations
A1  - Michael Laskey
A1  - Caleb Chuck
A1  - Jonathan Lee
A1  - Jeffrey Mahler
A1  - Sanjay Krishnan
A1  - Kevin Jamieson
A1  - Anca Dragan
A1  - Ken Goldberg
JO  - ArXiv e-prints
Y1  - 28 March, 2017
UR  - https://arxiv.org/abs/1610.00850
N2  - Motivated by recent advances in Deep Learning for robot control, this paper considers two learning algorithms in terms of how they acquire demonstrations. &#34;Human-Centric&#34; (HC) sampling is the standard supervised learning algorithm, where a human supervisor demonstrates the task by teleoperating the robot to provide trajectories consisting of state-control pairs. &#34;Robot-Centric&#34; (RC) sampling is an increasingly popular alternative used in algorithms such as DAgger, where a human supervisor observes the robot executing a learned policy and provides corrective control labels for each state visited. RC sampling can be challenging for human supervisors and prone to mislabeling. RC sampling can also induce error in policy performance because it repeatedly visits areas of the state space that are harder to learn. Although policies learned with RC sampling can be superior to HC sampling for standard learning models such as linear SVMs, policies learned with HC sampling may be comparable with highly-expressive learning models such as deep learning and hyper-parametric decision trees, which have little model error. We compare HC and RC using a grid world and a physical robot singulation task, where in the latter the input is a binary image of a connected set of objects on a planar worksurface and the policy generates a motion of the gripper to separate one object from the rest. We observe in simulation that for linear SVMs, policies learned with RC outperformed those learned with HC but that with deep models this advantage disappears. We also find that with RC, the corrective control labels provided by humans can be highly inconsistent. We prove there exists a class of examples where in the limit, HC is guaranteed to converge to an optimal policy while RC may fail to converge.
ER  -


TY  - Preprint
T1  - Adaptive Neuron Apoptosis for Accelerating Deep Learning on Large Scale Systems
A1  - Charles Siegel
A1  - Jeff Daily
A1  - Abhinav Vishnu
JO  - ArXiv e-prints
Y1  - 3 October, 2016
UR  - https://arxiv.org/abs/1610.00790
N2  - We present novel techniques to accelerate the convergence of Deep Learning algorithms by conducting low overhead removal of redundant neurons -- apoptosis of neurons -- which do not contribute to model learning, during the training phase itself. We provide in-depth theoretical underpinnings of our heuristics (bounding accuracy loss and handling apoptosis of several neuron types), and present the methods to conduct adaptive neuron apoptosis. Specifically, we are able to improve the training time for several datasets by 2-3x, while reducing the number of parameters by up to 30x (4-5x on average) on datasets such as ImageNet classification. For the Higgs Boson dataset, our implementation improves the accuracy (measured by Area Under Curve (AUC)) for classification from 0.88/1 to 0.94/1, while reducing the number of parameters by 3x in comparison to existing literature. The proposed methods achieve a 2.44x speedup in comparison to the default (no apoptosis) algorithm.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates
A1  - Shixiang Gu
A1  - Ethan Holly
A1  - Timothy Lillicrap
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 23 November, 2016
UR  - https://arxiv.org/abs/1610.00633
N2  - Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.
ER  -


TY  - Preprint
T1  - Deep Learning Algorithms for Signal Recognition in Long Perimeter Monitoring Distributed Fiber Optic Sensors
A1  - A. V. Makarenko
JO  - ArXiv e-prints
Y1  - 2 October, 2016
UR  - https://arxiv.org/abs/1610.00279
N2  - In this paper, we show an approach to build deep learning algorithms for recognizing signals in distributed fiber optic monitoring and security systems for long perimeters. Synthesizing such detection algorithms poses a non-trivial research and development challenge, because these systems face stringent error (type I and II) requirements and operate in difficult signal-jamming environments, with intensive signal-like jamming and a variety of changing possible signal portraits of possible recognized events. To address these issues, we have developed a twolevel event detection architecture, where the primary classifier is based on an ensemble of deep convolutional networks, can recognize 7 classes of signals and receives time-space data frames as input. Using real-life data, we have shown that the applied methods result in efficient and robust multiclass detection algorithms that have a high degree of adaptability.
ER  -


TY  - Preprint
T1  - Deep unsupervised learning through spatial contrasting
A1  - Elad Hoffer
A1  - Itay Hubara
A1  - Nir Ailon
JO  - ArXiv e-prints
Y1  - 2 October, 2016
UR  - https://arxiv.org/abs/1610.00243
N2  - Convolutional networks have marked their place over the last few years as the best performing model for various visual tasks. They are, however, most suited for supervised learning from large amounts of labeled data. Previous attempts have been made to use unlabeled data to improve model performance by applying unsupervised techniques. These attempts require different architectures and training methods. In this work we present a novel approach for unsupervised training of Convolutional networks that is based on contrasting between spatial regions within images. This criterion can be employed within conventional neural networks and trained using standard techniques such as SGD and back-propagation, thus complementing supervised methods.
ER  -


TY  - Preprint
T1  - Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge
A1  - Andy Zeng
A1  - Kuan-Ting Yu
A1  - Shuran Song
A1  - Daniel Suo
A1  - Ed Walker Jr.
A1  - Alberto Rodriguez
A1  - Jianxiong Xiao
JO  - ArXiv e-prints
Y1  - 7 May, 2017
UR  - https://arxiv.org/abs/1609.09475
N2  - Robot warehouse automation has attracted significant interest in recent years, perhaps most visibly in the Amazon Picking Challenge (APC). A fully autonomous warehouse pick-and-place system requires robust vision that reliably recognizes and locates objects amid cluttered environments, self-occlusions, sensor noise, and a large variety of objects. In this paper we present an approach that leverages multi-view RGB-D data and self-supervised, data-driven learning to overcome those difficulties. The approach was part of the MIT-Princeton Team system that took 3rd- and 4th- place in the stowing and picking tasks, respectively at APC 2016. In the proposed approach, we segment and label multiple views of a scene with a fully convolutional neural network, and then fit pre-scanned 3D object models to the resulting segmentation to get the 6D object pose. Training a deep neural network for segmentation typically requires a large amount of training data. We propose a self-supervised method to generate a large labeled dataset without tedious manual segmentation. We demonstrate that our system can reliably estimate the 6D pose of objects under a variety of scenarios. All code, data, and benchmarks are available at http://apc.cs.princeton.edu/
ER  -


TY  - Preprint
T1  - Deep Tracking on the Move: Learning to Track the World from a Moving Vehicle using Recurrent Neural Networks
A1  - Julie Dequaire
A1  - Dushyant Rao
A1  - Peter Ondruska
A1  - Dominic Wang
A1  - Ingmar Posner
JO  - ArXiv e-prints
Y1  - 19 April, 2017
UR  - https://arxiv.org/abs/1609.09365
N2  - This paper presents an end-to-end approach for tracking static and dynamic objects for an autonomous vehicle driving through crowded urban environments. Unlike traditional approaches to tracking, this method is learned end-to-end, and is able to directly predict a full unoccluded occupancy grid map from raw laser input data. Inspired by the recently presented DeepTracking approach [Ondruska, 2016], we employ a recurrent neural network (RNN) to capture the temporal evolution of the state of the environment, and propose to use Spatial Transformer modules to exploit estimates of the egomotion of the vehicle. Our results demonstrate the ability to track a range of objects, including cars, buses, pedestrians, and cyclists through occlusion, from both moving and stationary platforms, using a single learned model. Experimental results demonstrate that the model can also predict the future states of objects from current inputs, with greater accuracy than previous work.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Tensegrity Robot Locomotion
A1  - Marvin Zhang
A1  - Xinyang Geng
A1  - Jonathan Bruce
A1  - Ken Caluwaerts
A1  - Massimo Vespignani
A1  - Vytas SunSpiral
A1  - Pieter Abbeel
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 7 March, 2017
UR  - https://arxiv.org/abs/1609.09049
N2  - Tensegrity robots, composed of rigid rods connected by elastic cables, have a number of unique properties that make them appealing for use as planetary exploration rovers. However, control of tensegrity robots remains a difficult problem due to their unusual structures and complex dynamics. In this work, we show how locomotion gaits can be learned automatically using a novel extension of mirror descent guided policy search (MDGPS) applied to periodic locomotion movements, and we demonstrate the effectiveness of our approach on tensegrity robot locomotion. We evaluate our method with real-world and simulated experiments on the SUPERball tensegrity robot, showing that the learned policies generalize to changes in system parameters, unreliable sensor measurements, and variation in environmental conditions, including varied terrains and a range of different gravities. Our experiments demonstrate that our method not only learns fast, power-efficient feedback policies for rolling gaits, but that these policies can succeed with only the limited onboard sensing provided by SUPERball&#39;s accelerometers. We compare the learned feedback policies to learned open-loop policies and hand-engineered controllers, and demonstrate that the learned policy enables the first continuous, reliable locomotion gait for the real SUPERball robot. Our code and other supplementary materials are available from http://rll.berkeley.edu/drl_tensegrity
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Mention-Ranking Coreference Models
A1  - Kevin Clark
A1  - Christopher D. Manning
JO  - ArXiv e-prints
Y1  - 31 October, 2016
UR  - https://arxiv.org/abs/1609.08667
N2  - Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. We experiment with two approaches: the REINFORCE policy gradient algorithm and a reward-rescaled max-margin objective. We find the latter to be more effective, resulting in significant improvements over the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task.
ER  -


TY  - Preprint
T1  - Deep learning for detection of bird vocalisations
A1  - Ilyas Potamitis
JO  - ArXiv e-prints
Y1  - 25 September, 2016
UR  - https://arxiv.org/abs/1609.08408
N2  - This work focuses on reliable detection of bird sound emissions as recorded in the open field. Acoustic detection of avian sounds can be used for the automatized monitoring of multiple bird taxa and querying in long-term recordings for species of interest for researchers, conservation practitioners, and decision makers. Recordings in the wild can be very noisy due to the exposure of the microphones to a large number of audio sources originating from all distances and directions, the number and identity of which cannot be known a-priori. The co-existence of the target vocalizations with abiotic interferences in an unconstrained environment is inefficiently treated by current approaches of audio signal enhancement. A technique that would spot only bird vocalization while ignoring other audio sources is of prime importance. These difficulties are tackled in this work, presenting a deep autoencoder that maps the audio spectrogram of bird vocalizations to its corresponding binary mask that encircles the spectral blobs of vocalizations while suppressing other audio sources. The procedure requires minimum human attendance, it is very fast during execution, thus suitable to scan massive volumes of data, in order to analyze them, evaluate insights and hypotheses, identify patterns of bird activity that, hopefully, finally lead to design policies on biodiversity issues.
ER  -


TY  - Preprint
T1  - Decentralized Non-communicating Multiagent Collision Avoidance with Deep Reinforcement Learning
A1  - Yu Fan Chen
A1  - Miao Liu
A1  - Michael Everett
A1  - Jonathan P. How
JO  - ArXiv e-prints
Y1  - 28 September, 2016
UR  - https://arxiv.org/abs/1609.07845
N2  - Finding feasible, collision-free paths for multiagent systems can be challenging, particularly in non-communicating scenarios where each agent&#39;s intent (e.g. goal) is unobservable to the others. In particular, finding time efficient paths often requires anticipating interaction with neighboring agents, the process of which can be computationally prohibitive. This work presents a decentralized multiagent collision avoidance algorithm based on a novel application of deep reinforcement learning, which effectively offloads the online computation (for predicting interaction patterns) to an offline learning procedure. Specifically, the proposed approach develops a value network that encodes the estimated time to the goal given an agent&#39;s joint configuration (positions and velocities) with its neighbors. Use of the value network not only admits efficient (i.e., real-time implementable) queries for finding a collision-free velocity vector, but also considers the uncertainty in the other agents&#39; motion. Simulation results show more than 26 percent improvement in paths quality (i.e., time to reach the goal) when compared with optimal reciprocal collision avoidance (ORCA), a state-of-the-art collision avoidance strategy.
ER  -


TY  - Preprint
T1  - Deep learning based fence segmentation and removal from an image using a video sequence
A1  - Sankaraganesh Jonna
A1  - Krishna K. Nakka
A1  - Rajiv R. Sahay
JO  - ArXiv e-prints
Y1  - 21 October, 2016
UR  - https://arxiv.org/abs/1609.07727
N2  - Conventional approaches to image de-fencing use multiple adjacent frames for segmentation of fences in the reference image and are limited to restoring images of static scenes only. In this paper, we propose a de-fencing algorithm for images of dynamic scenes using an occlusion-aware optical flow method. We divide the problem of image de-fencing into the tasks of automated fence segmentation from a single image, motion estimation under known occlusions and fusion of data from multiple frames of a captured video of the scene. Specifically, we use a pre-trained convolutional neural network to segment fence pixels from a single image. The knowledge of spatial locations of fences is used to subsequently estimate optical flow in the occluded frames of the video for the final data fusion step. We cast the fence removal problem in an optimization framework by modeling the formation of the degraded observations. The inverse problem is solved using fast iterative shrinkage thresholding algorithm (FISTA). Experimental results show the effectiveness of proposed algorithm.
ER  -


TY  - Preprint
T1  - Deep Multi-Task Learning with Shared Memory
A1  - Pengfei Liu
A1  - Xipeng Qiu
A1  - Xuanjing Huang
JO  - ArXiv e-prints
Y1  - 22 September, 2016
UR  - https://arxiv.org/abs/1609.07222
N2  - Neural network based models have achieved impressive results on various specific tasks. However, in previous works, most models are learned separately based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks.
ER  -


TY  - Preprint
T1  - Deep Learning in Multi-Layer Architectures of Dense Nuclei
A1  - Yonghua Yin
A1  - Erol Gelenbe
JO  - ArXiv e-prints
Y1  - 29 September, 2016
UR  - https://arxiv.org/abs/1609.07160
N2  - We assume that, within the dense clusters of neurons that can be found in nuclei, cells may interconnect via soma-to-soma interactions, in addition to conventional synaptic connections. We illustrate this idea with a multi-layer architecture (MLA) composed of multiple clusters of recurrent sub-networks of spiking Random Neural Networks (RNN) with dense soma-to-soma interactions, and use this RNN-MLA architecture for deep learning. The inputs to the clusters are first normalised by adjusting the external arrival rates of spikes to each cluster. Then we apply this architecture to learning from multi-channel datasets. Numerical results based on both images and sensor based data, show the value of this novel architecture for deep learning.
ER  -


TY  - Preprint
T1  - How Useful is Region-based Classification of Remote Sensing Images in a Deep Learning Framework?
A1  - Nicolas Audebert
A1  - Bertrand Le Saux
A1  - SÃ©bastien LefÃ¨vre
JO  - ArXiv e-prints
Y1  - 22 September, 2016
UR  - https://arxiv.org/abs/1609.06861
N2  - In this paper,  we investigate the impact of segmentation algorithms as a preprocessing step for classification of remote sensing images in a deep learning framework. Especially, we address the issue of segmenting the image into regions to be classified using pre-trained deep neural networks as feature extractors for an SVM-based classifier.  An efficient segmentation  as  a  preprocessing  step  helps  learning  by  adding  a spatially-coherent structure to the data.  Therefore, we compare algorithms producing superpixels with more traditional remote  sensing  segmentation  algorithms  and  measure  the variation  in  terms  of  classification  accuracy.   We  establish that  superpixel  algorithms  allow  for  a  better  classification accuracy as a homogenous and compact segmentation favors better generalization of the training samples.
ER  -


TY  - Preprint
T1  - Deep-Learned Collision Avoidance Policy for Distributed Multi-Agent Navigation
A1  - Pinxin Long
A1  - Wenxi Liu
A1  - Jia Pan
JO  - ArXiv e-prints
Y1  - 6 July, 2017
UR  - https://arxiv.org/abs/1609.06838
N2  - High-speed, low-latency obstacle avoidance that is insensitive to sensor noise is essential for enabling multiple decentralized robots to function reliably in cluttered and dynamic environments. While other distributed multi-agent collision avoidance systems exist, these systems require online geometric optimization where tedious parameter tuning and perfect sensing are necessary.
ER  -


TY  - Preprint
T1  - Deep Learning for Video Classification and Captioning
A1  - Zuxuan Wu
A1  - Ting Yao
A1  - Yanwei Fu
A1  - Yu-Gang Jiang
JO  - ArXiv e-prints
Y1  - 22 February, 2018
UR  - https://arxiv.org/abs/1609.06782
N2  - Accelerated by the tremendous increase in Internet bandwidth and storage space, video data has been generated, published and spread explosively, becoming an indispensable part of today&#39;s big data. In this paper, we focus on reviewing two lines of research aiming to stimulate the comprehension of videos with deep learning: video classification and video captioning. While video classification concentrates on automatically labeling video clips based on their semantic contents like human actions or complex events, video captioning attempts to generate a complete and natural sentence, enriching the single label as in video classification, to capture the most informative dynamics in videos. In addition, we also provide a review of popular benchmarks and competitions, which are critical for evaluating the technical progress of this vibrant field.
ER  -


TY  - Preprint
T1  - Opponent Modeling in Deep Reinforcement Learning
A1  - He He
A1  - Jordan Boyd-Graber
A1  - Kevin Kwok
A1  - Hal DaumÃ© III
JO  - ArXiv e-prints
Y1  - 18 September, 2016
UR  - https://arxiv.org/abs/1609.05559
N2  - Opponent modeling is necessary in multi-agent settings where secondary agents with competing goals also adapt their strategies, yet it remains challenging because strategies interact with each other and change. Most previous work focuses on developing probabilistic models or parameterized strategies for specific applications. Inspired by the recent success of deep reinforcement learning, we present neural-based models that jointly learn a policy and the behavior of opponents. Instead of explicitly predicting the opponent&#39;s action, we encode observation of the opponents into a deep Q-Network (DQN); however, we retain explicit modeling (if desired) using multitasking. By using a Mixture-of-Experts architecture, our model automatically discovers different strategy patterns of opponents without extra supervision. We evaluate our models on a simulated soccer game and a popular trivia game, showing superior performance over DQN and its variants.
ER  -


TY  - Preprint
T1  - Playing FPS Games with Deep Reinforcement Learning
A1  - Guillaume Lample
A1  - Devendra Singh Chaplot
JO  - ArXiv e-prints
Y1  - 29 January, 2018
UR  - https://arxiv.org/abs/1609.05521
N2  - Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in AI agents of the game as well as humans in deathmatch scenarios.
ER  -


TY  - Preprint
T1  - Towards Deep Symbolic Reinforcement Learning
A1  - Marta Garnelo
A1  - Kai Arulkumaran
A1  - Murray Shanahan
JO  - ArXiv e-prints
Y1  - 1 October, 2016
UR  - https://arxiv.org/abs/1609.05518
N2  - Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system -- though just a prototype -- learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game.
ER  -


TY  - Preprint
T1  - Interactive Spoken Content Retrieval by Deep Reinforcement Learning
A1  - Yen-Chen Wu
A1  - Tzu-Hsiang Lin
A1  - Yang-De Chen
A1  - Hung-Yi Lee
A1  - Lin-Shan Lee
JO  - ArXiv e-prints
Y1  - 16 September, 2016
UR  - https://arxiv.org/abs/1609.05234
N2  - User-machine interaction is important for spoken content retrieval. For text content retrieval, the user can easily scan through and select on a list of retrieved item. This is impossible for spoken content retrieval, because the retrieved items are difficult to show on screen. Besides, due to the high degree of uncertainty for speech recognition, the retrieval results can be very noisy. One way to counter such difficulties is through user-machine interaction. The machine can take different actions to interact with the user to obtain better retrieval results before showing to the user. The suitable actions depend on the retrieval status, for example requesting for extra information from the user, returning a list of topics for user to select, etc. In our previous work, some hand-crafted states estimated from the present retrieval results are used to determine the proper actions. In this paper, we propose to use Deep-Q-Learning techniques instead to determine the machine actions for interactive spoken content retrieval. Deep-Q-Learning bypasses the need for estimation of the hand-crafted states, and directly determine the best action base on the present retrieval status even without any human knowledge. It is shown to achieve significantly better performance compared with the previous hand-crafted states.
ER  -


TY  - Preprint
T1  - Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning
A1  - Yuke Zhu
A1  - Roozbeh Mottaghi
A1  - Eric Kolve
A1  - Joseph J. Lim
A1  - Abhinav Gupta
A1  - Li Fei-Fei
A1  - Ali Farhadi
JO  - ArXiv e-prints
Y1  - 16 September, 2016
UR  - https://arxiv.org/abs/1609.05143
N2  - Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new target goals, and (2) data inefficiency i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to the task of target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows to better generalize. To address the second issue, we propose AI2-THOR framework, which provides an environment with high-quality 3D scenes and physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently.
ER  -


TY  - Preprint
T1  - On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima
A1  - Nitish Shirish Keskar
A1  - Dheevatsa Mudigere
A1  - Jorge Nocedal
A1  - Mikhail Smelyanskiy
A1  - Ping Tak Peter Tang
JO  - ArXiv e-prints
Y1  - 9 February, 2017
UR  - https://arxiv.org/abs/1609.04836
N2  - The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say $32$-$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.
ER  -


TY  - Preprint
T1  - A Large Contextual Dataset for Classification, Detection and Counting of Cars with Deep Learning
A1  - T. Nathan Mundhenk
A1  - Goran Konjevod
A1  - Wesam A. Sakla
A1  - Kofi Boakye
JO  - ArXiv e-prints
Y1  - 14 September, 2016
UR  - https://arxiv.org/abs/1609.04453
N2  - We have created a large diverse set of cars from overhead images, which are useful for training a deep learner to binary classify, detect and count them. The dataset and all related material will be made publically available. The set contains contextual matter to aid in identification of difficult targets. We demonstrate classification and detection on this dataset using a neural network we call ResCeption. This network combines residual learning with Inception-style layers and is used to count cars in one look. This is a new way to count objects rather than by localization or density estimation. It is fairly accurate, fast and easy to implement. Additionally, the counting method is not car or scene specific. It would be easy to train this method to count other kinds of objects and counting over new scenes requires no extra set up or assumptions about object locations.
ER  -


TY  - Preprint
T1  - HMD Vision-based Teleoperating UGV and UAV for Hostile Environment using Deep Learning
A1  - Abhishek Sawarkar
A1  - Vishal Chaudhari
A1  - Rahul Chavan
A1  - Varun Zope
A1  - Akshay Budale
A1  - Faruk Kazi
JO  - ArXiv e-prints
Y1  - 14 September, 2016
UR  - https://arxiv.org/abs/1609.04147
N2  - The necessity of maintaining a robust antiterrorist task force has become imperative in recent times with resurgence of rogue element in the society. A well equipped combat force warrants the safety and security of citizens and the integrity of the sovereign state. In this paper we propose a novel teleoperating robot which can play a major role in combat, rescue and reconnaissance missions by substantially reducing loss of human soldiers in such hostile environments. The proposed robotic solution consists of an unmanned ground vehicle equipped with an IP camera visual system broadcasting real-time video data to a remote cloud server. With the advancement in machine learning algorithms in the field of computer vision, we incorporate state of the art deep convolutional neural networks to identify and predict individuals with malevolent intent. The classification is performed on every frame of the video stream by the trained network in the cloud server. The predicted output of the network is overlaid on the video stream with specific colour marks and prediction percentage. Finally the data is resized into half-side by side format and streamed to the head mount display worn by the human controller which facilitates first person view of the scenario. The ground vehicle is also coupled with an unmanned aerial vehicle for aerial surveillance. The proposed scheme is an assistive system and the final decision evidently lies with the human handler.
ER  -


TY  - Preprint
T1  - 3D Simulation for Robot Arm Control with Deep Q-Learning
A1  - Stephen James
A1  - Edward Johns
JO  - ArXiv e-prints
Y1  - 13 December, 2016
UR  - https://arxiv.org/abs/1609.03759
N2  - Recent trends in robot arm control have seen a shift towards end-to-end solutions, using deep reinforcement learning to learn a controller directly from raw sensor data, rather than relying on a hand-crafted, modular pipeline. However, the high dimensionality of the state space often means that it is impractical to generate sufficient training data with real-world experiments. As an alternative solution, we propose to learn a robot controller in simulation, with the potential of then transferring this to a real robot. Building upon the recent success of deep Q-networks, we present an approach which uses 3D simulations to train a 7-DOF robotic arm in a control task without any prior knowledge. The controller accepts images of the environment as its only input, and outputs motor actions for the task of locating and grasping a cube, over a range of initial configurations. To encourage efficient learning, a structured reward function is designed with intermediate rewards. We also present preliminary results in direct transfer of policies over to a real robot, without any further training.
ER  -


TY  - Preprint
T1  - DeepSkeleton: Learning Multi-task Scale-associated Deep Side Outputs for Object Skeleton Extraction in Natural Images
A1  - Wei Shen
A1  - Kai Zhao
A1  - Yuan Jiang
A1  - Yan Wang
A1  - Xiang Bai
A1  - Alan Yuille
JO  - ArXiv e-prints
Y1  - 13 July, 2017
UR  - https://arxiv.org/abs/1609.03659
N2  - Object skeletons are useful for object representation and object detection. They are complementary to the object contour, and provide extra information, such as how object scale (thickness) varies among object parts. But object skeleton extraction from natural images is very challenging, because it requires the extractor to be able to capture both local and non-local image context in order to determine the scale of each skeleton pixel. In this paper, we present a novel fully convolutional network with multiple scale-associated side outputs to address this problem. By observing the relationship between the receptive field sizes of the different layers in the network and the skeleton scales they can capture, we introduce two scale-associated side outputs to each stage of the network. The network is trained by multi-task learning, where one task is skeleton localization to classify whether a pixel is a skeleton pixel or not, and the other is skeleton scale prediction to regress the scale of each skeleton pixel. Supervision is imposed at different stages by guiding the scale-associated side outputs toward the groundtruth skeletons at the appropriate scales. The responses of the multiple scale-associated side outputs are then fused in a scale-specific way to detect skeleton pixels using multiple scales effectively. Our method achieves promising results on two skeleton extraction datasets, and significantly outperforms other competitors. Additionally, the usefulness of the obtained skeletons and scales (thickness) are verified on two object detection applications: Foreground object segmentation and object proposal detection.
ER  -


TY  - Preprint
T1  - Stride Length Estimation with Deep Learning
A1  - Julius Hannink
A1  - Thomas Kautz
A1  - Cristian F. Pasluosta
A1  - Jens Barth
A1  - Samuel SchÃ¼lein
A1  - Karl-GÃ¼nter GaÃmann
A1  - Jochen Klucken
A1  - Bjoern M. Eskofier
JO  - ArXiv e-prints
Y1  - 9 March, 2017
UR  - https://arxiv.org/abs/1609.03321
N2  - Accurate estimation of spatial gait characteristics is critical to assess motor impairments resulting from neurological or musculoskeletal disease. Currently, however, methodological constraints limit clinical applicability of state-of-the-art double integration approaches to gait patterns with a clear zero-velocity phase. We describe a novel approach to stride length estimation that uses deep convolutional neural networks to map stride-specific inertial sensor data to the resulting stride length. The model is trained on a publicly available and clinically relevant benchmark dataset consisting of 1220 strides from 101 geriatric patients. Evaluation is done in a 10-fold cross validation and for three different stride definitions. Even though best results are achieved with strides defined from mid-stance to mid-stance with average accuracy and precision of 0.01 $\pm$ 5.37 cm, performance does not strongly depend on stride definition. The achieved precision outperforms state-of-the-art methods evaluated on this benchmark dataset by 3.0 cm (36%). Due to the independence of stride definition, the proposed method is not subject to the methodological constrains that limit applicability of state-of-the-art double integration methods. Furthermore, precision on the benchmark dataset could be improved. With more precise mobile stride length estimation, new insights to the progression of neurological disease or early indications might be gained. Due to the independence of stride definition, previously uncharted diseases in terms of mobile gait analysis can now be investigated by re-training and applying the proposed method.
ER  -


TY  - Preprint
T1  - Automated detection of smuggled high-risk security threats using Deep Learning
A1  - Nicolas Jaccard
A1  - Thomas W. Rogers
A1  - Edward J. Morton
A1  - Lewis D. Griffin
JO  - ArXiv e-prints
Y1  - 9 September, 2016
UR  - https://arxiv.org/abs/1609.02805
N2  - The security infrastructure is ill-equipped to detect and deter the smuggling of non-explosive devices that enable terror attacks such as those recently perpetrated in western Europe. The detection of so-called &#34;small metallic threats&#34; (SMTs) in cargo containers currently relies on statistical risk analysis, intelligence reports, and visual inspection of X-ray images by security officers. The latter is very slow and unreliable due to the difficulty of the task: objects potentially spanning less than 50 pixels have to be detected in images containing more than 2 million pixels against very complex and cluttered backgrounds. In this contribution, we demonstrate for the first time the use of Convolutional Neural Networks (CNNs), a type of Deep Learning, to automate the detection of SMTs in fullsize X-ray images of cargo containers. Novel approaches for dataset augmentation allowed to train CNNs from-scratch despite the scarcity of data available. We report fewer than 6% false alarms when detecting 90% SMTs synthetically concealed in stream-of-commerce images, which corresponds to an improvement of over an order of magnitude over conventional approaches such as Bag-of-Words (BoWs). The proposed scheme offers potentially super-human performance for a fraction of the time it would take for a security officers to carry out visual inspection (processing time is approximately 3.5s per container image).
ER  -


TY  - Preprint
T1  - INSIGHT-1 at SemEval-2016 Task 5: Deep Learning for Multilingual Aspect-based Sentiment Analysis
A1  - Sebastian Ruder
A1  - Parsa Ghaffari
A1  - John G. Breslin
JO  - ArXiv e-prints
Y1  - 22 September, 2016
UR  - https://arxiv.org/abs/1609.02748
N2  - This paper describes our deep learning-based approach to multilingual aspect-based sentiment analysis as part of SemEval 2016 Task 5. We use a convolutional neural network (CNN) for both aspect extraction and aspect-based sentiment analysis. We cast aspect extraction as a multi-label classification problem, outputting probabilities over aspects parameterized by a threshold. To determine the sentiment towards an aspect, we concatenate an aspect vector with every word embedding and apply a convolution over it. Our constrained system (unconstrained for English) achieves competitive results across all languages and domains, placing first or second in 5 and 7 out of 11 language-domain pairs for aspect category detection (slot 1) and sentiment polarity (slot 3) respectively, thereby demonstrating the viability of a deep learning-based approach for multilingual aspect-based sentiment analysis.
ER  -


TY  - Preprint
T1  - Extraction of Skin Lesions from Non-Dermoscopic Images Using Deep Learning
A1  - Mohammad H. Jafari
A1  - Ebrahim Nasr-Esfahani
A1  - Nader Karimi
A1  - S. M. Reza Soroushmehr
A1  - Shadrokh Samavi
A1  - Kayvan Najarian
JO  - ArXiv e-prints
Y1  - 8 September, 2016
UR  - https://arxiv.org/abs/1609.02374
N2  - Melanoma is amongst most aggressive types of cancer. However, it is highly curable if detected in its early stages. Prescreening of suspicious moles and lesions for malignancy is of great importance. Detection can be done by images captured by standard cameras, which are more preferable due to low cost and availability. One important step in computerized evaluation of skin lesions is accurate detection of lesion region, i.e. segmentation of an image into two regions as lesion and normal skin. Accurate segmentation can be challenging due to burdens such as illumination variation and low contrast between lesion and healthy skin. In this paper, a method based on deep neural networks is proposed for accurate extraction of a lesion region. The input image is preprocessed and then its patches are fed to a convolutional neural network (CNN). Local texture and global structure of the patches are processed in order to assign pixels to lesion or normal classes. A method for effective selection of training patches is used for more accurate detection of a lesion border. The output segmentation mask is refined by some post processing operations. The experimental results of qualitative and quantitative evaluations demonstrate that our method can outperform other state-of-the-art algorithms exist in the literature.
ER  -


TY  - Preprint
T1  - Object Specific Deep Learning Feature and Its Application to Face Detection
A1  - Xianxu Hou
A1  - Ke Sun
A1  - Linlin Shen
A1  - Guoping Qiu
JO  - ArXiv e-prints
Y1  - 5 September, 2016
UR  - https://arxiv.org/abs/1609.01366
N2  - We present a method for discovering and exploiting object specific deep learning features and use face detection as a case study. Motivated by the observation that certain convolutional channels of a Convolutional Neural Network (CNN) exhibit object specific responses, we seek to discover and exploit the convolutional channels of a CNN in which neurons are activated by the presence of specific objects in the input image. A method for explicitly fine-tuning a pre-trained CNN to induce an object specific channel (OSC) and systematically identifying it for the human face object has been developed. Based on the basic OSC features, we introduce a multi-resolution approach to constructing robust face heatmaps for fast face detection in unconstrained settings. We show that multi-resolution OSC can be used to develop state of the art face detectors which have the advantage of being simple and compact.
ER  -


TY  - Preprint
T1  - Label distribution based facial attractiveness computation by deep residual learning
A1  - Shu Liu
A1  - Bo Li
A1  - Yangyu Fan
A1  - Zhe Guo
A1  - Ashok Samal
JO  - ArXiv e-prints
Y1  - 7 September, 2016
UR  - https://arxiv.org/abs/1609.00496
N2  - Two challenges lie in the facial attractiveness computation research: the lack of true attractiveness labels (scores), and the lack of an accurate face representation. In order to address the first challenge, this paper recasts facial attractiveness computation as a label distribution learning (LDL) problem rather than a traditional single-label supervised learning task. In this way, the negative influence of the label incomplete problem can be reduced. Inspired by the recent promising work in face recognition using deep neural networks to learn effective features, the second challenge is expected to be solved from a deep learning point of view. A very deep residual network is utilized to enable automatic learning of hierarchical aesthetics representation. Integrating these two ideas, an end-to-end deep learning framework is established. Our approach achieves the best results on a standard benchmark SCUT-FBP dataset compared with other state-of-the-art work.
ER  -


TY  - Preprint
T1  - A deep learning model for estimating story points
A1  - Morakot Choetkiertikul
A1  - Hoa Khanh Dam
A1  - Truyen Tran
A1  - Trang Pham
A1  - Aditya Ghose
A1  - Tim Menzies
JO  - ArXiv e-prints
Y1  - 6 September, 2016
UR  - https://arxiv.org/abs/1609.00489
N2  - Although there has been substantial research in software analytics for effort estimation in traditional software projects, little work has been done for estimation in agile projects, especially estimating user stories or issues. Story points are the most common unit of measure used for estimating the effort involved in implementing a user story or resolving an issue. In this paper, we offer for the \emph{first} time a comprehensive dataset for story points-based estimation that contains 23,313 issues from 16 open source projects. We also propose a prediction model for estimating story points based on a novel combination of two powerful deep learning architectures: long short-term memory and recurrent highway network. Our prediction system is \emph{end-to-end} trainable from raw input data to prediction outcomes without any manual feature engineering. An empirical evaluation demonstrates that our approach consistently outperforms three common effort estimation baselines and two alternatives in both Mean Absolute Error and the Standardized Accuracy.
ER  -


TY  - Preprint
T1  - Defeating Image Obfuscation with Deep Learning
A1  - Richard McPherson
A1  - Reza Shokri
A1  - Vitaly Shmatikov
JO  - ArXiv e-prints
Y1  - 6 September, 2016
UR  - https://arxiv.org/abs/1609.00408
N2  - We demonstrate that modern image recognition methods based on artificial neural networks can recover hidden information from images protected by various forms of obfuscation. The obfuscation techniques considered in this paper are mosaicing (also known as pixelation), blurring (as used by YouTube), and P3, a recently proposed system for privacy-preserving photo sharing that encrypts the significant JPEG coefficients to make images unrecognizable by humans. We empirically show how to train artificial neural networks to successfully identify faces and recognize objects and handwritten digits even if the images are protected using any of the above obfuscation techniques.
ER  -


TY  - Preprint
T1  - Deep Learning Human Mind for Automated Visual Classification
A1  - Concetto Spampinato
A1  - Simone Palazzo
A1  - Isaak Kavasidis
A1  - Daniela Giordano
A1  - Mubarak Shah
A1  - Nasim Souly
JO  - ArXiv e-prints
Y1  - 1 September, 2016
UR  - https://arxiv.org/abs/1609.00344
N2  - What if we could effectively read the mind and transfer human visual capabilities to computer vision methods? In this paper, we aim at addressing this question by developing the first visual object classifier driven by human brain signals. In particular, we employ EEG data evoked by visual object stimuli combined with Recurrent Neural Networks (RNN) to learn a discriminative brain activity manifold of visual categories. Afterwards, we train a Convolutional Neural Network (CNN)-based regressor to project images onto the learned manifold, thus effectively allowing machines to employ human brain-based features for automated visual classification. We use a 32-channel EEG to record brain activity of seven subjects while looking at images of 40 ImageNet object classes. The proposed RNN based approach for discriminating object classes using brain signals reaches an average accuracy of about 40%, which outperforms existing methods attempting to learn EEG visual object representations. As for automated object categorization, our human brain-driven approach obtains competitive performance, comparable to those achieved by powerful CNN models, both on ImageNet and CalTech 101, thus demonstrating its classification and generalization capabilities. This gives us a real hope that, indeed, human mind can be read and transferred to machines.
ER  -


TY  - Preprint
T1  - CliqueCNN: Deep Unsupervised Exemplar Learning
A1  - Miguel A. Bautista
A1  - Artsiom Sanakoyeu
A1  - Ekaterina Sutter
A1  - BjÃ¶rn Ommer
JO  - ArXiv e-prints
Y1  - 31 August, 2016
UR  - https://arxiv.org/abs/1608.08792
N2  - Exemplar learning is a powerful paradigm for discovering visual similarities in an unsupervised manner. In this context, however, the recent breakthrough in deep learning could not yet unfold its full potential. With only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of Convolutional Neural networks is impaired. Given weak estimates of local distance we propose a single optimization problem to extract batches of samples with mutually consistent relations. Conflicting relations are distributed over different batches and similar samples are grouped into compact cliques. Learning exemplar similarities is framed as a sequence of clique categorization tasks. The CNN then consolidates transitivity relations within and between cliques and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification.
ER  -


TY  - Preprint
T1  - Approaching the Computational Color Constancy as a Classification Problem through Deep Learning
A1  - Seoung Wug Oh
A1  - Seon Joo Kim
JO  - ArXiv e-prints
Y1  - 29 August, 2016
UR  - https://arxiv.org/abs/1608.07951
N2  - Computational color constancy refers to the problem of computing the illuminant color so that the images of a scene under varying illumination can be normalized to an image under the canonical illumination. In this paper, we adopt a deep learning framework for the illumination estimation problem. The proposed method works under the assumption of uniform illumination over the scene and aims for the accurate illuminant color computation. Specifically, we trained the convolutional neural network to solve the problem by casting the color constancy problem as an illumination classification problem. We designed the deep learning architecture so that the output of the network can be directly used for computing the color of the illumination. Experimental results show that our deep network is able to extract useful features for the illumination estimation and our method outperforms all previous color constancy methods on multiple test datasets.
ER  -


TY  - Preprint
T1  - Benchmarking State-of-the-Art Deep Learning Software Tools
A1  - Shaohuai Shi
A1  - Qiang Wang
A1  - Pengfei Xu
A1  - Xiaowen Chu
JO  - ArXiv e-prints
Y1  - 17 February, 2017
UR  - https://arxiv.org/abs/1608.07249
N2  - Deep learning has been shown as a successful machine learning method for a variety of tasks, and its popularity results in numerous open-source deep learning software tools. Training a deep network is usually a very time-consuming process. To address the computational challenge in deep learning, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training time. However, different tools exhibit different features and running performance when training different types of deep networks on different hardware platforms, which makes it difficult for end users to select an appropriate pair of software and hardware. In this paper, we aim to make a comparative study of the state-of-the-art GPU-accelerated deep learning software tools, including Caffe, CNTK, MXNet, TensorFlow, and Torch. We first benchmark the running performance of these tools with three popular types of neural networks on two CPU platforms and three GPU platforms. We then benchmark some distributed versions on multiple GPUs. Our contribution is two-fold. First, for end users of deep learning tools, our benchmarking results can serve as a guide to selecting appropriate hardware platforms and software tools. Second, for software developers of deep learning tools, our in-depth analysis points out possible future directions to further optimize the running performance.
ER  -


TY  - Preprint
T1  - Fathom: Reference Workloads for Modern Deep Learning Methods
A1  - Robert Adolf
A1  - Saketh Rama
A1  - Brandon Reagen
A1  - Gu-Yeon Wei
A1  - David Brooks
JO  - ArXiv e-prints
Y1  - 23 August, 2016
UR  - https://arxiv.org/abs/1608.06581
N2  - Deep learning has been popularized by its recent successes on challenging artificial intelligence problems. One of the reasons for its dominance is also an ongoing challenge: the need for immense amounts of computational power. Hardware architects have responded by proposing a wide array of promising ideas, but to date, the majority of the work has focused on specific algorithms in somewhat narrow application domains. While their specificity does not diminish these approaches, there is a clear need for more flexible solutions. We believe the first step is to examine the characteristics of cutting edge models from across the deep learning community.
ER  -


TY  - Preprint
T1  - Deep Double Sparsity Encoder: Learning to Sparsify Not Only Features But Also Parameters
A1  - Zhangyang Wang
A1  - Thomas S. Huang
JO  - ArXiv e-prints
Y1  - 1 October, 2016
UR  - https://arxiv.org/abs/1608.06374
N2  - This paper emphasizes the significance to jointly exploit the problem structure and the parameter structure, in the context of deep modeling. As a specific and interesting example, we describe the deep double sparsity encoder (DDSE), which is inspired by the double sparsity model for dictionary learning. DDSE simultaneously sparsities the output features and the learned model parameters, under one unified framework. In addition to its intuitive model interpretation, DDSE also possesses compact model size and low complexity. Extensive simulations compare DDSE with several carefully-designed baselines, and verify the consistently superior performance of DDSE. We further apply DDSE to the novel application domain of brain encoding, with promising preliminary results achieved.
ER  -


TY  - Preprint
T1  - BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems
A1  - Zachary C. Lipton
A1  - Xiujun Li
A1  - Jianfeng Gao
A1  - Lihong Li
A1  - Faisal Ahmed
A1  - Li Deng
JO  - ArXiv e-prints
Y1  - 23 November, 2017
UR  - https://arxiv.org/abs/1608.05081
N2  - We present a new algorithm that significantly improves the efficiency of exploration for deep Q-learning agents in dialogue systems. Our agents explore via Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop neural network. Our algorithm learns much faster than common exploration strategies such as $Îµ$-greedy, Boltzmann, bootstrapping, and intrinsic-reward-based ones. Additionally, we show that spiking the replay buffer with experiences from just a few successful episodes can make Q-learning feasible when it might otherwise fail.
ER  -


TY  - Preprint
T1  - An image compression and encryption scheme based on deep learning
A1  - Fei Hu
A1  - Changjiu Pu
A1  - Haowei Gao
A1  - Mengzi Tang
A1  - Li Li
JO  - ArXiv e-prints
Y1  - 8 October, 2016
UR  - https://arxiv.org/abs/1608.05001
N2  - Stacked Auto-Encoder (SAE) is a kind of deep learning algorithm for unsupervised learning. Which has multi layers that project the vector representation of input data into a lower vector space. These projection vectors are dense representations of the input data. As a result, SAE can be used for image compression. Using chaotic logistic map, the compression ones can further be encrypted. In this study, an application of image compression and encryption is suggested using SAE and chaotic logistic map. Experiments show that this application is feasible and effective. It can be used for image transmission and image protection on internet simultaneously.
ER  -


TY  - Preprint
T1  - Stacked Approximated Regression Machine: A Simple Deep Learning Approach
A1  - Zhangyang Wang
A1  - Shiyu Chang
A1  - Qing Ling
A1  - Shuai Huang
A1  - Xia Hu
A1  - Honghui Shi
A1  - Thomas S. Huang
JO  - ArXiv e-prints
Y1  - 8 September, 2016
UR  - https://arxiv.org/abs/1608.04062
N2  - With the agreement of my coauthors, I Zhangyang Wang would like to withdraw the manuscript &#34;Stacked Approximated Regression Machine: A Simple Deep Learning Approach&#34;. Some experimental procedures were not included in the manuscript, which makes a part of important claims not meaningful. In the relevant research, I was solely responsible for carrying out the experiments; the other coauthors joined in the discussions leading to the main algorithm.
ER  -


TY  - Preprint
T1  - Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising
A1  - Kai Zhang
A1  - Wangmeng Zuo
A1  - Yunjin Chen
A1  - Deyu Meng
A1  - Lei Zhang
JO  - ArXiv e-prints
Y1  - 13 August, 2016
UR  - https://arxiv.org/abs/1608.03981
N2  - Discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, we take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, residual learning and batch normalization are utilized to speed up the training process as well as boost the denoising performance. Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise (AWGN) at a certain noise level, our DnCNN model is able to handle Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising). With the residual learning strategy, DnCNN implicitly removes the latent clean image in the hidden layers. This property motivates us to train a single DnCNN model to tackle with several general image denoising tasks such as Gaussian denoising, single image super-resolution and JPEG image deblocking. Our extensive experiments demonstrate that our DnCNN model can not only exhibit high effectiveness in several general image denoising tasks, but also be efficiently implemented by benefiting from GPU computing.
ER  -


TY  - Preprint
T1  - Applying Deep Learning to Basketball Trajectories
A1  - Rajiv Shah
A1  - Rob Romijnders
JO  - ArXiv e-prints
Y1  - 16 August, 2016
UR  - https://arxiv.org/abs/1608.03793
N2  - One of the emerging trends for sports analytics is the growing use of player and ball tracking data. A parallel development is deep learning predictive approaches that use vast quantities of data with less reliance on feature engineering. This paper applies recurrent neural networks in the form of sequence modeling to predict whether a three-point shot is successful. The models are capable of learning the trajectory of a basketball without any knowledge of physics. For comparison, a baseline static machine learning model with a full set of features, such as angle and velocity, in addition to the positional data is also tested. Using a dataset of over 20,000 three pointers from NBA SportVu data, the models based simply on sequential positional data outperform a static feature rich machine learning model in predicting whether a three-point shot is successful. This suggests deep learning models may offer an improvement to traditional feature based machine learning methods for tracking data.
ER  -


TY  - Preprint
T1  - Learning Structured Sparsity in Deep Neural Networks
A1  - Wei Wen
A1  - Chunpeng Wu
A1  - Yandan Wang
A1  - Yiran Chen
A1  - Hai Li
JO  - ArXiv e-prints
Y1  - 18 October, 2016
UR  - https://arxiv.org/abs/1608.03665
N2  - High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNNs evaluation. Experimental results show that SSL achieves on average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual Network (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%, which is still slightly higher than that of original ResNet with 32 layers. For AlexNet, structure regularization by SSL also reduces the error by around ~1%. Open source code is in https://github.com/wenwei202/caffe/tree/scnn
ER  -


TY  - Preprint
T1  - Deep Hashing: A Joint Approach for Image Signature Learning
A1  - Yadong Mu
A1  - Zhu Liu
JO  - ArXiv e-prints
Y1  - 11 August, 2016
UR  - https://arxiv.org/abs/1608.03658
N2  - Similarity-based image hashing represents crucial technique for visual data storage reduction and expedited image search. Conventional hashing schemes typically feed hand-crafted features into hash functions, which separates the procedures of feature extraction and hash function learning. In this paper, we propose a novel algorithm that concurrently performs feature engineering and non-linear supervised hashing function learning. Our technical contributions in this paper are two-folds: 1) deep network optimization is often achieved by gradient propagation, which critically requires a smooth objective function. The discrete nature of hash codes makes them not amenable for gradient-based optimization. To address this issue, we propose an exponentiated hashing loss function and its bilinear smooth approximation. Effective gradient calculation and propagation are thereby enabled; 2) pre-training is an important trick in supervised deep learning. The impact of pre-training on the hash code quality has never been discussed in current deep hashing literature. We propose a pre-training scheme inspired by recent advance in deep network based image classification, and experimentally demonstrate its effectiveness. Comprehensive quantitative evaluations are conducted on several widely-used image benchmarks. On all benchmarks, our proposed deep hashing algorithm outperforms all state-of-the-art competitors by significant margins. In particular, our algorithm achieves a near-perfect 0.99 in terms of Hamming ranking accuracy with only 12 bits on MNIST, and a new record of 0.74 on the CIFAR10 dataset. In comparison, the best accuracies obtained on CIFAR10 by existing hashing algorithms without or with deep networks are known to be 0.36 and 0.58 respectively.
ER  -


TY  - Preprint
T1  - Mining Fashion Outfit Composition Using An End-to-End Deep Learning Approach on Set Data
A1  - Yuncheng Li
A1  - LiangLiang Cao
A1  - Jiang Zhu
A1  - Jiebo Luo
JO  - ArXiv e-prints
Y1  - 15 April, 2017
UR  - https://arxiv.org/abs/1608.03016
N2  - Composing fashion outfits involves deep understanding of fashion standards while incorporating creativity for choosing multiple fashion items (e.g., Jewelry, Bag, Pants, Dress). In fashion websites, popular or high-quality fashion outfits are usually designed by fashion experts and followed by large audiences. In this paper, we propose a machine learning system to compose fashion outfits automatically. The core of the proposed automatic composition system is to score fashion outfit candidates based on the appearances and meta-data. We propose to leverage outfit popularity on fashion oriented websites to supervise the scoring component. The scoring component is a multi-modal multi-instance deep learning system that evaluates instance aesthetics and set compatibility simultaneously. In order to train and evaluate the proposed composition system, we have collected a large scale fashion outfit dataset with 195K outfits and 368K fashion items from Polyvore. Although the fashion outfit scoring and composition is rather challenging, we have achieved an AUC of 85% for the scoring component, and an accuracy of 77% for a constrained composition task.
ER  -


TY  - Preprint
T1  - Online Adaptation of Deep Architectures with Reinforcement Learning
A1  - Thushan Ganegedara
A1  - Lionel Ott
A1  - Fabio Ramos
JO  - ArXiv e-prints
Y1  - 7 August, 2016
UR  - https://arxiv.org/abs/1608.02292
N2  - Online learning has become crucial to many problems in machine learning. As more data is collected sequentially, quickly adapting to changes in the data distribution can offer several competitive advantages such as avoiding loss of prior knowledge and more efficient learning. However, adaptation to changes in the data distribution (also known as covariate shift) needs to be performed without compromising past knowledge already built in into the model to cope with voluminous and dynamic data. In this paper, we propose an online stacked Denoising Autoencoder whose structure is adapted through reinforcement learning. Our algorithm forces the network to exploit and explore favourable architectures employing an estimated utility function that maximises the accuracy of an unseen validation sequence. Different actions, such as Pool, Increment and Merge are available to modify the structure of the network. As we observe through a series of experiments, our approach is more responsive, robust, and principled than its counterparts for non-stationary as well as stationary data distributions. Experimental results indicate that our algorithm performs better at preserving gained prior knowledge and responding to changes in the data distribution.
ER  -


TY  - Preprint
T1  - Deep Learning a Grasp Function for Grasping under Gripper Pose Uncertainty
A1  - Edward Johns
A1  - Stefan Leutenegger
A1  - Andrew J. Davison
JO  - ArXiv e-prints
Y1  - 7 August, 2016
UR  - https://arxiv.org/abs/1608.02239
N2  - This paper presents a new method for parallel-jaw grasping of isolated objects from depth images, under large gripper pose uncertainty. Whilst most approaches aim to predict the single best grasp pose from an image, our method first predicts a score for every possible grasp pose, which we denote the grasp function. With this, it is possible to achieve grasping robust to the gripper&#39;s pose uncertainty, by smoothing the grasp function with the pose uncertainty function. Therefore, if the single best pose is adjacent to a region of poor grasp quality, that pose will no longer be chosen, and instead a pose will be chosen which is surrounded by a region of high grasp quality. To learn this function, we train a Convolutional Neural Network which takes as input a single depth image of an object, and outputs a score for each grasp pose across the image. Training data for this is generated by use of physics simulation and depth image simulation with 3D object meshes, to enable acquisition of sufficient data without requiring exhaustive real-world experiments. We evaluate with both synthetic and real experiments, and show that the learned grasp score is more robust to gripper pose uncertainty than when this uncertainty is not accounted for.
ER  -


TY  - Preprint
T1  - Deep Learning the City : Quantifying Urban Perception At A Global Scale
A1  - Abhimanyu Dubey
A1  - Nikhil Naik
A1  - Devi Parikh
A1  - Ramesh Raskar
A1  - CÃ©sar A. Hidalgo
JO  - ArXiv e-prints
Y1  - 12 September, 2016
UR  - https://arxiv.org/abs/1608.01769
N2  - Computer vision methods that quantify the perception of urban environment are increasingly being used to study the relationship between a city&#39;s physical appearance and the behavior and health of its residents. Yet, the throughput of current methods is too limited to quantify the perception of cities across the world. To tackle this challenge, we introduce a new crowdsourced dataset containing 110,988 images from 56 cities, and 1,170,000 pairwise comparisons provided by 81,630 online volunteers along six perceptual attributes: safe, lively, boring, wealthy, depressing, and beautiful. Using this data, we train a Siamese-like convolutional neural architecture, which learns from a joint classification and ranking loss, to predict human judgments of pairwise image comparisons. Our results show that crowdsourcing combined with neural networks can produce urban perception data at the global scale.
ER  -


TY  - Preprint
T1  - Deep Learning for Detecting Multiple Space-Time Action Tubes in Videos
A1  - Suman Saha
A1  - Gurkirt Singh
A1  - Michael Sapienza
A1  - Philip H. S. Torr
A1  - Fabio Cuzzolin
JO  - ArXiv e-prints
Y1  - 4 August, 2016
UR  - https://arxiv.org/abs/1608.01529
N2  - In this work, we propose an approach to the spatiotemporal localisation (detection) and classification of multiple concurrent actions within temporally untrimmed videos. Our framework is composed of three stages. In stage 1, appearance and motion detection networks are employed to localise and score actions from colour images and optical flow. In stage 2, the appearance network detections are boosted by combining them with the motion detection scores, in proportion to their respective spatial overlap. In stage 3, sequences of detection boxes most likely to be associated with a single action instance, called action tubes, are constructed by solving two energy maximisation problems via dynamic programming. While in the first pass, action paths spanning the whole video are built by linking detection boxes over time using their class-specific scores and their spatial overlap, in the second pass, temporal trimming is performed by ensuring label consistency for all constituting detection boxes. We demonstrate the performance of our algorithm on the challenging UCF101, J-HMDB-21 and LIRIS-HARL datasets, achieving new state-of-the-art results across the board and significantly increasing detection speed at test time. We achieve a huge leap forward in action detection performance and report a 20% and 11% gain in mAP (mean average precision) on UCF-101 and J-HMDB-21 datasets respectively when compared to the state-of-the-art.
ER  -


TY  - Preprint
T1  - Mitochondria-based Renal Cell Carcinoma Subtyping: Learning from Deep vs. Flat Feature Representations
A1  - Peter J. SchÃ¼ffler
A1  - Judy Sarungbam
A1  - Hassan Muhammad
A1  - Ed Reznik
A1  - Satish K. Tickoo
A1  - Thomas J. Fuchs
JO  - ArXiv e-prints
Y1  - 2 August, 2016
UR  - https://arxiv.org/abs/1608.00842
N2  - Accurate subtyping of renal cell carcinoma (RCC) is of crucial importance for understanding disease progression and for making informed treatment decisions. New discoveries of significant alterations to mitochondria between subtypes make immunohistochemical (IHC) staining based image classification an imperative. Until now, accurate quantification and subtyping was made impossible by huge IHC variations, the absence of cell membrane staining for cytoplasm segmentation as well as the complete lack of systems for robust and reproducible image based classification. In this paper we present a comprehensive classification framework to overcome these challenges for tissue microarrays (TMA) of RCCs. We compare and evaluate models based on domain specific hand-crafted &#34;flat&#34;-features versus &#34;deep&#34; feature representations from various layers of a pre-trained convolutional neural network (CNN). The best model reaches a cross-validation accuracy of 89%, which demonstrates for the first time, that robust mitochondria-based subtyping of renal cancer is feasible
ER  -


TY  - Preprint
T1  - Exploring Deep Space: Learning Personalized Ranking in a Semantic Space
A1  - Jeroen B. P. Vuurens
A1  - Martha Larson
A1  - Arjen P. de Vries
JO  - ArXiv e-prints
Y1  - 22 August, 2016
UR  - https://arxiv.org/abs/1608.00276
N2  - Recommender systems leverage both content and user interactions to generate recommendations that fit users&#39; preferences. The recent surge of interest in deep learning presents new opportunities for exploiting these two sources of information. To recommend items we propose to first learn a user-independent high-dimensional semantic space in which items are positioned according to their substitutability, and then learn a user-specific transformation function to transform this space into a ranking according to the user&#39;s past preferences. An advantage of the proposed architecture is that it can be used to effectively recommend items using either content that describes the items or user-item ratings. We show that this approach significantly outperforms state-of-the-art recommender systems on the MovieLens 1M dataset.
ER  -


TY  - Preprint
T1  - Learning Robust Features using Deep Learning for Automatic Seizure Detection
A1  - Pierre Thodoroff
A1  - Joelle Pineau
A1  - Andrew Lim
JO  - ArXiv e-prints
Y1  - 31 July, 2016
UR  - https://arxiv.org/abs/1608.00220
N2  - We present and evaluate the capacity of a deep neural network to learn robust features from EEG to automatically detect seizures. This is a challenging problem because seizure manifestations on EEG are extremely variable both inter- and intra-patient. By simultaneously capturing spectral, temporal and spatial information our recurrent convolutional neural network learns a general spatially invariant representation of a seizure. The proposed approach exceeds significantly previous results obtained on cross-patient classifiers both in terms of sensitivity and false positive rate. Furthermore, our model proves to be robust to missing channel and variable electrode montage.
ER  -


TY  - Preprint
T1  - Hyperparameter Transfer Learning through Surrogate Alignment for Efficient Deep Neural Network Training
A1  - Ilija Ilievski
A1  - Jiashi Feng
JO  - ArXiv e-prints
Y1  - 31 July, 2016
UR  - https://arxiv.org/abs/1608.00218
N2  - Recently, several optimization methods have been successfully applied to the hyperparameter optimization of deep neural networks (DNNs). The methods work by modeling the joint distribution of hyperparameter values and corresponding error. Those methods become less practical when applied to modern DNNs whose training may take a few days and thus one cannot collect sufficient observations to accurately model the distribution. To address this challenging issue, we propose a method that learns to transfer optimal hyperparameter values for a small source dataset to hyperparameter values with comparable performance on a dataset of interest. As opposed to existing transfer learning methods, our proposed method does not use hand-designed features. Instead, it uses surrogates to model the hyperparameter-error distributions of the two datasets and trains a neural network to learn the transfer function. Extensive experiments on three CV benchmark datasets clearly demonstrate the efficiency of our method.
ER  -


TY  - Preprint
T1  - Learning deep representation from coarse to fine for face alignment
A1  - Zhiwen Shao
A1  - Shouhong Ding
A1  - Yiru Zhao
A1  - Qinchuan Zhang
A1  - Lizhuang Ma
JO  - ArXiv e-prints
Y1  - 31 July, 2016
UR  - https://arxiv.org/abs/1608.00207
N2  - In this paper, we propose a novel face alignment method that trains deep convolutional network from coarse to fine. It divides given landmarks into principal subset and elaborate subset. We firstly keep a large weight for principal subset to make our network primarily predict their locations while slightly take elaborate subset into account. Next the weight of principal subset is gradually decreased until two subsets have equivalent weights. This process contributes to learn a good initial model and search the optimal model smoothly to avoid missing fairly good intermediate models in subsequent procedures. On the challenging COFW dataset [1], our method achieves 6.33% mean error with a reduction of 21.37% compared with the best previous result [2].
ER  -


TY  - Preprint
T1  - Efficient Hyperparameter Optimization of Deep Learning Algorithms Using Deterministic RBF Surrogates
A1  - Ilija Ilievski
A1  - Taimoor Akhtar
A1  - Jiashi Feng
A1  - Christine Annette Shoemaker
JO  - ArXiv e-prints
Y1  - 20 January, 2017
UR  - https://arxiv.org/abs/1607.08316
N2  - Automatically searching for optimal hyperparameter configurations is of crucial importance for applying deep learning algorithms in practice. Recently, Bayesian optimization has been proposed for optimizing hyperparameters of various machine learning algorithms. Those methods adopt probabilistic surrogate models like Gaussian processes to approximate and minimize the validation error function of hyperparameter values. However, probabilistic surrogates require accurate estimates of sufficient statistics (e.g., covariance) of the error distribution and thus need many function evaluations with a sizeable number of hyperparameters. This makes them inefficient for optimizing hyperparameters of deep learning algorithms, which are highly expensive to evaluate. In this work, we propose a new deterministic and efficient hyperparameter optimization method that employs radial basis functions as error surrogates. The proposed mixed integer algorithm, called HORD, searches the surrogate for the most promising hyperparameter values through dynamic coordinate search and requires many fewer function evaluations. HORD does well in low dimensions but it is exceptionally better in higher dimensions. Extensive evaluations on MNIST and CIFAR-10 for four deep neural networks demonstrate HORD significantly outperforms the well-established Bayesian optimization methods such as GP, SMAC, and TPE. For instance, on average, HORD is more than 6 times faster than GP-EI in obtaining the best configuration of 19 hyperparameters.
ER  -


TY  - Preprint
T1  - Local- and Holistic- Structure Preserving Image Super Resolution via Deep Joint Component Learning
A1  - Yukai Shi
A1  - Keze Wang
A1  - Li Xu
A1  - Liang Lin
JO  - ArXiv e-prints
Y1  - 25 July, 2016
UR  - https://arxiv.org/abs/1607.07220
N2  - Recently, machine learning based single image super resolution (SR) approaches focus on jointly learning representations for high-resolution (HR) and low-resolution (LR) image patch pairs to improve the quality of the super-resolved images. However, due to treat all image pixels equally without considering the salient structures, these approaches usually fail to produce visual pleasant images with sharp edges and fine details. To address this issue, in this work we present a new novel SR approach, which replaces the main building blocks of the classical interpolation pipeline by a flexible, content-adaptive deep neural networks. In particular, two well-designed structure-aware components, respectively capturing local- and holistic- image contents, are naturally incorporated into the fully-convolutional representation learning to enhance the image sharpness and naturalness. Extensively evaluations on several standard benchmarks (e.g., Set5, Set14 and BSD200) demonstrate that our approach can achieve superior results, especially on the image with salient structures, over many existing state-of-the-art SR methods under both quantitative and qualitative measures.
ER  -


TY  - Preprint
T1  - Deep nets for local manifold learning
A1  - Charles K. Chui
A1  - H. N. Mhaskar
JO  - ArXiv e-prints
Y1  - 24 July, 2016
UR  - https://arxiv.org/abs/1607.07110
N2  - The problem of extending a function $f$ defined on a training data $\mathcal{C}$ on an unknown manifold $\mathbb{X}$ to the entire manifold and a tubular neighborhood of this manifold is considered in this paper. For $\mathbb{X}$ embedded in a high dimensional ambient Euclidean space $\mathbb{R}^D$, a deep learning algorithm is developed for finding a local coordinate system for the manifold {\bf without eigen--decomposition}, which reduces the problem to the classical problem of function approximation on a low dimensional cube. Deep nets (or multilayered neural networks) are proposed to accomplish this approximation scheme by using the training data. Our methods do not involve such optimization techniques as back--propagation, while assuring optimal (a priori) error bounds on the output in terms of the number of derivatives of the target function. In addition, these methods are universal, in that they do not require a prior knowledge of the smoothness of the target function, but adjust the accuracy of approximation locally and automatically, depending only upon the local smoothness of the target function. Our ideas are easily extended to solve both the pre--image problem and the out--of--sample extension problem, with a priori bounds on the growth of the function thus extended.
ER  -


TY  - Preprint
T1  - Impact of Physical Activity on Sleep:A Deep Learning Based Exploration
A1  - Aarti Sathyanarayana
A1  - Shafiq Joty
A1  - Luis Fernandez-Luque
A1  - Ferda Ofli
A1  - Jaideep Srivastava
A1  - Ahmed Elmagarmid
A1  - Shahrad Taheri
A1  - Teresa Arora
JO  - ArXiv e-prints
Y1  - 24 July, 2016
UR  - https://arxiv.org/abs/1607.07034
N2  - The importance of sleep is paramount for maintaining physical, emotional and mental wellbeing. Though the relationship between sleep and physical activity is known to be important, it is not yet fully understood. The explosion in popularity of actigraphy and wearable devices, provides a unique opportunity to understand this relationship. Leveraging this information source requires new tools to be developed to facilitate data-driven research for sleep and activity patient-recommendations.
ER  -


TY  - Preprint
T1  - Classification of Alzheimer&#39;s Disease Structural MRI Data by Deep Learning Convolutional Neural Networks
A1  - Saman Sarraf
A1  - Ghassem Tofighi
JO  - ArXiv e-prints
Y1  - 19 May, 2017
UR  - https://arxiv.org/abs/1607.06583
N2  - Recently, machine learning techniques especially predictive modeling and pattern recognition in biomedical sciences from drug delivery system to medical imaging has become one of the important methods which are assisting researchers to have deeper understanding of entire issue and to solve complex medical problems. Deep learning is a powerful machine learning algorithm in classification while extracting low to high-level features. In this paper, we used convolutional neural network to classify Alzheimer&#39;s brain from normal healthy brain. The importance of classifying this kind of medical data is to potentially develop a predict model or system in order to recognize the type disease from normal subjects or to estimate the stage of the disease. Classification of clinical data such as Alzheimer&#39;s disease has been always challenging and most problematic part has been always selecting the most discriminative features. Using Convolutional Neural Network (CNN) and the famous architecture LeNet-5, we successfully classified structural MRI data of Alzheimer&#39;s subjects from normal controls where the accuracy of test data on trained data reached 98.84%. This experiment suggests us the shift and scale invariant features extracted by CNN followed by deep learning classification is most powerful method to distinguish clinical data from healthy data in fMRI. This approach also enables us to expand our methodology to predict more complicated systems.
ER  -


TY  - Preprint
T1  - Deep Learning of Local RGB-D Patches for 3D Object Detection and 6D Pose Estimation
A1  - Wadim Kehl
A1  - Fausto Milletari
A1  - Federico Tombari
A1  - Slobodan Ilic
A1  - Nassir Navab
JO  - ArXiv e-prints
Y1  - 20 July, 2016
UR  - https://arxiv.org/abs/1607.06038
N2  - We present a 3D object detection method that uses regressed descriptors of locally-sampled RGB-D patches for 6D vote casting. For regression, we employ a convolutional auto-encoder that has been trained on a large collection of random local patches. During testing, scene patch descriptors are matched against a database of synthetic model view patches and cast 6D object votes which are subsequently filtered to refined hypotheses. We evaluate on three datasets to show that our method generalizes well to previously unseen input data, delivers robust detection results that compete with and surpass the state-of-the-art while being scalable in the number of objects.
ER  -


TY  - Preprint
T1  - Onsager-corrected deep learning for sparse linear inverse problems
A1  - Mark Borgerding
A1  - Philip Schniter
JO  - ArXiv e-prints
Y1  - 20 July, 2016
UR  - https://arxiv.org/abs/1607.05966
N2  - Deep learning has gained great popularity due to its widespread success on many inference problems. We consider the application of deep learning to the sparse linear inverse problem encountered in compressive sensing, where one seeks to recover a sparse signal from a small number of noisy linear measurements. In this paper, we propose a novel neural-network architecture that decouples prediction errors across layers in the same way that the approximate message passing (AMP) algorithm decouples them across iterations: through Onsager correction. Numerical experiments suggest that our &#34;learned AMP&#34; network significantly improves upon Gregor and LeCun&#39;s &#34;learned ISTA&#34; network in both accuracy and complexity.
ER  -


TY  - Preprint
T1  - Improved Deep Learning of Object Category using Pose Information
A1  - Jiaping Zhao
A1  - Laurent Itti
JO  - ArXiv e-prints
Y1  - 22 January, 2017
UR  - https://arxiv.org/abs/1607.05836
N2  - Despite significant recent progress, the best available computer vision algorithms still lag far behind human capabilities, even for recognizing individual discrete objects under various poses, illuminations, and backgrounds. Here we present a new approach to using object pose information to improve deep network learning. While existing large-scale datasets, e.g. ImageNet, do not have pose information, we leverage the newly published turntable dataset, iLab-20M, which has ~22M images of 704 object instances shot under different lightings, camera viewpoints and turntable rotations, to do more controlled object recognition experiments. We introduce a new convolutional neural network architecture, what/where CNN (2W-CNN), built on a linear-chain feedforward CNN (e.g., AlexNet), augmented by hierarchical layers regularized by object poses. Pose information is only used as feedback signal during training, in addition to category information; during test, the feedforward network only predicts category. To validate the approach, we train both 2W-CNN and AlexNet using a fraction of the dataset, and 2W-CNN achieves 6% performance improvement in category prediction. We show mathematically that 2W-CNN has inherent advantages over AlexNet under the stochastic gradient descent (SGD) optimization procedure. Further more, we fine-tune object recognition on ImageNet by using the pretrained 2W-CNN and AlexNet features on iLab-20M, results show that significant improvements have been achieved, compared with training AlexNet from scratch. Moreover, fine-tuning 2W-CNN features performs even better than fine-tuning the pretrained AlexNet features. These results show pretrained features on iLab- 20M generalizes well to natural image datasets, and 2WCNN learns even better features for object recognition than AlexNet.
ER  -


TY  - Preprint
T1  - Collaborative Layer-wise Discriminative Learning in Deep Neural Networks
A1  - Xiaojie Jin
A1  - Yunpeng Chen
A1  - Jian Dong
A1  - Jiashi Feng
A1  - Shuicheng Yan
JO  - ArXiv e-prints
Y1  - 19 July, 2016
UR  - https://arxiv.org/abs/1607.05440
N2  - Intermediate features at different layers of a deep neural network are known to be discriminative for visual patterns of different complexities. However, most existing works ignore such cross-layer heterogeneities when classifying samples of different complexities. For example, if a training sample has already been correctly classified at a specific layer with high confidence, we argue that it is unnecessary to enforce rest layers to classify this sample correctly and a better strategy is to encourage those layers to focus on other samples.
ER  -


TY  - Preprint
T1  - Deep learning trends for focal brain pathology segmentation in MRI
A1  - Mohammad Havaei
A1  - Nicolas Guizard
A1  - Hugo Larochelle
A1  - Pierre-Marc Jodoin
JO  - ArXiv e-prints
Y1  - 23 January, 2017
UR  - https://arxiv.org/abs/1607.05258
N2  - Segmentation of focal (localized) brain pathologies such as brain tumors and brain lesions caused by multiple sclerosis and ischemic strokes are necessary for medical diagnosis, surgical planning and disease development as well as other applications such as tractography. Over the years, attempts have been made to automate this process for both clinical and research reasons. In this regard, machine learning methods have long been a focus of attention. Over the past two years, the medical imaging field has seen a rise in the use of a particular branch of machine learning commonly known as deep learning. In the non-medical computer vision world, deep learning based methods have obtained state-of-the-art results on many datasets. Recent studies in computer aided diagnostics have shown deep learning methods (and especially convolutional neural networks - CNN) to yield promising results. In this chapter, we provide a survey of CNN methods applied to medical imaging with a focus on brain pathology segmentation. In particular, we discuss their characteristic peculiarities and their specific configuration and adjustments that are best suited to segment medical images. We also underline the intrinsic differences deep learning methods have with other machine learning methods.
ER  -


TY  - Preprint
T1  - Learning to Hash with Binary Deep Neural Network
A1  - Thanh-Toan Do
A1  - Anh-Dzung Doan
A1  - Ngai-Man Cheung
JO  - ArXiv e-prints
Y1  - 18 July, 2016
UR  - https://arxiv.org/abs/1607.05140
N2  - This work proposes deep network models and learning algorithms for unsupervised and supervised binary hashing. Our novel network design constrains one hidden layer to directly output the binary codes. This addresses a challenging issue in some previous works: optimizing non-smooth objective functions due to binarization. Moreover, we incorporate independence and balance properties in the direct and strict forms in the learning. Furthermore, we include similarity preserving property in our objective function. Our resulting optimization with these binary, independence, and balance constraints is difficult to solve. We propose to attack it with alternating optimization and careful relaxation. Experimental results on three benchmark datasets show that our proposed methods compare favorably with the state of the art.
ER  -


TY  - Preprint
T1  - Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay
A1  - Ionel-Alexandru Hosu
A1  - Traian Rebedea
JO  - ArXiv e-prints
Y1  - 18 July, 2016
UR  - https://arxiv.org/abs/1607.05077
N2  - This paper introduces a novel method for learning how to play the most difficult Atari 2600 games from the Arcade Learning Environment using deep reinforcement learning. The proposed method, human checkpoint replay, consists in using checkpoints sampled from human gameplay as starting points for the learning process. This is meant to compensate for the difficulties of current exploration strategies, such as epsilon-greedy, to find successful control policies in games with sparse rewards. Like other deep reinforcement learning architectures, our model uses a convolutional neural network that receives only raw pixel inputs to estimate the state value function. We tested our method on Montezuma&#39;s Revenge and Private Eye, two of the most challenging games from the Atari platform. The results we obtained show a substantial improvement compared to previous learning approaches, as well as over a random player. We also propose a method for training deep reinforcement learning agents using human gameplay experience, which we call human experience replay.
ER  -


TY  - Preprint
T1  - An Empirical Evaluation of various Deep Learning Architectures for Bi-Sequence Classification Tasks
A1  - Anirban Laha
A1  - Vikas Raykar
JO  - ArXiv e-prints
Y1  - 2 October, 2016
UR  - https://arxiv.org/abs/1607.04853
N2  - Several tasks in argumentation mining and debating, question-answering, and natural language inference involve classifying a sequence in the context of another sequence (referred as bi-sequence classification). For several single sequence classification tasks, the current state-of-the-art approaches are based on recurrent and convolutional neural networks. On the other hand, for bi-sequence classification problems, there is not much understanding as to the best deep learning architecture. In this paper, we attempt to get an understanding of this category of problems by extensive empirical evaluation of 19 different deep learning architectures (specifically on different ways of handling context) for various problems originating in natural language processing like debating, textual entailment and question-answering. Following the empirical evaluation, we offer our insights and conclusions regarding the architectures we have considered. We also establish the first deep learning baselines for three argumentation mining tasks.
ER  -


TY  - Preprint
T1  - Learning to Decode Linear Codes Using Deep Learning
A1  - Eliya Nachmani
A1  - Yair Beery
A1  - David Burshtein
JO  - ArXiv e-prints
Y1  - 30 September, 2016
UR  - https://arxiv.org/abs/1607.04793
N2  - A novel deep learning method for improving the belief propagation algorithm is proposed. The method generalizes the standard belief propagation algorithm by assigning weights to the edges of the Tanner graph. These edges are then trained using deep learning techniques. A well-known property of the belief propagation algorithm is the independence of the performance on the transmitted codeword. A crucial property of our new method is that our decoder preserved this property. Furthermore, this property allows us to learn only a single codeword instead of exponential number of code-words. Improvements over the belief propagation algorithm are demonstrated for various high density parity check codes.
ER  -


TY  - Preprint
T1  - Analyzing features learned for Offline Signature Verification using Deep CNNs
A1  - Luiz G. Hafemann
A1  - Robert Sabourin
A1  - Luiz S. Oliveira
JO  - ArXiv e-prints
Y1  - 26 August, 2016
UR  - https://arxiv.org/abs/1607.04573
N2  - Research on Offline Handwritten Signature Verification explored a large variety of handcrafted feature extractors, ranging from graphology, texture descriptors to interest points. In spite of advancements in the last decades, performance of such systems is still far from optimal when we test the systems against skilled forgeries - signature forgeries that target a particular individual. In previous research, we proposed a formulation of the problem to learn features from data (signature images) in a Writer-Independent format, using Deep Convolutional Neural Networks (CNNs), seeking to improve performance on the task. In this research, we push further the performance of such method, exploring a range of architectures, and obtaining a large improvement in state-of-the-art performance on the GPDS dataset, the largest publicly available dataset on the task. In the GPDS-160 dataset, we obtained an Equal Error Rate of 2.74%, compared to 6.97% in the best result published in literature (that used a combination of multiple classifiers). We also present a visual analysis of the feature space learned by the model, and an analysis of the errors made by the classifier. Our analysis shows that the model is very effective in separating signatures that have a different global appearance, while being particularly vulnerable to forgeries that very closely resemble genuine signatures, even if their line quality is bad, which is the case of slowly-traced forgeries.
ER  -


TY  - Preprint
T1  - Efficient and Robust Pedestrian Detection using Deep Learning for Human-Aware Navigation
A1  - Andre Mateus
A1  - David Ribeiro
A1  - Pedro Miraldo
A1  - Jacinto C. Nascimento
JO  - ArXiv e-prints
Y1  - 28 September, 2017
UR  - https://arxiv.org/abs/1607.04441
N2  - This paper addresses the problem of Human-Aware Navigation (HAN), using multi camera sensors to implement a vision-based person tracking system. The main contributions of this paper are as follows: a novel and efficient Deep Learning person detection and a standardization of human-aware constraints. In the first stage of the approach, we propose to cascade the Aggregate Channel Features (ACF) detector with a deep Convolutional Neural Network (CNN) to achieve fast and accurate Pedestrian Detection (PD). Regarding the human awareness (that can be defined as constraints associated with the robot&#39;s motion), we use a mixture of asymmetric Gaussian functions, to define the cost functions associated to each constraint. Both methods proposed herein are evaluated individually to measure the impact of each of the components. The final solution (including both the proposed pedestrian detection and the human-aware constraints) is tested in a typical domestic indoor scenario, in four distinct experiments. The results show that the robot is able to cope with human-aware constraints, defined after common proxemics and social rules.
ER  -


TY  - Preprint
T1  - A Real-Time Deep Learning Pedestrian Detector for Robot Navigation
A1  - David Ribeiro
A1  - Andre Mateus
A1  - Pedro Miraldo
A1  - Jacinto C. Nascimento
JO  - ArXiv e-prints
Y1  - 19 September, 2017
UR  - https://arxiv.org/abs/1607.04436
N2  - A real-time Deep Learning based method for Pedestrian Detection (PD) is applied to the Human-Aware robot navigation problem. The pedestrian detector combines the Aggregate Channel Features (ACF) detector with a deep Convolutional Neural Network (CNN) in order to obtain fast and accurate performance. Our solution is firstly evaluated using a set of real images taken from onboard and offboard cameras and, then, it is validated in a typical robot navigation environment with pedestrians (two distinct experiments are conducted). The results on both tests show that our pedestrian detector is robust and fast enough to be used on robot navigation applications.
ER  -


TY  - Preprint
T1  - Deep Structured-Output Regression Learning for Computational Color Constancy
A1  - Yanlin Qian
A1  - Ke Chen
A1  - Joni-Kristian Kamarainen
A1  - Jarno Nikkanen
A1  - Jiri Matas
JO  - ArXiv e-prints
Y1  - 11 August, 2016
UR  - https://arxiv.org/abs/1607.03856
N2  - Computational color constancy that requires esti- mation of illuminant colors of images is a fundamental yet active problem in computer vision, which can be formulated into a regression problem. To learn a robust regressor for color constancy, obtaining meaningful imagery features and capturing latent correlations across output variables play a vital role. In this work, we introduce a novel deep structured-output regression learning framework to achieve both goals simultaneously. By borrowing the power of deep convolutional neural networks (CNN) originally designed for visual recognition, the proposed framework can automatically discover strong features for white balancing over different illumination conditions and learn a multi-output regressor beyond underlying relationships between features and targets to find the complex interdependence of dif- ferent dimensions of target variables. Experiments on two public benchmarks demonstrate that our method achieves competitive performance in comparison with the state-of-the-art approaches.
ER  -


TY  - Preprint
T1  - Unsupervised Feature Learning Based on Deep Models for Environmental Audio Tagging
A1  - Yong Xu
A1  - Qiang Huang
A1  - Wenwu Wang
A1  - Peter Foster
A1  - Siddharth Sigtia
A1  - Philip J. B. Jackson
A1  - Mark D. Plumbley
JO  - ArXiv e-prints
Y1  - 29 November, 2016
UR  - https://arxiv.org/abs/1607.03681
N2  - Environmental audio tagging aims to predict only the presence or absence of certain acoustic events in the interested acoustic scene. In this paper we make contributions to audio tagging in two parts, respectively, acoustic modeling and feature learning. We propose to use a shrinking deep neural network (DNN) framework incorporating unsupervised feature learning to handle the multi-label classification task. For the acoustic modeling, a large set of contextual frames of the chunk are fed into the DNN to perform a multi-label classification for the expected tags, considering that only chunk (or utterance) level rather than frame-level labels are available. Dropout and background noise aware training are also adopted to improve the generalization capability of the DNNs. For the unsupervised feature learning, we propose to use a symmetric or asymmetric deep de-noising auto-encoder (sDAE or aDAE) to generate new data-driven features from the Mel-Filter Banks (MFBs) features. The new features, which are smoothed against background noise and more compact with contextual information, can further improve the performance of the DNN baseline. Compared with the standard Gaussian Mixture Model (GMM) baseline of the DCASE 2016 audio tagging challenge, our proposed method obtains a significant equal error rate (EER) reduction from 0.21 to 0.13 on the development set. The proposed aDAE system can get a relative 6.7% EER reduction compared with the strong DNN baseline on the development set. Finally, the results also show that our approach obtains the state-of-the-art performance with 0.15 EER on the evaluation set of the DCASE 2016 audio tagging task while EER of the first prize of this challenge is 0.17.
ER  -


TY  - Preprint
T1  - Characterizing Driving Styles with Deep Learning
A1  - Weishan Dong
A1  - Jian Li
A1  - Renjie Yao
A1  - Changsheng Li
A1  - Ting Yuan
A1  - Lanjun Wang
JO  - ArXiv e-prints
Y1  - 8 October, 2016
UR  - https://arxiv.org/abs/1607.03611
N2  - Characterizing driving styles of human drivers using vehicle sensor data, e.g., GPS, is an interesting research problem and an important real-world requirement from automotive industries. A good representation of driving features can be highly valuable for autonomous driving, auto insurance, and many other application scenarios. However, traditional methods mainly rely on handcrafted features, which limit machine learning algorithms to achieve a better performance. In this paper, we propose a novel deep learning solution to this problem, which could be the first attempt of extending deep learning to driving behavior analysis based on GPS data. The proposed approach can effectively extract high level and interpretable features describing complex driving patterns. It also requires significantly less human experience and work. The power of the learned driving style representations are validated through the driver identification problem using a large real dataset.
ER  -


TY  - Preprint
T1  - Automatic Bridge Bidding Using Deep Reinforcement Learning
A1  - Chih-Kuan Yeh
A1  - Hsuan-Tien Lin
JO  - ArXiv e-prints
Y1  - 12 July, 2016
UR  - https://arxiv.org/abs/1607.03290
N2  - Bridge is among the zero-sum games for which artificial intelligence has not yet outperformed expert human players. The main difficulty lies in the bidding phase of bridge, which requires cooperative decision making under partial information. Existing artificial intelligence systems for bridge bidding rely on and are thus restricted by human-designed bidding systems or features. In this work, we propose a pioneering bridge bidding system without the aid of human domain knowledge. The system is based on a novel deep reinforcement learning model, which extracts sophisticated features and learns to bid automatically based on raw card data. The model includes an upper-confidence-bound algorithm and additional techniques to achieve a balance between exploration and exploitation. Our experiments validate the promising performance of our proposed model. In particular, the model advances from having no knowledge about bidding to achieving superior performance when compared with a champion-winning computer bridge program that implements a human-designed bidding system.
ER  -


TY  - Preprint
T1  - Deep Learning of Appearance Models for Online Object Tracking
A1  - Mengyao Zhai
A1  - Mehrsan Javan Roshtkhari
A1  - Greg Mori
JO  - ArXiv e-prints
Y1  - 9 July, 2016
UR  - https://arxiv.org/abs/1607.02568
N2  - This paper introduces a novel deep learning based approach for vision based single target tracking. We address this problem by proposing a network architecture which takes the input video frames and directly computes the tracking score for any candidate target location by estimating the probability distributions of the positive and negative examples. This is achieved by combining a deep convolutional neural network with a Bayesian loss layer in a unified framework. In order to deal with the limited number of positive training examples, the network is pre-trained offline for a generic image feature representation and then is fine-tuned in multiple steps. An online fine-tuning step is carried out at every frame to learn the appearance of the target. We adopt a two-stage iterative algorithm to adaptively update the network parameters and maintain a probability density for target/non-target regions. The tracker has been tested on the standard tracking benchmark and the results indicate that the proposed solution achieves state-of-the-art tracking results.
ER  -


TY  - Preprint
T1  - Applying Deep Learning to the Newsvendor Problem
A1  - Afshin Oroojlooyjadid
A1  - Lawrence Snyder
A1  - Martin TakÃ¡Ä
JO  - ArXiv e-prints
Y1  - 6 March, 2018
UR  - https://arxiv.org/abs/1607.02177
N2  - The newsvendor problem is one of the most basic and widely applied inventory models. There are numerous extensions of this problem. If the probability distribution of the demand is known, the problem can be solved analytically.
ER  -


TY  - Preprint
T1  - DeepChrome: Deep-learning for predicting gene expression from histone modifications
A1  - Ritambhara Singh
A1  - Jack Lanchantin
A1  - Gabriel Robins
A1  - Yanjun Qi
JO  - ArXiv e-prints
Y1  - 7 July, 2016
UR  - https://arxiv.org/abs/1607.02078
N2  - Motivation: Histone modifications are among the most important factors that control gene regulation. Computational methods that predict gene expression from histone modification signals are highly desirable for understanding their combinatorial effects in gene regulation. This knowledge can help in developing &#39;epigenetic drugs&#39; for diseases like cancer. Previous studies for quantifying the relationship between histone modifications and gene expression levels either failed to capture combinatorial effects or relied on multiple methods that separate predictions and combinatorial analysis. This paper develops a unified discriminative framework using a deep convolutional neural network to classify gene expression using histone modification data as input. Our system, called DeepChrome, allows automatic extraction of complex interactions among important features. To simultaneously visualize the combinatorial interactions among histone modifications, we propose a novel optimization-based technique that generates feature pattern maps from the learnt deep model. This provides an intuitive description of underlying epigenetic mechanisms that regulate genes. Results: We show that DeepChrome outperforms state-of-the-art models like Support Vector Machines and Random Forests for gene expression classification task on 56 different cell-types from REMC database. The output of our visualization technique not only validates the previous observations but also allows novel insights about combinatorial interactions among histone modification marks, some of which have recently been observed by experimental studies.
ER  -


TY  - Preprint
T1  - Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network
A1  - Xibin Song
A1  - Yuchao Dai
A1  - Xueying Qin
JO  - ArXiv e-prints
Y1  - 7 July, 2016
UR  - https://arxiv.org/abs/1607.01977
N2  - Depth image super-resolution is an extremely challenging task due to the information loss in sub-sampling. Deep convolutional neural network have been widely applied to color image super-resolution. Quite surprisingly, this success has not been matched to depth super-resolution. This is mainly due to the inherent difference between color and depth images. In this paper, we bridge up the gap and extend the success of deep convolutional neural network to depth super-resolution. The proposed deep depth super-resolution method learns the mapping from a low-resolution depth image to a high resolution one in an end-to-end style. Furthermore, to better regularize the learned depth map, we propose to exploit the depth field statistics and the local correlation between depth image and color image. These priors are integrated in an energy minimization formulation, where the deep neural network learns the unary term, the depth field statistics works as global model constraint and the color-depth correlation is utilized to enforce the local structure in depth images. Extensive experiments on various depth super-resolution benchmark datasets show that our method outperforms the state-of-the-art depth image super-resolution methods with a margin.
ER  -


TY  - Preprint
T1  - Iterative Multi-domain Regularized Deep Learning for Anatomical Structure Detection and Segmentation from Ultrasound Images
A1  - Hao Chen
A1  - Yefeng Zheng
A1  - Jin-Hyeong Park
A1  - Pheng-Ann Heng
A1  - S. Kevin Zhou
JO  - ArXiv e-prints
Y1  - 6 July, 2016
UR  - https://arxiv.org/abs/1607.01855
N2  - Accurate detection and segmentation of anatomical structures from ultrasound images are crucial for clinical diagnosis and biometric measurements. Although ultrasound imaging has been widely used with superiorities such as low cost and portability, the fuzzy border definition and existence of abounding artifacts pose great challenges for automatically detecting and segmenting the complex anatomical structures. In this paper, we propose a multi-domain regularized deep learning method to address this challenging problem. By leveraging the transfer learning from cross domains, the feature representations are effectively enhanced. The results are further improved by the iterative refinement. Moreover, our method is quite efficient by taking advantage of a fully convolutional network, which is formulated as an end-to-end learning framework of detection and segmentation. Extensive experimental results on a large-scale database corroborated that our method achieved a superior detection and segmentation accuracy, outperforming other methods by a significant margin and demonstrating competitive capability even compared to human performance.
ER  -


TY  - Preprint
T1  - Learning Discriminative Features using Encoder-Decoder type Deep Neural Nets
A1  - Vishwajeet Singh
A1  - Killamsetti Ravi Kumar
A1  - K Eswaran
JO  - ArXiv e-prints
Y1  - 22 March, 2016
UR  - https://arxiv.org/abs/1607.01354
N2  - As machine learning is applied to an increasing variety of complex problems, which are defined by high dimensional and complex data sets, the necessity for task oriented feature learning grows in importance. With the advancement of Deep Learning algorithms, various successful feature learning techniques have evolved. In this paper, we present a novel way of learning discriminative features by training Deep Neural Nets which have Encoder or Decoder type architecture similar to an Autoencoder. We demonstrate that our approach can learn discriminative features which can perform better at pattern classification tasks when the number of training samples is relatively small in size.
ER  -


TY  - Preprint
T1  - A Distributed Deep Representation Learning Model for Big Image Data Classification
A1  - Le Dong
A1  - Na Lv
A1  - Qianni Zhang
A1  - Shanshan Xie
A1  - Ling He
A1  - Mengdie Mao
JO  - ArXiv e-prints
Y1  - 2 July, 2016
UR  - https://arxiv.org/abs/1607.00501
N2  - This paper describes an effective and efficient image classification framework nominated distributed deep representation learning model (DDRL). The aim is to strike the balance between the computational intensive deep learning approaches (tuned parameters) which are intended for distributed computing, and the approaches that focused on the designed parameters but often limited by sequential computing and cannot scale up. In the evaluation of our approach, it is shown that DDRL is able to achieve state-of-art classification accuracy efficiently on both medium and large datasets. The result implies that our approach is more efficient than the conventional deep learning approaches, and can be applied to big data that is too complex for parameter designing focused approaches. More specifically, DDRL contains two main components, i.e., feature extraction and selection. A hierarchical distributed deep representation learning algorithm is designed to extract image statistics and a nonlinear mapping algorithm is used to map the inherent statistics into abstract features. Both algorithms are carefully designed to avoid millions of parameters tuning. This leads to a more compact solution for image classification of big data. We note that the proposed approach is designed to be friendly with parallel computing. It is generic and easy to be deployed to different distributed computing resources. In the experiments, the largescale image datasets are classified with a DDRM implementation on Hadoop MapReduce, which shows high scalability and resilience.
ER  -


TY  - Preprint
T1  - Automated 5-year Mortality Prediction using Deep Learning and Radiomics Features from Chest Computed Tomography
A1  - Gustavo Carneiro
A1  - Luke Oakden-Rayner
A1  - Andrew P. Bradley
A1  - Jacinto Nascimento
A1  - Lyle Palmer
JO  - ArXiv e-prints
Y1  - 1 July, 2016
UR  - https://arxiv.org/abs/1607.00267
N2  - We propose new methods for the prediction of 5-year mortality in elderly individuals using chest computed tomography (CT). The methods consist of a classifier that performs this prediction using a set of features extracted from the CT image and segmentation maps of multiple anatomic structures. We explore two approaches: 1) a unified framework based on deep learning, where features and classifier are automatically learned in a single optimisation process; and 2) a multi-stage framework based on the design and selection/extraction of hand-crafted radiomics features, followed by the classifier learning process. Experimental results, based on a dataset of 48 annotated chest CTs, show that the deep learning model produces a mean 5-year mortality prediction accuracy of 68.5%, while radiomics produces a mean accuracy that varies between 56% to 66% (depending on the feature selection/extraction method and classifier). The successful development of the proposed models has the potential to make a profound impact in preventive and personalised healthcare.
ER  -


TY  - Preprint
T1  - Less-forgetting Learning in Deep Neural Networks
A1  - Heechul Jung
A1  - Jeongwoo Ju
A1  - Minju Jung
A1  - Junmo Kim
JO  - ArXiv e-prints
Y1  - 1 July, 2016
UR  - https://arxiv.org/abs/1607.00122
N2  - A catastrophic forgetting problem makes deep neural networks forget the previously learned information, when learning data collected in new environments, such as by different sensors or in different light conditions. This paper presents a new method for alleviating the catastrophic forgetting problem. Unlike previous research, our method does not use any information from the source domain. Surprisingly, our method is very effective to forget less of the information in the source domain, and we show the effectiveness of our method using several experiments. Furthermore, we observed that the forgetting problem occurs between mini-batches when performing general training processes using stochastic gradient descent methods, and this problem is one of the factors that degrades generalization performance of the network. We also try to solve this problem using the proposed method. Finally, we show our less-forgetting learning method is also helpful to improve the performance of deep neural networks in terms of recognition rates.
ER  -


TY  - Preprint
T1  - Detection of concealed cars in complex cargo X-ray imagery using Deep Learning
A1  - Nicolas Jaccard
A1  - Thomas W. Rogers
A1  - Edward J. Morton
A1  - Lewis D. Griffin
JO  - ArXiv e-prints
Y1  - 9 September, 2016
UR  - https://arxiv.org/abs/1606.08078
N2  - Non-intrusive inspection systems based on X-ray radiography techniques are routinely used at transport hubs to ensure the conformity of cargo content with the supplied shipping manifest. As trade volumes increase and regulations become more stringent, manual inspection by trained operators is less and less viable due to low throughput. Machine vision techniques can assist operators in their task by automating parts of the inspection workflow. Since cars are routinely involved in trafficking, export fraud, and tax evasion schemes, they represent an attractive target for automated detection and flagging for subsequent inspection by operators. In this contribution, we describe a method for the detection of cars in X-ray cargo images based on trained-from-scratch Convolutional Neural Networks. By introducing an oversampling scheme that suitably addresses the low number of car images available for training, we achieved 100% car image classification rate for a false positive rate of 1-in-454. Cars that were partially or completely obscured by other goods, a modus operandi frequently adopted by criminals, were correctly detected. We believe that this level of performance suggests that the method is suitable for deployment in the field. It is expected that the generic object detection workflow described can be extended to other object classes given the availability of suitable training data.
ER  -


TY  - Preprint
T1  - Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles
A1  - Stefan Lee
A1  - Senthil Purushwalkam
A1  - Michael Cogswell
A1  - Viresh Ranjan
A1  - David Crandall
A1  - Dhruv Batra
JO  - ArXiv e-prints
Y1  - 5 October, 2016
UR  - https://arxiv.org/abs/1606.07839
N2  - Many practical perception systems exist within larger processes that include interactions with users or additional components capable of evaluating the quality of predicted solutions. In these contexts, it is beneficial to provide these oracle mechanisms with multiple highly likely hypotheses rather than a single prediction. In this work, we pose the task of producing multiple outputs as a learning problem over an ensemble of deep networks -- introducing a novel stochastic gradient descent based approach to minimize the loss with respect to an oracle. Our method is simple to implement, agnostic to both architecture and loss function, and parameter-free. Our approach achieves lower oracle error compared to existing methods on a wide range of tasks and deep architectures. We also show qualitatively that the diverse solutions produced often provide interpretable representations of task ambiguity.
ER  -


TY  - Preprint
T1  - Wide &amp; Deep Learning for Recommender Systems
A1  - Heng-Tze Cheng
A1  - Levent Koc
A1  - Jeremiah Harmsen
A1  - Tal Shaked
A1  - Tushar Chandra
A1  - Hrishi Aradhye
A1  - Glen Anderson
A1  - Greg Corrado
A1  - Wei Chai
A1  - Mustafa Ispir
A1  - Rohan Anil
A1  - Zakaria Haque
A1  - Lichan Hong
A1  - Vihan Jain
A1  - Xiaobing Liu
A1  - Hemal Shah
JO  - ArXiv e-prints
Y1  - 24 June, 2016
UR  - https://arxiv.org/abs/1606.07792
N2  - Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide &amp; Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide &amp; Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.
ER  -


TY  - Preprint
T1  - Deep Learning Relevance: Creating Relevant Information (as Opposed to Retrieving it)
A1  - Christina Lioma
A1  - Birger Larsen
A1  - Casper Petersen
A1  - Jakob Grue Simonsen
JO  - ArXiv e-prints
Y1  - 27 June, 2016
UR  - https://arxiv.org/abs/1606.07660
N2  - What if Information Retrieval (IR) systems did not just retrieve relevant information that is stored in their indices, but could also &#34;understand&#34; it and synthesise it into a single document? We present a preliminary study that makes a first step towards answering this question. Given a query, we train a Recurrent Neural Network (RNN) on existing relevant information to that query. We then use the RNN to &#34;deep learn&#34; a single, synthetic, and we assume, relevant document for that query. We design a crowdsourcing experiment to assess how relevant the &#34;deep learned&#34; document is, compared to existing relevant documents. Users are shown a query and four wordclouds (of three existing relevant documents and our deep learned synthetic document). The synthetic document is ranked on average most relevant of all.
ER  -


TY  - Preprint
T1  - Deep Learning Markov Random Field for Semantic Segmentation
A1  - Ziwei Liu
A1  - Xiaoxiao Li
A1  - Ping Luo
A1  - Chen Change Loy
A1  - Xiaoou Tang
JO  - ArXiv e-prints
Y1  - 8 August, 2017
UR  - https://arxiv.org/abs/1606.07230
N2  - Semantic segmentation tasks can be well modeled by Markov Random Field (MRF). This paper addresses semantic segmentation by incorporating high-order relations and mixture of label contexts into MRF. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN to model unary terms and additional layers are devised to approximate the mean field (MF) algorithm for pairwise terms. It has several appealing properties. First, different from the recent works that required many iterations of MF during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing models as its special cases. Furthermore, pairwise terms in DPN provide a unified framework to encode rich contextual information in high-dimensional data, such as images and videos. Third, DPN makes MF easier to be parallelized and speeded up, thus enabling efficient inference. DPN is thoroughly evaluated on standard semantic image/video segmentation benchmarks, where a single DPN model yields state-of-the-art segmentation accuracies on PASCAL VOC 2012, Cityscapes dataset and CamVid dataset.
ER  -


TY  - Preprint
T1  - DeepFood: Deep Learning-Based Food Image Recognition for Computer-Aided Dietary Assessment
A1  - Chang Liu
A1  - Yu Cao
A1  - Yan Luo
A1  - Guanling Chen
A1  - Vinod Vokkarane
A1  - Yunsheng Ma
JO  - ArXiv e-prints
Y1  - 17 June, 2016
UR  - https://arxiv.org/abs/1606.05675
N2  - Worldwide, in 2014, more than 1.9 billion adults, 18 years and older, were overweight. Of these, over 600 million were obese. Accurately documenting dietary caloric intake is crucial to manage weight loss, but also presents challenges because most of the current methods for dietary assessment must rely on memory to recall foods eaten. The ultimate goal of our research is to develop computer-aided technical solutions to enhance and improve the accuracy of current measurements of dietary intake. Our proposed system in this paper aims to improve the accuracy of dietary assessment by analyzing the food images captured by mobile devices (e.g., smartphone). The key technique innovation in this paper is the deep learning-based food image recognition algorithms. Substantial research has demonstrated that digital imaging accurately estimates dietary intake in many environments and it has many advantages over other methods. However, how to derive the food information (e.g., food type and portion size) from food image effectively and efficiently remains a challenging and open research problem. We propose a new Convolutional Neural Network (CNN)-based food image recognition algorithm to address this problem. We applied our proposed approach to two real-world food image data sets (UEC-256 and Food-101) and achieved impressive results. To the best of our knowledge, these results outperformed all other reported work using these two data sets. Our experiments have demonstrated that the proposed approach is a promising solution for addressing the food image recognition problem. Our future work includes further improving the performance of the algorithms and integrating our system into a real-world mobile and cloud computing-based system to enhance the accuracy of current measurements of dietary intake.
ER  -


TY  - Preprint
T1  - Learning Abstract Classes using Deep Learning
A1  - Sebastian Stabinger
A1  - Antonio Rodriguez-Sanchez
A1  - Justus Piater
JO  - ArXiv e-prints
Y1  - 17 June, 2016
UR  - https://arxiv.org/abs/1606.05506
N2  - Humans are generally good at learning abstract concepts about objects and scenes (e.g.\ spatial orientation, relative sizes, etc.). Over the last years convolutional neural networks have achieved almost human performance in recognizing concrete classes (i.e.\ specific object categories). This paper tests the performance of a current CNN (GoogLeNet) on the task of differentiating between abstract classes which are trivially differentiable for humans. We trained and tested the CNN on the two abstract classes of horizontal and vertical orientation and determined how well the network is able to transfer the learned classes to other, previously unseen objects.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning Discovers Internal Models
A1  - Nir Baram
A1  - Tom Zahavy
A1  - Shie Mannor
JO  - ArXiv e-prints
Y1  - 16 June, 2016
UR  - https://arxiv.org/abs/1606.05174
N2  - Deep Reinforcement Learning (DRL) is a trending field of research, showing great promise in challenging problems such as playing Atari, solving Go and controlling robots. While DRL agents perform well in practice we are still lacking the tools to analayze their performance. In this work we present the Semi-Aggregated MDP (SAMDP) model. A model best suited to describe policies exhibiting both spatial and temporal hierarchies. We describe its advantages for analyzing trained policies over other modeling approaches, and show that under the right state representation, like that of DQN agents, SAMDP can help to identify skills. We detail the automatic process of creating it from recorded trajectories, up to presenting it on t-SNE maps. We explain how to evaluate its fitness and show surprising results indicating high compatibility with the policy at hand. We conclude by showing how using the SAMDP model, an extra performance gain can be squeezed from the agent.
ER  -


TY  - Preprint
T1  - Deep Learning for Music
A1  - Allen Huang
A1  - Raymond Wu
JO  - ArXiv e-prints
Y1  - 15 June, 2016
UR  - https://arxiv.org/abs/1606.04930
N2  - Our goal is to be able to build a generative model from a deep neural network architecture to try to create music that has both harmony and melody and is passable as music composed by humans. Previous work in music generation has mainly been focused on creating a single melody. More recent work on polyphonic music modeling, centered around time series probability density estimation, has met some partial success. In particular, there has been a lot of work based off of Recurrent Neural Networks combined with Restricted Boltzmann Machines (RNN-RBM) and other similar recurrent energy based models. Our approach, however, is to perform end-to-end learning and generation with deep neural nets alone.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning With Macro-Actions
A1  - Ishan P. Durugkar
A1  - Clemens Rosenbaum
A1  - Stefan Dernbach
A1  - Sridhar Mahadevan
JO  - ArXiv e-prints
Y1  - 14 June, 2016
UR  - https://arxiv.org/abs/1606.04615
N2  - Deep reinforcement learning has been shown to be a powerful framework for learning policies from complex high-dimensional sensory inputs to actions in complex tasks, such as the Atari domain. In this paper, we explore output representation modeling in the form of temporal abstraction to improve convergence and reliability of deep reinforcement learning approaches. We concentrate on macro-actions, and evaluate these on different Atari 2600 games, where we show that they yield significant improvements in learning speed. Additionally, we show that they can even achieve better scores than DQN. We offer analysis and explanation for both convergence and final results, revealing a problem deep RL approaches have with sparse reward signals.
ER  -


TY  - Preprint
T1  - Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning
A1  - Mehdi Sajjadi
A1  - Mehran Javanmardi
A1  - Tolga Tasdizen
JO  - ArXiv e-prints
Y1  - 14 June, 2016
UR  - https://arxiv.org/abs/1606.04586
N2  - Effective convolutional neural networks are trained on large sets of labeled data. However, creating large labeled datasets is a very costly and time-consuming task. Semi-supervised learning uses unlabeled data to train a model with higher accuracy when there is a limited set of labeled data available. In this paper, we consider the problem of semi-supervised learning with convolutional neural networks. Techniques such as randomized data augmentation, dropout and random max-pooling provide better generalization and stability for classifiers that are trained using gradient descent. Multiple passes of an individual sample through the network might lead to different predictions due to the non-deterministic behavior of these techniques. We propose an unsupervised loss function that takes advantage of the stochastic nature of these methods and minimizes the difference between the predictions of multiple passes of a training sample through the network. We evaluate the proposed method on several benchmark datasets.
ER  -


TY  - Preprint
T1  - Omnivore: An Optimizer for Multi-device Deep Learning on CPUs and GPUs
A1  - Stefan Hadjis
A1  - Ce Zhang
A1  - Ioannis Mitliagkas
A1  - Dan Iter
A1  - Christopher RÃ©
JO  - ArXiv e-prints
Y1  - 19 October, 2016
UR  - https://arxiv.org/abs/1606.04487
N2  - We study the factors affecting training time in multi-device deep learning systems. Given a specification of a convolutional neural network, our goal is to minimize the time to train this model on a cluster of commodity CPUs and GPUs. We first focus on the single-node setting and show that by using standard batching and data-parallel techniques, throughput can be improved by at least 5.5x over state-of-the-art systems on CPUs. This ensures an end-to-end training speed directly proportional to the throughput of a device regardless of its underlying hardware, allowing each node in the cluster to be treated as a black box. Our second contribution is a theoretical and empirical study of the tradeoffs affecting end-to-end training time in a multiple-device setting. We identify the degree of asynchronous parallelization as a key factor affecting both hardware and statistical efficiency. We see that asynchrony can be viewed as introducing a momentum term. Our results imply that tuning momentum is critical in asynchronous parallel configurations, and suggest that published results that have not been fully tuned might report suboptimal performance for some configurations. For our third contribution, we use our novel understanding of the interaction between system and optimization dynamics to provide an efficient hyperparameter optimizer. Our optimizer involves a predictive model for the total time to convergence and selects an allocation of resources to minimize that time. We demonstrate that the most popular distributed deep learning systems fall within our tradeoff space, but do not optimize within the space. By doing this optimization, our prototype runs 1.9x to 12x faster than the fastest state-of-the-art systems.
ER  -


TY  - Preprint
T1  - Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge
A1  - Luciano Serafini
A1  - Artur d&#39;Avila Garcez
JO  - ArXiv e-prints
Y1  - 7 July, 2016
UR  - https://arxiv.org/abs/1606.04422
N2  - We propose Logic Tensor Networks: a uniform framework for integrating automatic learning and reasoning. A logic formalism called Real Logic is defined on a first-order language whereby formulas have truth-value in the interval [0,1] and semantics defined concretely on the domain of real numbers. Logical constants are interpreted as feature vectors of real numbers. Real Logic promotes a well-founded integration of deductive reasoning on a knowledge-base and efficient data-driven relational machine learning. We show how Real Logic can be implemented in deep Tensor Neural Networks with the use of Google&#39;s tensorflow primitives. The paper concludes with experiments applying Logic Tensor Networks on a simple but representative example of knowledge completion.
ER  -


TY  - Preprint
T1  - Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural Networks
A1  - Mohammad Javad Shafiee
A1  - Akshaya Mishra
A1  - Alexander Wong
JO  - ArXiv e-prints
Y1  - 6 February, 2017
UR  - https://arxiv.org/abs/1606.04393
N2  - Taking inspiration from biological evolution, we explore the idea of &#34;Can deep neural networks evolve naturally over successive generations into highly efficient deep neural networks?&#34; by introducing the notion of synthesizing new highly efficient, yet powerful deep neural networks over successive generations via an evolutionary process from ancestor deep neural networks. The architectural traits of ancestor deep neural networks are encoded using synaptic probability models, which can be viewed as the `DNA&#39; of these networks. New descendant networks with differing network architectures are synthesized based on these synaptic probability models from the ancestor networks and computational environmental factor models, in a random manner to mimic heredity, natural selection, and random mutation. These offspring networks are then trained into fully functional networks, like one would train a newborn, and have more efficient, more diverse network architectures than their ancestor networks, while achieving powerful modeling capabilities. Experimental results for the task of visual saliency demonstrated that the synthesized `evolved&#39; offspring networks can achieve state-of-the-art performance while having network architectures that are significantly more efficient (with a staggering $\sim$48-fold decrease in synapses by the fourth generation) compared to the original ancestor network.
ER  -


TY  - Preprint
T1  - Neither Quick Nor Proper -- Evaluation of QuickProp for Learning Deep Neural Networks
A1  - Clemens-Alexander Brust
A1  - Sven Sickert
A1  - Marcel Simon
A1  - Erik Rodner
A1  - Joachim Denzler
JO  - ArXiv e-prints
Y1  - 15 June, 2016
UR  - https://arxiv.org/abs/1606.04333
N2  - Neural networks and especially convolutional neural networks are of great interest in current computer vision research. However, many techniques, extensions, and modifications have been published in the past, which are not yet used by current approaches. In this paper, we study the application of a method called QuickProp for training of deep neural networks. In particular, we apply QuickProp during learning and testing of fully convolutional networks for the task of semantic segmentation. We compare QuickProp empirically with gradient descent, which is the current standard method. Experiments suggest that QuickProp can not compete with standard gradient descent techniques for complex computer vision tasks like semantic segmentation.
ER  -


TY  - Preprint
T1  - Trace Norm Regularised Deep Multi-Task Learning
A1  - Yongxin Yang
A1  - Timothy M. Hospedales
JO  - ArXiv e-prints
Y1  - 16 February, 2017
UR  - https://arxiv.org/abs/1606.04038
N2  - We propose a framework for training multiple neural networks simultaneously. The parameters from all models are regularised by the tensor trace norm, so that each neural network is encouraged to reuse others&#39; parameters if possible -- this is the main motivation behind multi-task learning. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning with a Combinatorial Action Space for Predicting Popular Reddit Threads
A1  - Ji He
A1  - Mari Ostendorf
A1  - Xiaodong He
A1  - Jianshu Chen
A1  - Jianfeng Gao
A1  - Lihong Li
A1  - Li Deng
JO  - ArXiv e-prints
Y1  - 16 September, 2016
UR  - https://arxiv.org/abs/1606.03667
N2  - We introduce an online popularity prediction and tracking task as a benchmark task for reinforcement learning with a combinatorial, natural language action space. A specified number of discussion threads predicted to be popular are recommended, chosen from a fixed window of recent comments to track. Novel deep reinforcement learning architectures are studied for effective modeling of the value function associated with actions comprised of interdependent sub-actions. The proposed model, which represents dependence between sub-actions through a bi-directional LSTM, gives the best performance across different experimental configurations and domains, and it also generalizes well with varying numbers of recommendation requests.
ER  -


TY  - Preprint
T1  - Mutual Exclusivity Loss for Semi-Supervised Deep Learning
A1  - Mehdi Sajjadi
A1  - Mehran Javanmardi
A1  - Tolga Tasdizen
JO  - ArXiv e-prints
Y1  - 9 June, 2016
UR  - https://arxiv.org/abs/1606.03141
N2  - In this paper we consider the problem of semi-supervised learning with deep Convolutional Neural Networks (ConvNets). Semi-supervised learning is motivated on the observation that unlabeled data is cheap and can be used to improve the accuracy of classifiers. In this paper we propose an unsupervised regularization term that explicitly forces the classifier&#39;s prediction for multiple classes to be mutually-exclusive and effectively guides the decision boundary to lie on the low density space between the manifolds corresponding to different classes of data. Our proposed approach is general and can be used with any backpropagation-based learning method. We show through different experiments that our method can improve the object recognition performance of ConvNets using unlabeled data.
ER  -


TY  - Preprint
T1  - Apparent Age Estimation Using Ensemble of Deep Learning Models
A1  - Refik Can Malli
A1  - Mehmet Aygun
A1  - Hazim Kemal Ekenel
JO  - ArXiv e-prints
Y1  - 9 June, 2016
UR  - https://arxiv.org/abs/1606.02909
N2  - In this paper, we address the problem of apparent age estimation. Different from estimating the real age of individuals, in which each face image has a single age label, in this problem, face images have multiple age labels, corresponding to the ages perceived by the annotators, when they look at these images. This provides an intriguing computer vision problem, since in generic image or object classification tasks, it is typical to have a single ground truth label per class. To account for multiple labels per image, instead of using average age of the annotated face image as the class label, we have grouped the face images that are within a specified age range. Using these age groups and their age-shifted groupings, we have trained an ensemble of deep learning models. Before feeding an input face image to a deep learning model, five facial landmark points are detected and used for 2-D alignment. We have employed and fine tuned convolutional neural networks (CNNs) that are based on VGG-16 [24] architecture and pretrained on the IMDB-WIKI dataset [22]. The outputs of these deep learning models are then combined to produce the final estimation. Proposed method achieves 0.3668 error in the final ChaLearn LAP 2016 challenge test set [5].
ER  -


TY  - Preprint
T1  - A Comprehensive Analysis of Deep Learning Based Representation for Face Recognition
A1  - Mostafa Mehdipour Ghazi
A1  - Hazim Kemal Ekenel
JO  - ArXiv e-prints
Y1  - 9 June, 2016
UR  - https://arxiv.org/abs/1606.02894
N2  - Deep learning based approaches have been dominating the face recognition field due to the significant performance improvement they have provided on the challenging wild datasets. These approaches have been extensively tested on such unconstrained datasets, on the Labeled Faces in the Wild and YouTube Faces, to name a few. However, their capability to handle individual appearance variations caused by factors such as head pose, illumination, occlusion, and misalignment has not been thoroughly assessed till now. In this paper, we present a comprehensive study to evaluate the performance of deep learning based face representation under several conditions including the varying head pose angles, upper and lower face occlusion, changing illumination of different strengths, and misalignment due to erroneous facial feature localization. Two successful and publicly available deep learning models, namely VGG-Face and Lightened CNN have been utilized to extract face representations. The obtained results show that although deep learning provides a powerful representation for face recognition, it can still benefit from preprocessing, for example, for pose and illumination normalization in order to achieve better performance under various conditions. Particularly, if these variations are not included in the dataset used to train the deep learning model, the role of preprocessing becomes more crucial. Experimental results also show that deep learning based representation is robust to misalignment and can tolerate facial feature localization errors up to 10% of the interocular distance.
ER  -


TY  - Preprint
T1  - Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning
A1  - Tiancheng Zhao
A1  - Maxine Eskenazi
JO  - ArXiv e-prints
Y1  - 15 September, 2016
UR  - https://arxiv.org/abs/1606.02560
N2  - This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent Q-Networks (DRQN). The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state.
ER  -


TY  - Preprint
T1  - Structured Convolution Matrices for Energy-efficient Deep learning
A1  - Rathinakumar Appuswamy
A1  - Tapan Nayak
A1  - John Arthur
A1  - Steven Esser
A1  - Paul Merolla
A1  - Jeffrey Mckinstry
A1  - Timothy Melano
A1  - Myron Flickner
A1  - Dharmendra Modha
JO  - ArXiv e-prints
Y1  - 8 June, 2016
UR  - https://arxiv.org/abs/1606.02407
N2  - We derive a relationship between network representation in energy-efficient neuromorphic architectures and block Toplitz convolutional matrices. Inspired by this connection, we develop deep convolutional networks using a family of structured convolutional matrices and achieve state-of-the-art trade-off between energy efficiency and classification accuracy for well-known image recognition tasks. We also put forward a novel method to train binary convolutional networks by utilising an existing connection between noisy-rectified linear units and binary activations.
ER  -


TY  - Preprint
T1  - Deep Learning Convolutional Networks for Multiphoton Microscopy Vasculature Segmentation
A1  - Petteri Teikari
A1  - Marc Santos
A1  - Charissa Poon
A1  - Kullervo Hynynen
JO  - ArXiv e-prints
Y1  - 7 June, 2016
UR  - https://arxiv.org/abs/1606.02382
N2  - Recently there has been an increasing trend to use deep learning frameworks for both 2D consumer images and for 3D medical images. However, there has been little effort to use deep frameworks for volumetric vascular segmentation. We wanted to address this by providing a freely available dataset of 12 annotated two-photon vasculature microscopy stacks. We demonstrated the use of deep learning framework consisting both 2D and 3D convolutional filters (ConvNet). Our hybrid 2D-3D architecture produced promising segmentation result. We derived the architectures from Lee et al. who used the ZNN framework initially designed for electron microscope image segmentation. We hope that by sharing our volumetric vasculature datasets, we will inspire other researchers to experiment with vasculature dataset and improve the used network architectures.
ER  -


TY  - Preprint
T1  - SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks
A1  - Arunkumar Byravan
A1  - Dieter Fox
JO  - ArXiv e-prints
Y1  - 30 March, 2017
UR  - https://arxiv.org/abs/1606.02378
N2  - We introduce SE3-Nets, which are deep neural networks designed to model and learn rigid body motion from raw point cloud data. Based only on sequences of depth images along with action vectors and point wise data associations, SE3-Nets learn to segment effected object parts and predict their motion resulting from the applied force. Rather than learning point wise flow vectors, SE3-Nets predict SE3 transformations for different parts of the scene. Using simulated depth data of a table top scene and a robot manipulator, we show that the structure underlying SE3-Nets enables them to generate a far more consistent prediction of object motion than traditional flow based networks. Additional experiments with a depth camera observing a Baxter robot pushing objects on a table show that SE3-Nets also work well on real data.
ER  -


TY  - Preprint
T1  - Learning deep structured network for weakly supervised change detection
A1  - Salman H Khan
A1  - Xuming He
A1  - Fatih Porikli
A1  - Mohammed Bennamoun
A1  - Ferdous Sohel
A1  - Roberto Togneri
JO  - ArXiv e-prints
Y1  - 22 May, 2017
UR  - https://arxiv.org/abs/1606.02009
N2  - Conventional change detection methods require a large number of images to learn background models or depend on tedious pixel-level labeling by humans. In this paper, we present a weakly supervised approach that needs only image-level labels to simultaneously detect and localize changes in a pair of images. To this end, we employ a deep neural network with DAG topology to learn patterns of change from image-level labeled training data. On top of the initial CNN activations, we define a CRF model to incorporate the local differences and context with the dense connections between individual pixels. We apply a constrained mean-field algorithm to estimate the pixel-level labels, and use the estimated labels to update the parameters of the CNN in an iterative EM framework. This enables imposing global constraints on the observed foreground probability mass function. Our evaluations on four benchmark datasets demonstrate superior detection and localization performance.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning for Dialogue Generation
A1  - Jiwei Li
A1  - Will Monroe
A1  - Alan Ritter
A1  - Michel Galley
A1  - Jianfeng Gao
A1  - Dan Jurafsky
JO  - ArXiv e-prints
Y1  - 29 September, 2016
UR  - https://arxiv.org/abs/1606.01541
N2  - Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.
ER  -


TY  - Preprint
T1  - A Deep Learning Approach to Block-based Compressed Sensing of Images
A1  - Amir Adler
A1  - David Boublil
A1  - Michael Elad
A1  - Michael Zibulevsky
JO  - ArXiv e-prints
Y1  - 5 June, 2016
UR  - https://arxiv.org/abs/1606.01519
N2  - Compressed sensing (CS) is a signal processing framework for efficiently reconstructing a signal from a small number of measurements, obtained by linear projections of the signal. Block-based CS is a lightweight CS approach that is mostly suitable for processing very high-dimensional images and videos: it operates on local patches, employs a low-complexity reconstruction operator and requires significantly less memory to store the sensing matrix. In this paper we present a deep learning approach for block-based CS, in which a fully-connected network performs both the block-based linear sensing and non-linear reconstruction stages. During the training phase, the sensing matrix and the non-linear reconstruction operator are \emph{jointly} optimized, and the proposed approach outperforms state-of-the-art both in terms of reconstruction quality and computation time. For example, at a 25% sensing rate the average PSNR advantage is 0.77dB and computation time is over 200-times faster.
ER  -


TY  - Preprint
T1  - Exploiting Multi-typed Treebanks for Parsing with Deep Multi-task Learning
A1  - Jiang Guo
A1  - Wanxiang Che
A1  - Haifeng Wang
A1  - Ting Liu
JO  - ArXiv e-prints
Y1  - 3 June, 2016
UR  - https://arxiv.org/abs/1606.01161
N2  - Various treebanks have been released for dependency parsing. Despite that treebanks may belong to different languages or have different annotation schemes, they contain syntactic knowledge that is potential to benefit each other. This paper presents an universal framework for exploiting these multi-typed treebanks to improve parsing with deep multi-task learning. We consider two kinds of treebanks as source: the multilingual universal treebanks and the monolingual heterogeneous treebanks. Multiple treebanks are trained jointly and interacted with multi-level parameter sharing. Experiments on several benchmark datasets in various languages demonstrate that our approach can make effective use of arbitrary source treebanks to improve target parsing models.
ER  -


TY  - Preprint
T1  - The use of deep learning in image segmentation, classification and detection
A1  - M. S. Badea
A1  - I. I. Felea
A1  - L. M. Florea
A1  - C. Vertan
JO  - ArXiv e-prints
Y1  - 31 May, 2016
UR  - https://arxiv.org/abs/1605.09612
N2  - Recent years have shown that deep learned neural networks are a valuable tool in the field of computer vision. This paper addresses the use of two different kinds of network architectures, namely LeNet and Network in Network (NiN). They will be compared in terms of both performance and computational efficiency by addressing the classification and detection problems. In this paper, multiple databases will be used to test the networks. One of them contains images depicting burn wounds from pediatric cases, another one contains an extensive number of art images and other facial databases were used for facial keypoints detection.
ER  -


TY  - Preprint
T1  - Adaptive Learning Rate via Covariance Matrix Based Preconditioning for Deep Neural Networks
A1  - Yasutoshi Ida
A1  - Yasuhiro Fujiwara
A1  - Sotetsu Iwamura
JO  - ArXiv e-prints
Y1  - 28 September, 2017
UR  - https://arxiv.org/abs/1605.09593
N2  - Adaptive learning rate algorithms such as RMSProp are widely used for training deep neural networks. RMSProp offers efficient training since it uses first order gradients to approximate Hessian-based preconditioning. However, since the first order gradients include noise caused by stochastic optimization, the approximation may be inaccurate. In this paper, we propose a novel adaptive learning rate algorithm called SDProp. Its key idea is effective handling of the noise by preconditioning based on covariance matrix. For various neural networks, our approach is more efficient and effective than RMSProp and its variant.
ER  -


TY  - Preprint
T1  - Robust Deep-Learning-Based Road-Prediction for Augmented Reality Navigation Systems
A1  - Matthias Limmer
A1  - Julian Forster
A1  - Dennis Baudach
A1  - Florian SchÃ¼le
A1  - Roland Schweiger
A1  - Hendrik P. A. Lensch
JO  - ArXiv e-prints
Y1  - 31 May, 2016
UR  - https://arxiv.org/abs/1605.09533
N2  - This paper proposes an approach that predicts the road course from camera sensors leveraging deep learning techniques. Road pixels are identified by training a multi-scale convolutional neural network on a large number of full-scene-labeled night-time road images including adverse weather conditions. A framework is presented that applies the proposed approach to longer distance road course estimation, which is the basis for an augmented reality navigation application. In this framework long range sensor data (radar) and data from a map database are fused with short range sensor data (camera) to produce a precise longitudinal and lateral localization and road course estimation. The proposed approach reliably detects roads with and without lane markings and thus increases the robustness and availability of road course estimations and augmented reality navigation. Evaluations on an extensive set of high precision ground truth data taken from a differential GPS and an inertial measurement unit show that the proposed approach reaches state-of-the-art performance without the limitation of requiring existing lane markings.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning Radio Control and Signal Detection with KeRLym, a Gym RL Agent
A1  - Timothy J. O&#39;Shea
A1  - T. Charles Clancy
JO  - ArXiv e-prints
Y1  - 30 May, 2016
UR  - https://arxiv.org/abs/1605.09221
N2  - This paper presents research in progress investigating the viability and adaptation of reinforcement learning using deep neural network based function approximation for the task of radio control and signal detection in the wireless domain. We demonstrate a successful initial method for radio control which allows naive learning of search without the need for expert features, heuristics, or search strategies. We also introduce Kerlym, an open Keras based reinforcement learning agent collection for OpenAI&#39;s Gym.
ER  -


TY  - Preprint
T1  - Deep API Learning
A1  - Xiaodong Gu
A1  - Hongyu Zhang
A1  - Dongmei Zhang
A1  - Sunghun Kim
JO  - ArXiv e-prints
Y1  - 13 July, 2017
UR  - https://arxiv.org/abs/1605.08535
N2  - Developers often wonder how to implement a certain functionality (e.g., how to parse XML files) using APIs. Obtaining an API usage sequence based on an API-related natural language query is very helpful in this regard. Given a query, existing approaches utilize information retrieval models to search for matching API sequences. These approaches treat queries and APIs as bag-of-words (i.e., keyword matching or word-to-word alignment) and lack a deep understanding of the semantics of the query.
ER  -


TY  - Preprint
T1  - Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning
A1  - William Lotter
A1  - Gabriel Kreiman
A1  - David Cox
JO  - ArXiv e-prints
Y1  - 28 February, 2017
UR  - https://arxiv.org/abs/1605.08104
N2  - While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (&#34;PredNet&#34;) architecture that is inspired by the concept of &#34;predictive coding&#34; from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.
ER  -


TY  - Preprint
T1  - Self Paced Deep Learning for Weakly Supervised Object Detection
A1  - Enver Sangineto
A1  - Moin Nabi
A1  - Dubravko Culibrk
A1  - Nicu Sebe
JO  - ArXiv e-prints
Y1  - 21 February, 2018
UR  - https://arxiv.org/abs/1605.07651
N2  - In a weakly-supervised scenario object detectors need to be trained using image-level annotation alone. Since bounding-box-level ground truth is not available, most of the solutions proposed so far are based on an iterative, Multiple Instance Learning framework in which the current classifier is used to select the highest-confidence boxes in each image, which are treated as pseudo-ground truth in the next training iteration. However, the errors of an immature classifier can make the process drift, usually introducing many of false positives in the training dataset. To alleviate this problem, we propose in this paper a training protocol based on the self-paced learning paradigm. The main idea is to iteratively select a subset of images and boxes that are the most reliable, and use them for training. While in the past few years similar strategies have been adopted for SVMs and other classifiers, we are the first showing that a self-paced approach can be used with deep-network-based classifiers in an end-to-end training pipeline. The method we propose is built on the fully-supervised Fast-RCNN architecture and can be applied to similar architectures which represent the input image as a bag of boxes. We show state-of-the-art results on Pascal VOC 2007, Pascal VOC 2010 and ILSVRC 2013. On ILSVRC 2013 our results based on a low-capacity AlexNet network outperform even those weakly-supervised approaches which are based on much higher-capacity networks.
ER  -


TY  - Preprint
T1  - Generative Choreography using Deep Learning
A1  - Luka Crnkovic-Friis
A1  - Louise Crnkovic-Friis
JO  - ArXiv e-prints
Y1  - 23 May, 2016
UR  - https://arxiv.org/abs/1605.06921
N2  - Recent advances in deep learning have enabled the extraction of high-level features from raw sensor data which has opened up new possibilities in many different fields, including computer generated choreography. In this paper we present a system chor-rnn for generating novel choreographic material in the nuanced choreographic language and style of an individual choreographer. It also shows promising results in producing a higher level compositional cohesion, rather than just generating sequences of movement. At the core of chor-rnn is a deep recurrent neural network trained on raw motion capture data and that can generate new dance sequences for a solo dancer. Chor-rnn can be used for collaborative human-machine choreography or as a creative catalyst, serving as inspiration for a choreographer.
ER  -


TY  - Preprint
T1  - DLAU: A Scalable Deep Learning Accelerator Unit on FPGA
A1  - Chao Wang
A1  - Qi Yu
A1  - Lei Gong
A1  - Xi Li
A1  - Yuan Xie
A1  - Xuehai Zhou
JO  - ArXiv e-prints
Y1  - 23 May, 2016
UR  - https://arxiv.org/abs/1605.06894
N2  - As the emerging field of machine learning, deep learning shows excellent ability in solving complex learning problems. However, the size of the networks becomes increasingly large scale due to the demands of the practical applications, which poses significant challenge to construct a high performance implementations of deep learning neural networks. In order to improve the performance as well to maintain the low power cost, in this paper we design DLAU, which is a scalable accelerator architecture for large-scale deep learning networks using FPGA as the hardware prototype. The DLAU accelerator employs three pipelined processing units to improve the throughput and utilizes tile techniques to explore locality for deep learning applications. Experimental results on the state-of-the-art Xilinx FPGA board demonstrate that the DLAU accelerator is able to achieve up to 36.1x speedup comparing to the Intel Core2 processors, with the power consumption at 234mW.
ER  -


TY  - Preprint
T1  - Learning to Communicate with Deep Multi-Agent Reinforcement Learning
A1  - Jakob N. Foerster
A1  - Yannis M. Assael
A1  - Nando de Freitas
A1  - Shimon Whiteson
JO  - ArXiv e-prints
Y1  - 24 May, 2016
UR  - https://arxiv.org/abs/1605.06676
N2  - We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.
ER  -


TY  - Preprint
T1  - Deep Transfer Learning with Joint Adaptation Networks
A1  - Mingsheng Long
A1  - Han Zhu
A1  - Jianmin Wang
A1  - Michael I. Jordan
JO  - ArXiv e-prints
Y1  - 17 August, 2017
UR  - https://arxiv.org/abs/1605.06636
N2  - Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets.
ER  -


TY  - Preprint
T1  - Swapout: Learning an ensemble of deep architectures
A1  - Saurabh Singh
A1  - Derek Hoiem
A1  - David Forsyth
JO  - ArXiv e-prints
Y1  - 20 May, 2016
UR  - https://arxiv.org/abs/1605.06465
N2  - We describe Swapout, a new stochastic training method, that outperforms ResNets of identical network structure yielding impressive results on CIFAR-10 and CIFAR-100. Swapout samples from a rich set of architectures including dropout, stochastic depth and residual architectures as special cases. When viewed as a regularization method swapout not only inhibits co-adaptation of units in a layer, similar to dropout, but also across network layers. We conjecture that swapout achieves strong regularization by implicitly tying the parameters across layers. When viewed as an ensemble training method, it samples a much richer set of architectures than existing methods such as dropout or stochastic depth. We propose a parameterization that reveals connections to exiting architectures and suggests a much richer set of architectures to be explored. We show that our formulation suggests an efficient training method and validate our conclusions on CIFAR-10 and CIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider model performs similar to a 1001 layer ResNet model.
ER  -


TY  - Preprint
T1  - Deep Multi-task Representation Learning: A Tensor Factorisation Approach
A1  - Yongxin Yang
A1  - Timothy Hospedales
JO  - ArXiv e-prints
Y1  - 16 February, 2017
UR  - https://arxiv.org/abs/1605.06391
N2  - Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.
ER  -


TY  - Preprint
T1  - Learning Deep Representations of Fine-grained Visual Descriptions
A1  - Scott Reed
A1  - Zeynep Akata
A1  - Bernt Schiele
A1  - Honglak Lee
JO  - ArXiv e-prints
Y1  - 17 May, 2016
UR  - https://arxiv.org/abs/1605.05395
N2  - State-of-the-art methods for zero-shot visual recognition formulate learning as a joint embedding problem of images and side information. In these formulations the current best complement to visual features are attributes: manually encoded vectors describing shared characteristics among categories. Despite good performance, attributes have limitations: (1) finer-grained recognition requires commensurately more attributes, and (2) attributes do not provide a natural language interface. We propose to overcome these limitations by training neural language models from scratch; i.e. without pre-training and only consuming words and characters. Our proposed models train end-to-end to align with the fine-grained and category-specific content of images. Natural language provides a flexible and compact way of encoding only the salient visual aspects for distinguishing categories. By training on raw text, our model can do inference on raw text as well, providing humans a familiar mode both for annotation and retrieval. Our model achieves strong performance on zero-shot text-based image retrieval and significantly outperforms the attribute-based state-of-the-art for zero-shot classification on the Caltech UCSD Birds 200-2011 dataset.
ER  -


TY  - Preprint
T1  - Deep Action Sequence Learning for Causal Shape Transformation
A1  - Kin Gwn Lore
A1  - Daniel Stoecklein
A1  - Michael Davies
A1  - Baskar Ganapathysubramanian
A1  - Soumik Sarkar
JO  - ArXiv e-prints
Y1  - 8 November, 2016
UR  - https://arxiv.org/abs/1605.05368
N2  - Deep learning became the method of choice in recent year for solving a wide variety of predictive analytics tasks. For sequence prediction, recurrent neural networks (RNN) are often the go-to architecture for exploiting sequential information where the output is dependent on previous computation. However, the dependencies of the computation lie in the latent domain which may not be suitable for certain applications involving the prediction of a step-wise transformation sequence that is dependent on the previous computation only in the visible domain. We propose that a hybrid architecture of convolution neural networks (CNN) and stacked autoencoders (SAE) is sufficient to learn a sequence of actions that nonlinearly transforms an input shape or distribution into a target shape or distribution with the same support. While such a framework can be useful in a variety of problems such as robotic path planning, sequential decision-making in games, and identifying material processing pathways to achieve desired microstructures, the application of the framework is exemplified by the control of fluid deformations in a microfluidic channel by deliberately placing a sequence of pillars. Learning of a multistep topological transform has significant implications for rapid advances in material science and biomedical applications.
ER  -


TY  - Preprint
T1  - DeepLearningKit - an GPU Optimized Deep Learning Framework for Apple&#39;s iOS, OS X and tvOS developed in Metal and Swift
A1  - Amund Tveit
A1  - TorbjÃ¸rn Morland
A1  - Thomas Brox RÃ¸st
JO  - ArXiv e-prints
Y1  - 15 May, 2016
UR  - https://arxiv.org/abs/1605.04614
N2  - In this paper we present DeepLearningKit - an open source framework that supports using pretrained deep learning models (convolutional neural networks) for iOS, OS X and tvOS. DeepLearningKit is developed in Metal in order to utilize the GPU efficiently and Swift for integration with applications, e.g. iOS-based mobile apps on iPhone/iPad, tvOS-based apps for the big screen, or OS X desktop applications. The goal is to support using deep learning models trained with popular frameworks such as Caffe, Torch, TensorFlow, Theano, Pylearn, Deeplearning4J and Mocha. Given the massive GPU resources and time required to train Deep Learning models we suggest an App Store like model to distribute and download pretrained and reusable Deep Learning models.
ER  -


TY  - Preprint
T1  - LightNet: A Versatile, Standalone Matlab-based Environment for Deep Learning
A1  - Chengxi Ye
A1  - Chen Zhao
A1  - Yezhou Yang
A1  - Cornelia Fermuller
A1  - Yiannis Aloimonos
JO  - ArXiv e-prints
Y1  - 2 August, 2016
UR  - https://arxiv.org/abs/1605.02766
N2  - LightNet is a lightweight, versatile and purely Matlab-based deep learning framework. The idea underlying its design is to provide an easy-to-understand, easy-to-use and efficient computational platform for deep learning research. The implemented framework supports major deep learning architectures such as Multilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). The framework also supports both CPU and GPU computation, and the switch between them is straightforward. Different applications in computer vision, natural language processing and robotics are demonstrated as experiments.
ER  -


TY  - Preprint
T1  - Ask Your Neurons: A Deep Learning Approach to Visual Question Answering
A1  - Mateusz Malinowski
A1  - Marcus Rohrbach
A1  - Mario Fritz
JO  - ArXiv e-prints
Y1  - 24 November, 2016
UR  - https://arxiv.org/abs/1605.02697
N2  - We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Ask Your Neurons, a scalable, jointly trained, end-to-end formulation to this problem.
ER  -


TY  - Preprint
T1  - Distributed stochastic optimization for deep learning (thesis)
A1  - Sixin Zhang
JO  - ArXiv e-prints
Y1  - 7 May, 2016
UR  - https://arxiv.org/abs/1605.02216
N2  - We study the problem of how to distribute the training of large-scale deep learning models in the parallel computing environment. We propose a new distributed stochastic optimization method called Elastic Averaging SGD (EASGD). We analyze the convergence rate of the EASGD method in the synchronous scenario and compare its stability condition with the existing ADMM method in the round-robin scheme. An asynchronous and momentum variant of the EASGD method is applied to train deep convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Our approach accelerates the training and furthermore achieves better test accuracy. It also requires a much smaller amount of communication than other common baseline approaches such as the DOWNPOUR method.
ER  -


TY  - Preprint
T1  - Accelerating Deep Learning with Shrinkage and Recall
A1  - Shuai Zheng
A1  - Abhinav Vishnu
A1  - Chris Ding
JO  - ArXiv e-prints
Y1  - 19 September, 2016
UR  - https://arxiv.org/abs/1605.01369
N2  - Deep Learning is a very powerful machine learning model. Deep Learning trains a large number of parameters for multiple layers and is very slow when data is in large scale and the architecture size is large. Inspired from the shrinking technique used in accelerating computation of Support Vector Machines (SVM) algorithm and screening technique used in LASSO, we propose a shrinking Deep Learning with recall (sDLr) approach to speed up deep learning computation. We experiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network (DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data sets. Results show that the speedup using shrinking Deep Learning with recall (sDLr) can reach more than 2.0 while still giving competitive classification performance.
ER  -


TY  - Preprint
T1  - Unsupervised Total Variation Loss for Semi-supervised Deep Learning of Semantic Segmentation
A1  - Mehran Javanmardi
A1  - Mehdi Sajjadi
A1  - Ting Liu
A1  - Tolga Tasdizen
JO  - ArXiv e-prints
Y1  - 7 August, 2018
UR  - https://arxiv.org/abs/1605.01368
N2  - We introduce a novel unsupervised loss function for learning semantic segmentation with deep convolutional neural nets (ConvNet) when densely labeled training images are not available. More specifically, the proposed loss function penalizes the L1-norm of the gradient of the label probability vector image , i.e. total variation, produced by the ConvNet. This can be seen as a regularization term that promotes piecewise smoothness of the label probability vector image produced by the ConvNet during learning. The unsupervised loss function is combined with a supervised loss in a semi-supervised setting to learn ConvNets that can achieve high semantic segmentation accuracy even when only a tiny percentage of the pixels in the training images are labeled. We demonstrate significant improvements over the purely supervised setting in the Weizmann horse, Stanford background and Sift Flow datasets. Furthermore, we show that using the proposed piecewise smoothness constraint in the learning phase significantly outperforms post-processing results from a purely supervised approach with Markov Random Fields (MRF). Finally, we note that the framework we introduce is general and can be used to learn to label other types of structures such as curvilinear structures by modifying the unsupervised loss function accordingly.
ER  -


TY  - Preprint
T1  - Phase 3: DCL System Using Deep Learning Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals - Bioacoustic Applicaitons
A1  - Peter J. Dugan
A1  - Christopher W. Clark
A1  - Yann AndrÃ© LeCun
A1  - Sofie M. Van Parijs
JO  - ArXiv e-prints
Y1  - 5 May, 2016
UR  - https://arxiv.org/abs/1605.00983
N2  - Goals of this research phase is to investigate advanced detection and classification pardims useful for data-mining passive large passive acoustic archives. Technical objectives are to develop and refine a High Performance Computing, Acoustic Data Accelerator (HPC-ADA) along with MATLAB based software based on time series acoustic signal Detection cLassification using Machine learning Algorithms, called DeLMA. Data scientists and biologists integrate to use the HPC-ADA and DeLMA technologies to explore data using newly developed techniques aimed at inspection of data extracted at large spatial and temporal scales.
ER  -


TY  - Preprint
T1  - Phase 4: DCL System Using Deep Learning Approaches for Land-Based or Ship-Based Real-Time Recognition and Localization of Marine Mammals - Distributed Processing and Big Data Applications
A1  - Peter J. Dugan
A1  - Christopher W. Clark
A1  - Yann AndrÃ© LeCun
A1  - Sofie M. Van Parijs
JO  - ArXiv e-prints
Y1  - 5 May, 2016
UR  - https://arxiv.org/abs/1605.00982
N2  - While the animal bioacoustics community at large is collecting huge amounts of acoustic data at an unprecedented pace, processing these data is problematic. Currently in bioacoustics, there is no effective way to achieve high performance computing using commericial off the shelf (COTS) or government off the shelf (GOTS) tools. Although several advances have been made in the open source and commercial software community, these offerings either support specific applications that do not integrate well with data formats in bioacoustics or they are too general. Furthermore, complex algorithms that use deep learning strategies require special considerations, such as very large libraiers of exemplars (whale sounds) readily available for algorithm training and testing. Detection-classification for passive acoustics is a data-mining strategy and our goals are aligned with best practices that appeal to the general data mining and machine learning communities where the problem of processing large data is common. Therefore, the objective of this work is to advance the state-of-the art for data-mining large passive acoustic datasets as they pertain to bioacoustics. With this basic deficiency recognized at the forefront, portions of the grant were dedicated to fostering deep-learning by way of international competitions (kaggle.com) meant to attract deep-learning solutions. The focus of this early work was targeted to make significant progress in addressing big data systems and advanced algorithms over the duration of the grant from 2012 to 2015. This early work provided simulataneous advances in systems-algorithms research while supporting various collaborations and projects.
ER  -


TY  - Preprint
T1  - Phase 2: DCL System Using Deep Learning Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals - Machine Learning Detection Algorithms
A1  - Peter J. Dugan
A1  - Christopher W. Clark
A1  - Yann AndrÃ© LeCun
A1  - Sofie M. Van Parijs
JO  - ArXiv e-prints
Y1  - 5 May, 2016
UR  - https://arxiv.org/abs/1605.00972
N2  - Overarching goals for this work aim to advance the state of the art for detection, classification and localization (DCL) in the field of bioacoustics. This goal is primarily achieved by building a generic framework for detection-classification (DC) using a fast, efficient and scalable architecture, demonstrating the capabilities of this system using on a variety of low-frequency mid-frequency cetacean sounds. Two primary goals are to develop transferable technologies for detection and classification in, one: the area of advanced algorithms, such as deep learning and other methods; and two: advanced systems, capable of real-time and archival processing. For each key area, we will focus on producing publications from this work and providing tools and software to the community where/when possible. Currently massive amounts of acoustic data are being collected by various institutions, corporations and national defense agencies. The long-term goal is to provide technical capability to analyze the data using automatic algorithms for (DC) based on machine intelligence. The goal of the automation is to provide effective and efficient mechanisms by which to process large acoustic datasets for understanding the bioacoustic behaviors of marine mammals. This capability will provide insights into the potential ecological impacts and influences of anthropogenic ocean sounds. This work focuses on building technologies using a maturity model based on DARPA 6.1 and 6.2 processes, for basic and applied research, respectively.
ER  -


TY  - Preprint
T1  - Music transcription modelling and composition using deep learning
A1  - Bob L. Sturm
A1  - JoÃ£o Felipe Santos
A1  - Oded Ben-Tal
A1  - Iryna Korshunova
JO  - ArXiv e-prints
Y1  - 29 April, 2016
UR  - https://arxiv.org/abs/1604.08723
N2  - We apply deep learning methods, specifically long short-term memory (LSTM) networks, to music transcription modelling and composition. We build and train LSTM networks using approximately 23,000 music transcriptions expressed with a high-level vocabulary (ABC notation), and use them to generate new transcriptions. Our practical aim is to create music transcription models useful in particular contexts of music composition. We present results from three perspectives: 1) at the population level, comparing descriptive statistics of the set of training transcriptions and generated transcriptions; 2) at the individual level, examining how a generated transcription reflects the conventions of a music practice in the training transcriptions (Celtic folk); 3) at the application level, using the system for idea generation in music composition. We make our datasets, software and sound examples open and available: \url{https://github.com/IraKorshunova/folk-rnn}.
ER  -


TY  - Preprint
T1  - Deep Edge Guided Recurrent Residual Learning for Image Super-Resolution
A1  - Wenhan Yang
A1  - Jiashi Feng
A1  - Jianchao Yang
A1  - Fang Zhao
A1  - Jiaying Liu
A1  - Zongming Guo
A1  - Shuicheng Yan
JO  - ArXiv e-prints
Y1  - 17 July, 2016
UR  - https://arxiv.org/abs/1604.08671
N2  - In this work, we consider the image super-resolution (SR) problem. The main challenge of image SR is to recover high-frequency details of a low-resolution (LR) image that are important for human perception. To address this essentially ill-posed problem, we introduce a Deep Edge Guided REcurrent rEsidual~(DEGREE) network to progressively recover the high-frequency details. Different from most of existing methods that aim at predicting high-resolution (HR) images directly, DEGREE investigates an alternative route to recover the difference between a pair of LR and HR images by recurrent residual learning. DEGREE further augments the SR process with edge-preserving capability, namely the LR image and its edge map can jointly infer the sharp edge details of the HR image during the recurrent recovery process. To speed up its training convergence rate, by-pass connections across multiple layers of DEGREE are constructed. In addition, we offer an understanding on DEGREE from the view-point of sub-band frequency decomposition on image signal and experimentally demonstrate how DEGREE can recover different frequency bands separately. Extensive experiments on three benchmark datasets clearly demonstrate the superiority of DEGREE over well-established baselines and DEGREE also provides new state-of-the-arts on these datasets.
ER  -


TY  - Preprint
T1  - Classifying Options for Deep Reinforcement Learning
A1  - Kai Arulkumaran
A1  - Nat Dilokthanakul
A1  - Murray Shanahan
A1  - Anil Anthony Bharath
JO  - ArXiv e-prints
Y1  - 19 June, 2017
UR  - https://arxiv.org/abs/1604.08153
N2  - In this paper we combine one method for hierarchical reinforcement learning - the options framework - with deep Q-networks (DQNs) through the use of different &#34;option heads&#34; on the policy network, and a supervisory network for choosing between the different options. We utilise our setup to investigate the effects of architectural constraints in subtasks with positive and negative transfer, across a range of network capacities. We empirically show that our augmented DQN has lower sample complexity when simultaneously learning subtasks with negative transfer, without degrading performance when learning subtasks with positive transfer.
ER  -


TY  - Preprint
T1  - Deep Learning for Saliency Prediction in Natural Video
A1  - Souad Chaabouni
A1  - Jenny Benois-Pineau
A1  - Ofer Hadar
A1  - Chokri Ben Amar
JO  - ArXiv e-prints
Y1  - 27 April, 2016
UR  - https://arxiv.org/abs/1604.08010
N2  - The purpose of this paper is the detection of salient areas in natural video by using the new deep learning techniques. Salient patches in video frames are predicted first. Then the predicted visual fixation maps are built upon them. We design the deep architecture on the basis of CaffeNet implemented with Caffe toolkit. We show that changing the way of data selection for optimisation of network parameters, we can save computation cost up to 12 times. We extend deep learning approaches for saliency prediction in still images with RGB values to specificity of video using the sensitivity of the human visual system to residual motion. Furthermore, we complete primary colour pixel values by contrast features proposed in classical visual attention prediction models. The experiments are conducted on two publicly available datasets. The first is IRCCYN video database containing 31 videos with an overall amount of 7300 frames and eye fixations of 37 subjects. The second one is HOLLYWOOD2 provided 2517 movie clips with the eye fixations of 19 subjects. On IRCYYN dataset, the accuracy obtained is of 89.51%. On HOLLYWOOD2 dataset, results in prediction of saliency of patches show the improvement up to 2% with regard to RGB use only. The resulting accuracy of 76, 6% is obtained. The AUC metric in comparison of predicted saliency maps with visual fixation maps shows the increase up to 16% on a sample of video clips from this dataset.
ER  -


TY  - Preprint
T1  - Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification
A1  - Tong Xiao
A1  - Hongsheng Li
A1  - Wanli Ouyang
A1  - Xiaogang Wang
JO  - ArXiv e-prints
Y1  - 26 April, 2016
UR  - https://arxiv.org/abs/1604.07528
N2  - Learning generic and robust feature representations with data from multiple domains for the same problem is of great value, especially for the problems that have multiple datasets but none of them are large enough to provide abundant data variations. In this work, we present a pipeline for learning deep feature representations from multiple domains with Convolutional Neural Networks (CNNs). When training a CNN with data from all the domains, some neurons learn representations shared across several domains, while some others are effective only for a specific one. Based on this important observation, we propose a Domain Guided Dropout algorithm to improve the feature learning procedure. Experiments show the effectiveness of our pipeline and the proposed algorithm. Our methods on the person re-identification problem outperform state-of-the-art methods on multiple datasets by large margins.
ER  -


TY  - Preprint
T1  - A Deep Hierarchical Approach to Lifelong Learning in Minecraft
A1  - Chen Tessler
A1  - Shahar Givony
A1  - Tom Zahavy
A1  - Daniel J. Mankowitz
A1  - Shie Mannor
JO  - ArXiv e-prints
Y1  - 30 November, 2016
UR  - https://arxiv.org/abs/1604.07255
N2  - We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks, are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using two techniques: (1) a deep skill array and (2) skill distillation, our novel variation of policy distillation (Rusu et. al. 2015) for learning skills. Skill distillation enables the HDRLN to efficiently retain knowledge and therefore scale in lifelong learning, by accumulating knowledge and encapsulating multiple reusable skills into a single distilled network. The H-DRLN exhibits superior performance and lower learning sample complexity compared to the regular Deep Q Network (Mnih et. al. 2015) in sub-domains of Minecraft.
ER  -


TY  - Preprint
T1  - Neurohex: A Deep Q-learning Hex Agent
A1  - Kenny Young
A1  - Ryan Hayward
A1  - Gautham Vasan
JO  - ArXiv e-prints
Y1  - 25 April, 2016
UR  - https://arxiv.org/abs/1604.07097
N2  - DeepMind&#39;s recent spectacular success in using deep convolutional neural nets and machine learning to build superhuman level agents --- e.g. for Atari games via deep Q-learning and for the game of Go via Reinforcement Learning --- raises many questions, including to what extent these methods will succeed in other domains. In this paper we consider DQL for the game of Hex: after supervised initialization, we use selfplay to train NeuroHex, an 11-layer CNN that plays Hex on the 13x13 board. Hex is the classic two-player alternate-turn stone placement game played on a rhombus of hexagonal cells in which the winner is whomever connects their two opposing sides. Despite the large action and state space, our system trains a Q-network capable of strong play with no search. After two weeks of Q-learning, NeuroHex achieves win-rates of 20.4% as first player and 2.1% as second player against a 1-second/move version of MoHex, the current ICGA Olympiad Hex champion. Our data suggests further improvement might be possible with more training time.
ER  -


TY  - Preprint
T1  - Deep Learning for Reward Design to Improve Monte Carlo Tree Search in ATARI Games
A1  - Xiaoxiao Guo
A1  - Satinder Singh
A1  - Richard Lewis
A1  - Honglak Lee
JO  - ArXiv e-prints
Y1  - 24 April, 2016
UR  - https://arxiv.org/abs/1604.07095
N2  - Monte Carlo Tree Search (MCTS) methods have proven powerful in planning for sequential decision-making problems such as Go and video games, but their performance can be poor when the planning depth and sampling trajectories are limited or when the rewards are sparse. We present an adaptation of PGRD (policy-gradient for reward-design) for learning a reward-bonus function to improve UCT (a MCTS algorithm). Unlike previous applications of PGRD in which the space of reward-bonus functions was limited to linear functions of hand-coded state-action-features, we use PGRD with a multi-layer convolutional neural network to automatically learn features from raw perception as well as to adapt the non-linear reward-bonus function parameters. We also adopt a variance-reducing gradient method to improve PGRD&#39;s performance. The new method improves UCT&#39;s performance on multiple ATARI games compared to UCT without the reward bonus. Combining PGRD and Deep Learning in this way should make adapting rewards for MCTS algorithms far more widely and practically applicable than before.
ER  -


TY  - Preprint
T1  - Deep Learning with Eigenvalue Decay Regularizer
A1  - Oswaldo Ludwig
JO  - ArXiv e-prints
Y1  - 8 May, 2016
UR  - https://arxiv.org/abs/1604.06985
N2  - This paper extends our previous work on regularization of neural networks using Eigenvalue Decay by employing a soft approximation of the dominant eigenvalue in order to enable the calculation of its derivatives in relation to the synaptic weights, and therefore the application of back-propagation, which is a primary demand for deep learning. Moreover, we extend our previous theoretical analysis to deep neural networks and multiclass classification problems. Our method is implemented as an additional regularizer in Keras, a modular neural networks library written in Python, and evaluated in the benchmark data sets Reuters Newswire Topics Classification, IMDB database for binary sentiment classification, MNIST database of handwritten digits and CIFAR-10 data set for image classification.
ER  -


TY  - Preprint
T1  - Benchmarking Deep Reinforcement Learning for Continuous Control
A1  - Yan Duan
A1  - Xi Chen
A1  - Rein Houthooft
A1  - John Schulman
A1  - Pieter Abbeel
JO  - ArXiv e-prints
Y1  - 27 May, 2016
UR  - https://arxiv.org/abs/1604.06778
N2  - Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers.
ER  -


TY  - Preprint
T1  - Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation
A1  - Tejas D. Kulkarni
A1  - Karthik R. Narasimhan
A1  - Ardavan Saeedi
A1  - Joshua B. Tenenbaum
JO  - ArXiv e-prints
Y1  - 31 May, 2016
UR  - https://arxiv.org/abs/1604.06057
N2  - Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma&#39;s Revenge&#39;.
ER  -


TY  - Preprint
T1  - M$^2$S-Net: Multi-Modal Similarity Metric Learning based Deep Convolutional Network for Answer Selection
A1  - Lingxun Meng
A1  - Yan Li
JO  - ArXiv e-prints
Y1  - 16 March, 2018
UR  - https://arxiv.org/abs/1604.05519
N2  - Recent works using artificial neural networks based on distributed word representation greatly boost performance on various natural language processing tasks, especially the answer selection problem. Nevertheless, most of the previous works used deep learning methods (like LSTM-RNN, CNN, etc.) only to capture semantic representation of each sentence separately, without considering the interdependence between each other. In this paper, we propose a novel end-to-end learning framework which constitutes deep convolutional neural network based on multi-modal similarity metric learning (M$^2$S-Net) on pairwise tokens. The proposed model demonstrates its performance by surpassing previous state-of-the-art systems on the answer selection benchmark, i.e., TREC-QA dataset, in both MAP and MRR metrics.
ER  -


TY  - Preprint
T1  - Self-taught learning of a deep invariant representation for visual tracking via temporal slowness principle
A1  - Jason Kuen
A1  - Kian Ming Lim
A1  - Chin Poo Lee
JO  - ArXiv e-prints
Y1  - 14 April, 2016
UR  - https://arxiv.org/abs/1604.04144
N2  - Visual representation is crucial for a visual tracking method&#39;s performances. Conventionally, visual representations adopted in visual tracking rely on hand-crafted computer vision descriptors. These descriptors were developed generically without considering tracking-specific information. In this paper, we propose to learn complex-valued invariant representations from tracked sequential image patches, via strong temporal slowness constraint and stacked convolutional autoencoders. The deep slow local representations are learned offline on unlabeled data and transferred to the observational model of our proposed tracker. The proposed observational model retains old training samples to alleviate drift, and collect negative samples which are coherent with target&#39;s motion pattern for better discriminative tracking. With the learned representation and online training samples, a logistic regression classifier is adopted to distinguish target from background, and retrained online to adapt to appearance changes. Subsequently, the observational model is integrated into a particle filter framework to peform visual tracking. Experimental results on various challenging benchmark sequences demonstrate that the proposed tracker performs favourably against several state-of-the-art trackers.
ER  -


TY  - Preprint
T1  - VConv-DAE: Deep Volumetric Shape Learning Without Object Labels
A1  - Abhishek Sharma
A1  - Oliver Grau
A1  - Mario Fritz
JO  - ArXiv e-prints
Y1  - 9 September, 2016
UR  - https://arxiv.org/abs/1604.03755
N2  - With the advent of affordable depth sensors, 3D capture becomes more and more ubiquitous and already has made its way into commercial products. Yet, capturing the geometry or complete shapes of everyday objects using scanning devices (e.g. Kinect) still comes with several challenges that result in noise or even incomplete shapes. Recent success in deep learning has shown how to learn complex shape distributions in a data-driven way from large scale 3D CAD Model collections and to utilize them for 3D processing on volumetric representations and thereby circumventing problems of topology and tessellation. Prior work has shown encouraging results on problems ranging from shape completion to recognition. We provide an analysis of such approaches and discover that training as well as the resulting representation are strongly and unnecessarily tied to the notion of object labels. Thus, we propose a full convolutional volumetric auto encoder that learns volumetric representation from noisy data by estimating the voxel occupancy grids. The proposed method outperforms prior work on challenging tasks like denoising and shape completion. We also show that the obtained deep embedding gives competitive performance when used for classification and promising results for shape interpolation.
ER  -


TY  - Preprint
T1  - Joint Unsupervised Learning of Deep Representations and Image Clusters
A1  - Jianwei Yang
A1  - Devi Parikh
A1  - Dhruv Batra
JO  - ArXiv e-prints
Y1  - 20 June, 2016
UR  - https://arxiv.org/abs/1604.03628
N2  - In this paper, we propose a recurrent framework for Joint Unsupervised LEarning (JULE) of deep representations and image clusters. In our framework, successive operations in a clustering algorithm are expressed as steps in a recurrent process, stacked on top of representations output by a Convolutional Neural Network (CNN). During training, image clusters and representations are updated jointly: image clustering is conducted in the forward pass, while representation learning in the backward pass. Our key idea behind this framework is that good representations are beneficial to image clustering and clustering results provide supervisory signals to representation learning. By integrating two processes into a single model with a unified weighted triplet loss and optimizing it end-to-end, we can obtain not only more powerful representations, but also more precise image clusters. Extensive experiments show that our method outperforms the state-of-the-art on image clustering across a variety of image datasets. Moreover, the learned representations generalize well when transferred to other tasks.
ER  -


TY  - Preprint
T1  - Using Deep Learning for Image-Based Plant Disease Detection
A1  - Sharada Prasanna Mohanty
A1  - David Hughes
A1  - Marcel Salathe
JO  - ArXiv e-prints
Y1  - 15 April, 2016
UR  - https://arxiv.org/abs/1604.03169
N2  - Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35% on a held-out test set, demonstrating the feasibility of this approach. When testing the model on a set of images collected from trusted online sources - i.e. taken under conditions different from the images used for training - the model still achieves an accuracy of 31.4%. While this accuracy is much higher than the one based on random selection (2.6%), a more diverse set of training data is needed to improve the general accuracy. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path towards smartphone-assisted crop disease diagnosis on a massive global scale.
ER  -


TY  - Preprint
T1  - Deep Structured Scene Parsing by Learning with Image Descriptions
A1  - Liang Lin
A1  - Guangrun Wang
A1  - Rui Zhang
A1  - Ruimao Zhang
A1  - Xiaodan Liang
A1  - Wangmeng Zuo
JO  - ArXiv e-prints
Y1  - 27 February, 2018
UR  - https://arxiv.org/abs/1604.02271
N2  - This paper addresses a fundamental problem of scene understanding: How to parse the scene image into a structured configuration (i.e., a semantic object hierarchy with object interaction relations) that finely accords with human perception. We propose a deep architecture consisting of two networks: i) a convolutional neural network (CNN) extracting the image representation for pixelwise object labeling and ii) a recursive neural network (RNN) discovering the hierarchical object structure and the inter-object relations. Rather than relying on elaborative user annotations (e.g., manually labeling semantic maps and relations), we train our deep model in a weakly-supervised manner by leveraging the descriptive sentences of the training images. Specifically, we decompose each sentence into a semantic tree consisting of nouns and verb phrases, and facilitate these trees discovering the configurations of the training images. Once these scene configurations are determined, then the parameters of both the CNN and RNN are updated accordingly by back propagation. The entire model training is accomplished through an Expectation-Maximization method. Extensive experiments suggest that our model is capable of producing meaningful and structured scene configurations and achieving more favorable scene labeling performance on PASCAL VOC 2012 over other state-of-the-art weakly-supervised methods.
ER  -


TY  - Preprint
T1  - Learning to Track at 100 FPS with Deep Regression Networks
A1  - David Held
A1  - Sebastian Thrun
A1  - Silvio Savarese
JO  - ArXiv e-prints
Y1  - 15 August, 2016
UR  - https://arxiv.org/abs/1604.01802
N2  - Machine learning techniques are often used in computer vision due to their ability to leverage large amounts of training data to improve performance. Unfortunately, most generic object trackers are still trained from scratch online and do not benefit from the large number of videos that are readily available for offline training. We propose a method for offline training of neural networks that can track novel objects at test-time at 100 fps. Our tracker is significantly faster than previous methods that use neural networks for tracking, which are typically very slow to run and not practical for real-time applications. Our tracker uses a simple feed-forward network with no online training required. The tracker learns a generic relationship between object motion and appearance and can be used to track novel objects that do not appear in the training set. We test our network on a standard tracking benchmark to demonstrate our tracker&#39;s state-of-the-art performance. Further, our performance improves as we add more videos to our offline training set. To the best of our knowledge, our tracker is the first neural-network tracker that learns to track generic objects at 100 fps.
ER  -


TY  - Preprint
T1  - Correlated and Individual Multi-Modal Deep Learning for RGB-D Object Recognition
A1  - Ziyan Wang
A1  - Jiwen Lu
A1  - Ruogu Lin
A1  - Jianjiang Feng
A1  - Jie zhou
JO  - ArXiv e-prints
Y1  - 9 December, 2016
UR  - https://arxiv.org/abs/1604.01655
N2  - In this paper, we propose a new correlated and individual multi-modal deep learning (CIMDL) method for RGB-D object recognition. Unlike most conventional RGB-D object recognition methods which extract features from the RGB and depth channels individually, our CIMDL jointly learns feature representations from raw RGB-D data with a pair of deep neural networks, so that the sharable and modal-specific information can be simultaneously exploited. Specifically, we construct a pair of deep convolutional neural networks (CNNs) for the RGB and depth data, and concatenate them at the top layer of the network with a loss function which learns a new feature space where both correlated part and the individual part of the RGB-D information are well modelled. The parameters of the whole networks are updated by using the back-propagation criterion. Experimental results on two widely used RGB-D object image benchmark datasets clearly show that our method outperforms state-of-the-arts.
ER  -


TY  - Preprint
T1  - Learning A Deep $\ell_\infty$ Encoder for Hashing
A1  - Zhangyang Wang
A1  - Yingzhen Yang
A1  - Shiyu Chang
A1  - Qing Ling
A1  - Thomas S. Huang
JO  - ArXiv e-prints
Y1  - 5 April, 2016
UR  - https://arxiv.org/abs/1604.01475
N2  - We investigate the $\ell_\infty$-constrained representation which demonstrates robustness to quantization errors, utilizing the tool of deep learning. Based on the Alternating Direction Method of Multipliers (ADMM), we formulate the original convex minimization problem as a feed-forward neural network, named \textit{Deep $\ell_\infty$ Encoder}, by introducing the novel Bounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as network biases. Such a structural prior acts as an effective network regularization, and facilitates the model initialization. We then investigate the effective use of the proposed model in the application of hashing, by coupling the proposed encoders under a supervised pairwise loss, to develop a \textit{Deep Siamese $\ell_\infty$ Network}, which can be optimized from end to end. Extensive experiments demonstrate the impressive performances of the proposed model. We also provide an in-depth analysis of its behaviors against the competitors.
ER  -


TY  - Preprint
T1  - Deep Cross Residual Learning for Multitask Visual Recognition
A1  - Brendan Jou
A1  - Shih-Fu Chang
JO  - ArXiv e-prints
Y1  - 19 July, 2016
UR  - https://arxiv.org/abs/1604.01335
N2  - Residual learning has recently surfaced as an effective means of constructing very deep neural networks for object recognition. However, current incarnations of residual networks do not allow for the modeling and integration of complex relations between closely coupled recognition tasks or across domains. Such problems are often encountered in multimedia applications involving large-scale content recognition. We propose a novel extension of residual learning for deep networks that enables intuitive learning across multiple related tasks using cross-connections called cross-residuals. These cross-residuals connections can be viewed as a form of in-network regularization and enables greater network generalization. We show how cross-residual learning (CRL) can be integrated in multitask networks to jointly train and detect visual concepts across several tasks. We present a single multitask cross-residual network with &gt;40% less parameters that is able to achieve competitive, or even better, detection performance on a visual sentiment concept detection problem normally requiring multiple specialized single-task networks. The resulting multitask cross-residual network also achieves better detection performance by about 10.4% over a standard multitask residual network without cross-residuals with even a small amount of cross-task weighting.
ER  -


TY  - Preprint
T1  - Deep Image Retrieval: Learning global representations for image search
A1  - Albert Gordo
A1  - Jon Almazan
A1  - Jerome Revaud
A1  - Diane Larlus
JO  - ArXiv e-prints
Y1  - 28 July, 2016
UR  - https://arxiv.org/abs/1604.01325
N2  - We propose a novel approach for instance-level image retrieval. It produces a global and compact fixed-length representation for each image by aggregating many region-wise descriptors. In contrast to previous works employing pre-trained deep networks as a black box to produce features, our method leverages a deep architecture trained for the specific task of image retrieval. Our contribution is twofold: (i) we leverage a ranking framework to learn convolution and projection weights that are used to build the region features; and (ii) we employ a region proposal network to learn which regions should be pooled to form the final global descriptor. We show that using clean training data is key to the success of our approach. To that aim, we use a large scale but noisy landmark dataset and develop an automatic cleaning approach. The proposed architecture produces a global image representation in a single forward pass. Our approach significantly outperforms previous approaches based on global descriptors on standard datasets. It even surpasses most prior works based on costly local descriptor indexing and spatial verification. Additional material is available at www.xrce.xerox.com/Deep-Image-Retrieval.
ER  -


TY  - Preprint
T1  - Comparative Deep Learning of Hybrid Representations for Image Recommendations
A1  - Chenyi Lei
A1  - Dong Liu
A1  - Weiping Li
A1  - Zheng-Jun Zha
A1  - Houqiang Li
JO  - ArXiv e-prints
Y1  - 5 April, 2016
UR  - https://arxiv.org/abs/1604.01252
N2  - In many image-related tasks, learning expressive and discriminative representations of images is essential, and deep learning has been studied for automating the learning of such representations. Some user-centric tasks, such as image recommendations, call for effective representations of not only images but also preferences and intents of users over images. Such representations are termed \emph{hybrid} and addressed via a deep learning approach in this paper. We design a dual-net deep network, in which the two sub-networks map input images and preferences of users into a same latent semantic space, and then the distances between images and users in the latent space are calculated to make decisions. We further propose a comparative deep learning (CDL) method to train the deep network, using a pair of images compared against one user to learn the pattern of their relative distances. The CDL embraces much more training data than naive deep learning, and thus achieves superior performance than the latter, with no cost of increasing network complexity. Experimental results with real-world data sets for image recommendations have shown the proposed dual-net network and CDL greatly outperform other state-of-the-art image recommendation solutions.
ER  -


TY  - Preprint
T1  - Writer-independent Feature Learning for Offline Signature Verification using Deep Convolutional Neural Networks
A1  - Luiz G. Hafemann
A1  - Robert Sabourin
A1  - Luiz S. Oliveira
JO  - ArXiv e-prints
Y1  - 4 April, 2016
UR  - https://arxiv.org/abs/1604.00974
N2  - Automatic Offline Handwritten Signature Verification has been researched over the last few decades from several perspectives, using insights from graphology, computer vision, signal processing, among others. In spite of the advancements on the field, building classifiers that can separate between genuine signatures and skilled forgeries (forgeries made targeting a particular signature) is still hard. We propose approaching the problem from a feature learning perspective. Our hypothesis is that, in the absence of a good model of the data generation process, it is better to learn the features from data, instead of using hand-crafted features that have no resemblance to the signature generation process. To this end, we use Deep Convolutional Neural Networks to learn features in a writer-independent format, and use this model to obtain a feature representation on another set of users, where we train writer-dependent classifiers. We tested our method in two datasets: GPDS-960 and Brazilian PUC-PR. Our experimental results show that the features learned in a subset of the users are discriminative for the other users, including across different datasets, reaching close to the state-of-the-art in the GPDS dataset, and improving the state-of-the-art in the Brazilian PUC-PR dataset.
ER  -


TY  - Preprint
T1  - deepTarget: End-to-end Learning Framework for microRNA Target Prediction using Deep Recurrent Neural Networks
A1  - Byunghan Lee
A1  - Junghwan Baek
A1  - Seunghyun Park
A1  - Sungroh Yoon
JO  - ArXiv e-prints
Y1  - 19 August, 2016
UR  - https://arxiv.org/abs/1603.09123
N2  - MicroRNAs (miRNAs) are short sequences of ribonucleic acids that control the expression of target messenger RNAs (mRNAs) by binding them. Robust prediction of miRNA-mRNA pairs is of utmost importance in deciphering gene regulations but has been challenging because of high false positive rates, despite a deluge of computational tools that normally require laborious manual feature extraction. This paper presents an end-to-end machine learning framework for miRNA target prediction. Leveraged by deep recurrent neural networks-based auto-encoding and sequence-sequence interaction learning, our approach not only delivers an unprecedented level of accuracy but also eliminates the need for manual feature extraction. The performance gap between the proposed method and existing alternatives is substantial (over 25% increase in F-measure), and deepTarget delivers a quantum leap in the long-standing challenge of robust miRNA target prediction.
ER  -


TY  - Preprint
T1  - Classification of Alzheimer&#39;s Disease using fMRI Data and Deep Learning Convolutional Neural Networks
A1  - Saman Sarraf
A1  - Ghassem Tofighi
JO  - ArXiv e-prints
Y1  - 29 March, 2016
UR  - https://arxiv.org/abs/1603.08631
N2  - Over the past decade, machine learning techniques especially predictive modeling and pattern recognition in biomedical sciences from drug delivery system to medical imaging has become one of the important methods which are assisting researchers to have deeper understanding of entire issue and to solve complex medical problems. Deep learning is power learning machine learning algorithm in classification while extracting high-level features. In this paper, we used convolutional neural network to classify Alzheimer&#39;s brain from normal healthy brain. The importance of classifying this kind of medical data is to potentially develop a predict model or system in order to recognize the type disease from normal subjects or to estimate the stage of the disease. Classification of clinical data such as Alzheimer&#39;s disease has been always challenging and most problematic part has been always selecting the most discriminative features. Using Convolutional Neural Network (CNN) and the famous architecture LeNet-5, we successfully classified functional MRI data of Alzheimer&#39;s subjects from normal controls where the accuracy of test data on trained data reached 96.85%. This experiment suggests us the shift and scale invariant features extracted by CNN followed by deep learning classification is most powerful method to distinguish clinical data from healthy data in fMRI. This approach also enables us to expand our methodology to predict more complicated systems.
ER  -


TY  - Preprint
T1  - Deep Learning At Scale and At Ease
A1  - Wei Wang
A1  - Gang Chen
A1  - Haibo Chen
A1  - Tien Tuan Anh Dinh
A1  - Jinyang Gao
A1  - Beng Chin Ooi
A1  - Kian-Lee Tan
A1  - Sheng Wang
JO  - ArXiv e-prints
Y1  - 25 March, 2016
UR  - https://arxiv.org/abs/1603.07846
N2  - Recently, deep learning techniques have enjoyed success in various multimedia applications, such as image classification and multi-modal data analysis. Large deep learning models are developed for learning rich representations of complex data. There are two challenges to overcome before deep learning can be widely adopted in multimedia and other applications. One is usability, namely the implementation of different models and training algorithms must be done by non-experts without much effort especially when the model is large and complex. The other is scalability, that is the deep learning system must be able to provision for a huge demand of computing resources for training large models with massive datasets. To address these two challenges, in this paper, we design a distributed deep learning platform called SINGA which has an intuitive programming model based on the common layer abstraction of deep learning models. Good scalability is achieved through flexible distributed training architecture and specific optimization techniques. SINGA runs on GPUs as well as on CPUs, and we show that it outperforms many other state-of-the-art deep learning systems. Our experience with developing and training deep learning models for real-life multimedia applications in SINGA shows that the platform is both usable and scalable.
ER  -


TY  - Preprint
T1  - Co-occurrence Feature Learning for Skeleton based Action Recognition using Regularized Deep LSTM Networks
A1  - Wentao Zhu
A1  - Cuiling Lan
A1  - Junliang Xing
A1  - Wenjun Zeng
A1  - Yanghao Li
A1  - Li Shen
A1  - Xiaohui Xie
JO  - ArXiv e-prints
Y1  - 24 March, 2016
UR  - https://arxiv.org/abs/1603.07772
N2  - Skeleton based action recognition distinguishes human actions using the trajectories of skeleton joints, which provide a very good representation for describing actions. Considering that recurrent neural networks (RNNs) with Long Short-Term Memory (LSTM) can learn feature representations and model long-term temporal dependencies automatically, we propose an end-to-end fully connected deep LSTM network for skeleton based action recognition. Inspired by the observation that the co-occurrences of the joints intrinsically characterize human actions, we take the skeleton as the input at each time slot and introduce a novel regularization scheme to learn the co-occurrence features of skeleton joints. To train the deep LSTM network effectively, we propose a new dropout algorithm which simultaneously operates on the gates, cells, and output responses of the LSTM neurons. Experimental results on three human action recognition datasets consistently demonstrate the effectiveness of the proposed model.
ER  -


TY  - Preprint
T1  - Probabilistic Reasoning via Deep Learning: Neural Association Models
A1  - Quan Liu
A1  - Hui Jiang
A1  - Andrew Evdokimov
A1  - Zhen-Hua Ling
A1  - Xiaodan Zhu
A1  - Si Wei
A1  - Yu Hu
JO  - ArXiv e-prints
Y1  - 3 August, 2016
UR  - https://arxiv.org/abs/1603.07704
N2  - In this paper, we propose a new deep learning approach, called neural association model (NAM), for probabilistic reasoning in artificial intelligence. We propose to use neural networks to model association between any two events in a domain. Neural networks take one event as input and compute a conditional probability of the other event to model how likely these two events are to be associated. The actual meaning of the conditional probabilities varies between applications and depends on how the models are trained. In this work, as two case studies, we have investigated two NAM structures, namely deep neural networks (DNN) and relation-modulated neural nets (RMNN), on several probabilistic reasoning tasks in AI, including recognizing textual entailment, triple classification in multi-relational knowledge bases and commonsense reasoning. Experimental results on several popular datasets derived from WordNet, FreeBase and ConceptNet have all demonstrated that both DNNs and RMNNs perform equally well and they can significantly outperform the conventional methods available for these reasoning tasks. Moreover, compared with DNNs, RMNNs are superior in knowledge transfer, where a pre-trained model can be quickly extended to an unseen relation after observing only a few training samples. To further prove the effectiveness of the proposed models, in this work, we have applied NAMs to solving challenging Winograd Schema (WS) problems. Experiments conducted on a set of WS problems prove that the proposed models have the potential for commonsense reasoning.
ER  -


TY  - Preprint
T1  - CSI-based Fingerprinting for Indoor Localization: A Deep Learning Approach
A1  - Xuyu Wang
A1  - Lingjun Gao
A1  - Shiwen Mao
A1  - Santosh Pandey
JO  - ArXiv e-prints
Y1  - 23 March, 2016
UR  - https://arxiv.org/abs/1603.07080
N2  - With the fast growing demand of location-based services in indoor environments, indoor positioning based on fingerprinting has attracted a lot of interest due to its high accuracy. In this paper, we present a novel deep learning based indoor fingerprinting system using Channel State Information (CSI), which is termed DeepFi. Based on three hypotheses on CSI, the DeepFi system architecture includes an off-line training phase and an on-line localization phase. In the off-line training phase, deep learning is utilized to train all the weights of a deep network as fingerprints. Moreover, a greedy learning algorithm is used to train the weights layer-by-layer to reduce complexity. In the on-line localization phase, we use a probabilistic method based on the radial basis function to obtain the estimated location. Experimental results are presented to confirm that DeepFi can effectively reduce location error compared with three existing methods in two representative indoor environments.
ER  -


TY  - Preprint
T1  - Frankenstein: Learning Deep Face Representations using Small Data
A1  - Guosheng Hu
A1  - Xiaojiang Peng
A1  - Yongxin Yang
A1  - Timothy Hospedales
A1  - Jakob Verbeek
JO  - ArXiv e-prints
Y1  - 21 September, 2017
UR  - https://arxiv.org/abs/1603.06470
N2  - Deep convolutional neural networks have recently proven extremely effective for difficult face recognition problems in uncontrolled settings. To train such networks, very large training sets are needed with millions of labeled images. For some applications, such as near-infrared (NIR) face recognition, such large training datasets are not publicly available and difficult to collect. In this work, we propose a method to generate very large training datasets of synthetic images by compositing real face images in a given dataset. We show that this method enables to learn models from as few as 10,000 training images, which perform on par with models trained from 500,000 images. Using our approach we also obtain state-of-the-art results on the CASIA NIR-VIS2.0 heterogeneous face recognition dataset.
ER  -


TY  - Preprint
T1  - Deep Learning in Bioinformatics
A1  - Seonwoo Min
A1  - Byunghan Lee
A1  - Sungroh Yoon
JO  - ArXiv e-prints
Y1  - 19 June, 2016
UR  - https://arxiv.org/abs/1603.06430
N2  - In the era of big data, transformation of biomedical big data into valuable knowledge has been one of the most important challenges in bioinformatics. Deep learning has advanced rapidly since the early 2000s and now demonstrates state-of-the-art performance in various fields. Accordingly, application of deep learning in bioinformatics to gain insight from data has been emphasized in both academia and industry. Here, we review deep learning in bioinformatics, presenting examples of current research. To provide a useful and comprehensive perspective, we categorize research both by the bioinformatics domain (i.e., omics, biomedical imaging, biomedical signal processing) and deep learning architecture (i.e., deep neural networks, convolutional neural networks, recurrent neural networks, emergent architectures) and present brief descriptions of each study. Additionally, we discuss theoretical and practical issues of deep learning in bioinformatics and suggest future research directions. We believe that this review will provide valuable insights and serve as a starting point for researchers to apply deep learning approaches in their bioinformatics studies.
ER  -


TY  - Preprint
T1  - Emotion Classification from Noisy Speech - A Deep Learning Approach
A1  - Rajib Rana
JO  - ArXiv e-prints
Y1  - 12 April, 2016
UR  - https://arxiv.org/abs/1603.05901
N2  - This paper investigates the performance of Deep Learning for speech emotion classification when the speech is compounded with noise. It reports on the classification accuracy and concludes with the future directions for achieving greater robustness for emotion recognition from noisy speech.
ER  -


TY  - Preprint
T1  - Comparing Time and Frequency Domain for Audio Event Recognition Using Deep Learning
A1  - Lars Hertel
A1  - Huy Phan
A1  - Alfred Mertins
JO  - ArXiv e-prints
Y1  - 18 March, 2016
UR  - https://arxiv.org/abs/1603.05824
N2  - Recognizing acoustic events is an intricate problem for a machine and an emerging field of research. Deep neural networks achieve convincing results and are currently the state-of-the-art approach for many tasks. One advantage is their implicit feature learning, opposite to an explicit feature extraction of the input signal. In this work, we analyzed whether more discriminative features can be learned from either the time-domain or the frequency-domain representation of the audio signal. For this purpose, we trained multiple deep networks with different architectures on the Freiburg-106 and ESC-10 datasets. Our results show that feature learning from the frequency domain is superior to the time domain. Moreover, additionally using convolution and pooling layers, to explore local structures of the audio signal, significantly improves the recognition performance and achieves state-of-the-art results.
ER  -


TY  - Preprint
T1  - Bank distress in the news: Describing events through deep learning
A1  - Samuel RÃ¶nnqvist
A1  - Peter Sarlin
JO  - ArXiv e-prints
Y1  - 27 December, 2016
UR  - https://arxiv.org/abs/1603.05670
N2  - While many models are purposed for detecting the occurrence of significant events in financial systems, the task of providing qualitative detail on the developments is not usually as well automated. We present a deep learning approach for detecting relevant discussion in text and extracting natural language descriptions of events. Supervised by only a small set of event information, comprising entity names and dates, the model is leveraged by unsupervised learning of semantic vector representations on extensive text data. We demonstrate applicability to the study of financial risk based on news (6.6M articles), particularly bank distress and government interventions (243 events), where indices can signal the level of bank-stress-related reporting at the entity level, or aggregated at national or European level, while being coupled with explanations. Thus, we exemplify how text, as timely, widely available and descriptive data, can serve as a useful complementary source of information for financial and systemic risk analytics.
ER  -


TY  - Preprint
T1  - Ensemble of Deep Convolutional Neural Networks for Learning to Detect Retinal Vessels in Fundus Images
A1  - Debapriya Maji
A1  - Anirban Santara
A1  - Pabitra Mitra
A1  - Debdoot Sheet
JO  - ArXiv e-prints
Y1  - 15 March, 2016
UR  - https://arxiv.org/abs/1603.04833
N2  - Vision impairment due to pathological damage of the retina can largely be prevented through periodic screening using fundus color imaging. However the challenge with large scale screening is the inability to exhaustively detect fine blood vessels crucial to disease diagnosis. In this work we present a computational imaging framework using deep and ensemble learning for reliable detection of blood vessels in fundus color images. An ensemble of deep convolutional neural networks is trained to segment vessel and non-vessel areas of a color fundus image. During inference, the responses of the individual ConvNets of the ensemble are averaged to form the final segmentation. In experimental evaluation with the DRIVE database, we achieve the objective of vessel detection with maximum average accuracy of 94.7\% and area under ROC curve of 0.9283.
ER  -


TY  - Preprint
T1  - Revealing the Hidden Patterns of News Photos: Analysis of Millions of News Photos Using GDELT and Deep Learning-based Vision APIs
A1  - Haewoon Kwak
A1  - Jisun An
JO  - ArXiv e-prints
Y1  - 24 March, 2016
UR  - https://arxiv.org/abs/1603.04531
N2  - In this work, we analyze more than two million news photos published in January 2016. We demonstrate i) which objects appear the most in news photos; ii) what the sentiments of news photos are; iii) whether the sentiment of news photos is aligned with the tone of the text; iv) how gender is treated; and v) how differently political candidates are portrayed. To our best knowledge, this is the first large-scale study of news photo contents using deep learning-based vision APIs.
ER  -


TY  - Preprint
T1  - Faster learning of deep stacked autoencoders on multi-core systems using synchronized layer-wise pre-training
A1  - Anirban Santara
A1  - Debapriya Maji
A1  - DP Tejas
A1  - Pabitra Mitra
A1  - Arobinda Gupta
JO  - ArXiv e-prints
Y1  - 9 March, 2016
UR  - https://arxiv.org/abs/1603.02836
N2  - Deep neural networks are capable of modelling highly non-linear functions by capturing different levels of abstraction of data hierarchically. While training deep networks, first the system is initialized near a good optimum by greedy layer-wise unsupervised pre-training. However, with burgeoning data and increasing dimensions of the architecture, the time complexity of this approach becomes enormous. Also, greedy pre-training of the layers often turns detrimental by over-training a layer causing it to lose harmony with the rest of the network. In this paper a synchronized parallel algorithm for pre-training deep networks on multi-core machines has been proposed. Different layers are trained by parallel threads running on different cores with regular synchronization. Thus the pre-training process becomes faster and chances of over-training are reduced. This is experimentally validated using a stacked autoencoder for dimensionality reduction of MNIST handwritten digit database. The proposed algorithm achieved 26\% speed-up compared to greedy layer-wise pre-training for achieving the same reconstruction accuracy substantiating its potential as an alternative.
ER  -


TY  - Preprint
T1  - DROW: Real-Time Deep Learning based Wheelchair Detection in 2D Range Data
A1  - Lucas Beyer
A1  - Alexander Hermans
A1  - Bastian Leibe
JO  - ArXiv e-prints
Y1  - 5 December, 2016
UR  - https://arxiv.org/abs/1603.02636
N2  - We introduce the DROW detector, a deep learning based detector for 2D range data. Laser scanners are lighting invariant, provide accurate range data, and typically cover a large field of view, making them interesting sensors for robotics applications. So far, research on detection in laser range data has been dominated by hand-crafted features and boosted classifiers, potentially losing performance due to suboptimal design choices. We propose a Convolutional Neural Network (CNN) based detector for this task. We show how to effectively apply CNNs for detection in 2D range data, and propose a depth preprocessing step and voting scheme that significantly improve CNN performance. We demonstrate our approach on wheelchairs and walkers, obtaining state of the art detection results. Apart from the training data, none of our design choices limits the detector to these two classes, though. We provide a ROS node for our detector and release our dataset containing 464k laser scans, out of which 24k were annotated.
ER  -


TY  - Preprint
T1  - Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection
A1  - Sergey Levine
A1  - Peter Pastor
A1  - Alex Krizhevsky
A1  - Deirdre Quillen
JO  - ArXiv e-prints
Y1  - 28 August, 2016
UR  - https://arxiv.org/abs/1603.02199
N2  - We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.
ER  -


TY  - Preprint
T1  - Deep Contrast Learning for Salient Object Detection
A1  - Guanbin Li
A1  - Yizhou Yu
JO  - ArXiv e-prints
Y1  - 7 March, 2016
UR  - https://arxiv.org/abs/1603.01976
N2  - Salient object detection has recently witnessed substantial progress due to powerful features extracted using deep convolutional neural networks (CNNs). However, existing CNN-based methods operate at the patch level instead of the pixel level. Resulting saliency maps are typically blurry, especially near the boundary of salient objects. Furthermore, image patches are treated as independent samples even when they are overlapping, giving rise to significant redundancy in computation and storage. In this CVPR 2016 paper, we propose an end-to-end deep contrast network to overcome the aforementioned limitations. Our deep network consists of two complementary components, a pixel-level fully convolutional stream and a segment-wise spatial pooling stream. The first stream directly produces a saliency map with pixel-level accuracy from an input image. The second stream extracts segment-wise features very efficiently, and better models saliency discontinuities along object boundaries. Finally, a fully connected CRF model can be optionally incorporated to improve spatial coherence and contour localization in the fused result from these two streams. Experimental results demonstrate that our deep model significantly improves the state of the art.
ER  -


TY  - Preprint
T1  - Variational methods for Conditional Multimodal Deep Learning
A1  - Gaurav Pandey
A1  - Ambedkar Dukkipati
JO  - ArXiv e-prints
Y1  - 26 August, 2016
UR  - https://arxiv.org/abs/1603.01801
N2  - In this paper, we address the problem of conditional modality learning, whereby one is interested in generating one modality given the other. While it is straightforward to learn a joint distribution over multiple modalities using a deep multimodal architecture, we observe that such models aren&#39;t very effective at conditional generation. Hence, we address the problem by learning conditional distributions between the modalities. We use variational methods for maximizing the corresponding conditional log-likelihood. The resultant deep model, which we refer to as conditional multimodal autoencoder (CMMA), forces the latent representation obtained from a single modality alone to be `close&#39; to the joint representation obtained from multiple modalities. We use the proposed model to generate faces from attributes. We show that the faces generated from attributes using the proposed model, are qualitatively and quantitatively more representative of the attributes from which they were generated, than those obtained by other deep generative models. We also propose a secondary task, whereby the existing faces are modified by modifying the corresponding attributes. We observe that the modifications in face introduced by the proposed model are representative of the corresponding modifications in attributes.
ER  -


TY  - Preprint
T1  - Integrated Sequence Tagging for Medieval Latin Using Deep Representation Learning
A1  - Mike Kestemont
A1  - Jeroen De Gussem
JO  - ArXiv e-prints
Y1  - 3 August, 2017
UR  - https://arxiv.org/abs/1603.01597
N2  - In this paper we consider two sequence tagging tasks for medieval Latin: part-of-speech tagging and lemmatization. These are both basic, yet foundational preprocessing steps in applications such as text re-use detection. Nevertheless, they are generally complicated by the considerable orthographic variation which is typical of medieval Latin. In Digital Classics, these tasks are traditionally solved in a (i) cascaded and (ii) lexicon-dependent fashion. For example, a lexicon is used to generate all the potential lemma-tag pairs for a token, and next, a context-aware PoS-tagger is used to select the most appropriate tag-lemma pair. Apart from the problems with out-of-lexicon items, error percolation is a major downside of such approaches. In this paper we explore the possibility to elegantly solve these tasks using a single, integrated approach. For this, we make use of a layered neural network architecture from the field of deep representation learning.
ER  -


TY  - Preprint
T1  - HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition
A1  - Rajeev Ranjan
A1  - Vishal M. Patel
A1  - Rama Chellappa
JO  - ArXiv e-prints
Y1  - 5 December, 2017
UR  - https://arxiv.org/abs/1603.01249
N2  - We present an algorithm for simultaneous face detection, landmarks localization, pose estimation and gender recognition using deep convolutional neural networks (CNN). The proposed method called, HyperFace, fuses the intermediate layers of a deep CNN using a separate CNN followed by a multi-task learning algorithm that operates on the fused features. It exploits the synergy among the tasks which boosts up their individual performances. Additionally, we propose two variants of HyperFace: (1) HyperFace-ResNet that builds on the ResNet-101 model and achieves significant improvement in performance, and (2) Fast-HyperFace that uses a high recall fast face detector for generating region proposals to improve the speed of the algorithm. Extensive experiments show that the proposed models are able to capture both global and local information in faces and performs significantly better than many competitive algorithms for each of these four tasks.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning from Self-Play in Imperfect-Information Games
A1  - Johannes Heinrich
A1  - David Silver
JO  - ArXiv e-prints
Y1  - 28 June, 2016
UR  - https://arxiv.org/abs/1603.01121
N2  - Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge. Our method combines fictitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a strategy that approached the performance of state-of-the-art, superhuman algorithms based on significant domain expertise.
ER  -


TY  - Preprint
T1  - Learning Functions: When Is Deep Better Than Shallow
A1  - Hrushikesh Mhaskar
A1  - Qianli Liao
A1  - Tomaso Poggio
JO  - ArXiv e-prints
Y1  - 29 May, 2016
UR  - https://arxiv.org/abs/1603.00988
N2  - While the universal approximation property holds both for hierarchical and shallow networks, we prove that deep (hierarchical) networks can approximate the class of compositional functions with the same accuracy as shallow networks but with exponentially lower number of training parameters as well as VC-dimension. This theorem settles an old conjecture by Bengio on the role of depth in networks. We then define a general class of scalable, shift-invariant algorithms to show a simple and natural set of requirements that justify deep convolutional networks.
ER  -


TY  - Preprint
T1  - Continuous Deep Q-Learning with Model-based Acceleration
A1  - Shixiang Gu
A1  - Timothy Lillicrap
A1  - Ilya Sutskever
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 2 March, 2016
UR  - https://arxiv.org/abs/1603.00748
N2  - Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.
ER  -


TY  - Preprint
T1  - Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization
A1  - Chelsea Finn
A1  - Sergey Levine
A1  - Pieter Abbeel
JO  - ArXiv e-prints
Y1  - 27 May, 2016
UR  - https://arxiv.org/abs/1603.00448
N2  - Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.
ER  -


TY  - Preprint
T1  - Evaluation of Deep Learning based Pose Estimation for Sign Language Recognition
A1  - Srujana Gattupalli
A1  - Amir Ghaderi
A1  - Vassilis Athitsos
JO  - ArXiv e-prints
Y1  - 19 April, 2016
UR  - https://arxiv.org/abs/1602.09065
N2  - Human body pose estimation and hand detection are two important tasks for systems that perform computer vision-based sign language recognition(SLR). However, both tasks are challenging, especially when the input is color videos, with no depth information. Many algorithms have been proposed in the literature for these tasks, and some of the most successful recent algorithms are based on deep learning. In this paper, we introduce a dataset for human pose estimation for SLR domain. We evaluate the performance of two deep learning based pose estimation methods, by performing user-independent experiments on our dataset. We also perform transfer learning, and we obtain results that demonstrate that transfer learning can improve pose estimation accuracy. The dataset and results from these methods can create a useful baseline for future works.
ER  -


TY  - Preprint
T1  - Multimodal Emotion Recognition Using Multimodal Deep Learning
A1  - Wei Liu
A1  - Wei-Long Zheng
A1  - Bao-Liang Lu
JO  - ArXiv e-prints
Y1  - 26 February, 2016
UR  - https://arxiv.org/abs/1602.08225
N2  - To enhance the performance of affective models and reduce the cost of acquiring physiological signals for real-world applications, we adopt multimodal deep learning approach to construct affective models from multiple physiological signals. For unimodal enhancement task, we indicate that the best recognition accuracy of 82.11% on SEED dataset is achieved with shared representations generated by Deep AutoEncoder (DAE) model. For multimodal facilitation tasks, we demonstrate that the Bimodal Deep AutoEncoder (BDAE) achieves the mean accuracies of 91.01% and 83.25% on SEED and DEAP datasets, respectively, which are much superior to the state-of-the-art approaches. For cross-modal learning task, our experimental results demonstrate that the mean accuracy of 66.34% is achieved on SEED dataset through shared representations generated by EEG-based DAE as training samples and shared representations generated by eye-based DAE as testing sample, and vice versa.
ER  -


TY  - Preprint
T1  - DeepSpark: A Spark-Based Distributed Deep Learning Framework for Commodity Clusters
A1  - Hanjoo Kim
A1  - Jaehong Park
A1  - Jaehee Jang
A1  - Sungroh Yoon
JO  - ArXiv e-prints
Y1  - 30 September, 2016
UR  - https://arxiv.org/abs/1602.08191
N2  - The increasing complexity of deep neural networks (DNNs) has made it challenging to exploit existing large-scale data processing pipelines for handling massive data and parameters involved in DNN training. Distributed computing platforms and GPGPU-based acceleration provide a mainstream solution to this computational challenge. In this paper, we propose DeepSpark, a distributed and parallel deep learning framework that exploits Apache Spark on commodity clusters. To support parallel operations, DeepSpark automatically distributes workloads and parameters to Caffe/Tensorflow-running nodes using Spark, and iteratively aggregates training results by a novel lock-free asynchronous variant of the popular elastic averaging stochastic gradient descent based update scheme, effectively complementing the synchronized processing capabilities of Spark. DeepSpark is an on-going project, and the current release is available at http://deepspark.snu.ac.kr.
ER  -


TY  - Preprint
T1  - Hierarchical Conflict Propagation: Sequence Learning in a Recurrent Deep Neural Network
A1  - Andrew J. R. Simpson
JO  - ArXiv e-prints
Y1  - 25 February, 2016
UR  - https://arxiv.org/abs/1602.08118
N2  - Recurrent neural networks (RNN) are capable of learning to encode and exploit activation history over an arbitrary timescale. However, in practice, state of the art gradient descent based training methods are known to suffer from difficulties in learning long term dependencies. Here, we describe a novel training method that involves concurrent parallel cloned networks, each sharing the same weights, each trained at different stimulus phase and each maintaining independent activation histories. Training proceeds by recursively performing batch-updates over the parallel clones as activation history is progressively increased. This allows conflicts to propagate hierarchically from short-term contexts towards longer-term contexts until they are resolved. We illustrate the parallel clones method and hierarchical conflict propagation with a character-level deep RNN tasked with memorizing a paragraph of Moby Dick (by Herman Melville).
ER  -


TY  - Preprint
T1  - Mobile Big Data Analytics Using Deep Learning and Apache Spark
A1  - Mohammad Abu Alsheikh
A1  - Dusit Niyato
A1  - Shaowei Lin
A1  - Hwee-Pink Tan
A1  - Zhu Han
JO  - ArXiv e-prints
Y1  - 22 February, 2016
UR  - https://arxiv.org/abs/1602.07031
N2  - The proliferation of mobile devices, such as smartphones and Internet of Things (IoT) gadgets, results in the recent mobile big data (MBD) era. Collecting MBD is unprofitable unless suitable analytics and learning methods are utilized for extracting meaningful information and hidden patterns from data. This article presents an overview and brief tutorial of deep learning in MBD analytics and discusses a scalable learning framework over Apache Spark. Specifically, a distributed deep learning is executed as an iterative MapReduce computing on many Spark workers. Each Spark worker learns a partial deep model on a partition of the overall MBD, and a master deep model is then built by averaging the parameters of all partial models. This Spark-based framework speeds up the learning of deep models consisting of many hidden layers and millions of parameters. We use a context-aware activity recognition application with a real-world dataset containing millions of samples to validate our framework and assess its speedup effectiveness.
ER  -


TY  - Preprint
T1  - Semi-supervised Clustering for Short Text via Deep Representation Learning
A1  - Zhiguo Wang
A1  - Haitao Mi
A1  - Abraham Ittycheriah
JO  - ArXiv e-prints
Y1  - 14 July, 2017
UR  - https://arxiv.org/abs/1602.06797
N2  - In this work, we propose a semi-supervised method for short text clustering, where we represent texts as distributed vectors with neural networks, and use a small amount of labeled data to specify our intention for clustering. We design a novel objective to combine the representation learning process and the k-means clustering process together, and optimize the objective with both labeled data and unlabeled data iteratively until convergence through three steps: (1) assign each short text to its nearest centroid based on its representation from the current neural networks; (2) re-estimate the cluster centroids based on cluster assignments from step (1); (3) update neural networks according to the objective by keeping centroids and cluster assignments fixed. Experimental results on four datasets show that our method works significantly better than several other text clustering methods.
ER  -


TY  - Preprint
T1  - Distributed Deep Learning Using Synchronous Stochastic Gradient Descent
A1  - Dipankar Das
A1  - Sasikanth Avancha
A1  - Dheevatsa Mudigere
A1  - Karthikeyan Vaidynathan
A1  - Srinivas Sridharan
A1  - Dhiraj Kalamkar
A1  - Bharat Kaul
A1  - Pradeep Dubey
JO  - ArXiv e-prints
Y1  - 22 February, 2016
UR  - https://arxiv.org/abs/1602.06709
N2  - We design and implement a distributed multinode synchronous SGD algorithm, without altering hyper parameters, or compressing data, or altering algorithmic behavior. We perform a detailed analysis of scaling, and identify optimal design points for different networks. We demonstrate scaling of CNNs on 100s of nodes, and present what we believe to be record training throughputs. A 512 minibatch VGG-A CNN training run is scaled 90X on 128 nodes. Also 256 minibatch VGG-A and OverFeat-FAST networks are scaled 53X and 42X respectively on a 64 node cluster. We also demonstrate the generality of our approach via best-in-class 6.5X scaling for a 7-layer DNN on 16 nodes. Thereafter we attempt to democratize deep-learning by training on an Ethernet based AWS cluster and show ~14X scaling on 16 nodes.
ER  -


TY  - Preprint
T1  - Deep Learning in Finance
A1  - J. B. Heaton
A1  - N. G. Polson
A1  - J. H. Witte
JO  - ArXiv e-prints
Y1  - 14 January, 2018
UR  - https://arxiv.org/abs/1602.06561
N2  - We explore the use of deep learning hierarchical models for problems in financial prediction and classification. Financial prediction problems -- such as those presented in designing and pricing securities, constructing portfolios, and risk management -- often involve large data sets with complex data interactions that currently are difficult or impossible to specify in a full economic model. Applying deep learning methods to these problems can produce more useful results than standard methods in finance. In particular, deep learning can detect and exploit interactions in the data that are, at least currently, invisible to any existing financial economic theory.
ER  -


TY  - Preprint
T1  - Node-By-Node Greedy Deep Learning for Interpretable Features
A1  - Ke Wu
A1  - Malik Magdon-Ismail
JO  - ArXiv e-prints
Y1  - 19 February, 2016
UR  - https://arxiv.org/abs/1602.06183
N2  - Multilayer networks have seen a resurgence under the umbrella of deep learning. Current deep learning algorithms train the layers of the network sequentially, improving algorithmic performance as well as providing some regularization. We present a new training algorithm for deep networks which trains \emph{each node in the network} sequentially. Our algorithm is orders of magnitude faster, creates more interpretable internal representations at the node level, while not sacrificing on the ultimate out-of-sample performance.
ER  -


TY  - Preprint
T1  - Audio Recording Device Identification Based on Deep Learning
A1  - Simeng Qi
A1  - Zheng Huang
A1  - Yan Li
A1  - Shaopei Shi
JO  - ArXiv e-prints
Y1  - 26 April, 2016
UR  - https://arxiv.org/abs/1602.05682
N2  - In this paper we present a research on identification of audio recording devices from background noise, thus providing a method for forensics. The audio signal is the sum of speech signal and noise signal. Usually, people pay more attention to speech signal, because it carries the information to deliver. So a great amount of researches have been dedicated to getting higher Signal-Noise-Ratio (SNR). There are many speech enhancement algorithms to improve the quality of the speech, which can be seen as reducing the noise. However, noises can be regarded as the intrinsic fingerprint traces of an audio recording device. These digital traces can be characterized and identified by new machine learning techniques. Therefore, in our research, we use the noise as the intrinsic features. As for the identification, multiple classifiers of deep learning methods are used and compared. The identification result shows that the method of getting feature vector from the noise of each device and identifying them with deep learning techniques is viable, and well-preformed.
ER  -


TY  - Preprint
T1  - Communication-Efficient Learning of Deep Networks from Decentralized Data
A1  - H. Brendan McMahan
A1  - Eider Moore
A1  - Daniel Ramage
A1  - Seth Hampson
A1  - Blaise AgÃ¼era y Arcas
JO  - ArXiv e-prints
Y1  - 28 February, 2017
UR  - https://arxiv.org/abs/1602.05629
N2  - Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.
ER  -


TY  - Preprint
T1  - On the Use of Deep Learning for Blind Image Quality Assessment
A1  - Simone Bianco
A1  - Luigi Celona
A1  - Paolo Napoletano
A1  - Raimondo Schettini
JO  - ArXiv e-prints
Y1  - 4 April, 2017
UR  - https://arxiv.org/abs/1602.05531
N2  - In this work we investigate the use of deep learning for distortion-generic blind image quality assessment. We report on different design choices, ranging from the use of features extracted from pre-trained Convolutional Neural Networks (CNNs) as a generic image description, to the use of features extracted from a CNN fine-tuned for the image quality task. Our best proposal, named DeepBIQ, estimates the image quality by average pooling the scores predicted on multiple sub-regions of the original image. The score of each sub-region is computed using a Support Vector Regression (SVR) machine taking as input features extracted using a CNN fine-tuned for category-based image quality assessment. Experimental results on the LIVE In the Wild Image Quality Challenge Database and on the LIVE Image Quality Assessment Database show that DeepBIQ outperforms the state-of-the-art methods compared, having a Linear Correlation Coefficient (LCC) with human subjective scores of almost 0.91 and 0.98 respectively. Furthermore, in most of the cases, the quality score predictions of DeepBIQ are closer to the average observer than those of a generic human observer.
ER  -


TY  - Preprint
T1  - Deep Learning on FPGAs: Past, Present, and Future
A1  - Griffin Lacey
A1  - Graham W. Taylor
A1  - Shawki Areibi
JO  - ArXiv e-prints
Y1  - 12 February, 2016
UR  - https://arxiv.org/abs/1602.04283
N2  - The rapid growth of data size and accessibility in recent years has instigated a shift of philosophy in algorithm design for artificial intelligence. Instead of engineering algorithms by hand, the ability to learn composable systems automatically from massive amounts of data has led to ground-breaking performance in important domains such as computer vision, speech recognition, and natural language processing. The most popular class of techniques used in these domains is called deep learning, and is seeing significant attention from industry. However, these models require incredible amounts of data and compute power to train, and are limited by the need for better hardware acceleration to accommodate scaling beyond current data and model sizes. While the current solution has been to use clusters of graphics processing units (GPU) as general purpose processors (GPGPU), the use of field programmable gate arrays (FPGA) provide an interesting alternative. Current trends in design tools for FPGAs have made them more compatible with the high-level software practices typically practiced in the deep learning community, making FPGAs more accessible to those who build and deploy models. Since FPGA architectures are flexible, this could also allow researchers the ability to explore model-level optimizations beyond what is possible on fixed architectures such as GPUs. As well, FPGAs tend to provide high performance per watt of power consumption, which is of particular importance for application scientists interested in large scale server-based deployment or resource-limited embedded applications. This review takes a look at deep learning and FPGAs from a hardware acceleration perspective, identifying trends and innovations that make these technologies a natural fit, and motivates a discussion on how FPGAs may best serve the needs of the deep learning community moving forward.
ER  -


TY  - Preprint
T1  - Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks
A1  - Anh Nguyen
A1  - Jason Yosinski
A1  - Jeff Clune
JO  - ArXiv e-prints
Y1  - 7 May, 2016
UR  - https://arxiv.org/abs/1602.03616
N2  - We can better understand deep neural networks by identifying which features each of their neurons have learned to detect. To do so, researchers have created Deep Visualization techniques including activation maximization, which synthetically generates inputs (e.g. images) that maximally activate each neuron. A limitation of current techniques is that they assume each neuron detects only one type of feature, but we know that neurons can be multifaceted, in that they fire in response to many different types of features: for example, a grocery store class neuron must activate either for rows of produce or for a storefront. Previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron, creating inappropriate mixes of colors, parts of objects, scales, orientations, etc. Here, we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron. We also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization. By separately synthesizing each type of image a neuron fires in response to, the visualizations have more appropriate colors and coherent global structure. Multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron.
ER  -


TY  - Preprint
T1  - Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning
A1  - Hoo-Chang Shin
A1  - Holger R. Roth
A1  - Mingchen Gao
A1  - Le Lu
A1  - Ziyue Xu
A1  - Isabella Nogues
A1  - Jianhua Yao
A1  - Daniel Mollura
A1  - Ronald M. Summers
JO  - ArXiv e-prints
Y1  - 10 February, 2016
UR  - https://arxiv.org/abs/1602.03409
N2  - Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and the revival of deep CNN. CNNs enable learning data-driven, highly representative, layered hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, with 85% sensitivity at 3 false positive per patient, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.
ER  -


TY  - Preprint
T1  - Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks
A1  - Jakob N. Foerster
A1  - Yannis M. Assael
A1  - Nando de Freitas
A1  - Shimon Whiteson
JO  - ArXiv e-prints
Y1  - 8 February, 2016
UR  - https://arxiv.org/abs/1602.02672
N2  - We propose deep distributed recurrent Q-networks (DDRQN), which enable teams of agents to learn to solve communication-based coordination tasks. In these tasks, the agents are not given any pre-designed communication protocol. Therefore, in order to successfully communicate, they must first automatically develop and agree upon their own communication protocol. We present empirical results on two multi-agent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols to do so. To our knowledge, this is the first time deep reinforcement learning has succeeded in learning communication protocols. In addition, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success.
ER  -


TY  - Preprint
T1  - Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms
A1  - Tom Zahavy
A1  - Bingyi Kang
A1  - Alex Sivak
A1  - Jiashi Feng
A1  - Huan Xu
A1  - Shie Mannor
JO  - ArXiv e-prints
Y1  - 5 November, 2017
UR  - https://arxiv.org/abs/1602.02389
N2  - The question why deep learning algorithms generalize so well has attracted increasing research interest. However, most of the well-established approaches, such as hypothesis capacity, stability or sparseness, have not provided complete explanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus on the robustness approach (Xu &amp; Mannor, 2012), i.e., if the error of a hypothesis will not change much due to perturbations of its training examples, then it will also generalize well. As most deep learning algorithms are stochastic (e.g., Stochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness arguments of Xu &amp; Mannor, and introduce a new approach, ensemble robustness, that concerns the robustness of a population of hypotheses. Through the lens of ensemble robustness, we reveal that a stochastic learning algorithm can generalize well as long as its sensitiveness to adversarial perturbations is bounded in average over training examples. Moreover, an algorithm may be sensitive to some adversarial examples (Goodfellow et al., 2015) but still generalize well. To support our claims, we provide extensive simulations for different deep learning algorithms and different network architectures exhibiting a strong correlation between ensemble robustness and the ability to generalize.
ER  -


TY  - Preprint
T1  - Improved Dropout for Shallow and Deep Learning
A1  - Zhe Li
A1  - Boqing Gong
A1  - Tianbao Yang
JO  - ArXiv e-prints
Y1  - 4 December, 2016
UR  - https://arxiv.org/abs/1602.02220
N2  - Dropout has been witnessed with great success in training deep neural networks by independently zeroing out the outputs of neurons at random. It has also received a surge of interest for shallow learning, e.g., logistic regression. However, the independent sampling for dropout could be suboptimal for the sake of convergence. In this paper, we propose to use multinomial sampling for dropout, i.e., sampling features or neurons according to a multinomial distribution with different probabilities for different features/neurons. To exhibit the optimal dropout probabilities, we analyze the shallow learning with multinomial dropout and establish the risk bound for stochastic optimization. By minimizing a sampling dependent factor in the risk bound, we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution. To tackle the issue of evolving distribution of neurons in deep learning, we propose an efficient adaptive dropout (named \textbf{evolutional dropout}) that computes the sampling probabilities on-the-fly from a mini-batch of examples. Empirical studies on several benchmark datasets demonstrate that the proposed dropouts achieve not only much faster convergence and but also a smaller testing error than the standard dropout. For example, on the CIFAR-100 data, the evolutional dropout achieves relative improvements over 10\% on the prediction performance and over 50\% on the convergence speed compared to the standard dropout.
ER  -


TY  - Preprint
T1  - Asynchronous Methods for Deep Reinforcement Learning
A1  - Volodymyr Mnih
A1  - AdriÃ  PuigdomÃ¨nech Badia
A1  - Mehdi Mirza
A1  - Alex Graves
A1  - Timothy P. Lillicrap
A1  - Tim Harley
A1  - David Silver
A1  - Koray Kavukcuoglu
JO  - ArXiv e-prints
Y1  - 16 June, 2016
UR  - https://arxiv.org/abs/1602.01783
N2  - We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.
ER  -


TY  - Preprint
T1  - Unsupervised Regenerative Learning of Hierarchical Features in Spiking Deep Networks for Object Recognition
A1  - Priyadarshini Panda
A1  - Kaushik Roy
JO  - ArXiv e-prints
Y1  - 3 February, 2016
UR  - https://arxiv.org/abs/1602.01510
N2  - We present a spike-based unsupervised regenerative learning scheme to train Spiking Deep Networks (SpikeCNN) for object recognition problems using biologically realistic leaky integrate-and-fire neurons. The training methodology is based on the Auto-Encoder learning model wherein the hierarchical network is trained layer wise using the encoder-decoder principle. Regenerative learning uses spike-timing information and inherent latencies to update the weights and learn representative levels for each convolutional layer in an unsupervised manner. The features learnt from the final layer in the hierarchy are then fed to an output layer. The output layer is trained with supervision by showing a fraction of the labeled training dataset and performs the overall classification of the input. Our proposed methodology yields 0.92%/29.84% classification error on MNIST/CIFAR10 datasets which is comparable with state-of-the-art results. The proposed methodology also introduces sparsity in the hierarchical feature representations on account of event-based coding resulting in computationally efficient learning.
ER  -


TY  - Preprint
T1  - Learning scale-variant and scale-invariant features for deep image classification
A1  - Nanne van Noord
A1  - Eric Postma
JO  - ArXiv e-prints
Y1  - 13 May, 2016
UR  - https://arxiv.org/abs/1602.01255
N2  - Convolutional Neural Networks (CNNs) require large image corpora to be trained on classification tasks. The variation in image resolutions, sizes of objects and patterns depicted, and image scales, hampers CNN training and performance, because the task-relevant information varies over spatial scales. Previous work attempting to deal with such scale variations focused on encouraging scale-invariant CNN representations. However, scale-invariant representations are incomplete representations of images, because images contain scale-variant information as well. This paper addresses the combined development of scale-invariant and scale-variant representations. We propose a multi- scale CNN method to encourage the recognition of both types of features and evaluate it on a challenging image classification task involving task-relevant characteristics at multiple scales. The results show that our multi-scale CNN outperforms single-scale CNN. This leads to the conclusion that encouraging the combined development of a scale-invariant and scale-variant representation in CNNs is beneficial to image recognition performance.
ER  -


TY  - Preprint
T1  - On Deep Multi-View Representation Learning: Objectives and Optimization
A1  - Weiran Wang
A1  - Raman Arora
A1  - Karen Livescu
A1  - Jeff Bilmes
JO  - ArXiv e-prints
Y1  - 2 February, 2016
UR  - https://arxiv.org/abs/1602.01024
N2  - We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for learning while only one view is available for downstream tasks. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a batch-style correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them empirically on image, speech, and text tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE). We also explore a stochastic optimization procedure for minibatch correlation-based objectives and discuss the time/performance trade-offs for kernel-based and neural network-based implementations.
ER  -


TY  - Preprint
T1  - Learning a Deep Model for Human Action Recognition from Novel Viewpoints
A1  - Hossein Rahmani
A1  - Ajmal Mian
A1  - Mubarak Shah
JO  - ArXiv e-prints
Y1  - 2 February, 2016
UR  - https://arxiv.org/abs/1602.00828
N2  - Recognizing human actions from unknown and unseen (novel) views is a challenging problem. We propose a Robust Non-Linear Knowledge Transfer Model (R-NKTM) for human action recognition from novel views. The proposed R-NKTM is a deep fully-connected neural network that transfers knowledge of human actions from any unknown view to a shared high-level virtual view by finding a non-linear virtual path that connects the views. The R-NKTM is learned from dense trajectories of synthetic 3D human models fitted to real motion capture data and generalizes to real videos of human actions. The strength of our technique is that we learn a single R-NKTM for all actions and all viewpoints for knowledge transfer of any real human action video without the need for re-training or fine-tuning the model. Thus, R-NKTM can efficiently scale to incorporate new action classes. R-NKTM is learned with dummy labels and does not require knowledge of the camera viewpoint at any stage. Experiments on three benchmark cross-view human action datasets show that our method outperforms existing state-of-the-art.
ER  -


TY  - Preprint
T1  - A Deep Learning Based Fast Image Saliency Detection Algorithm
A1  - Hengyue Pan
A1  - Hui Jiang
JO  - ArXiv e-prints
Y1  - 1 February, 2016
UR  - https://arxiv.org/abs/1602.00577
N2  - In this paper, we propose a fast deep learning method for object saliency detection using convolutional neural networks. In our approach, we use a gradient descent method to iteratively modify the input images based on the pixel-wise gradients to reduce a pre-defined cost function, which is defined to measure the class-specific objectness and clamp the class-irrelevant outputs to maintain image background. The pixel-wise gradients can be efficiently computed using the back-propagation algorithm. We further apply SLIC superpixels and LAB color based low level saliency features to smooth and refine the gradients. Our methods are quite computationally efficient, much faster than other deep learning based saliency methods. Experimental results on two benchmark tasks, namely Pascal VOC 2012 and MSRA10k, have shown that our proposed methods can generate high-quality salience maps, at least comparable with many slow and complicated deep learning methods. Comparing with the pure low-level methods, our approach excels in handling many difficult images, which contain complex background, highly-variable salient objects, multiple objects, and/or very small salient objects.
ER  -


TY  - Preprint
T1  - An Iterative Deep Learning Framework for Unsupervised Discovery of Speech Features and Linguistic Units with Applications on Spoken Term Detection
A1  - Cheng-Tao Chung
A1  - Cheng-Yu Tsai
A1  - Hsiang-Hung Lu
A1  - Chia-Hsiang Liu
A1  - Hung-yi Lee
A1  - Lin-shan Lee
JO  - ArXiv e-prints
Y1  - 1 February, 2016
UR  - https://arxiv.org/abs/1602.00426
N2  - In this work we aim to discover high quality speech features and linguistic units directly from unlabeled speech data in a zero resource scenario. The results are evaluated using the metrics and corpora proposed in the Zero Resource Speech Challenge organized at Interspeech 2015. A Multi-layered Acoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters that describe the model configuration. These sets of acoustic tokens carry different characteristics fof the given corpus and the language behind, thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target Deep Neural Network (MDNN) trained on low-level acoustic features. Bottleneck features extracted from the MDNN are then used as the feedback input to the MAT and the MDNN itself in the next iteration. We call this iterative deep learning framework the Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), which generates both high quality speech features for the Track 1 of the Challenge and acoustic tokens for the Track 2 of the Challenge. In addition, we performed extra experiments on the same corpora on the application of query-by-example spoken term detection. The experimental results showed the iterative deep learning framework of MAT-DNN improved the detection performance due to better underlying speech features and acoustic tokens.
ER  -


TY  - Preprint
T1  - Greedy Deep Dictionary Learning
A1  - Snigdha Tariyal
A1  - Angshul Majumdar
A1  - Richa Singh
A1  - Mayank Vatsa
JO  - ArXiv e-prints
Y1  - 31 January, 2016
UR  - https://arxiv.org/abs/1602.00203
N2  - In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVD. Our method yields better results than all.
ER  -


TY  - Preprint
T1  - Deep Learning For Smile Recognition
A1  - Patrick O. Glauner
JO  - ArXiv e-prints
Y1  - 25 July, 2017
UR  - https://arxiv.org/abs/1602.00172
N2  - Inspired by recent successes of deep learning in computer vision, we propose a novel application of deep convolutional neural networks to facial expression recognition, in particular smile recognition. A smile recognition test accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action (DISFA) database, significantly outperforming existing approaches based on hand-crafted features with accuracies ranging from 65.55% to 79.67%. The novelty of this approach includes a comprehensive model selection of the architecture parameters, allowing to find an appropriate architecture for each expression such as smile. This is feasible because all experiments were run on a Tesla K40c GPU, allowing a speedup of factor 10 over traditional computations on a CPU.
ER  -


TY  - Preprint
T1  - Deep Learning Based Semantic Video Indexing and Retrieval
A1  - Anna Podlesnaya
A1  - Sergey Podlesnyy
JO  - ArXiv e-prints
Y1  - 28 January, 2016
UR  - https://arxiv.org/abs/1601.07754
N2  - We share the implementation details and testing results for video retrieval system based exclusively on features extracted by convolutional neural networks. We show that deep learned features might serve as universal signature for semantic content of video useful in many search and retrieval tasks. We further show that graph-based storage structure for video index allows to efficiently retrieving the content with complicated spatial and temporal search queries.
ER  -


TY  - Preprint
T1  - Deep Learning Driven Visual Path Prediction from a Single Image
A1  - Siyu Huang
A1  - Xi Li
A1  - Zhongfei Zhang
A1  - Zhouzhou He
A1  - Fei Wu
A1  - Wei Liu
A1  - Jinhui Tang
A1  - Yueting Zhuang
JO  - ArXiv e-prints
Y1  - 27 January, 2016
UR  - https://arxiv.org/abs/1601.07265
N2  - Capabilities of inference and prediction are significant components of visual systems. In this paper, we address an important and challenging task of them: visual path prediction. Its goal is to infer the future path for a visual object in a static scene. This task is complicated as it needs high-level semantic understandings of both the scenes and motion patterns underlying video sequences. In practice, cluttered situations have also raised higher demands on the effectiveness and robustness of the considered models. Motivated by these observations, we propose a deep learning framework which simultaneously performs deep feature learning for visual representation in conjunction with spatio-temporal context modeling. After that, we propose a unified path planning scheme to make accurate future path prediction based on the analytic results of the context models. The highly effective visual representation and deep context models ensure that our framework makes a deep semantic understanding of the scene and motion pattern, consequently improving the performance of the visual path prediction task. In order to comprehensively evaluate the model&#39;s performance on the visual path prediction task, we construct two large benchmark datasets from the adaptation of video tracking datasets. The qualitative and quantitative experimental results show that our approach outperforms the existing approaches and owns a better generalization capability.
ER  -


TY  - Preprint
T1  - Hough-CNN: Deep Learning for Segmentation of Deep Brain Regions in MRI and Ultrasound
A1  - Fausto Milletari
A1  - Seyed-Ahmad Ahmadi
A1  - Christine Kroll
A1  - Annika Plate
A1  - Verena Rozanski
A1  - Juliana Maiostre
A1  - Johannes Levin
A1  - Olaf Dietrich
A1  - Birgit Ertl-Wagner
A1  - Kai BÃ¶tzel
A1  - Nassir Navab
JO  - ArXiv e-prints
Y1  - 31 January, 2016
UR  - https://arxiv.org/abs/1601.07014
N2  - In this work we propose a novel approach to perform segmentation by leveraging the abstraction capabilities of convolutional neural networks (CNNs). Our method is based on Hough voting, a strategy that allows for fully automatic localisation and segmentation of the anatomies of interest. This approach does not only use the CNN classification outcomes, but it also implements voting by exploiting the features produced by the deepest portion of the network. We show that this learning-based segmentation method is robust, multi-region, flexible and can be easily adapted to different modalities. In the attempt to show the capabilities and the behaviour of CNNs when they are applied to medical image analysis, we perform a systematic study of the performances of six different network architectures, conceived according to state-of-the-art criteria, in various situations. We evaluate the impact of both different amount of training data and different data dimensionality (2D, 2.5D and 3D) on the final results. We show results on both MRI and transcranial US volumes depicting respectively 26 regions of the basal ganglia and the midbrain.
ER  -


TY  - Preprint
T1  - PN-Net: Conjoined Triple Deep Network for Learning Local Image Descriptors
A1  - Vassileios Balntas
A1  - Edward Johns
A1  - Lilian Tang
A1  - Krystian Mikolajczyk
JO  - ArXiv e-prints
Y1  - 19 January, 2016
UR  - https://arxiv.org/abs/1601.05030
N2  - In this paper we propose a new approach for learning local descriptors for matching image patches. It has recently been demonstrated that descriptors based on convolutional neural networks (CNN) can significantly improve the matching performance. Unfortunately their computational complexity is prohibitive for any practical application. We address this problem and propose a CNN based descriptor with improved matching performance, significantly reduced training and execution time, as well as low dimensionality.
ER  -


TY  - Preprint
T1  - SimpleDS: A Simple Deep Reinforcement Learning Dialogue System
A1  - Heriberto CuayÃ¡huitl
JO  - ArXiv e-prints
Y1  - 18 January, 2016
UR  - https://arxiv.org/abs/1601.04574
N2  - This paper presents &#39;SimpleDS&#39;, a simple and publicly available dialogue system trained with deep reinforcement learning. In contrast to previous reinforcement learning dialogue systems, this system avoids manual feature engineering by performing action selection directly from raw text of the last system and (noisy) user responses. Our initial results, in the restaurant domain, show that it is indeed possible to induce reasonable dialogue behaviour with an approach that aims for high levels of automation in dialogue control for intelligent interactive agents.
ER  -


TY  - Preprint
T1  - Deep Learning Applied to Image and Text Matching
A1  - Afroze Ibrahim Baqapuri
JO  - ArXiv e-prints
Y1  - 14 September, 2015
UR  - https://arxiv.org/abs/1601.03478
N2  - The ability to describe images with natural language sentences is the hallmark for image and language understanding. Such a system has wide ranging applications such as annotating images and using natural sentences to search for images.In this project we focus on the task of bidirectional image retrieval: such asystem is capable of retrieving an image based on a sentence (image search) andretrieve sentence based on an image query (image annotation). We present asystem based on a global ranking objective function which uses a combinationof convolutional neural networks (CNN) and multi layer perceptrons (MLP).It takes a pair of image and sentence and processes them in different channels,finally embedding it into a common multimodal vector space. These embeddingsencode abstract semantic information about the two inputs and can be comparedusing traditional information retrieval approaches. For each such pair, the modelreturns a score which is interpretted as a similarity metric. If this score is high,the image and sentence are likely to convey similar meaning, and if the score is low then they are likely not to.
ER  -


TY  - Preprint
T1  - Deep Learning of Part-based Representation of Data Using Sparse Autoencoders with Nonnegativity Constraints
A1  - Ehsan Hosseini-Asl
A1  - Jacek M. Zurada
A1  - Olfa Nasraoui
JO  - ArXiv e-prints
Y1  - 12 January, 2016
UR  - https://arxiv.org/abs/1601.02733
N2  - We demonstrate a new deep learning autoencoder network, trained by a nonnegativity constraint algorithm (NCAE), that learns features which show part-based representation of data. The learning algorithm is based on constraining negative weights. The performance of the algorithm is assessed based on decomposing data into parts and its prediction performance is tested on three standard image data sets and one text dataset. The results indicate that the nonnegativity constraint forces the autoencoder to learn features that amount to a part-based representation of data, while improving sparsity and reconstruction quality in comparison with the traditional sparse autoencoder and Nonnegative Matrix Factorization. It is also shown that this newly acquired representation improves the prediction performance of a deep neural network.
ER  -


TY  - Preprint
T1  - Robobarista: Learning to Manipulate Novel Objects via Deep Multimodal Embedding
A1  - Jaeyong Sung
A1  - Seok Hyun Jin
A1  - Ian Lenz
A1  - Ashutosh Saxena
JO  - ArXiv e-prints
Y1  - 11 January, 2016
UR  - https://arxiv.org/abs/1601.02705
N2  - There is a large variety of objects and appliances in human environments, such as stoves, coffee dispensers, juice extractors, and so on. It is challenging for a roboticist to program a robot for each of these object types and for each of their instantiations. In this work, we present a novel approach to manipulation planning based on the idea that many household objects share similarly-operated object parts. We formulate the manipulation planning as a structured prediction problem and learn to transfer manipulation strategy across different objects by embedding point-cloud, natural language, and manipulation trajectory data into a shared embedding space using a deep neural network. In order to learn semantically meaningful spaces throughout our network, we introduce a method for pre-training its lower layers for multimodal feature embedding and a method for fine-tuning this embedding space using a loss-based margin. In order to collect a large number of manipulation demonstrations for different objects, we develop a new crowd-sourcing platform called Robobarista. We test our model on our dataset consisting of 116 objects and appliances with 249 parts along with 250 language instructions, for which there are 1225 crowd-sourced manipulation demonstrations. We further show that our robot with our model can even prepare a cup of a latte with appliances it has never seen before.
ER  -


TY  - Preprint
T1  - Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction
A1  - Weinan Zhang
A1  - Tianming Du
A1  - Jun Wang
JO  - ArXiv e-prints
Y1  - 11 January, 2016
UR  - https://arxiv.org/abs/1601.02376
N2  - Predicting user responses, such as click-through rate and conversion rate, are critical in many web applications including web search, personalised recommendation, and online advertising. Different from continuous raw features that we usually found in the image and audio domains, the input features in web space are always of multi-field and are mostly discrete and categorical while their dependencies are little known. Major user response prediction models have to either limit themselves to linear models or require manually building up high-order combination features. The former loses the ability of exploring feature interactions, while the latter results in a heavy computation in the large feature space. To tackle the issue, we propose two novel models using deep neural networks (DNNs) to automatically learn effective patterns from categorical feature interactions and make predictions of users&#39; ad clicks. To get our DNNs efficiently work, we propose to leverage three feature transformation methods, i.e., factorisation machines (FMs), restricted Boltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paper presents the structure of our models and their efficient training algorithms. The large-scale experiments with real-world data demonstrate that our methods work better than major state-of-the-art models.
ER  -


TY  - Preprint
T1  - Brain4Cars: Car That Knows Before You Do via Sensory-Fusion Deep Learning Architecture
A1  - Ashesh Jain
A1  - Hema S Koppula
A1  - Shane Soh
A1  - Bharad Raghavan
A1  - Avi Singh
A1  - Ashutosh Saxena
JO  - ArXiv e-prints
Y1  - 5 January, 2016
UR  - https://arxiv.org/abs/1601.00740
N2  - Advanced Driver Assistance Systems (ADAS) have made driving safer over the last decade. They prepare vehicles for unsafe road conditions and alert drivers if they perform a dangerous maneuver. However, many accidents are unavoidable because by the time drivers are alerted, it is already too late. Anticipating maneuvers beforehand can alert drivers before they perform the maneuver and also give ADAS more time to avoid or prepare for the danger.
ER  -


TY  - Preprint
T1  - Learning Local Image Descriptors with Deep Siamese and Triplet Convolutional Networks by Minimising Global Loss Functions
A1  - Vijay Kumar B G
A1  - Gustavo Carneiro
A1  - Ian Reid
JO  - ArXiv e-prints
Y1  - 1 August, 2016
UR  - https://arxiv.org/abs/1512.09272
N2  - Recent innovations in training deep convolutional neural network (ConvNet) models have motivated the design of new methods to automatically learn local image descriptors. The latest deep ConvNets proposed for this task consist of a siamese network that is trained by penalising misclassification of pairs of local image patches. Current results from machine learning show that replacing this siamese by a triplet network can improve the classification accuracy in several problems, but this has yet to be demonstrated for local image descriptor learning. Moreover, current siamese and triplet networks have been trained with stochastic gradient descent that computes the gradient from individual pairs or triplets of local image patches, which can make them prone to overfitting. In this paper, we first propose the use of triplet networks for the problem of local image descriptor learning. Furthermore, we also propose the use of a global loss that minimises the overall classification error in the training set, which can improve the generalisation capability of the model. Using the UBC benchmark dataset for comparing local image descriptors, we show that the triplet network produces a more accurate embedding than the siamese network in terms of the UBC dataset errors. Moreover, we also demonstrate that a combination of the triplet and global losses produces the best embedding in the field, using this triplet network. Finally, we also show that the use of the central-surround siamese network trained with the global loss produces the best result of the field on the UBC dataset. Pre-trained models are available online at https://github.com/vijaykbg/deep-patchmatch
ER  -


TY  - Preprint
T1  - Inverse Reinforcement Learning via Deep Gaussian Process
A1  - Ming Jin
A1  - Andreas Damianou
A1  - Pieter Abbeel
A1  - Costas Spanos
JO  - ArXiv e-prints
Y1  - 4 May, 2017
UR  - https://arxiv.org/abs/1512.08065
N2  - We propose a new approach to inverse reinforcement learning (IRL) based on the deep Gaussian process (deep GP) model, which is capable of learning complicated reward structures with few demonstrations. Our model stacks multiple latent GP layers to learn abstract representations of the state feature space, which is linked to the demonstrations through the Maximum Entropy learning framework. Incorporating the IRL engine into the nonlinear latent structure renders existing deep GP inference approaches intractable. To tackle this, we develop a non-standard variational approximation framework which extends previous inference schemes. This allows for approximate Bayesian treatment of the feature space and guards against overfitting. Carrying out representation and inverse reinforcement learning simultaneously within our model outperforms state-of-the-art approaches, as we demonstrate with experiments on standard benchmarks (&#34;object world&#34;,&#34;highway driving&#34;) and a new benchmark (&#34;binary world&#34;).
ER  -


TY  - Preprint
T1  - A Combined Deep-Learning and Deformable-Model Approach to Fully Automatic Segmentation of the Left Ventricle in Cardiac MRI
A1  - M. R. Avendi
A1  - A. Kheradvar
A1  - H. Jafarkhani
JO  - ArXiv e-prints
Y1  - 24 December, 2015
UR  - https://arxiv.org/abs/1512.07951
N2  - Segmentation of the left ventricle (LV) from cardiac magnetic resonance imaging (MRI) datasets is an essential step for calculation of clinical indices such as ventricular volume and ejection fraction. In this work, we employ deep learning algorithms combined with deformable models to develop and evaluate a fully automatic segmentation tool for the LV from short-axis cardiac MRI datasets. The method employs deep learning algorithms to learn the segmentation task from the ground true data. Convolutional networks are employed to automatically detect the LV chamber in MRI dataset. Stacked autoencoders are utilized to infer the shape of the LV. The inferred shape is incorporated into deformable models to improve the accuracy and robustness of the segmentation. We validated our method using 45 cardiac MR datasets taken from the MICCAI 2009 LV segmentation challenge and showed that it outperforms the state-of-the art methods. Excellent agreement with the ground truth was achieved. Validation metrics, percentage of good contours, Dice metric, average perpendicular distance and conformity, were computed as 96.69%, 0.94, 1.81mm and 0.86, versus those of 79.2%-95.62%, 0.87-0.9, 1.76-2.97mm and 0.67-0.78, obtained by other methods, respectively.
ER  -


TY  - Preprint
T1  - Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network
A1  - Seunghoon Hong
A1  - Junhyuk Oh
A1  - Bohyung Han
A1  - Honglak Lee
JO  - ArXiv e-prints
Y1  - 24 December, 2015
UR  - https://arxiv.org/abs/1512.07928
N2  - We propose a novel weakly-supervised semantic segmentation algorithm based on Deep Convolutional Neural Network (DCNN). Contrary to existing weakly-supervised approaches, our algorithm exploits auxiliary segmentation annotations available for different categories to guide segmentations on images with only image-level class labels. To make the segmentation knowledge transferrable across categories, we design a decoupled encoder-decoder architecture with attention model. In this architecture, the model generates spatial highlights of each category presented in an image using an attention model, and subsequently generates foreground segmentation for each highlighted region using decoder. Combining attention model, we show that the decoder trained with segmentation annotations in different categories can boost the performance of weakly-supervised semantic segmentation. The proposed algorithm demonstrates substantially improved performance compared to the state-of-the-art weakly-supervised techniques in challenging PASCAL VOC 2012 dataset when our model is trained with the annotations in 60 exclusive categories in Microsoft COCO dataset.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning in Large Discrete Action Spaces
A1  - Gabriel Dulac-Arnold
A1  - Richard Evans
A1  - Hado van Hasselt
A1  - Peter Sunehag
A1  - Timothy Lillicrap
A1  - Jonathan Hunt
A1  - Timothy Mann
A1  - Theophane Weber
A1  - Thomas Degris
A1  - Ben Coppin
JO  - ArXiv e-prints
Y1  - 4 April, 2016
UR  - https://arxiv.org/abs/1512.07679
N2  - Being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. Recommender systems, industrial plants and language models are only some of the many real-world tasks involving large numbers of discrete actions for which current methods are difficult or even often impossible to apply. An ability to generalize over the set of actions as well as sub-linear complexity relative to the size of the set are both necessary to handle such tasks. Current approaches are not able to provide both of these, which motivates the work in this paper. Our proposed approach leverages prior information about the actions to embed them in a continuous space upon which it can generalize. Additionally, approximate nearest-neighbor methods allow for logarithmic-time lookup complexity relative to the number of actions, which is necessary for time-wise tractable training. This combined approach allows reinforcement learning methods to be applied to large-scale learning problems previously intractable with current methods. We demonstrate our algorithm&#39;s abilities on a series of tasks having up to one million actions.
ER  -


TY  - Preprint
T1  - Implementation of deep learning algorithm for automatic detection of brain tumors using intraoperative IR-thermal mapping data
A1  - A. V. Makarenko
A1  - M. G. Volovik
JO  - ArXiv e-prints
Y1  - 22 December, 2015
UR  - https://arxiv.org/abs/1512.07041
N2  - The efficiency of deep machine learning for automatic delineation of tumor areas has been demonstrated for intraoperative neuronavigation using active IR-mapping with the use of the cold test. The proposed approach employs a matrix IR-imager to remotely register the space-time distribution of surface temperature pattern, which is determined by the dynamics of local cerebral blood flow. The advantages of this technique are non-invasiveness, zero risks for the health of patients and medical staff, low implementation and operational costs, ease and speed of use. Traditional IR-diagnostic technique has a crucial limitation - it involves a diagnostician who determines the boundaries of tumor areas, which gives rise to considerable uncertainty, which can lead to diagnosis errors that are difficult to control. The current study demonstrates that implementing deep learning algorithms allows to eliminate the explained drawback.
ER  -


TY  - Preprint
T1  - Deep Learning with S-shaped Rectified Linear Activation Units
A1  - Xiaojie Jin
A1  - Chunyan Xu
A1  - Jiashi Feng
A1  - Yunchao Wei
A1  - Junjun Xiong
A1  - Shuicheng Yan
JO  - ArXiv e-prints
Y1  - 22 December, 2015
UR  - https://arxiv.org/abs/1512.07030
N2  - Rectified linear activation units are important components for state-of-the-art deep convolutional networks. In this paper, we propose a novel S-shaped rectified linear activation unit (SReLU) to learn both convex and non-convex functions, imitating the multiple function forms given by the two fundamental laws, namely the Webner-Fechner law and the Stevens law, in psychophysics and neural sciences. Specifically, SReLU consists of three piecewise linear functions, which are formulated by four learnable parameters. The SReLU is learned jointly with the training of the whole deep network through back propagation. During the training phase, to initialize SReLU in different layers, we propose a &#34;freezing&#34; method to degenerate SReLU into a predefined leaky rectified linear unit in the initial several training epochs and then adaptively learn the good initial values. SReLU can be universally used in the existing deep networks with negligible additional parameters and computation cost. Experiments with two popular CNN architectures, Network in Network and GoogLeNet on scale-various benchmarks including CIFAR10, CIFAR100, MNIST and ImageNet demonstrate that SReLU achieves remarkable improvement compared to other activation functions.
ER  -


TY  - Preprint
T1  - A C++ library for Multimodal Deep Learning
A1  - Jian Jin
JO  - ArXiv e-prints
Y1  - 12 April, 2016
UR  - https://arxiv.org/abs/1512.06927
N2  - MDL, Multimodal Deep Learning Library, is a deep learning framework that supports multiple models, and this document explains its philosophy and functionality. MDL runs on Linux, Mac, and Unix platforms. It depends on OpenCV.
ER  -


TY  - Preprint
T1  - Deep Learning for Surface Material Classification Using Haptic And Visual Information
A1  - Haitian Zheng
A1  - Lu Fang
A1  - Mengqi Ji
A1  - Matti Strese
A1  - Yigitcan Ozer
A1  - Eckehard Steinbach
JO  - ArXiv e-prints
Y1  - 1 May, 2016
UR  - https://arxiv.org/abs/1512.06658
N2  - When a user scratches a hand-held rigid tool across an object surface, an acceleration signal can be captured, which carries relevant information about the surface. More importantly, such a haptic signal is complementary to the visual appearance of the surface, which suggests the combination of both modalities for the recognition of the surface material. In this paper, we present a novel deep learning method dealing with the surface material classification problem based on a Fully Convolutional Network (FCN), which takes as input the aforementioned acceleration signal and a corresponding image of the surface texture. Compared to previous surface material classification solutions, which rely on a careful design of hand-crafted domain-specific features, our method automatically extracts discriminative features utilizing the advanced deep learning methodologies. Experiments performed on the TUM surface material database demonstrate that our method achieves state-of-the-art classification accuracy robustly and efficiently.
ER  -


TY  - Preprint
T1  - Poseidon: A System Architecture for Efficient GPU-based Deep Learning on Multiple Machines
A1  - Hao Zhang
A1  - Zhiting Hu
A1  - Jinliang Wei
A1  - Pengtao Xie
A1  - Gunhee Kim
A1  - Qirong Ho
A1  - Eric Xing
JO  - ArXiv e-prints
Y1  - 19 December, 2015
UR  - https://arxiv.org/abs/1512.06216
N2  - Deep learning (DL) has achieved notable successes in many machine learning tasks. A number of frameworks have been developed to expedite the process of designing and training deep neural networks (DNNs), such as Caffe, Torch and Theano. Currently they can harness multiple GPUs on a single machine, but are unable to use GPUs that are distributed across multiple machines; as even average-sized DNNs can take days to train on a single GPU with 100s of GBs to TBs of data, distributed GPUs present a prime opportunity for scaling up DL. However, the limited bandwidth available on commodity Ethernet networks presents a bottleneck to distributed GPU training, and prevents its trivial realization.
ER  -


TY  - Preprint
T1  - Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks
A1  - Li Shen
A1  - Zhouchen Lin
A1  - Qingming Huang
JO  - ArXiv e-prints
Y1  - 3 April, 2016
UR  - https://arxiv.org/abs/1512.05830
N2  - Learning deeper convolutional neural networks becomes a tendency in recent years. However, many empirical evidences suggest that performance improvement cannot be gained by simply stacking more layers. In this paper, we consider the issue from an information theoretical perspective, and propose a novel method Relay Backpropagation, that encourages the propagation of effective information through the network in training stage. By virtue of the method, we achieved the first place in ILSVRC 2015 Scene Classification Challenge. Extensive experiments on two challenging large scale datasets demonstrate the effectiveness of our method is not restricted to a specific dataset or network architecture. Our models will be available to the research community later.
ER  -


TY  - Preprint
T1  - Deep-Spying: Spying using Smartwatch and Deep Learning
A1  - Tony Beltramelli
A1  - Sebastian Risi
JO  - ArXiv e-prints
Y1  - 17 December, 2015
UR  - https://arxiv.org/abs/1512.05616
N2  - Wearable technologies are today on the rise, becoming more common and broadly available to mainstream users. In fact, wristband and armband devices such as smartwatches and fitness trackers already took an important place in the consumer electronics market and are becoming ubiquitous. By their very nature of being wearable, these devices, however, provide a new pervasive attack surface threatening users privacy, among others.
ER  -


TY  - Preprint
T1  - Fine-grained Categorization and Dataset Bootstrapping using Deep Metric Learning with Humans in the Loop
A1  - Yin Cui
A1  - Feng Zhou
A1  - Yuanqing Lin
A1  - Serge Belongie
JO  - ArXiv e-prints
Y1  - 11 April, 2016
UR  - https://arxiv.org/abs/1512.05227
N2  - Existing fine-grained visual categorization methods often suffer from three challenges: lack of training data, large number of fine-grained categories, and high intraclass vs. low inter-class variance. In this work we propose a generic iterative framework for fine-grained categorization and dataset bootstrapping that handles these three challenges. Using deep metric learning with humans in the loop, we learn a low dimensional feature embedding with anchor points on manifolds for each category. These anchor points capture intra-class variances and remain discriminative between classes. In each round, images with high confidence scores from our model are sent to humans for labeling. By comparing with exemplar images, labelers mark each candidate image as either a &#34;true positive&#34; or a &#34;false positive&#34;. True positives are added into our current dataset and false positives are regarded as &#34;hard negatives&#34; for our metric learning model. Then the model is retrained with an expanded dataset and hard negatives for the next round. To demonstrate the effectiveness of the proposed framework, we bootstrap a fine-grained flower dataset with 620 categories from Instagram images. The proposed deep metric learning scheme is evaluated on both our dataset and the CUB-200-2001 Birds dataset. Experimental evaluations show significant performance gain using dataset bootstrapping and demonstrate state-of-the-art results achieved by the proposed deep metric learning methods.
ER  -


TY  - Preprint
T1  - On Deep Representation Learning from Noisy Web Images
A1  - Phong D. Vo
A1  - Alexandru Ginsca
A1  - HervÃ© Le Borgne
A1  - Adrian Popescu
JO  - ArXiv e-prints
Y1  - 15 July, 2016
UR  - https://arxiv.org/abs/1512.04785
N2  - The keep-growing content of Web images may be the next important data source to scale up deep neural networks, which recently obtained a great success in the ImageNet classification challenge and related tasks. This prospect, however, has not been validated on convolutional networks (convnet) -- one of best performing deep models -- because of their supervised regime. While unsupervised alternatives are not so good as convnet in generalizing the learned model to new domains, we use convnet to leverage semi-supervised representation learning. Our approach is to use massive amounts of unlabeled and noisy Web images to train convnets as general feature detectors despite challenges coming from data such as high level of mislabeled data, outliers, and data biases. Extensive experiments are conducted at several data scales, different network architectures, and data reranking techniques. The learned representations are evaluated on nine public datasets of various topics. The best results obtained by our convnets, trained on 3.14 million Web images, outperform AlexNet trained on 1.2 million clean images of ILSVRC 2012 and is closing the gap with VGG-16. These prominent results suggest a budget solution to use deep learning in practice and motivate more research in semi-supervised representation learning.
ER  -


TY  - Preprint
T1  - Learning Deep Features for Discriminative Localization
A1  - Bolei Zhou
A1  - Aditya Khosla
A1  - Agata Lapedriza
A1  - Aude Oliva
A1  - Antonio Torralba
JO  - ArXiv e-prints
Y1  - 13 December, 2015
UR  - https://arxiv.org/abs/1512.04150
N2  - In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them
ER  -


TY  - Preprint
T1  - Deep Learning-Based Image Kernel for Inductive Transfer
A1  - Neeraj Kumar
A1  - Animesh Karmakar
A1  - Ranti Dev Sharma
A1  - Abhinav Mittal
A1  - Amit Sethi
JO  - ArXiv e-prints
Y1  - 16 February, 2016
UR  - https://arxiv.org/abs/1512.04086
N2  - We propose a method to classify images from target classes with a small number of training examples based on transfer learning from non-target classes. Without using any more information than class labels for samples from non-target classes, we train a Siamese net to estimate the probability of two images to belong to the same class. With some post-processing, output of the Siamese net can be used to form a gram matrix of a Mercer kernel. Coupled with a support vector machine (SVM), such a kernel gave reasonable classification accuracy on target classes without any fine-tuning. When the Siamese net was only partially fine-tuned using a small number of samples from the target classes, the resulting classifier outperformed the state-of-the-art and other alternatives. We share class separation capabilities and insights into the learning process of such a kernel on MNIST, Dogs vs. Cats, and CIFAR-10 datasets.
ER  -


TY  - Preprint
T1  - Efficient Deep Feature Learning and Extraction via StochasticNets
A1  - Mohammad Javad Shafiee
A1  - Parthipan Siva
A1  - Paul Fieguth
A1  - Alexander Wong
JO  - ArXiv e-prints
Y1  - 11 December, 2015
UR  - https://arxiv.org/abs/1512.03844
N2  - Deep neural networks are a powerful tool for feature learning and extraction given their ability to model high-level abstractions in highly complex data. One area worth exploring in feature learning and extraction using deep neural networks is efficient neural connectivity formation for faster feature learning and extraction. Motivated by findings of stochastic synaptic connectivity formation in the brain as well as the brain&#39;s uncanny ability to efficiently represent information, we propose the efficient learning and extraction of features via StochasticNets, where sparsely-connected deep neural networks can be formed via stochastic connectivity between neurons. To evaluate the feasibility of such a deep neural network architecture for feature learning and extraction, we train deep convolutional StochasticNets to learn abstract features using the CIFAR-10 dataset, and extract the learned features from images to perform classification on the SVHN and STL-10 datasets. Experimental results show that features learned using deep convolutional StochasticNets, with fewer neural connections than conventional deep convolutional neural networks, can allow for better or comparable classification accuracy than conventional deep neural networks: relative test error decrease of ~4.5% for classification on the STL-10 dataset and ~1% for classification on the SVHN dataset. Furthermore, it was shown that the deep features extracted using deep convolutional StochasticNets can provide comparable classification accuracy even when only 10% of the training data is used for feature learning. Finally, it was also shown that significant gains in feature extraction speed can be achieved in embedded applications using StochasticNets. As such, StochasticNets allow for faster feature learning and extraction performance while facilitate for better or comparable accuracy performances.
ER  -


TY  - Preprint
T1  - Deep Feature Learning with Relative Distance Comparison for Person Re-identification
A1  - Shengyong Ding
A1  - Liang Lin
A1  - Guangrun Wang
A1  - Hongyang Chao
JO  - ArXiv e-prints
Y1  - 11 December, 2015
UR  - https://arxiv.org/abs/1512.03622
N2  - Identifying the same individual across different scenes is an important yet difficult task in intelligent video surveillance. Its main difficulty lies in how to preserve similarity of the same person against large appearance and structure variation while discriminating different individuals. In this paper, we present a scalable distance driven feature learning framework based on the deep neural network for person re-identification, and demonstrate its effectiveness to handle the existing challenges. Specifically, given the training images with the class labels (person IDs), we first produce a large number of triplet units, each of which contains three images, i.e. one person with a matched reference and a mismatched reference. Treating the units as the input, we build the convolutional neural network to generate the layered representations, and follow with the $L2$ distance metric. By means of parameter optimization, our framework tends to maximize the relative distance between the matched pair and the mismatched pair for each triplet unit. Moreover, a nontrivial issue arising with the framework is that the triplet organization cubically enlarges the number of training triplets, as one image can be involved into several triplet units. To overcome this problem, we develop an effective triplet generation scheme and an optimized gradient descent algorithm, making the computational load mainly depends on the number of original images instead of the number of triplets. On several challenging databases, our approach achieves very promising results and outperforms other state-of-the-art approaches.
ER  -


TY  - Preprint
T1  - Deep Residual Learning for Image Recognition
A1  - Kaiming He
A1  - Xiangyu Zhang
A1  - Shaoqing Ren
A1  - Jian Sun
JO  - ArXiv e-prints
Y1  - 10 December, 2015
UR  - https://arxiv.org/abs/1512.03385
N2  - Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.
ER  -


TY  - Preprint
T1  - Deep Learning Algorithms with Applications to Video Analytics for A Smart City: A Survey
A1  - Li Wang
A1  - Dennis Sng
JO  - ArXiv e-prints
Y1  - 9 December, 2015
UR  - https://arxiv.org/abs/1512.03131
N2  - Deep learning has recently achieved very promising results in a wide range of areas such as computer vision, speech recognition and natural language processing. It aims to learn hierarchical representations of data by using deep architecture models. In a smart city, a lot of data (e.g. videos captured from many distributed sensors) need to be automatically processed and analyzed. In this paper, we review the deep learning algorithms applied to video analytics of smart city in terms of different research topics: object detection, object tracking, face recognition, image classification and scene labeling.
ER  -


TY  - Preprint
T1  - Deep Learning for Single and Multi-Session i-Vector Speaker Recognition
A1  - Omid Ghahabi
A1  - Javier Hernando
JO  - ArXiv e-prints
Y1  - 8 December, 2015
UR  - https://arxiv.org/abs/1512.02560
N2  - The promising performance of Deep Learning (DL) in speech recognition has motivated the use of DL in other speech technology applications such as speaker recognition. Given i-vectors as inputs, the authors proposed an impostor selection algorithm and a universal model adaptation process in a hybrid system based on Deep Belief Networks (DBN) and Deep Neural Networks (DNN) to discriminatively model each target speaker. In order to have more insight into the behavior of DL techniques in both single and multi-session speaker enrollment tasks, some experiments have been carried out in this paper in both scenarios. Additionally, the parameters of the global model, referred to as universal DBN (UDBN), are normalized before adaptation. UDBN normalization facilitates training DNNs specifically with more than one hidden layer. Experiments are performed on the NIST SRE 2006 corpus. It is shown that the proposed impostor selection algorithm and UDBN adaptation process enhance the performance of conventional DNNs 8-20 % and 16-20 % in terms of EER for the single and multi-session tasks, respectively. In both scenarios, the proposed architectures outperform the baseline systems obtaining up to 17 % reduction in EER.
ER  -


TY  - Preprint
T1  - How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies
A1  - Vincent FranÃ§ois-Lavet
A1  - Raphael Fonteneau
A1  - Damien Ernst
JO  - ArXiv e-prints
Y1  - 20 January, 2016
UR  - https://arxiv.org/abs/1512.02011
N2  - Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity. Using these results as a benchmark, we discuss the role that the discount factor may play in the quality of the learning process of a deep Q-network (DQN). When the discount factor progressively increases up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps. When used in conjunction with a varying learning rate, we empirically show that it outperforms original DQN on several experiments. We relate this phenomenon with the instabilities of neural networks when they are used in an approximate Dynamic Programming setting. We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma.
ER  -


TY  - Preprint
T1  - Proposition of a Theoretical Model for Missing Data Imputation using Deep Learning and Evolutionary Algorithms
A1  - Collins Leke
A1  - Tshilidzi Marwala
A1  - Satyakama Paul
JO  - ArXiv e-prints
Y1  - 4 December, 2015
UR  - https://arxiv.org/abs/1512.01362
N2  - In the last couple of decades, there has been major advancements in the domain of missing data imputation. The techniques in the domain include amongst others: Expectation Maximization, Neural Networks with Evolutionary Algorithms or optimization techniques and K-Nearest Neighbor approaches to solve the problem. The presence of missing data entries in databases render the tasks of decision-making and data analysis nontrivial. As a result this area has attracted a lot of research interest with the aim being to yield accurate and time efficient and sensitive missing data imputation techniques especially when time sensitive applications are concerned like power plants and winding processes. In this article, considering arbitrary and monotone missing data patterns, we hypothesize that the use of deep neural networks built using autoencoders and denoising autoencoders in conjunction with genetic algorithms, swarm intelligence and maximum likelihood estimator methods as novel data imputation techniques will lead to better imputed values than existing techniques. Also considered are the missing at random, missing completely at random and missing not at random missing data mechanisms. We also intend to use fuzzy logic in tandem with deep neural networks to perform the missing data imputation tasks, as well as different building blocks for the deep neural networks like Stacked Restricted Boltzmann Machines and Deep Belief Networks to test our hypothesis. The motivation behind this article is the need for missing data imputation techniques that lead to better imputed values than existing methods with higher accuracies and lower errors.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning with Attention for Slate Markov Decision Processes with High-Dimensional States and Actions
A1  - Peter Sunehag
A1  - Richard Evans
A1  - Gabriel Dulac-Arnold
A1  - Yori Zwols
A1  - Daniel Visentin
A1  - Ben Coppin
JO  - ArXiv e-prints
Y1  - 16 December, 2015
UR  - https://arxiv.org/abs/1512.01124
N2  - Many real-world problems come with action spaces represented as feature vectors. Although high-dimensional control is a largely unsolved problem, there has recently been progress for modest dimensionalities. Here we report on a successful attempt at addressing problems of dimensionality as high as $2000$, of a particular form. Motivated by important applications such as recommendation systems that do not fit the standard reinforcement learning frameworks, we introduce Slate Markov Decision Processes (slate-MDPs). A Slate-MDP is an MDP with a combinatorial action space consisting of slates (tuples) of primitive actions of which one is executed in an underlying MDP. The agent does not control the choice of this executed action and the action might not even be from the slate, e.g., for recommendation systems for which all recommendations can be ignored. We use deep Q-learning based on feature representations of both the state and action to learn the value of whole slates. Unlike existing methods, we optimize for both the combinatorial and sequential aspects of our tasks. The new agent&#39;s superiority over agents that either ignore the combinatorial or sequential long-term value aspect is demonstrated on a range of environments with dynamics from a real-world recommendation system. Further, we use deep deterministic policy gradients to learn a policy that for each position of the slate, guides attention towards the part of the action space in which the value is the highest and we only evaluate actions in this area. The attention is used within a sequentially greedy procedure leveraging submodularity. Finally, we show how introducing risk-seeking can dramatically improve the agents performance and ability to discover more far reaching strategies.
ER  -


TY  - Preprint
T1  - Recognizing Semantic Features in Faces using Deep Learning
A1  - Amogh Gudi
JO  - ArXiv e-prints
Y1  - 19 October, 2016
UR  - https://arxiv.org/abs/1512.00743
N2  - The human face constantly conveys information, both consciously and subconsciously. However, as basic as it is for humans to visually interpret this information, it is quite a big challenge for machines. Conventional semantic facial feature recognition and analysis techniques are already in use and are based on physiological heuristics, but they suffer from lack of robustness and high computation time. This thesis aims to explore ways for machines to learn to interpret semantic information available in faces in an automated manner without requiring manual design of feature detectors, using the approach of Deep Learning. This thesis provides a study of the effects of various factors and hyper-parameters of deep neural networks in the process of determining an optimal network configuration for the task of semantic facial feature recognition. This thesis explores the effectiveness of the system to recognize the various semantic features (like emotions, age, gender, ethnicity etc.) present in faces. Furthermore, the relation between the effect of high-level concepts on low level features is explored through an analysis of the similarities in low-level descriptors of different semantic features. This thesis also demonstrates a novel idea of using a deep network to generate 3-D Active Appearance Models of faces from real-world 2-D images.
ER  -


TY  - Preprint
T1  - Cost-aware Pre-training for Multiclass Cost-sensitive Deep Learning
A1  - Yu-An Chung
A1  - Hsuan-Tien Lin
A1  - Shao-Wen Yang
JO  - ArXiv e-prints
Y1  - 24 May, 2016
UR  - https://arxiv.org/abs/1511.09337
N2  - Deep learning has been one of the most prominent machine learning techniques nowadays, being the state-of-the-art on a broad range of applications where automatic feature extraction is needed. Many such applications also demand varying costs for different types of mis-classification errors, but it is not clear whether or how such cost information can be incorporated into deep learning to improve performance. In this work, we propose a novel cost-aware algorithm that takes into account the cost information into not only the training stage but also the pre-training stage of deep learning. The approach allows deep learning to conduct automatic feature extraction with the cost information effectively. Extensive experimental results demonstrate that the proposed approach outperforms other deep learning models that do not digest the cost information in the pre-training stage.
ER  -


TY  - Preprint
T1  - Applying deep learning to classify pornographic images and videos
A1  - Mohamed Moustafa
JO  - ArXiv e-prints
Y1  - 28 November, 2015
UR  - https://arxiv.org/abs/1511.08899
N2  - It is no secret that pornographic material is now a one-click-away from everyone, including children and minors. General social media networks are striving to isolate adult images and videos from normal ones. Intelligent image analysis methods can help to automatically detect and isolate questionable images in media. Unfortunately, these methods require vast experience to design the classifier including one or more of the popular computer vision feature descriptors. We propose to build a classifier based on one of the recently flourishing deep learning techniques. Convolutional neural networks contain many layers for both automatic features extraction and classification. The benefit is an easier system to build (no need for hand-crafting features and classifiers). Additionally, our experiments show that it is even more accurate than the state of the art methods on the most recent benchmark dataset.
ER  -


TY  - Preprint
T1  - Multiagent Cooperation and Competition with Deep Reinforcement Learning
A1  - Ardi Tampuu
A1  - Tambet Matiisen
A1  - Dorian Kodelja
A1  - Ilya Kuzovkin
A1  - Kristjan Korjus
A1  - Juhan Aru
A1  - Jaan Aru
A1  - Raul Vicente
JO  - ArXiv e-prints
Y1  - 27 November, 2015
UR  - https://arxiv.org/abs/1511.08779
N2  - Multiagent systems appear in most social, economical, and political situations. In the present work we extend the Deep Q-Learning Network architecture proposed by Google DeepMind to multiagent environments and investigate how two agents controlled by independent Deep Q-Networks interact in the classic videogame Pong. By manipulating the classical rewarding scheme of Pong we demonstrate how competitive and collaborative behaviors emerge. Competitive agents learn to play and score efficiently. Agents trained under collaborative rewarding schemes find an optimal strategy to keep the ball in the game as long as possible. We also describe the progression from competitive to collaborative behavior. The present work demonstrates that Deep Q-Networks can become a practical tool for studying the decentralized learning of multiagent systems living in highly complex environments.
ER  -


TY  - Preprint
T1  - Strategic Dialogue Management via Deep Reinforcement Learning
A1  - Heriberto CuayÃ¡huitl
A1  - Simon Keizer
A1  - Oliver Lemon
JO  - ArXiv e-prints
Y1  - 25 November, 2015
UR  - https://arxiv.org/abs/1511.08099
N2  - Artificially intelligent agents equipped with strategic skills that can negotiate during their interactions with other natural or artificial agents are still underdeveloped. This paper describes a successful application of Deep Reinforcement Learning (DRL) for training intelligent agents with strategic conversational skills, in a situated dialogue setting. Previous studies have modelled the behaviour of strategic agents using supervised learning and traditional reinforcement learning techniques, the latter using tabular representations or learning with linear function approximation. In this study, we apply DRL with a high-dimensional state space to the strategic board game of Settlers of Catan---where players can offer resources in exchange for others and they can also reply to offers made by other players. Our experimental results report that the DRL-based learnt policies significantly outperformed several baselines including random, rule-based, and supervised-based behaviours. The DRL-based policy has a 53% win rate versus 3 automated players (`bots&#39;), whereas a supervised player trained on a dialogue corpus in this setting achieved only 27%, versus the same 3 bots. This result supports the claim that DRL is a promising framework for training dialogue systems, and strategic agents with negotiation abilities.
ER  -


TY  - Preprint
T1  - Constrained Deep Metric Learning for Person Re-identification
A1  - Hailin Shi
A1  - Xiangyu Zhu
A1  - Shengcai Liao
A1  - Zhen Lei
A1  - Yang Yang
A1  - Stan Z. Li
JO  - ArXiv e-prints
Y1  - 23 November, 2015
UR  - https://arxiv.org/abs/1511.07545
N2  - Person re-identification aims to re-identify the probe image from a given set of images under different camera views. It is challenging due to large variations of pose, illumination, occlusion and camera view. Since the convolutional neural networks (CNN) have excellent capability of feature extraction, certain deep learning methods have been recently applied in person re-identification. However, in person re-identification, the deep networks often suffer from the over-fitting problem. In this paper, we propose a novel CNN-based method to learn a discriminative metric with good robustness to the over-fitting problem in person re-identification. Firstly, a novel deep architecture is built where the Mahalanobis metric is learned with a weight constraint. This weight constraint is used to regularize the learning, so that the learned metric has a better generalization ability. Secondly, we find that the selection of intra-class sample pairs is crucial for learning but has received little attention. To cope with the large intra-class variations in pedestrian images, we propose a novel training strategy named moderate positive mining to prevent the training process from over-fitting to the extreme samples in intra-class pairs. Experiments show that our approach significantly outperforms state-of-the-art methods on several benchmarks of person re-identification.
ER  -


TY  - Preprint
T1  - The Limitations of Deep Learning in Adversarial Settings
A1  - Nicolas Papernot
A1  - Patrick McDaniel
A1  - Somesh Jha
A1  - Matt Fredrikson
A1  - Z. Berkay Celik
A1  - Ananthram Swami
JO  - ArXiv e-prints
Y1  - 23 November, 2015
UR  - https://arxiv.org/abs/1511.07528
N2  - Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.
ER  -


TY  - Preprint
T1  - Pushing the Boundaries of Boundary Detection using Deep Learning
A1  - Iasonas Kokkinos
JO  - ArXiv e-prints
Y1  - 22 January, 2016
UR  - https://arxiv.org/abs/1511.07386
N2  - In this work we show that adapting Deep Convolutional Neural Network training to the task of boundary detection can result in substantial improvements over the current state-of-the-art in boundary detection.
ER  -


TY  - Preprint
T1  - Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)
A1  - Djork-ArnÃ© Clevert
A1  - Thomas Unterthiner
A1  - Sepp Hochreiter
JO  - ArXiv e-prints
Y1  - 22 February, 2016
UR  - https://arxiv.org/abs/1511.07289
N2  - We introduce the &#34;exponential linear unit&#34; (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network.
ER  -


TY  - Preprint
T1  - Detecting Road Surface Wetness from Audio: A Deep Learning Approach
A1  - Irman AbdiÄ
A1  - Lex Fridman
A1  - Erik Marchi
A1  - Daniel E Brown
A1  - William Angell
A1  - Bryan Reimer
A1  - BjÃ¶rn Schuller
JO  - ArXiv e-prints
Y1  - 4 December, 2015
UR  - https://arxiv.org/abs/1511.07035
N2  - We introduce a recurrent neural network architecture for automated road surface wetness detection from audio of tire-surface interaction. The robustness of our approach is evaluated on 785,826 bins of audio that span an extensive range of vehicle speeds, noises from the environment, road surface types, and pavement conditions including international roughness index (IRI) values from 25 in/mi to 1400 in/mi. The training and evaluation of the model are performed on different roads to minimize the impact of environmental and other external factors on the accuracy of the classification. We achieve an unweighted average recall (UAR) of 93.2% across all vehicle speeds including 0 mph. The classifier still works at 0 mph because the discriminating signal is present in the sound of other vehicles driving by.
ER  -


TY  - Preprint
T1  - Online Semi-Supervised Learning with Deep Hybrid Boltzmann Machines and Denoising Autoencoders
A1  - Alexander G. Ororbia II
A1  - C. Lee Giles
A1  - David Reitter
JO  - ArXiv e-prints
Y1  - 18 January, 2016
UR  - https://arxiv.org/abs/1511.06964
N2  - Two novel deep hybrid architectures, the Deep Hybrid Boltzmann Machine and the Deep Hybrid Denoising Auto-encoder, are proposed for handling semi-supervised learning problems. The models combine experts that model relevant distributions at different levels of abstraction to improve overall predictive performance on discriminative tasks. Theoretical motivations and algorithms for joint learning for each are presented. We apply the new models to the domain of data-streams in work towards life-long learning. The proposed architectures show improved performance compared to a pseudo-labeled, drop-out rectifier network.
ER  -


TY  - Preprint
T1  - Images Don&#39;t Lie: Transferring Deep Visual Semantic Features to Large-Scale Multimodal Learning to Rank
A1  - Corey Lynch
A1  - Kamelia Aryafar
A1  - Josh Attenberg
JO  - ArXiv e-prints
Y1  - 20 November, 2015
UR  - https://arxiv.org/abs/1511.06746
N2  - Search is at the heart of modern e-commerce. As a result, the task of ranking search results automatically (learning to rank) is a multibillion dollar machine learning problem. Traditional models optimize over a few hand-constructed features based on the item&#39;s text. In this paper, we introduce a multimodal learning to rank model that combines these traditional features with visual semantic features transferred from a deep convolutional neural network. In a large scale experiment using data from the online marketplace Etsy, we verify that moving to a multimodal representation significantly improves ranking quality. We show how image features can capture fine-grained style information not available in a text-only representation. In addition, we show concrete examples of how image information can successfully disentangle pairs of highly different items that are ranked similarly by a text-only model.
ER  -


TY  - Preprint
T1  - Dueling Network Architectures for Deep Reinforcement Learning
A1  - Ziyu Wang
A1  - Tom Schaul
A1  - Matteo Hessel
A1  - Hado van Hasselt
A1  - Marc Lanctot
A1  - Nando de Freitas
JO  - ArXiv e-prints
Y1  - 5 April, 2016
UR  - https://arxiv.org/abs/1511.06581
N2  - In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.
ER  -


TY  - Preprint
T1  - Deep Metric Learning via Lifted Structured Feature Embedding
A1  - Hyun Oh Song
A1  - Yu Xiang
A1  - Stefanie Jegelka
A1  - Silvio Savarese
JO  - ArXiv e-prints
Y1  - 19 November, 2015
UR  - https://arxiv.org/abs/1511.06452
N2  - Learning the distance metric between pairs of examples is of great importance for learning and visual recognition. With the remarkable success from the state of the art convolutional neural networks, recent works have shown promising results on discriminatively training the networks to learn semantic feature embeddings where similar examples are mapped close to each other and dissimilar examples are mapped farther apart. In this paper, we describe an algorithm for taking full advantage of the training batches in the neural network training by lifting the vector of pairwise distances within the batch to the matrix of pairwise distances. This step enables the algorithm to learn the state of the art feature embedding by optimizing a novel structured prediction objective on the lifted problem. Additionally, we collected Online Products dataset: 120k images of 23k classes of online products for metric learning. Our experiments on the CUB-200-2011, CARS196, and Online Products datasets demonstrate significant improvement over existing deep feature embedding methods on all experimented embedding sizes with the GoogLeNet network.
ER  -


TY  - Preprint
T1  - Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks
A1  - Pouya Bashivan
A1  - Irina Rish
A1  - Mohammed Yeasin
A1  - Noel Codella
JO  - ArXiv e-prints
Y1  - 29 February, 2016
UR  - https://arxiv.org/abs/1511.06448
N2  - One of the challenges in modeling cognitive events from electroencephalogram (EEG) data is finding representations that are invariant to inter- and intra-subject differences, as well as to inherent noise associated with such data. Herein, we propose a novel approach for learning such representations from multi-channel EEG time-series, and demonstrate its advantages in the context of mental load classification task. First, we transform EEG activities into a sequence of topology-preserving multi-spectral images, as opposed to standard EEG analysis techniques that ignore such spatial information. Next, we train a deep recurrent-convolutional network inspired by state-of-the-art video classification to learn robust representations from the sequence of images. The proposed approach is designed to preserve the spatial, spectral, and temporal structure of EEG which leads to finding features that are less sensitive to variations and distortions within each dimension. Empirical evaluation on the cognitive load classification task demonstrated significant improvements in classification accuracy over current state-of-the-art approaches in this field.
ER  -


TY  - Preprint
T1  - Fast Metric Learning For Deep Neural Networks
A1  - Henry Gouk
A1  - Bernhard Pfahringer
A1  - Michael Cree
JO  - ArXiv e-prints
Y1  - 5 April, 2016
UR  - https://arxiv.org/abs/1511.06442
N2  - Similarity metrics are a core component of many information retrieval and machine learning systems. In this work we propose a method capable of learning a similarity metric from data equipped with a binary relation. By considering only the similarity constraints, and initially ignoring the features, we are able to learn target vectors for each instance using one of several appropriately designed loss functions. A regression model can then be constructed that maps novel feature vectors to the same target vector space, resulting in a feature extractor that computes vectors for which a predefined metric is a meaningful measure of similarity. We present results on both multiclass and multi-label classification datasets that demonstrate considerably faster convergence, as well as higher accuracy on the majority of the intrinsic evaluation tasks and all extrinsic evaluation tasks.
ER  -


TY  - Preprint
T1  - Comparative Study of Deep Learning Software Frameworks
A1  - Soheil Bahrampour
A1  - Naveen Ramakrishnan
A1  - Lukas Schott
A1  - Mohak Shah
JO  - ArXiv e-prints
Y1  - 29 March, 2016
UR  - https://arxiv.org/abs/1511.06435
N2  - Deep learning methods have resulted in significant performance improvements in several application domains and as such several software frameworks have been developed to facilitate their implementation. This paper presents a comparative study of five deep learning frameworks, namely Caffe, Neon, TensorFlow, Theano, and Torch, on three aspects: extensibility, hardware utilization, and speed. The study is performed on several types of deep learning architectures and we evaluate the performance of the above frameworks when employed on a single machine for both (multi-threaded) CPU and GPU (Nvidia Titan X) settings. The speed performance metrics used here include the gradient computation time, which is important during the training phase of deep networks, and the forward time, which is important from the deployment perspective of trained networks. For convolutional networks, we also report how each of these frameworks support various convolutional algorithms and their corresponding performance. From our experiments, we observe that Theano and Torch are the most easily extensible frameworks. We observe that Torch is best suited for any deep architecture on CPU, followed by Theano. It also achieves the best performance on the GPU for large convolutional and fully connected networks, followed closely by Neon. Theano achieves the best performance on GPU for training and deployment of LSTM networks. Caffe is the easiest for evaluating the performance of standard deep architectures. Finally, TensorFlow is a very flexible framework, similar to Theano, but its performance is currently not competitive compared to the other studied frameworks.
ER  -


TY  - Preprint
T1  - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks
A1  - Alec Radford
A1  - Luke Metz
A1  - Soumith Chintala
JO  - ArXiv e-prints
Y1  - 7 January, 2016
UR  - https://arxiv.org/abs/1511.06434
N2  - In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.
ER  -


TY  - Preprint
T1  - How much data is needed to train a medical image deep learning system to achieve necessary high accuracy?
A1  - Junghwan Cho
A1  - Kyewook Lee
A1  - Ellie Shin
A1  - Garry Choy
A1  - Synho Do
JO  - ArXiv e-prints
Y1  - 7 January, 2016
UR  - https://arxiv.org/abs/1511.06348
N2  - The use of Convolutional Neural Networks (CNN) in natural image classification systems has produced very impressive results. Combined with the inherent nature of medical images that make them ideal for deep-learning, further application of such systems to medical image classification holds much promise. However, the usefulness and potential impact of such a system can be completely negated if it does not reach a target accuracy. In this paper, we present a study on determining the optimum size of the training data set necessary to achieve high classification accuracy with low variance in medical image classification systems. The CNN was applied to classify axial Computed Tomography (CT) images into six anatomical classes. We trained the CNN using six different sizes of training data set (5, 10, 20, 50, 100, and 200) and then tested the resulting system with a total of 6000 CT images. All images were acquired from the Massachusetts General Hospital (MGH) Picture Archiving and Communication System (PACS). Using this data, we employ the learning curve approach to predict classification accuracy at a given training sample size. Our research will present a general methodology for determining the training data set size necessary to achieve a certain target classification accuracy that can be easily applied to other problems within such systems.
ER  -


TY  - Preprint
T1  - Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning
A1  - Emilio Parisotto
A1  - Jimmy Lei Ba
A1  - Ruslan Salakhutdinov
JO  - ArXiv e-prints
Y1  - 22 February, 2016
UR  - https://arxiv.org/abs/1511.06342
N2  - The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed &#34;Actor-Mimic&#34;, exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.
ER  -


TY  - Preprint
T1  - Predicting online user behaviour using deep learning algorithms
A1  - Armando Vieira
JO  - ArXiv e-prints
Y1  - 26 May, 2016
UR  - https://arxiv.org/abs/1511.06247
N2  - We propose a robust classifier to predict buying intentions based on user behaviour within a large e-commerce website. In this work we compare traditional machine learning techniques with the most advanced deep learning approaches. We show that both Deep Belief Networks and Stacked Denoising auto-Encoders achieved a substantial improvement by extracting features from high dimensional data during the pre-train phase. They prove also to be more convenient to deal with severe class imbalance.
ER  -


TY  - Preprint
T1  - Learning Deep Structure-Preserving Image-Text Embeddings
A1  - Liwei Wang
A1  - Yin Li
A1  - Svetlana Lazebnik
JO  - ArXiv e-prints
Y1  - 13 April, 2016
UR  - https://arxiv.org/abs/1511.06078
N2  - This paper proposes a method for learning joint embeddings of images and text using a two-branch neural network with multiple layers of linear projections followed by nonlinearities. The network is trained using a large margin objective that combines cross-view ranking constraints with within-view neighborhood structure preservation constraints inspired by metric learning literature. Extensive experiments show that our approach gains significant improvements in accuracy for image-to-text and text-to-image retrieval. Our method achieves new state-of-the-art results on the Flickr30K and MSCOCO image-sentence datasets and shows promise on the new task of phrase localization on the Flickr30K Entities dataset.
ER  -


TY  - Preprint
T1  - Deep Learning for Tactile Understanding From Visual and Haptic Data
A1  - Yang Gao
A1  - Lisa Anne Hendricks
A1  - Katherine J. Kuchenbecker
A1  - Trevor Darrell
JO  - ArXiv e-prints
Y1  - 11 April, 2016
UR  - https://arxiv.org/abs/1511.06065
N2  - Robots which interact with the physical world will benefit from a fine-grained tactile understanding of objects and surfaces. Additionally, for certain tasks, robots may need to know the haptic properties of an object before touching it. To enable better tactile understanding for robots, we propose a method of classifying surfaces with haptic adjectives (e.g., compressible or smooth) from both visual and physical interaction data. Humans typically combine visual predictions and feedback from physical interactions to accurately predict haptic properties and interact with the world. Inspired by this cognitive pattern, we propose and explore a purely visual haptic prediction model. Purely visual models enable a robot to &#34;feel&#34; without physical interaction. Furthermore, we demonstrate that using both visual and physical interaction signals together yields more accurate haptic classification. Our models take advantage of recent advances in deep neural networks by employing a unified approach to learning features for physical interaction and visual observations. Even though we employ little domain specific knowledge, our model still achieves better results than methods based on hand-designed features.
ER  -


TY  - Preprint
T1  - Active Object Localization with Deep Reinforcement Learning
A1  - Juan C. Caicedo
A1  - Svetlana Lazebnik
JO  - ArXiv e-prints
Y1  - 18 November, 2015
UR  - https://arxiv.org/abs/1511.06015
N2  - We present an active detection model for localizing objects in scenes. The model is class-specific and allows an agent to focus attention on candidate regions for identifying the correct location of a target object. This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most specific location of target objects following top-down reasoning. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.
ER  -


TY  - Preprint
T1  - Staleness-aware Async-SGD for Distributed Deep Learning
A1  - Wei Zhang
A1  - Suyog Gupta
A1  - Xiangru Lian
A1  - Ji Liu
JO  - ArXiv e-prints
Y1  - 5 April, 2016
UR  - https://arxiv.org/abs/1511.05950
N2  - Deep neural networks have been shown to achieve state-of-the-art performance in several machine learning tasks. Stochastic Gradient Descent (SGD) is the preferred optimization algorithm for training these networks and asynchronous SGD (ASGD) has been widely adopted for accelerating the training of large-scale deep networks in a distributed computing environment. However, in practice it is quite challenging to tune the training hyperparameters (such as learning rate) when using ASGD so as achieve convergence and linear speedup, since the stability of the optimization algorithm is strongly influenced by the asynchronous nature of parameter updates. In this paper, we propose a variant of the ASGD algorithm in which the learning rate is modulated according to the gradient staleness and provide theoretical guarantees for convergence of this algorithm. Experimental verification is performed on commonly-used image classification benchmarks: CIFAR10 and Imagenet to demonstrate the superior effectiveness of the proposed approach, compared to SSGD (Synchronous SGD) and the conventional ASGD algorithm.
ER  -


TY  - Preprint
T1  - Identifying the Absorption Bump with Deep Learning
A1  - Min Li
A1  - Sudeep Gaddam
A1  - Xiaolin Li
A1  - Yinan Zhao
A1  - Jingzhe Ma
A1  - Jian Ge
JO  - ArXiv e-prints
Y1  - 20 November, 2015
UR  - https://arxiv.org/abs/1511.05607
N2  - The pervasive interstellar dust grains provide significant insights to understand the formation and evolution of the stars, planetary systems, and the galaxies, and may harbor the building blocks of life. One of the most effective way to analyze the dust is via their interaction with the light from background sources. The observed extinction curves and spectral features carry the size and composition information of dust. The broad absorption bump at 2175 Angstrom is the most prominent feature in the extinction curves. Traditionally, statistical methods are applied to detect the existence of the absorption bump. These methods require heavy preprocessing and the co-existence of other reference features to alleviate the influence from the noises. In this paper, we apply Deep Learning techniques to detect the broad absorption bump. We demonstrate the key steps for training the selected models and their results. The success of Deep Learning based method inspires us to generalize a common methodology for broader science discovery problems. We present our on-going work to build the DeepDis system for such kind of applications.
ER  -


TY  - Preprint
T1  - Structural-RNN: Deep Learning on Spatio-Temporal Graphs
A1  - Ashesh Jain
A1  - Amir R. Zamir
A1  - Silvio Savarese
A1  - Ashutosh Saxena
JO  - ArXiv e-prints
Y1  - 11 April, 2016
UR  - https://arxiv.org/abs/1511.05298
N2  - Deep Recurrent Neural Network architectures, though remarkably capable at modeling sequences, lack an intuitive high-level spatio-temporal structure. That is while many problems in computer vision inherently have an underlying high-level structure and can benefit from it. Spatio-temporal graphs are a popular tool for imposing such high-level intuitions in the formulation of real world problems. In this paper, we propose an approach for combining the power of high-level spatio-temporal graphs and sequence learning success of Recurrent Neural Networks~(RNNs). We develop a scalable method for casting an arbitrary spatio-temporal graph as a rich RNN mixture that is feedforward, fully differentiable, and jointly trainable. The proposed method is generic and principled as it can be used for transforming any spatio-temporal graph through employing a certain set of well defined steps. The evaluations of the proposed approach on a diverse set of problems, ranging from modeling human motion to object interactions, shows improvement over the state-of-the-art with a large margin. We expect this method to empower new approaches to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks.
ER  -


TY  - Preprint
T1  - On the interplay of network structure and gradient convergence in deep learning
A1  - Vamsi K Ithapu
A1  - Sathya N Ravi
A1  - Vikas Singh
JO  - ArXiv e-prints
Y1  - 22 February, 2017
UR  - https://arxiv.org/abs/1511.05297
N2  - The regularization and output consistency behavior of dropout and layer-wise pretraining for learning deep networks have been fairly well studied. However, our understanding of how the asymptotic convergence of backpropagation in deep architectures is related to the structural properties of the network and other design choices (like denoising and dropout rate) is less clear at this time. An interesting question one may ask is whether the network architecture and input data statistics may guide the choices of learning parameters and vice versa. In this work, we explore the association between such structural, distributional and learnability aspects vis-Ã -vis their interaction with parameter convergence rates. We present a framework to address these questions based on convergence of backpropagation for general nonconvex objectives using first-order information. This analysis suggests an interesting relationship between feature denoising and dropout. Building upon these results, we obtain a setup that provides systematic guidance regarding the choice of learning parameters and network sizes that achieve a certain level of convergence (in the optimization sense) often mediated by statistical attributes of the inputs. Our results are supported by a set of experimental evaluations as well as independent empirical observations reported by other groups.
ER  -


TY  - Preprint
T1  - Deep learning is a good steganalysis tool when embedding key is reused for different images, even if there is a cover source-mismatch
A1  - Lionel Pibre
A1  - Pasquet JÃ©rÃ´me
A1  - Dino Ienco
A1  - Marc Chaumont
JO  - ArXiv e-prints
Y1  - 12 January, 2018
UR  - https://arxiv.org/abs/1511.04855
N2  - Since the BOSS competition, in 2010, most steganalysis approaches use a learning methodology involving two steps: feature extraction, such as the Rich Models (RM), for the image representation, and use of the Ensemble Classifier (EC) for the learning step. In 2015, Qian et al. have shown that the use of a deep learning approach that jointly learns and computes the features, is very promising for the steganalysis. In this paper, we follow-up the study of Qian et al., and show that, due to intrinsic joint minimization, the results obtained from a Convolutional Neural Network (CNN) or a Fully Connected Neural Network (FNN), if well parameterized, surpass the conventional use of a RM with an EC. First, numerous experiments were conducted in order to find the best &#34; shape &#34; of the CNN. Second, experiments were carried out in the clairvoyant scenario in order to compare the CNN and FNN to an RM with an EC. The results show more than 16% reduction in the classification error with our CNN or FNN. Third, experiments were also performed in a cover-source mismatch setting. The results show that the CNN and FNN are naturally robust to the mismatch problem. In Addition to the experiments, we provide discussions on the internal mechanisms of a CNN, and weave links with some previously stated ideas, in order to understand the impressive results we obtained.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning with a Natural Language Action Space
A1  - Ji He
A1  - Jianshu Chen
A1  - Xiaodong He
A1  - Jianfeng Gao
A1  - Lihong Li
A1  - Li Deng
A1  - Mari Ostendorf
JO  - ArXiv e-prints
Y1  - 8 June, 2016
UR  - https://arxiv.org/abs/1511.04636
N2  - This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text.
ER  -


TY  - Preprint
T1  - 8-Bit Approximations for Parallelism in Deep Learning
A1  - Tim Dettmers
JO  - ArXiv e-prints
Y1  - 19 February, 2016
UR  - https://arxiv.org/abs/1511.04561
N2  - The creation of practical deep learning data-products often requires parallelization across processors and computers to make deep learning feasible on large data sets, but bottlenecks in communication bandwidth make it difficult to attain good speedups through parallelism. Here we develop and test 8-bit approximation algorithms which make better use of the available bandwidth by compressing 32-bit gradients and nonlinear activations to 8-bit approximations. We show that these approximations do not decrease predictive performance on MNIST, CIFAR10, and ImageNet for both model and data parallelism and provide a data transfer speedup of 2x relative to 32-bit parallelism. We build a predictive model for speedups based on our experimental data, verify its validity on known speedup data, and show that we can obtain a speedup of 50x and more on a system of 96 GPUs compared to a speedup of 23x for 32-bit. We compare our data types with other methods and show that 8-bit approximations achieve state-of-the-art speedups for model parallelism. Thus 8-bit approximation is an efficient method to parallelize convolutional networks on very large systems of GPUs.
ER  -


TY  - Preprint
T1  - Deep Feature Learning for EEG Recordings
A1  - Sebastian Stober
A1  - Avital Sternin
A1  - Adrian M. Owen
A1  - Jessica A. Grahn
JO  - ArXiv e-prints
Y1  - 7 January, 2016
UR  - https://arxiv.org/abs/1511.04306
N2  - We introduce and compare several strategies for learning discriminative features from electroencephalography (EEG) recordings using deep learning techniques. EEG data are generally only available in small quantities, they are high-dimensional with a poor signal-to-noise ratio, and there is considerable variability between individual subjects and recording sessions. Our proposed techniques specifically address these challenges for feature learning. Cross-trial encoding forces auto-encoders to focus on features that are stable across trials. Similarity-constraint encoders learn features that allow to distinguish between classes by demanding that two trials from the same class are more similar to each other than to trials from other classes. This tuple-based training approach is especially suitable for small datasets. Hydra-nets allow for separate processing pathways adapting to subsets of a dataset and thus combine the advantages of individual feature learning (better adaptation of early, low-level processing) with group model training (better generalization of higher-level processing in deeper layers). This way, models can, for instance, adapt to each subject individually to compensate for differences in spatial patterns due to anatomical differences or variance in electrode positions. The different techniques are evaluated using the publicly available OpenMIIR dataset of EEG recordings taken while participants listened to and imagined music.
ER  -


TY  - Preprint
T1  - DISC: Deep Image Saliency Computing via Progressive Representation Learning
A1  - Tianshui Chen
A1  - Liang Lin
A1  - Lingbo Liu
A1  - Xiaonan Luo
A1  - Xuelong Li
JO  - ArXiv e-prints
Y1  - 10 December, 2015
UR  - https://arxiv.org/abs/1511.04192
N2  - Salient object detection increasingly receives attention as an important component or step in several pattern recognition and image processing tasks. Although a variety of powerful saliency models have been intensively proposed, they usually involve heavy feature (or model) engineering based on priors (or assumptions) about the properties of objects and backgrounds. Inspired by the effectiveness of recently developed feature learning, we provide a novel Deep Image Saliency Computing (DISC) framework for fine-grained image saliency computing. In particular, we model the image saliency from both the coarse- and fine-level observations, and utilize the deep convolutional neural network (CNN) to learn the saliency representation in a progressive manner. Specifically, our saliency model is built upon two stacked CNNs. The first CNN generates a coarse-level saliency map by taking the overall image as the input, roughly identifying saliency regions in the global context. Furthermore, we integrate superpixel-based local context information in the first CNN to refine the coarse-level saliency map. Guided by the coarse saliency map, the second CNN focuses on the local context to produce fine-grained and accurate saliency map while preserving object details. For a testing image, the two CNNs collaboratively conduct the saliency computing in one shot. Our DISC framework is capable of uniformly highlighting the objects-of-interest from complex background while preserving well object details. Extensive experiments on several standard benchmarks suggest that DISC outperforms other state-of-the-art methods and it also generalizes well across datasets without additional training. The executable version of DISC is available online: http://vision.sysu.edu.cn/projects/DISC.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning in Parameterized Action Space
A1  - Matthew Hausknecht
A1  - Peter Stone
JO  - ArXiv e-prints
Y1  - 16 February, 2016
UR  - https://arxiv.org/abs/1511.04143
N2  - Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces. However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces. To fill this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which features a small set of discrete action types, each of which is parameterized with continuous variables. The best learned agent can score goals more reliably than the 2012 RoboCup champion agent. As such, this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space MDPs.
ER  -


TY  - Preprint
T1  - LSTM-based Deep Learning Models for Non-factoid Answer Selection
A1  - Ming Tan
A1  - Cicero dos Santos
A1  - Bing Xiang
A1  - Bowen Zhou
JO  - ArXiv e-prints
Y1  - 28 March, 2016
UR  - https://arxiv.org/abs/1511.04108
N2  - In this paper, we apply a general deep learning (DL) framework for the answer selection task, which does not depend on manually defined features or linguistic tools. The basic framework is to build the embeddings of questions and answers based on bidirectional long short-term memory (biLSTM) models, and measure their closeness by cosine similarity. We further extend this basic model in two directions. One direction is to define a more composite representation for questions and answers by combining convolutional neural network with the basic framework. The other direction is to utilize a simple but efficient attention mechanism in order to generate the answer representation according to the question context. Several variations of models are provided. The models are examined by two datasets, including TREC-QA and InsuranceQA. Experimental results demonstrate that the proposed models substantially outperform several strong baselines.
ER  -


TY  - Preprint
T1  - Representational Distance Learning for Deep Neural Networks
A1  - Patrick McClure
A1  - Nikolaus Kriegeskorte
JO  - ArXiv e-prints
Y1  - 7 November, 2016
UR  - https://arxiv.org/abs/1511.03979
N2  - Deep neural networks (DNNs) provide useful models of visual representational transformations. We present a method that enables a DNN (student) to learn from the internal representational spaces of a reference model (teacher), which could be another DNN or, in the future, a biological brain. Representational spaces of the student and the teacher are characterized by representational distance matrices (RDMs). We propose representational distance learning (RDL), a stochastic gradient descent method that drives the RDMs of the student to approximate the RDMs of the teacher. We demonstrate that RDL is competitive with other transfer learning techniques for two publicly available benchmark computer vision datasets (MNIST and CIFAR-100), while allowing for architectural differences between student and teacher. By pulling the student&#39;s RDMs towards those of the teacher, RDL significantly improved visual classification performance when compared to baseline networks that did not use transfer learning. In the future, RDL may enable combined supervised training of deep neural networks using task constraints (e.g. images and category labels) and constraints from brain-activity measurements, so as to build models that replicate the internal representational spaces of biological brains.
ER  -


TY  - Preprint
T1  - Feature Learning based Deep Supervised Hashing with Pairwise Labels
A1  - Wu-Jun Li
A1  - Sheng Wang
A1  - Wang-Cheng Kang
JO  - ArXiv e-prints
Y1  - 21 April, 2016
UR  - https://arxiv.org/abs/1511.03855
N2  - Recent years have witnessed wide application of hashing for large-scale image retrieval. However, most existing hashing methods are based on hand-crafted features which might not be optimally compatible with the hashing procedure. Recently, deep hashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks, which have shown better performance than traditional hashing methods with hand-crafted features. Most of these deep hashing methods are supervised whose supervised information is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and hash-code learning. In this paper, we propose a novel deep hashing method, called deep pairwise-supervised hashing(DPSH), to perform simultaneous feature learning and hash-code learning for applications with pairwise labels. Experiments on real datasets show that our DPSH method can outperform other methods to achieve the state-of-the-art performance in image retrieval applications.
ER  -


TY  - Preprint
T1  - Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control
A1  - Fangyi Zhang
A1  - JÃ¼rgen Leitner
A1  - Michael Milford
A1  - Ben Upcroft
A1  - Peter Corke
JO  - ArXiv e-prints
Y1  - 13 November, 2015
UR  - https://arxiv.org/abs/1511.03791
N2  - This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only. The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time. We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation. A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.
ER  -


TY  - Preprint
T1  - A new humanlike facial attractiveness predictor with cascaded fine-tuning deep learning model
A1  - Jie Xu
A1  - Lianwen Jin
A1  - Lingyu Liang
A1  - Ziyong Feng
A1  - Duorui Xie
JO  - ArXiv e-prints
Y1  - 8 November, 2015
UR  - https://arxiv.org/abs/1511.02465
N2  - This paper proposes a deep leaning method to address the challenging facial attractiveness prediction problem. The method constructs a convolutional neural network of facial beauty prediction using a new deep cascaded fine-turning scheme with various face inputting channels, such as the original RGB face image, the detail layer image, and the lighting layer image. With a carefully designed CNN model of deep structure, large input size and small convolutional kernels, we have achieved a high prediction correlation of 0.88. This result convinces us that the problem of facial attractiveness prediction can be solved by deep learning approach, and it also shows the important roles of the facial smoothness, lightness, and color information that were involved in facial beauty perception, which is consistent with the result of recent psychology studies. Furthermore, we analyze the high-level features learnt by CNN through visualization of its hidden layers, and some interesting phenomena were observed. It is found that the contours and appearance of facial features, especially eyes and moth, are the most significant facial attributes for facial attractiveness prediction, which is also consistent with the visual perception intuition of human.
ER  -


TY  - Preprint
T1  - Deep Kernel Learning
A1  - Andrew Gordon Wilson
A1  - Zhiting Hu
A1  - Ruslan Salakhutdinov
A1  - Eric P. Xing
JO  - ArXiv e-prints
Y1  - 6 November, 2015
UR  - https://arxiv.org/abs/1511.02222
N2  - We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost $O(n)$ for $n$ training points, and predictions cost $O(1)$ per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.
ER  -


TY  - Preprint
T1  - Distributed Deep Learning for Question Answering
A1  - Minwei Feng
A1  - Bing Xiang
A1  - Bowen Zhou
JO  - ArXiv e-prints
Y1  - 4 August, 2016
UR  - https://arxiv.org/abs/1511.01158
N2  - This paper is an empirical study of the distributed deep learning for question answering subtasks: answer selection and question classification. Comparison studies of SGD, MSGD, ADADELTA, ADAGRAD, ADAM/ADAMAX, RMSPROP, DOWNPOUR and EASGD/EAMSGD algorithms have been presented. Experimental results show that the distributed framework based on the message passing interface can accelerate the convergence speed at a sublinear scale. This paper demonstrates the importance of distributed training. For example, with 48 workers, a 24x speedup is achievable for the answer selection task and running time is decreased from 138.2 hours to 5.81 hours, which will increase the productivity significantly.
ER  -


TY  - Preprint
T1  - Generating Text with Deep Reinforcement Learning
A1  - Hongyu Guo
JO  - ArXiv e-prints
Y1  - 30 October, 2015
UR  - https://arxiv.org/abs/1510.09202
N2  - We introduce a novel schema for sequence to sequence learning with a Deep Q-Network (DQN), which decodes the output sequence iteratively. The aim here is to enable the decoder to first tackle easier portions of the sequences, and then turn to cope with difficult parts. Specifically, in each iteration, an encoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the input sequence, automatically create features to represent the internal states of and formulate a list of potential actions for the DQN. Take rephrasing a natural sentence as an example. This list can contain ranked potential words. Next, the DQN learns to make decision on which action (e.g., word) will be selected from the list to modify the current decoded sequence. The newly modified output sequence is subsequently used as the input to the DQN for the next decoding iteration. In each iteration, we also bias the reinforcement learning&#39;s attention to explore sequence portions which are previously difficult to be decoded. For evaluation, the proposed strategy was trained to decode ten thousands natural sentences. Our experiments indicate that, when compared to a left-to-right greedy beam search LSTM decoder, the proposed method performed competitively well when decoding sentences from the training set, but significantly outperformed the baseline when decoding unseen sentences, in terms of BLEU score obtained.
ER  -


TY  - Preprint
T1  - Empirical Study on Deep Learning Models for Question Answering
A1  - Yang Yu
A1  - Wei Zhang
A1  - Chung-Wei Hang
A1  - Bing Xiang
A1  - Bowen Zhou
JO  - ArXiv e-prints
Y1  - 20 November, 2015
UR  - https://arxiv.org/abs/1510.07526
N2  - In this paper we explore deep learning models with memory component or attention mechanism for question answering task. We combine and compare three models, Neural Machine Translation, Neural Turing Machine, and Memory Networks for a simulated QA data set. This paper is the first one that uses Neural Machine Translation and Neural Turing Machines for solving QA tasks. Our results suggest that the combination of attention and memory have potential to solve certain QA problem.
ER  -


TY  - Preprint
T1  - A Framework for Distributed Deep Learning Layer Design in Python
A1  - Clay McLeod
JO  - ArXiv e-prints
Y1  - 25 October, 2015
UR  - https://arxiv.org/abs/1510.07303
N2  - In this paper, a framework for testing Deep Neural Network (DNN) design in Python is presented. First, big data, machine learning (ML), and Artificial Neural Networks (ANNs) are discussed to familiarize the reader with the importance of such a system. Next, the benefits and detriments of implementing such a system in Python are presented. Lastly, the specifics of the system are explained, and some experimental results are presented to prove the effectiveness of the system.
ER  -


TY  - Preprint
T1  - Vehicle Speed Prediction using Deep Learning
A1  - Joe Lemieux
A1  - Yuan Ma
JO  - ArXiv e-prints
Y1  - 25 October, 2015
UR  - https://arxiv.org/abs/1510.07208
N2  - Global optimization of the energy consumption of dual power source vehicles such as hybrid electric vehicles, plug-in hybrid electric vehicles, and plug in fuel cell electric vehicles requires knowledge of the complete route characteristics at the beginning of the trip. One of the main characteristics is the vehicle speed profile across the route. The profile will translate directly into energy requirements for a given vehicle. However, the vehicle speed that a given driver chooses will vary from driver to driver and from time to time, and may be slower, equal to, or faster than the average traffic flow. If the specific driver speed profile can be predicted, the energy usage can be optimized across the route chosen. The purpose of this paper is to research the application of Deep Learning techniques to this problem to identify at the beginning of a drive cycle the driver specific vehicle speed profile for an individual driver repeated drive cycle, which can be used in an optimization algorithm to minimize the amount of fossil fuel energy used during the trip.
ER  -


TY  - Preprint
T1  - A Survey: Time Travel in Deep Learning Space: An Introduction to Deep Learning Models and How Deep Learning Models Evolved from the Initial Ideas
A1  - Haohan Wang
A1  - Bhiksha Raj
JO  - ArXiv e-prints
Y1  - 6 November, 2015
UR  - https://arxiv.org/abs/1510.04781
N2  - This report will show the history of deep learning evolves. It will trace back as far as the initial belief of connectionism modelling of brain, and come back to look at its early stage realization: neural networks. With the background of neural network, we will gradually introduce how convolutional neural network, as a representative of deep discriminative models, is developed from neural networks, together with many practical techniques that can help in optimization of neural networks. On the other hand, we will also trace back to see the evolution history of deep generative models, to see how researchers balance the representation power and computation complexity to reach Restricted Boltzmann Machine and eventually reach Deep Belief Nets. Further, we will also look into the development history of modelling time series data with neural networks. We start with Time Delay Neural Networks and move further to currently famous model named Recurrent Neural Network and its extension Long Short Term Memory. We will also briefly look into how to construct deep recurrent neural networks. Finally, we will conclude this report with some interesting open-ended questions of deep neural networks.
ER  -


TY  - Preprint
T1  - Layer-Specific Adaptive Learning Rates for Deep Networks
A1  - Bharat Singh
A1  - Soham De
A1  - Yangmuzi Zhang
A1  - Thomas Goldstein
A1  - Gavin Taylor
JO  - ArXiv e-prints
Y1  - 15 October, 2015
UR  - https://arxiv.org/abs/1510.04609
N2  - The increasing complexity of deep learning architectures is resulting in training time requiring weeks or even months. This slow training is due in part to vanishing gradients, in which the gradients used by back-propagation are extremely large for weights connecting deep layers (layers near the output layer), and extremely small for shallow layers (near the input layer); this results in slow learning in the shallow layers. Additionally, it has also been shown that in highly non-convex problems, such as deep neural networks, there is a proliferation of high-error low curvature saddle points, which slows down learning dramatically. In this paper, we attempt to overcome the two above problems by proposing an optimization method for training deep neural networks which uses learning rates which are both specific to each layer in the network and adaptive to the curvature of the function, increasing the learning rate at low curvature points. This enables us to speed up learning in the shallow layers of the network and quickly escape high-error low curvature saddle points. We test our method on standard image classification datasets such as MNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy as well as reduces the required training time over standard algorithms.
ER  -


TY  - Preprint
T1  - Improved Deep Learning Baselines for Ubuntu Corpus Dialogs
A1  - Rudolf Kadlec
A1  - Martin Schmid
A1  - Jan Kleindienst
JO  - ArXiv e-prints
Y1  - 3 November, 2015
UR  - https://arxiv.org/abs/1510.03753
N2  - This paper presents results of our experiments for the next utterance ranking on the Ubuntu Dialog Corpus -- the largest publicly available multi-turn dialog corpus. First, we use an in-house implementation of previously reported models to do an independent evaluation using the same data. Second, we evaluate the performances of various LSTMs, Bi-LSTMs and CNNs on the dataset. Third, we create an ensemble by averaging predictions of multiple models. The ensemble further improves the performance and it achieves a state-of-the-art result for the next utterance ranking on this dataset. Finally, we discuss our future plans using this corpus.
ER  -


TY  - Preprint
T1  - Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?
A1  - Pooya Khorrami
A1  - Tom Le Paine
A1  - Thomas S. Huang
JO  - ArXiv e-prints
Y1  - 15 March, 2017
UR  - https://arxiv.org/abs/1510.02969
N2  - Despite being the appearance-based classifier of choice in recent years, relatively few works have examined how much convolutional neural networks (CNNs) can improve performance on accepted expression recognition benchmarks and, more importantly, examine what it is they actually learn. In this work, not only do we show that CNNs can achieve strong performance, but we also introduce an approach to decipher which portions of the face influence the CNN&#39;s predictions. First, we train a zero-bias CNN on facial expression data and achieve, to our knowledge, state-of-the-art performance on two expression recognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD). We then qualitatively analyze the network by visualizing the spatial patterns that maximally excite different neurons in the convolutional layers and show how they resemble Facial Action Units (FAUs). Finally, we use the FAU labels provided in the CK+ dataset to verify that the FAUs observed in our filter visualizations indeed align with the subject&#39;s facial movements.
ER  -


TY  - Preprint
T1  - Large-scale Artificial Neural Network: MapReduce-based Deep Learning
A1  - Kairan Sun
A1  - Xu Wei
A1  - Gengtao Jia
A1  - Risheng Wang
A1  - Ruizhi Li
JO  - ArXiv e-prints
Y1  - 9 October, 2015
UR  - https://arxiv.org/abs/1510.02709
N2  - Faced with continuously increasing scale of data, original back-propagation neural network based machine learning algorithm presents two non-trivial challenges: huge amount of data makes it difficult to maintain both efficiency and accuracy; redundant data aggravates the system workload. This project is mainly focused on the solution to the issues above, combining deep learning algorithm with cloud computing platform to deal with large-scale data. A MapReduce-based handwriting character recognizer will be designed in this project to verify the efficiency improvement this mechanism will achieve on training and practical large-scale data. Careful discussion and experiment will be developed to illustrate how deep learning algorithm works to train handwritten digits data, how MapReduce is implemented on deep learning neural network, and why this combination accelerates computation. Besides performance, the scalability and robustness will be mentioned in this report as well. Our system comes with two demonstration software that visually illustrates our handwritten digit recognition/encoding application.
ER  -


TY  - Preprint
T1  - Uniform Learning in a Deep Neural Network via &#34;Oddball&#34; Stochastic Gradient Descent
A1  - Andrew J. R. Simpson
JO  - ArXiv e-prints
Y1  - 8 October, 2015
UR  - https://arxiv.org/abs/1510.02442
N2  - When training deep neural networks, it is typically assumed that the training examples are uniformly difficult to learn. Or, to restate, it is assumed that the training error will be uniformly distributed across the training examples. Based on these assumptions, each training example is used an equal number of times. However, this assumption may not be valid in many cases. &#34;Oddball SGD&#34; (novelty-driven stochastic gradient descent) was recently introduced to drive training probabilistically according to the error distribution - training frequency is proportional to training error magnitude. In this article, using a deep neural network to encode a video, we show that oddball SGD can be used to enforce uniform error across the training set.
ER  -


TY  - Preprint
T1  - Data-Efficient Learning of Feedback Policies from Image Pixels using Deep Dynamical Models
A1  - John-Alexander M. Assael
A1  - Niklas WahlstrÃ¶m
A1  - Thomas B. SchÃ¶n
A1  - Marc Peter Deisenroth
JO  - ArXiv e-prints
Y1  - 9 October, 2015
UR  - https://arxiv.org/abs/1510.02173
N2  - Data-efficient reinforcement learning (RL) in continuous state-action spaces using very high-dimensional observations remains a key challenge in developing fully autonomous systems. We consider a particularly important instance of this challenge, the pixels-to-torques problem, where an RL agent learns a closed-loop control policy (&#34;torques&#34;) from pixel information only. We introduce a data-efficient, model-based reinforcement learning algorithm that learns such a closed-loop policy directly from pixel information. The key ingredient is a deep dynamical model for learning a low-dimensional feature embedding of images jointly with a predictive model in this low-dimensional feature space. Joint learning is crucial for long-term predictions, which lie at the core of the adaptive nonlinear model predictive control strategy that we use for closed-loop control. Compared to state-of-the-art RL methods for continuous states and actions, our approach learns quickly, scales to high-dimensional state spaces, is lightweight and an important step toward fully autonomous end-to-end learning from pixels to torques.
ER  -


TY  - Preprint
T1  - Predicting Daily Activities From Egocentric Images Using Deep Learning
A1  - Daniel Castro
A1  - Steven Hickson
A1  - Vinay Bettadapura
A1  - Edison Thomaz
A1  - Gregory Abowd
A1  - Henrik Christensen
A1  - Irfan Essa
JO  - ArXiv e-prints
Y1  - 6 October, 2015
UR  - https://arxiv.org/abs/1510.01576
N2  - We present a method to analyze images taken from a passive egocentric wearable camera along with the contextual information, such as time and day of week, to learn and predict everyday activities of an individual. We collected a dataset of 40,103 egocentric images over a 6 month period with 19 activity classes and demonstrate the benefit of state-of-the-art deep learning techniques for learning and predicting daily activities. Classification is conducted using a Convolutional Neural Network (CNN) with a classification method we introduce called a late fusion ensemble. This late fusion ensemble incorporates relevant contextual information and increases our classification accuracy. Our technique achieves an overall accuracy of 83.07% in predicting a person&#39;s activity across the 19 activity classes. We also demonstrate some promising results from two additional users by fine-tuning the classifier with one day of training data.
ER  -


TY  - Preprint
T1  - Learning Deep Representations of Appearance and Motion for Anomalous Event Detection
A1  - Dan Xu
A1  - Elisa Ricci
A1  - Yan Yan
A1  - Jingkuan Song
A1  - Nicu Sebe
JO  - ArXiv e-prints
Y1  - 6 October, 2015
UR  - https://arxiv.org/abs/1510.01553
N2  - We present a novel unsupervised deep learning framework for anomalous event detection in complex video scenes. While most existing works merely use hand-crafted appearance and motion features, we propose Appearance and Motion DeepNet (AMDN) which utilizes deep neural networks to automatically learn feature representations. To exploit the complementary information of both appearance and motion patterns, we introduce a novel double fusion framework, combining both the benefits of traditional early fusion and late fusion strategies. Specifically, stacked denoising autoencoders are proposed to separately learn both appearance and motion features as well as a joint representation (early fusion). Based on the learned representations, multiple one-class SVM models are used to predict the anomaly scores of each input, which are then integrated with a late fusion strategy for final anomaly detection. We evaluate the proposed method on two publicly available video surveillance datasets, showing competitive performance with respect to state of the art approaches.
ER  -


TY  - Preprint
T1  - Transfer Learning from Deep Features for Remote Sensing and Poverty Mapping
A1  - Michael Xie
A1  - Neal Jean
A1  - Marshall Burke
A1  - David Lobell
A1  - Stefano Ermon
JO  - ArXiv e-prints
Y1  - 27 February, 2016
UR  - https://arxiv.org/abs/1510.00098
N2  - The lack of reliable data in developing countries is a major obstacle to sustainable development, food security, and disaster relief. Poverty data, for example, is typically scarce, sparse in coverage, and labor-intensive to obtain. Remote sensing data such as high-resolution satellite imagery, on the other hand, is becoming increasingly available and inexpensive. Unfortunately, such data is highly unstructured and currently no techniques exist to automatically extract useful insights to inform policy decisions and help direct humanitarian efforts. We propose a novel machine learning approach to extract large-scale socioeconomic indicators from high-resolution satellite imagery. The main challenge is that training data is very scarce, making it difficult to apply modern techniques such as Convolutional Neural Networks (CNN). We therefore propose a transfer learning approach where nighttime light intensities are used as a data-rich proxy. We train a fully convolutional CNN model to predict nighttime lights from daytime imagery, simultaneously learning features that are useful for poverty prediction. The model learns filters identifying different terrains and man-made structures, including roads, buildings, and farmlands, without any supervision beyond nighttime lights. We demonstrate that these learned features are highly informative for poverty mapping, even approaching the predictive performance of survey data collected in the field.
ER  -


TY  - Preprint
T1  - Conditional Deep Learning for Energy-Efficient and Enhanced Pattern Recognition
A1  - Priyadarshini Panda
A1  - Abhronil Sengupta
A1  - Kaushik Roy
JO  - ArXiv e-prints
Y1  - 28 January, 2016
UR  - https://arxiv.org/abs/1509.08971
N2  - Deep learning neural networks have emerged as one of the most powerful classification tools for vision related applications. However, the computational and energy requirements associated with such deep nets can be quite high, and hence their energy-efficient implementation is of great interest. Although traditionally the entire network is utilized for the recognition of all inputs, we observe that the classification difficulty varies widely across inputs in real-world datasets; only a small fraction of inputs require the full computational effort of a network, while a large majority can be classified correctly with very low effort. In this paper, we propose Conditional Deep Learning (CDL) where the convolutional layer features are used to identify the variability in the difficulty of input instances and conditionally activate the deeper layers of the network. We achieve this by cascading a linear network of output neurons for each convolutional layer and monitoring the output of the linear network to decide whether classification can be terminated at the current stage or not. The proposed methodology thus enables the network to dynamically adjust the computational effort depending upon the difficulty of the input data while maintaining competitive classification accuracy. We evaluate our approach on the MNIST dataset. Our experiments demonstrate that our proposed CDL yields 1.91x reduction in average number of operations per input, which translates to 1.84x improvement in energy. In addition, our results show an improvement in classification accuracy from 97.5% to 98.9% as compared to the original network.
ER  -


TY  - Preprint
T1  - Semantics, Representations and Grammars for Deep Learning
A1  - David Balduzzi
JO  - ArXiv e-prints
Y1  - 29 September, 2015
UR  - https://arxiv.org/abs/1509.08627
N2  - Deep learning is currently the subject of intensive study. However, fundamental concepts such as representations are not formally defined -- researchers &#34;know them when they see them&#34; -- and there is no common language for describing and analyzing algorithms. This essay proposes an abstract framework that identifies the essential features of current practice and may provide a foundation for future developments.
ER  -


TY  - Preprint
T1  - Deep Trans-layer Unsupervised Networks for Representation Learning
A1  - Wentao Zhu
A1  - Jun Miao
A1  - Laiyun Qing
A1  - Xilin Chen
JO  - ArXiv e-prints
Y1  - 26 September, 2015
UR  - https://arxiv.org/abs/1509.08038
N2  - Learning features from massive unlabelled data is a vast prevalent topic for high-level tasks in many machine learning applications. The recent great improvements on benchmark data sets achieved by increasingly complex unsupervised learning methods and deep learning models with lots of parameters usually requires many tedious tricks and much expertise to tune. However, filters learned by these complex architectures are quite similar to standard hand-crafted features visually. In this paper, unsupervised learning methods, such as PCA or auto-encoder, are employed as the building block to learn filter banks at each layer. The lower layer responses are transferred to the last layer (trans-layer) to form a more complete representation retaining more information. In addition, some beneficial methods such as local contrast normalization and whitening are added to the proposed deep trans-layer networks to further boost performance. The trans-layer representations are followed by block histograms with binary encoder schema to learn translation and rotation invariant representations, which are utilized to do high-level tasks such as recognition and classification. Compared to traditional deep learning methods, the implemented feature learning method has much less parameters and is validated in several typical experiments, such as digit recognition on MNIST and MNIST variations, object recognition on Caltech 101 dataset and face verification on LFW dataset. The deep trans-layer unsupervised learning achieves 99.45% accuracy on MNIST dataset, 67.11% accuracy on 15 samples per class and 75.98% accuracy on 30 samples per class on Caltech 101 dataset, 87.10% on LFW dataset.
ER  -


TY  - Preprint
T1  - Learning Deep Control Policies for Autonomous Aerial Vehicles with MPC-Guided Policy Search
A1  - Tianhao Zhang
A1  - Gregory Kahn
A1  - Sergey Levine
A1  - Pieter Abbeel
JO  - ArXiv e-prints
Y1  - 16 February, 2016
UR  - https://arxiv.org/abs/1509.06791
N2  - Model predictive control (MPC) is an effective method for controlling robotic systems, particularly autonomous aerial vehicles such as quadcopters. However, application of MPC can be computationally demanding, and typically requires estimating the state of the system, which can be challenging in complex, unstructured environments. Reinforcement learning can in principle forego the need for explicit state estimation and acquire a policy that directly maps sensor readings to actions, but is difficult to apply to unstable systems that are liable to fail catastrophically during training before an effective policy has been found. We propose to combine MPC with reinforcement learning in the framework of guided policy search, where MPC is used to generate data at training time, under full state observations provided by an instrumented training environment. This data is used to train a deep neural network policy, which is allowed to access only the raw observations from the vehicle&#39;s onboard sensors. After training, the neural network policy can successfully control the robot without knowledge of the full state, and at a fraction of the computational cost of MPC. We evaluate our method by learning obstacle avoidance policies for a simulated quadrotor, using simulated onboard sensors and no explicit state estimation at test time.
ER  -


TY  - Preprint
T1  - Deep Reinforcement Learning with Double Q-learning
A1  - Hado van Hasselt
A1  - Arthur Guez
A1  - David Silver
JO  - ArXiv e-prints
Y1  - 8 December, 2015
UR  - https://arxiv.org/abs/1509.06461
N2  - The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.
ER  -


TY  - Preprint
T1  - From Facial Parts Responses to Face Detection: A Deep Learning Approach
A1  - Shuo Yang
A1  - Ping Luo
A1  - Chen Change Loy
A1  - Xiaoou Tang
JO  - ArXiv e-prints
Y1  - 21 September, 2015
UR  - https://arxiv.org/abs/1509.06451
N2  - In this paper, we propose a novel deep convolutional network (DCN) that achieves outstanding performance on FDDB, PASCAL Face, and AFW. Specifically, our method achieves a high recall rate of 90.99% on the challenging FDDB benchmark, outperforming the state-of-the-art method by a large margin of 2.91%. Importantly, we consider finding faces from a new perspective through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variation, which are the main difficulty and bottleneck of most existing face detection approaches. We show that despite the use of DCN, our network can achieve practical runtime speed.
ER  -


TY  - Preprint
T1  - Evaluating the visualization of what a Deep Neural Network has learned
A1  - Wojciech Samek
A1  - Alexander Binder
A1  - GrÃ©goire Montavon
A1  - Sebastian Bach
A1  - Klaus-Robert MÃ¼ller
JO  - ArXiv e-prints
Y1  - 21 September, 2015
UR  - https://arxiv.org/abs/1509.06321
N2  - Deep Neural Networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multi-layer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the &#39;&#39;importance&#39;&#39; of individual pixels wrt the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012 and MIT Places data sets. Our main result is that the recently proposed Layer-wise Relevance Propagation (LRP) algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of neural network performance.
ER  -


TY  - Preprint
T1  - Deep Spatial Autoencoders for Visuomotor Learning
A1  - Chelsea Finn
A1  - Xin Yu Tan
A1  - Yan Duan
A1  - Trevor Darrell
A1  - Sergey Levine
A1  - Pieter Abbeel
JO  - ArXiv e-prints
Y1  - 1 March, 2016
UR  - https://arxiv.org/abs/1509.06113
N2  - Reinforcement learning provides a powerful and flexible framework for automated acquisition of robotic motion skills. However, applying reinforcement learning requires a sufficiently detailed representation of the state, including the configuration of task-relevant objects. We present an approach that automates state-space construction by learning a state representation directly from camera images. Our method uses a deep spatial autoencoder to acquire a set of feature points that describe the environment for the current task, such as the positions of objects, and then learns a motion skill with these feature points using an efficient reinforcement learning method based on local linear models. The resulting controller reacts continuously to the learned feature points, allowing the robot to dynamically manipulate objects in the world with closed-loop control. We demonstrate our method with a PR2 robot on tasks that include pushing a free-standing toy block, picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions. In each task, our method automatically learns to track task-relevant objects and manipulate their configuration with the robot&#39;s arm.
ER  -


TY  - Preprint
T1  - Modelling Uncertainty in Deep Learning for Camera Relocalization
A1  - Alex Kendall
A1  - Roberto Cipolla
JO  - ArXiv e-prints
Y1  - 18 February, 2016
UR  - https://arxiv.org/abs/1509.05909
N2  - We present a robust and real-time monocular six degree of freedom visual relocalization system. We use a Bayesian convolutional neural network to regress the 6-DOF camera pose from a single RGB image. It is trained in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking under 6ms to compute. It obtains approximately 2m and 6 degrees accuracy for very large scale outdoor scenes and 0.5m and 10 degrees accuracy indoors. Using a Bayesian convolutional neural network implementation we obtain an estimate of the model&#39;s relocalization uncertainty and improve state of the art localization accuracy on a large scale outdoor dataset. We leverage the uncertainty measure to estimate metric relocalization error and to detect the presence or absence of the scene in the input image. We show that the model&#39;s uncertainty is caused by images being dissimilar to the training dataset in either pose or appearance.
ER  -


TY  - Preprint
T1  - Deep Multi-task Learning for Railway Track Inspection
A1  - Xavier Gibert
A1  - Vishal M. Patel
A1  - Rama Chellappa
JO  - ArXiv e-prints
Y1  - 17 September, 2015
UR  - https://arxiv.org/abs/1509.05267
N2  - Railroad tracks need to be periodically inspected and monitored to ensure safe transportation. Automated track inspection using computer vision and pattern recognition methods have recently shown the potential to improve safety by allowing for more frequent inspections while reducing human errors. Achieving full automation is still very challenging due to the number of different possible failure modes as well as the broad range of image variations that can potentially trigger false alarms. Also, the number of defective components is very small, so not many training examples are available for the machine to learn a robust anomaly detector. In this paper, we show that detection performance can be improved by combining multiple detectors within a multi-task learning framework. We show that this approach results in better accuracy in detecting defects on railway ties and fasteners.
ER  -


TY  - Preprint
T1  - On the Expressive Power of Deep Learning: A Tensor Analysis
A1  - Nadav Cohen
A1  - Or Sharir
A1  - Amnon Shashua
JO  - ArXiv e-prints
Y1  - 27 May, 2016
UR  - https://arxiv.org/abs/1509.05009
N2  - It has long been conjectured that hypotheses spaces suitable for data that is compositional in nature, such as text or images, may be more efficiently represented with deep hierarchical networks than with shallow ones. Despite the vast empirical evidence supporting this belief, theoretical justifications to date are limited. In particular, they do not account for the locality, sharing and pooling constructs of convolutional networks, the most successful deep learning architecture to date. In this work we derive a deep network architecture based on arithmetic circuits that inherently employs locality, sharing and pooling. An equivalence between the networks and hierarchical tensor factorizations is established. We show that a shallow network corresponds to CP (rank-1) decomposition, whereas a deep network corresponds to Hierarchical Tucker decomposition. Using tools from measure theory and matrix algebra, we prove that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be realized (or even approximated) by a shallow network. Since log-space computation transforms our networks into SimNets, the result applies directly to a deep learning architecture demonstrating promising empirical performance. The construction and theory developed in this paper shed new light on various practices and ideas employed by the deep learning community.
ER  -


TY  - Preprint
T1  - Adapting Resilient Propagation for Deep Learning
A1  - Alan Mosca
A1  - George D. Magoulas
JO  - ArXiv e-prints
Y1  - 16 September, 2015
UR  - https://arxiv.org/abs/1509.04612
N2  - The Resilient Propagation (Rprop) algorithm has been very popular for backpropagation training of multilayer feed-forward neural networks in various applications. The standard Rprop however encounters difficulties in the context of deep neural networks as typically happens with gradient-based learning algorithms. In this paper, we propose a modification of the Rprop that combines standard Rprop steps with a special drop out technique. We apply the method for training Deep Neural Networks as standalone components and in ensemble formulations. Results on the MNIST dataset show that the proposed modification alleviates standard Rprop&#39;s problems demonstrating improved learning speed and accuracy.
ER  -


TY  - Preprint
T1  - Hessian-free Optimization for Learning Deep Multidimensional Recurrent Neural Networks
A1  - Minhyung Cho
A1  - Chandra Shekhar Dhir
A1  - Jaehyung Lee
JO  - ArXiv e-prints
Y1  - 23 October, 2015
UR  - https://arxiv.org/abs/1509.03475
N2  - Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable performance in the area of speech and handwriting recognition. The performance of an MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by using Hessian-free (HF) optimization. Given that connectionist temporal classification (CTC) is utilized as an objective of learning an MDRNN for sequence labeling, the non-convexity of CTC poses a problem when applying HF to the network. As a solution, a convex approximation of CTC is formulated and its relationship with the EM algorithm and the Fisher information matrix is discussed. An MDRNN up to a depth of 15 layers is successfully trained using HF, resulting in an improved performance for sequence labeling.
ER  -


TY  - Preprint
T1  - Learning Sparse Feature Representations using Probabilistic Quadtrees and Deep Belief Nets
A1  - Saikat Basu
A1  - Manohar Karki
A1  - Sangram Ganguly
A1  - Robert DiBiano
A1  - Supratik Mukhopadhyay
A1  - Ramakrishna Nemani
JO  - ArXiv e-prints
Y1  - 11 September, 2015
UR  - https://arxiv.org/abs/1509.03413
N2  - Learning sparse feature representations is a useful instrument for solving an unsupervised learning problem. In this paper, we present three labeled handwritten digit datasets, collectively called n-MNIST. Then, we propose a novel framework for the classification of handwritten digits that learns sparse representations using probabilistic quadtrees and Deep Belief Nets. On the MNIST and n-MNIST datasets, our framework shows promising results and significantly outperforms traditional Deep Belief Networks.
ER  -


TY  - Preprint
T1  - A deep matrix factorization method for learning attribute representations
A1  - George Trigeorgis
A1  - Konstantinos Bousmalis
A1  - Stefanos Zafeiriou
A1  - Bjoern W. Schuller
JO  - ArXiv e-prints
Y1  - 10 September, 2015
UR  - https://arxiv.org/abs/1509.03248
N2  - Semi-Non-negative Matrix Factorization is a technique that learns a low-dimensional representation of a dataset that lends itself to a clustering interpretation. It is possible that the mapping between this new representation and our original data matrix contains rather complex hierarchical information with implicit lower-level hidden attributes, that classical one level clustering methodologies can not interpret. In this work we propose a novel model, Deep Semi-NMF, that is able to learn such hidden representations that allow themselves to an interpretation of clustering according to different, unknown attributes of a given dataset. We also present a semi-supervised version of the algorithm, named Deep WSF, that allows the use of (partial) prior information for each of the known attributes of a dataset, that allows the model to be used on datasets with mixed attribute knowledge. Finally, we show that our models are able to learn low-dimensional representations that are better suited for clustering, but also classification, outperforming Semi-Non-negative Matrix Factorization, but also other state-of-the-art methodologies variants.
ER  -


TY  - Preprint
T1  - Compatible Value Gradients for Reinforcement Learning of Continuous Deep Policies
A1  - David Balduzzi
A1  - Muhammad Ghifary
JO  - ArXiv e-prints
Y1  - 10 September, 2015
UR  - https://arxiv.org/abs/1509.03005
N2  - This paper proposes GProp, a deep reinforcement learning algorithm for continuous policies with compatible function approximation. The algorithm is based on two innovations. Firstly, we present a temporal-difference based method for learning the gradient of the value-function. Secondly, we present the deviator-actor-critic (DAC) model, which comprises three neural networks that estimate the value function, its gradient, and determine the actor&#39;s policy respectively. We evaluate GProp on two challenging tasks: a contextual bandit problem constructed from nonparametric regression datasets that is designed to probe the ability of reinforcement learning algorithms to accurately estimate gradients; and the octopus arm, a challenging reinforcement learning benchmark. GProp is competitive with fully supervised methods on the bandit task and achieves the best performance to date on the octopus arm.
ER  -


TY  - Preprint
T1  - Continuous control with deep reinforcement learning
A1  - Timothy P. Lillicrap
A1  - Jonathan J. Hunt
A1  - Alexander Pritzel
A1  - Nicolas Heess
A1  - Tom Erez
A1  - Yuval Tassa
A1  - David Silver
A1  - Daan Wierstra
JO  - ArXiv e-prints
Y1  - 29 February, 2016
UR  - https://arxiv.org/abs/1509.02971
N2  - We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.
ER  -


TY  - Preprint
T1  - Hierarchical Deep Learning Architecture For 10K Objects Classification
A1  - Atul Laxman Katole
A1  - Krishna Prasad Yellapragada
A1  - Amish Kumar Bedi
A1  - Sehaj Singh Kalra
A1  - Mynepalli Siva Chaitanya
JO  - ArXiv e-prints
Y1  - 7 September, 2015
UR  - https://arxiv.org/abs/1509.01951
N2  - Evolution of visual object recognition architectures based on Convolutional Neural Networks &amp; Convolutional Deep Belief Networks paradigms has revolutionized artificial Vision Science. These architectures extract &amp; learn the real world hierarchical visual features utilizing supervised &amp; unsupervised learning approaches respectively. Both the approaches yet cannot scale up realistically to provide recognition for a very large number of objects as high as 10K. We propose a two level hierarchical deep learning architecture inspired by divide &amp; conquer principle that decomposes the large scale recognition architecture into root &amp; leaf level model architectures. Each of the root &amp; leaf level models is trained exclusively to provide superior results than possible by any 1-level deep learning architecture prevalent today. The proposed architecture classifies objects in two steps. In the first step the root level model classifies the object in a high level category. In the second step, the leaf level recognition model for the recognized high level category is selected among all the leaf models. This leaf level model is presented with the same input object image which classifies it in a specific category. Also we propose a blend of leaf level models trained with either supervised or unsupervised learning approaches. Unsupervised learning is suitable whenever labelled data is scarce for the specific leaf level models. Currently the training of leaf level models is in progress; where we have trained 25 out of the total 47 leaf level models as of now. We have trained the leaf models with the best case top-5 error rate of 3.2% on the validation data set for the particular leaf models. Also we demonstrate that the validation error of the leaf level models saturates towards the above mentioned accuracy as the number of epochs are increased to more than sixty.
ER  -


TY  - Preprint
T1  - Giraffe: Using Deep Reinforcement Learning to Play Chess
A1  - Matthew Lai
JO  - ArXiv e-prints
Y1  - 14 September, 2015
UR  - https://arxiv.org/abs/1509.01549
N2  - This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer. Unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions, Giraffe&#39;s learning system also performs automatic feature extraction and pattern recognition. The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters. Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess.
ER  -


TY  - Preprint
T1  - Deep Broad Learning - Big Models for Big Data
A1  - Nayyar A. Zaidi
A1  - Geoffrey I. Webb
A1  - Mark J. Carman
A1  - Francois Petitjean
JO  - ArXiv e-prints
Y1  - 4 September, 2015
UR  - https://arxiv.org/abs/1509.01346
N2  - Deep learning has demonstrated the power of detailed modeling of complex high-order (multivariate) interactions in data. For some learning tasks there is power in learning models that are not only Deep but also Broad. By Broad, we mean models that incorporate evidence from large numbers of features. This is of especial value in applications where many different features and combinations of features all carry small amounts of information about the class. The most accurate models will integrate all that information. In this paper, we propose an algorithm for Deep Broad Learning called DBL. The proposed algorithm has a tunable parameter $n$, that specifies the depth of the model. It provides straightforward paths towards out-of-core learning for large data. We demonstrate that DBL learns models from large quantities of data with accuracy that is highly competitive with the state-of-the-art.
ER  -


TY  - Preprint
T1  - Learning Deep $\ell_0$ Encoders
A1  - Zhangyang Wang
A1  - Qing Ling
A1  - Thomas S. Huang
JO  - ArXiv e-prints
Y1  - 22 November, 2015
UR  - https://arxiv.org/abs/1509.00153
N2  - Despite its nonconvex nature, $\ell_0$ sparse approximation is desirable in many theoretical and application cases. We study the $\ell_0$ sparse approximation problem with the tool of deep learning, by proposing Deep $\ell_0$ Encoders. Two typical forms, the $\ell_0$ regularized problem and the $M$-sparse problem, are investigated. Based on solid iterative algorithms, we model them as feed-forward neural networks, through introducing novel neurons and pooling functions. Enforcing such structural priors acts as an effective network regularization. The deep encoders also enjoy faster inference, larger learning capacity, and better scalability compared to conventional sparse coding solutions. Furthermore, under task-driven losses, the models can be conveniently optimized from end to end. Numerical results demonstrate the impressive performances of the proposed encoders.
ER  -


TY  - Preprint
T1  - Learning A Task-Specific Deep Architecture For Clustering
A1  - Zhangyang Wang
A1  - Shiyu Chang
A1  - Jiayu Zhou
A1  - Meng Wang
A1  - Thomas S. Huang
JO  - ArXiv e-prints
Y1  - 16 October, 2015
UR  - https://arxiv.org/abs/1509.00151
N2  - While sparse coding-based clustering methods have shown to be successful, their bottlenecks in both efficiency and scalability limit the practical usage. In recent years, deep learning has been proved to be a highly effective, efficient and scalable feature learning tool. In this paper, we propose to emulate the sparse coding-based clustering pipeline in the context of deep learning, leading to a carefully crafted deep model benefiting from both. A feed-forward network structure, named TAGnet, is constructed based on a graph-regularized sparse coding algorithm. It is then trained with task-specific loss functions from end to end. We discover that connecting deep learning to sparse coding benefits not only the model performance, but also its initialization and interpretation. Moreover, by introducing auxiliary clustering tasks to the intermediate feature hierarchy, we formulate DTAGnet and obtain a further performance boost. Extensive experiments demonstrate that the proposed model gains remarkable margins over several state-of-the-art methods.
ER  -


TY  - Preprint
T1  - Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose Estimation
A1  - Sijin Li
A1  - Weichen Zhang
A1  - Antoni B. Chan
JO  - ArXiv e-prints
Y1  - 26 August, 2015
UR  - https://arxiv.org/abs/1508.06708
N2  - This paper focuses on structured-output learning using deep neural networks for 3D human pose estimation from monocular images. Our network takes an image and 3D pose as inputs and outputs a score value, which is high when the image-pose pair matches and low otherwise. The network structure consists of a convolutional neural network for image feature extraction, followed by two sub-networks for transforming the image features and pose into a joint embedding. The score function is then the dot-product between the image and pose embeddings. The image-pose embedding and score function are jointly trained using a maximum-margin cost function. Our proposed framework can be interpreted as a special form of structured support vector machines where the joint feature space is discriminatively learned using deep neural networks. We test our framework on the Human3.6m dataset and obtain state-of-the-art results compared to other recent methods. Finally, we present visualizations of the image-pose embedding space, demonstrating the network has learned a high-level embedding of body-orientation and pose-configuration.
ER  -


TY  - Preprint
T1  - Exemplar Based Deep Discriminative and Shareable Feature Learning for Scene Image Classification
A1  - Zhen Zuo
A1  - Gang Wang
A1  - Bing Shuai
A1  - Lifan Zhao
A1  - Qingxiong Yang
JO  - ArXiv e-prints
Y1  - 21 August, 2015
UR  - https://arxiv.org/abs/1508.05306
N2  - In order to encode the class correlation and class specific information in image representation, we propose a new local feature learning approach named Deep Discriminative and Shareable Feature Learning (DDSFL). DDSFL aims to hierarchically learn feature transformation filter banks to transform raw pixel image patches to features. The learned filter banks are expected to: (1) encode common visual patterns of a flexible number of categories; (2) encode discriminative information; and (3) hierarchically extract patterns at different visual levels. Particularly, in each single layer of DDSFL, shareable filters are jointly learned for classes which share the similar patterns. Discriminative power of the filters is achieved by enforcing the features from the same category to be close, while features from different categories to be far away from each other. Furthermore, we also propose two exemplar selection methods to iteratively select training data for more efficient and effective learning. Based on the experimental results, DDSFL can achieve very promising performance, and it also shows great complementary effect to the state-of-the-art Caffe features.
ER  -


TY  - Preprint
T1  - Distributed Compressive Sensing: A Deep Learning Approach
A1  - Hamid Palangi
A1  - Rabab Ward
A1  - Li Deng
JO  - ArXiv e-prints
Y1  - 11 May, 2016
UR  - https://arxiv.org/abs/1508.04924
N2  - Various studies that address the compressed sensing problem with Multiple Measurement Vectors (MMVs) have been recently carried. These studies assume the vectors of the different channels to be jointly sparse. In this paper, we relax this condition. Instead we assume that these sparse vectors depend on each other but that this dependency is unknown. We capture this dependency by computing the conditional probability of each entry in each vector being non-zero, given the &#34;residuals&#34; of all previous vectors. To estimate these probabilities, we propose the use of the Long Short-Term Memory (LSTM)[1], a data driven model for sequence modelling that is deep in time. To calculate the model parameters, we minimize a cross entropy cost function. To reconstruct the sparse vectors at the decoder, we propose a greedy solver that uses the above model to estimate the conditional probabilities. By performing extensive experiments on two real world datasets, we show that the proposed method significantly outperforms the general MMV solver (the Simultaneous Orthogonal Matching Pursuit (SOMP)) and a number of the model-based Bayesian methods. The proposed method does not add any complexity to the general compressive sensing encoder. The trained model is used just at the decoder. As the proposed method is a data driven method, it is only applicable when training data is available. In many applications however, training data is indeed available, e.g. in recorded images and videos.
ER  -


TY  - Preprint
T1  - Bit-Scalable Deep Hashing with Regularized Similarity Learning for Image Retrieval and Person Re-identification
A1  - Ruimao Zhang
A1  - Liang Lin
A1  - Rui Zhang
A1  - Wangmeng Zuo
A1  - Lei Zhang
JO  - ArXiv e-prints
Y1  - 21 August, 2015
UR  - https://arxiv.org/abs/1508.04535
N2  - Extracting informative image features and learning effective approximate hashing functions are two crucial steps in image retrieval . Conventional methods often study these two steps separately, e.g., learning hash functions from a predefined hand-crafted feature space. Meanwhile, the bit lengths of output hashing codes are preset in most previous methods, neglecting the significance level of different bits and restricting their practical flexibility. To address these issues, we propose a supervised learning framework to generate compact and bit-scalable hashing codes directly from raw images. We pose hashing learning as a problem of regularized similarity learning. Specifically, we organize the training images into a batch of triplet samples, each sample containing two images with the same label and one with a different label. With these triplet samples, we maximize the margin between matched pairs and mismatched pairs in the Hamming space. In addition, a regularization term is introduced to enforce the adjacency consistency, i.e., images of similar appearances should have similar codes. The deep convolutional neural network is utilized to train the model in an end-to-end fashion, where discriminative image features and hash functions are simultaneously optimized. Furthermore, each bit of our hashing codes is unequally weighted so that we can manipulate the code lengths by truncating the insignificant bits. Our framework outperforms state-of-the-arts on public benchmarks of similar image search and also achieves promising results in the application of person re-identification in surveillance. It is also shown that the generated bit-scalable hashing codes well preserve the discriminative powers with shorter code lengths.
ER  -


TY  - Preprint
T1  - Distributed Deep Q-Learning
A1  - Hao Yi Ong
A1  - Kevin Chavez
A1  - Augustus Hong
JO  - ArXiv e-prints
Y1  - 15 October, 2015
UR  - https://arxiv.org/abs/1508.04186
N2  - We propose a distributed deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is based on the deep Q-network, a convolutional neural network trained with a variant of Q-learning. Its input is raw pixels and its output is a value function estimating future rewards from taking an action given a system state. To distribute the deep Q-network training, we adapt the DistBelief software framework to the context of efficiently training reinforcement learning agents. As a result, the method is completely asynchronous and scales well with the number of machines. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to achieve reasonable success on a simple game with minimal parameter tuning.
ER  -


TY  - Preprint
T1  - A Deep Learning Approach to Structured Signal Recovery
A1  - Ali Mousavi
A1  - Ankit B. Patel
A1  - Richard G. Baraniuk
JO  - ArXiv e-prints
Y1  - 17 August, 2015
UR  - https://arxiv.org/abs/1508.04065
N2  - In this paper, we develop a new framework for sensing and recovering structured signals. In contrast to compressive sensing (CS) systems that employ linear measurements, sparse representations, and computationally complex convex/greedy algorithms, we introduce a deep learning framework that supports both linear and mildly nonlinear measurements, that learns a structured representation from training data, and that efficiently computes a signal estimate. In particular, we apply a stacked denoising autoencoder (SDA), as an unsupervised feature learner. SDA enables us to capture statistical dependencies between the different elements of certain signals and improve signal recovery performance as compared to the CS approach.
ER  -


TY  - Preprint
T1  - Pose-Guided Human Parsing with Deep Learned Features
A1  - Fangting Xia
A1  - Jun Zhu
A1  - Peng Wang
A1  - Alan Yuille
JO  - ArXiv e-prints
Y1  - 24 November, 2015
UR  - https://arxiv.org/abs/1508.03881
N2  - Parsing human body into semantic regions is crucial to human-centric analysis. In this paper, we propose a segment-based parsing pipeline that explores human pose information, i.e. the joint location of a human model, which improves the part proposal, accelerates the inference and regularizes the parsing process at the same time. Specifically, we first generate part segment proposals with respect to human joints predicted by a deep model, then part- specific ranking models are trained for segment selection using both pose-based features and deep-learned part potential features. Finally, the best ensemble of the proposed part segments are inferred though an And-Or Graph.
ER  -


TY  - Preprint
T1  - Cost Sensitive Learning of Deep Feature Representations from Imbalanced Data
A1  - Salman H. Khan
A1  - Munawar Hayat
A1  - Mohammed Bennamoun
A1  - Ferdous Sohel
A1  - Roberto Togneri
JO  - ArXiv e-prints
Y1  - 23 March, 2017
UR  - https://arxiv.org/abs/1508.03422
N2  - Class imbalance is a common problem in the case of real-world object detection and classification tasks. Data of some classes is abundant making them an over-represented majority, and data of other classes is scarce, making them an under-represented minority. This imbalance makes it challenging for a classifier to appropriately learn the discriminating boundaries of the majority and minority classes. In this work, we propose a cost sensitive deep neural network which can automatically learn robust feature representations for both the majority and minority classes. During training, our learning procedure jointly optimizes the class dependent costs and the neural network parameters. The proposed approach is applicable to both binary and multi-class problems without any modification. Moreover, as opposed to data level approaches, we do not alter the original data distribution which results in a lower computational cost during the training process. We report the results of our experiments on six major image classification datasets and show that the proposed approach significantly outperforms the baseline algorithms. Comparisons with popular data sampling techniques and cost sensitive classifiers demonstrate the superior performance of our proposed method.
ER  -


TY  - Preprint
T1  - End-to-end Learning of LDA by Mirror-Descent Back Propagation over a Deep Architecture
A1  - Jianshu Chen
A1  - Ji He
A1  - Yelong Shen
A1  - Lin Xiao
A1  - Xiaodong He
A1  - Jianfeng Gao
A1  - Xinying Song
A1  - Li Deng
JO  - ArXiv e-prints
Y1  - 1 November, 2015
UR  - https://arxiv.org/abs/1508.03398
N2  - We develop a fully discriminative learning approach for supervised Latent Dirichlet Allocation (LDA) model using Back Propagation (i.e., BP-sLDA), which maximizes the posterior probability of the prediction variable given the input document. Different from traditional variational learning or Gibbs sampling approaches, the proposed learning method applies (i) the mirror descent algorithm for maximum a posterior inference and (ii) back propagation over a deep architecture together with stochastic gradient/mirror descent for model parameter estimation, leading to scalable and end-to-end discriminative learning of the model. As a byproduct, we also apply this technique to develop a new learning method for the traditional unsupervised LDA model (i.e., BP-LDA). Experimental results on three real-world regression and classification tasks show that the proposed methods significantly outperform the previous supervised topic models, neural networks, and is on par with deep neural networks.
ER  -


TY  - Preprint
T1  - Deep Boosting: Joint Feature Selection and Analysis Dictionary Learning in Hierarchy
A1  - Zhanglin Peng
A1  - Ya Li
A1  - Zhaoquan Cai
A1  - Liang Lin
JO  - ArXiv e-prints
Y1  - 11 August, 2015
UR  - https://arxiv.org/abs/1508.01887
N2  - This work investigates how the traditional image classification pipelines can be extended into a deep architecture, inspired by recent successes of deep neural networks. We propose a deep boosting framework based on layer-by-layer joint feature boosting and dictionary learning. In each layer, we construct a dictionary of filters by combining the filters from the lower layer, and iteratively optimize the image representation with a joint discriminative-generative formulation, i.e. minimization of empirical classification error plus regularization of analysis image generation over training images. For optimization, we perform two iterating steps: i) to minimize the classification error, select the most discriminative features using the gentle adaboost algorithm; ii) according to the feature selection, update the filters to minimize the regularization on analysis image representation using the gradient descent method. Once the optimization is converged, we learn the higher layer representation in the same way. Our model delivers several distinct advantages. First, our layer-wise optimization provides the potential to build very deep architectures. Second, the generated image representation is compact and meaningful. In several visual recognition tasks, our framework outperforms existing state-of-the-art approaches.
ER  -


TY  - Preprint
T1  - Using Deep Learning for Detecting Spoofing Attacks on Speech Signals
A1  - Alan Godoy
A1  - FlÃ¡vio SimÃµes
A1  - JosÃ© Augusto Stuchi
A1  - Marcus de Assis Angeloni
A1  - MÃ¡rio Uliani
A1  - Ricardo Violato
JO  - ArXiv e-prints
Y1  - 19 January, 2016
UR  - https://arxiv.org/abs/1508.01746
N2  - It is well known that speaker verification systems are subject to spoofing attacks. The Automatic Speaker Verification Spoofing and Countermeasures Challenge -- ASVSpoof2015 -- provides a standard spoofing database, containing attacks based on synthetic speech, along with a protocol for experiments. This paper describes CPqD&#39;s systems submitted to the ASVSpoof2015 Challenge, based on deep neural networks, working both as a classifier and as a feature extraction module for a GMM and a SVM classifier. Results show the validity of this approach, achieving less than 0.5\% EER for known attacks.
ER  -


TY  - Preprint
T1  - Applying Deep Learning to Answer Selection: A Study and An Open Task
A1  - Minwei Feng
A1  - Bing Xiang
A1  - Michael R. Glass
A1  - Lidan Wang
A1  - Bowen Zhou
JO  - ArXiv e-prints
Y1  - 2 October, 2015
UR  - https://arxiv.org/abs/1508.01585
N2  - We apply a general deep learning framework to address the non-factoid question answering task. Our approach does not rely on any linguistic tools and can be applied to different languages or domains. Various architectures are presented and compared. We create and release a QA corpus and setup a new QA task in the insurance domain. Experimental results demonstrate superior performance compared to the baseline methods and various technologies give further improvements. For this highly challenging task, the top-1 accuracy can reach up to 65.3% on a test set, which indicates a great potential for practical use.
ER  -


TY  - Preprint
T1  - Learning from LDA using Deep Neural Networks
A1  - Dongxu Zhang
A1  - Tianyi Luo
A1  - Dong Wang
A1  - Rong Liu
JO  - ArXiv e-prints
Y1  - 5 August, 2015
UR  - https://arxiv.org/abs/1508.01011
N2  - Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian model for topic inference. In spite of its great success, inferring the latent topic distribution with LDA is time-consuming. Motivated by the transfer learning approach proposed by~\newcite{hinton2015distilling}, we present a novel method that uses LDA to supervise the training of a deep neural network (DNN), so that the DNN can approximate the costly LDA inference with less computation. Our experiments on a document classification task show that a simple DNN can learn the LDA behavior pretty well, while the inference is speeded up tens or hundreds of times.
ER  -


TY  - Preprint
T1  - On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units
A1  - Zhibin Liao
A1  - Gustavo Carneiro
JO  - ArXiv e-prints
Y1  - 1 November, 2015
UR  - https://arxiv.org/abs/1508.00330
N2  - Deep feedforward neural networks with piecewise linear activations are currently producing the state-of-the-art results in several public datasets. The combination of deep learning models and piecewise linear activation functions allows for the estimation of exponentially complex functions with the use of a large number of subnetworks specialized in the classification of similar input examples. During the training process, these subnetworks avoid overfitting with an implicit regularization scheme based on the fact that they must share their parameters with other subnetworks. Using this framework, we have made an empirical observation that can improve even more the performance of such models. We notice that these models assume a balanced initial distribution of data points with respect to the domain of the piecewise linear activation function. If that assumption is violated, then the piecewise linear activation units can degenerate into purely linear activation units, which can result in a significant reduction of their capacity to learn complex functions. Furthermore, as the number of model layers increases, this unbalanced initial distribution makes the model ill-conditioned. Therefore, we propose the introduction of batch normalisation units into deep feedforward neural networks with piecewise linear activations, which drives a more balanced use of these activation units, where each region of the activation function is trained with a relatively large proportion of training samples. Also, this batch normalisation promotes the pre-conditioning of very deep learning models. We show that by introducing maxout and batch normalisation units to the network in network model results in a model that produces classification results that are better than or comparable to the current state of the art in CIFAR-10, CIFAR-100, MNIST, and SVHN datasets.
ER  -


TY  - Preprint
T1  - Deep Learning for Single-View Instance Recognition
A1  - David Held
A1  - Sebastian Thrun
A1  - Silvio Savarese
JO  - ArXiv e-prints
Y1  - 29 July, 2015
UR  - https://arxiv.org/abs/1507.08286
N2  - Deep learning methods have typically been trained on large datasets in which many training examples are available. However, many real-world product datasets have only a small number of images available for each product. We explore the use of deep learning methods for recognizing object instances when we have only a single training example per class. We show that feedforward neural networks outperform state-of-the-art methods for recognizing objects from novel viewpoints even when trained from just a single image per object. To further improve our performance on this task, we propose to take advantage of a supplementary dataset in which we observe a separate set of objects from multiple viewpoints. We introduce a new approach for training deep learning methods for instance recognition with limited training data, in which we use an auxiliary multi-view dataset to train our network to be robust to viewpoint changes. We find that this approach leads to a more robust classifier for recognizing objects from novel viewpoints, outperforming previous state-of-the-art approaches including keypoint-matching, template-based techniques, and sparse coding.
ER  -


TY  - Preprint
T1  - Multimodal Deep Learning for Robust RGB-D Object Recognition
A1  - Andreas Eitel
A1  - Jost Tobias Springenberg
A1  - Luciano Spinello
A1  - Martin Riedmiller
A1  - Wolfram Burgard
JO  - ArXiv e-prints
Y1  - 18 August, 2015
UR  - https://arxiv.org/abs/1507.06821
N2  - Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset and show recognition in challenging RGB-D real-world noisy settings.
ER  -


TY  - Preprint
T1  - Deep Recurrent Q-Learning for Partially Observable MDPs
A1  - Matthew Hausknecht
A1  - Peter Stone
JO  - ArXiv e-prints
Y1  - 11 January, 2017
UR  - https://arxiv.org/abs/1507.06527
N2  - Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting \textit{Deep Recurrent Q-Network} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN&#39;s performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN&#39;s performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN&#39;s performance degrades less than DQN&#39;s. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN&#39;s input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.
ER  -


TY  - Preprint
T1  - Learning Complexity-Aware Cascades for Deep Pedestrian Detection
A1  - Zhaowei Cai
A1  - Mohammad Saberian
A1  - Nuno Vasconcelos
JO  - ArXiv e-prints
Y1  - 19 July, 2015
UR  - https://arxiv.org/abs/1507.05348
N2  - The design of complexity-aware cascaded detectors, combining features of very different complexities, is considered. A new cascade design procedure is introduced, by formulating cascade learning as the Lagrangian optimization of a risk that accounts for both accuracy and complexity. A boosting algorithm, denoted as complexity aware cascade training (CompACT), is then derived to solve this optimization. CompACT cascades are shown to seek an optimal trade-off between accuracy and complexity by pushing features of higher complexity to the later cascade stages, where only a few difficult candidate patches remain to be classified. This enables the use of features of vastly different complexities in a single detector. In result, the feature pool can be expanded to features previously impractical for cascade design, such as the responses of a deep convolutional neural network (CNN). This is demonstrated through the design of a pedestrian detector with a pool of features whose complexities span orders of magnitude. The resulting cascade generalizes the combination of a CNN with an object proposal mechanism: rather than a pre-processing stage, CompACT cascades seamlessly integrate CNNs in their stages. This enables state of the art performance on the Caltech and KITTI datasets, at fairly fast speeds.
ER  -


TY  - Preprint
T1  - Maximum Entropy Deep Inverse Reinforcement Learning
A1  - Markus Wulfmeier
A1  - Peter Ondruska
A1  - Ingmar Posner
JO  - ArXiv e-prints
Y1  - 11 March, 2016
UR  - https://arxiv.org/abs/1507.04888
N2  - This paper presents a general framework for exploiting the representational capacity of neural networks to approximate complex, nonlinear reward functions in the context of solving the inverse reinforcement learning (IRL) problem. We show in this context that the Maximum Entropy paradigm for IRL lends itself naturally to the efficient training of deep architectures. At test time, the approach leads to a computational complexity independent of the number of demonstrations, which makes it especially well-suited for applications in life-long learning scenarios. Our approach achieves performance commensurate to the state-of-the-art on existing benchmarks while exceeding on an alternative benchmark based on highly varying reward structures. Finally, we extend the basic architecture - which is equivalent to a simplified subclass of Fully Convolutional Neural Networks (FCNNs) with width one - to include larger convolutions in order to eliminate dependency on precomputed spatial features and work on raw input representations.
ER  -


TY  - Preprint
T1  - Learning Robust Deep Face Representation
A1  - Xiang Wu
JO  - ArXiv e-prints
Y1  - 17 July, 2015
UR  - https://arxiv.org/abs/1507.04844
N2  - With the development of convolution neural network, more and more researchers focus their attention on the advantage of CNN for face recognition task. In this paper, we propose a deep convolution network for learning a robust face representation. The deep convolution net is constructed by 4 convolution layers, 4 max pooling layers and 2 fully connected layers, which totally contains about 4M parameters. The Max-Feature-Map activation function is used instead of ReLU because the ReLU might lead to the loss of information due to the sparsity while the Max-Feature-Map can get the compact and discriminative feature vectors. The model is trained on CASIA-WebFace dataset and evaluated on LFW dataset. The result on LFW achieves 97.77% on unsupervised setting for single net.
ER  -


TY  - Preprint
T1  - Deep Learning and Music Adversaries
A1  - Corey Kereliuk
A1  - Bob L. Sturm
A1  - Jan Larsen
JO  - ArXiv e-prints
Y1  - 16 July, 2015
UR  - https://arxiv.org/abs/1507.04761
N2  - An adversary is essentially an algorithm intent on making a classification system perform in some particular way given an input, e.g., increase the probability of a false negative. Recent work builds adversaries for deep learning systems applied to image object recognition, which exploits the parameters of the system to find the minimal perturbation of the input image such that the network misclassifies it with high confidence. We adapt this approach to construct and deploy an adversary of deep learning systems applied to music content analysis. In our case, however, the input to the systems is magnitude spectral frames, which requires special care in order to produce valid input audio signals from network-derived perturbations. For two different train-test partitionings of two benchmark datasets, and two different deep architectures, we find that this adversary is very effective in defeating the resulting systems. We find the convolutional networks are more robust, however, compared with systems based on a majority vote over individually classified audio frames. Furthermore, we integrate the adversary into the training of new deep systems, but do not find that this improves their resilience against the same adversary.
ER  -


TY  - Preprint
T1  - A Deep Hashing Learning Network
A1  - Guoqiang Zhong
A1  - Pan Yang
A1  - Sijiang Wang
A1  - Junyu Dong
JO  - ArXiv e-prints
Y1  - 15 July, 2015
UR  - https://arxiv.org/abs/1507.04437
N2  - Hashing-based methods seek compact and efficient binary codes that preserve the neighborhood structure in the original data space. For most existing hashing methods, an image is first encoded as a vector of hand-crafted visual feature, followed by a hash projection and quantization step to get the compact binary vector. Most of the hand-crafted features just encode the low-level information of the input, the feature may not preserve the semantic similarities of images pairs. Meanwhile, the hashing function learning process is independent with the feature representation, so the feature may not be optimal for the hashing projection. In this paper, we propose a supervised hashing method based on a well designed deep convolutional neural network, which tries to learn hashing code and compact representations of data simultaneously. The proposed model learn the binary codes by adding a compact sigmoid layer before the loss layer. Experiments on several image data sets show that the proposed model outperforms other state-of-the-art methods.
ER  -


TY  - Preprint
T1  - Massively Parallel Methods for Deep Reinforcement Learning
A1  - Arun Nair
A1  - Praveen Srinivasan
A1  - Sam Blackwell
A1  - Cagdas Alcicek
A1  - Rory Fearon
A1  - Alessandro De Maria
A1  - Vedavyas Panneershelvam
A1  - Mustafa Suleyman
A1  - Charles Beattie
A1  - Stig Petersen
A1  - Shane Legg
A1  - Volodymyr Mnih
A1  - Koray Kavukcuoglu
A1  - David Silver
JO  - ArXiv e-prints
Y1  - 16 July, 2015
UR  - https://arxiv.org/abs/1507.04296
N2  - We present the first massively distributed architecture for deep reinforcement learning. This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience. We used our architecture to implement the Deep Q-Network algorithm (DQN). Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters. Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.
ER  -


TY  - Preprint
T1  - Achieving Synergy in Cognitive Behavior of Humanoids via Deep Learning of Dynamic Visuo-Motor-Attentional Coordination
A1  - Jungsik Hwang
A1  - Minju Jung
A1  - Naveen Madapana
A1  - Jinhyung Kim
A1  - Minkyu Choi
A1  - Jun Tani
JO  - ArXiv e-prints
Y1  - 8 July, 2015
UR  - https://arxiv.org/abs/1507.02347
N2  - The current study examines how adequate coordination among different cognitive processes including visual recognition, attention switching, action preparation and generation can be developed via learning of robots by introducing a novel model, the Visuo-Motor Deep Dynamic Neural Network (VMDNN). The proposed model is built on coupling of a dynamic vision network, a motor generation network, and a higher level network allocated on top of these two. The simulation experiments using the iCub simulator were conducted for cognitive tasks including visual object manipulation responding to human gestures. The results showed that synergetic coordination can be developed via iterative learning through the whole network when spatio-temporal hierarchy and temporal one can be self-organized in the visual pathway and in the motor pathway, respectively, such that the higher level can manipulate them with abstraction.
ER  -


TY  - Preprint
T1  - Learning Deep Neural Network Policies with Continuous Memory States
A1  - Marvin Zhang
A1  - Zoe McCarthy
A1  - Chelsea Finn
A1  - Sergey Levine
A1  - Pieter Abbeel
JO  - ArXiv e-prints
Y1  - 23 September, 2015
UR  - https://arxiv.org/abs/1507.01273
N2  - Policy learning for partially observed control tasks requires policies that can remember salient information from past observations. In this paper, we present a method for learning policies with internal memory for high-dimensional, continuous systems, such as robotic manipulators. Our approach consists of augmenting the state and action space of the system with continuous-valued memory states that the policy can read from and write to. Learning general-purpose policies with this type of memory representation directly is difficult, because the policy must automatically figure out the most salient information to memorize at each time step. We show that, by decomposing this policy search problem into a trajectory optimization phase and a supervised learning phase through a method called guided policy search, we can acquire policies with effective memorization and recall strategies. Intuitively, the trajectory optimization phase chooses the values of the memory states that will make it easier for the policy to produce the right action in future states, while the supervised learning phase encourages the policy to use memorization actions to produce those memory states. We evaluate our method on tasks involving continuous control in manipulation and navigation settings, and show that our method can learn complex policies that successfully complete a range of tasks that require memory.
ER  -


TY  - Preprint
T1  - Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models
A1  - Bradly C. Stadie
A1  - Sergey Levine
A1  - Pieter Abbeel
JO  - ArXiv e-prints
Y1  - 19 November, 2015
UR  - https://arxiv.org/abs/1507.00814
N2  - Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.
ER  -


TY  - Preprint
T1  - Pose Embeddings: A Deep Architecture for Learning to Match Human Poses
A1  - Greg Mori
A1  - Caroline Pantofaru
A1  - Nisarg Kothari
A1  - Thomas Leung
A1  - George Toderici
A1  - Alexander Toshev
A1  - Weilong Yang
JO  - ArXiv e-prints
Y1  - 1 July, 2015
UR  - https://arxiv.org/abs/1507.00302
N2  - We present a method for learning an embedding that places images of humans in similar poses nearby. This embedding can be used as a direct method of comparing images based on human pose, avoiding potential challenges of estimating body joint positions. Pose embedding learning is formulated under a triplet-based distance criterion. A deep architecture is used to allow learning of a representation capable of making distinctions between different poses. Experiments on human pose matching and retrieval from video data demonstrate the potential of the method.
ER  -


TY  - Preprint
T1  - Supervised Learning of Semantics-Preserving Hash via Deep Convolutional Neural Networks
A1  - Huei-Fang Yang
A1  - Kevin Lin
A1  - Chu-Song Chen
JO  - ArXiv e-prints
Y1  - 14 February, 2017
UR  - https://arxiv.org/abs/1507.00101
N2  - This paper presents a simple yet effective supervised deep hash approach that constructs binary hash codes from labeled data for large-scale image search. We assume that the semantic labels are governed by several latent attributes with each attribute on or off, and classification relies on these attributes. Based on this assumption, our approach, dubbed supervised semantics-preserving deep hashing (SSDH), constructs hash functions as a latent layer in a deep network and the binary codes are learned by minimizing an objective function defined over classification error and other desirable hash codes properties. With this design, SSDH has a nice characteristic that classification and retrieval are unified in a single learning model. Moreover, SSDH performs joint learning of image representations, hash codes, and classification in a point-wised manner, and thus is scalable to large-scale datasets. SSDH is simple and can be realized by a slight enhancement of an existing deep architecture for classification; yet it is effective and outperforms other hashing approaches on several benchmarks and large datasets. Compared with state-of-the-art approaches, SSDH achieves higher retrieval accuracy, while the classification performance is not sacrificed.
ER  -


TY  - Preprint
T1  - The Potential of the Intel Xeon Phi for Supervised Deep Learning
A1  - Andre Viebke
A1  - Sabri Pllana
JO  - ArXiv e-prints
Y1  - 30 June, 2015
UR  - https://arxiv.org/abs/1506.09067
N2  - Supervised learning of Convolutional Neural Networks (CNNs), also known as supervised Deep Learning, is a computationally demanding process. To find the most suitable parameters of a network for a given application, numerous training sessions are required. Therefore, reducing the training time per session is essential to fully utilize CNNs in practice. While numerous research groups have addressed the training of CNNs using GPUs, so far not much attention has been paid to the Intel Xeon Phi coprocessor. In this paper we investigate empirically and theoretically the potential of the Intel Xeon Phi for supervised learning of CNNs. We design and implement a parallelization scheme named CHAOS that exploits both the thread- and SIMD-parallelism of the coprocessor. Our approach is evaluated on the Intel Xeon Phi 7120P using the MNIST dataset of handwritten digits for various thread counts and CNN architectures. Results show a 103.5x speed up when training our large network for 15 epochs using 244 threads, compared to one thread on the coprocessor. Moreover, we develop a performance model and use it to assess our implementation and answer what-if questions.
ER  -


TY  - Preprint
T1  - Language Understanding for Text-based Games Using Deep Reinforcement Learning
A1  - Karthik Narasimhan
A1  - Tejas Kulkarni
A1  - Regina Barzilay
JO  - ArXiv e-prints
Y1  - 11 September, 2015
UR  - https://arxiv.org/abs/1506.08941
N2  - In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations.
ER  -


TY  - Preprint
T1  - Improved Deep Speaker Feature Learning for Text-Dependent Speaker Recognition
A1  - Lantian Li
A1  - Yiye Lin
A1  - Zhiyong Zhang
A1  - Dong Wang
JO  - ArXiv e-prints
Y1  - 27 June, 2015
UR  - https://arxiv.org/abs/1506.08349
N2  - A deep learning approach has been proposed recently to derive speaker identifies (d-vector) by a deep neural network (DNN). This approach has been applied to text-dependent speaker recognition tasks and shows reasonable performance gains when combined with the conventional i-vector approach. Although promising, the existing d-vector implementation still can not compete with the i-vector baseline. This paper presents two improvements for the deep learning approach: a phonedependent DNN structure to normalize phone variation, and a new scoring approach based on dynamic time warping (DTW). Experiments on a text-dependent speaker recognition task demonstrated that the proposed methods can provide considerable performance improvement over the existing d-vector implementation.
ER  -


TY  - Preprint
T1  - Learning Representations from Deep Networks Using Mode Synthesizers
A1  - N. E. Osegi
A1  - P. Enyindah
JO  - ArXiv e-prints
Y1  - 24 June, 2015
UR  - https://arxiv.org/abs/1506.07545
N2  - Deep learning Networks play a crucial role in the evolution of a vast number of current machine learning models for solving a variety of real world non-trivial tasks. Such networks use big data which is generally unlabeled unsupervised and multi-layered requiring no form of supervision for training and learning data and has been used to successfully build automatic supervisory neural networks. However the question still remains how well the learned data represents interestingness, and their effectiveness i.e. efficiency in deep learning models or applications. If the output of a network of deep learning models can be beamed unto a scene of observables, we could learn the variational frequencies of these stacked networks in a parallel and distributive way.This paper seeks to discover and represent interesting patterns in an efficient and less complex way by incorporating the concept of Mode synthesizers in the deep learning process models
ER  -


TY  - Preprint
T1  - Global Optimality in Tensor Factorization, Deep Learning, and Beyond
A1  - Benjamin D. Haeffele
A1  - Rene Vidal
JO  - ArXiv e-prints
Y1  - 24 June, 2015
UR  - https://arxiv.org/abs/1506.07540
N2  - Techniques involving factorization are found in a wide range of applications and have enjoyed significant empirical success in many fields. However, common to a vast majority of these problems is the significant disadvantage that the associated optimization problems are typically non-convex due to a multilinear form or other convexity destroying transformation. Here we build on ideas from convex relaxations of matrix factorizations and present a very general framework which allows for the analysis of a wide range of non-convex factorization problems - including matrix factorization, tensor factorization, and deep neural network training formulations. We derive sufficient conditions to guarantee that a local minimum of the non-convex optimization problem is a global minimum and show that if the size of the factorized variables is large enough then from any initialization it is possible to find a global minimizer using a purely local descent algorithm. Our framework also provides a partial theoretical justification for the increasingly common use of Rectified Linear Units (ReLUs) in deep neural networks and offers guidance on deep network architectures and regularization strategies to facilitate efficient optimization.
ER  -


TY  - Preprint
T1  - A Deep Memory-based Architecture for Sequence-to-Sequence Learning
A1  - Fandong Meng
A1  - Zhengdong Lu
A1  - Zhaopeng Tu
A1  - Hang Li
A1  - Qun Liu
JO  - ArXiv e-prints
Y1  - 7 January, 2016
UR  - https://arxiv.org/abs/1506.06442
N2  - We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequence learning, which performs the task through a series of nonlinear transformations from the representation of the input sequence (e.g., a Chinese sentence) to the final output sequence (e.g., translation to English). Inspired by the recently proposed Neural Turing Machine (Graves et al., 2014), we store the intermediate representations in stacked layers of memories, and use read-write operations on the memories to realize the nonlinear transformations between the representations. The types of transformations are designed in advance but the parameters are learned from data. Through layer-by-layer transformations, DEEPMEMORY can model complicated relations between sequences necessary for applications such as machine translation between distant languages. The architecture can be trained with normal back-propagation on sequenceto-sequence data, and the learning can be easily scaled up to a large corpus. DEEPMEMORY is broad enough to subsume the state-of-the-art neural translation model in (Bahdanau et al., 2015) as its special case, while significantly improving upon the model with its deeper architecture. Remarkably, DEEPMEMORY, being purely neural network-based, can achieve performance comparable to the traditional phrase-based machine translation system Moses with a small vocabulary and a modest parameter size.
ER  -


TY  - Preprint
T1  - Learning Deep Generative Models with Doubly Stochastic MCMC
A1  - Chao Du
A1  - Jun Zhu
A1  - Bo Zhang
JO  - ArXiv e-prints
Y1  - 7 March, 2016
UR  - https://arxiv.org/abs/1506.04557
N2  - We present doubly stochastic gradient MCMC, a simple and generic method for (approximate) Bayesian inference of deep generative models (DGMs) in a collapsed continuous parameter space. At each MCMC sampling step, the algorithm randomly draws a mini-batch of data samples to estimate the gradient of log-posterior and further estimates the intractable expectation over hidden variables via a neural adaptive importance sampler, where the proposal distribution is parameterized by a deep neural network and learnt jointly. We demonstrate the effectiveness on learning various DGMs in a wide range of tasks, including density estimation, data generation and missing data imputation. Our method outperforms many state-of-the-art competitors.
ER  -


TY  - Preprint
T1  - Dual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy
A1  - Sang-Woo Lee
A1  - Min-Oh Heo
A1  - Jiwon Kim
A1  - Jeonghee Kim
A1  - Byoung-Tak Zhang
JO  - ArXiv e-prints
Y1  - 15 June, 2015
UR  - https://arxiv.org/abs/1506.04477
N2  - The online learning of deep neural networks is an interesting problem of machine learning because, for example, major IT companies want to manage the information of the massive data uploaded on the web daily, and this technology can contribute to the next generation of lifelong learning. We aim to train deep models from new data that consists of new classes, distributions, and tasks at minimal computational cost, which we call online deep learning. Unfortunately, deep neural network learning through classical online and incremental methods does not work well in both theory and practice. In this paper, we introduce dual memory architectures for online incremental deep learning. The proposed architecture consists of deep representation learners and fast learnable shallow kernel networks, both of which synergize to track the information of new data. During the training phase, we use various online, incremental ensemble, and transfer learning techniques in order to achieve lower error of the architecture. On the MNIST, CIFAR-10, and ImageNet image recognition tasks, the proposed dual memory architectures performs much better than the classical online and incremental ensemble algorithm, and their accuracies are similar to that of the batch learner.
ER  -


TY  - Preprint
T1  - LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop
A1  - Fisher Yu
A1  - Ari Seff
A1  - Yinda Zhang
A1  - Shuran Song
A1  - Thomas Funkhouser
A1  - Jianxiong Xiao
JO  - ArXiv e-prints
Y1  - 4 June, 2016
UR  - https://arxiv.org/abs/1506.03365
N2  - While there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classification confidence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and find that they achieve substantial performance gains when trained on this dataset.
ER  -


TY  - Preprint
T1  - Learning to Select Pre-Trained Deep Representations with Bayesian Evidence Framework
A1  - Yong-Deok Kim
A1  - Taewoong Jang
A1  - Bohyung Han
A1  - Seungjin Choi
JO  - ArXiv e-prints
Y1  - 24 April, 2016
UR  - https://arxiv.org/abs/1506.02565
N2  - We propose a Bayesian evidence framework to facilitate transfer learning from pre-trained deep convolutional neural networks (CNNs). Our framework is formulated on top of a least squares SVM (LS-SVM) classifier, which is simple and fast in both training and testing, and achieves competitive performance in practice. The regularization parameters in LS-SVM is estimated automatically without grid search and cross-validation by maximizing evidence, which is a useful measure to select the best performing CNN out of multiple candidates for transfer learning; the evidence is optimized efficiently by employing Aitken&#39;s delta-squared process, which accelerates convergence of fixed point update. The proposed Bayesian evidence framework also provides a good solution to identify the best ensemble of heterogeneous CNNs through a greedy algorithm. Our Bayesian evidence framework for transfer learning is tested on 12 visual recognition datasets and illustrates the state-of-the-art performance consistently in terms of prediction accuracy and modeling efficiency.
ER  -


TY  - Preprint
T1  - Blocks and Fuel: Frameworks for deep learning
A1  - Bart van MerriÃ«nboer
A1  - Dzmitry Bahdanau
A1  - Vincent Dumoulin
A1  - Dmitriy Serdyuk
A1  - David Warde-Farley
A1  - Jan Chorowski
A1  - Yoshua Bengio
JO  - ArXiv e-prints
Y1  - 1 June, 2015
UR  - https://arxiv.org/abs/1506.00619
N2  - We introduce two Python frameworks to train neural networks on large datasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler with CUDA-support. It facilitates the training of complex neural network models by providing parametrized Theano operations, attaching metadata to Theano&#39;s symbolic computational graph, and providing an extensive set of utilities to assist training the networks, e.g. training algorithms, logging, monitoring, visualization, and serialization. Fuel provides a standard format for machine learning datasets. It allows the user to easily iterate over large datasets, performing many types of pre-processing on the fly.
ER  -


TY  - Preprint
T1  - Learning to count with deep object features
A1  - Santi SeguÃ­
A1  - Oriol Pujol
A1  - Jordi VitriÃ 
JO  - ArXiv e-prints
Y1  - 29 May, 2015
UR  - https://arxiv.org/abs/1505.08082
N2  - Learning to count is a learning strategy that has been recently proposed in the literature for dealing with problems where estimating the number of object instances in a scene is the final objective. In this framework, the task of learning to detect and localize individual object instances is seen as a harder task that can be evaded by casting the problem as that of computing a regression value from hand-crafted image features. In this paper we explore the features that are learned when training a counting convolutional neural network in order to understand their underlying representation. To this end we define a counting problem for MNIST data and show that the internal representation of the network is able to classify digits in spite of the fact that no direct supervision was provided for them during training. We also present preliminary results about a deep network that is able to count the number of pedestrians in a scene.
ER  -


TY  - Preprint
T1  - Deep Ranking for Person Re-identification via Joint Representation Learning
A1  - Shi-Zhe Chen
A1  - Chun-Chao Guo
A1  - Jian-Huang Lai
JO  - ArXiv e-prints
Y1  - 22 March, 2016
UR  - https://arxiv.org/abs/1505.06821
N2  - This paper proposes a novel approach to person re-identification, a fundamental task in distributed multi-camera surveillance systems. Although a variety of powerful algorithms have been presented in the past few years, most of them usually focus on designing hand-crafted features and learning metrics either individually or sequentially. Different from previous works, we formulate a unified deep ranking framework that jointly tackles both of these key components to maximize their strengths. We start from the principle that the correct match of the probe image should be positioned in the top rank within the whole gallery set. An effective learning-to-rank algorithm is proposed to minimize the cost corresponding to the ranking disorders of the gallery. The ranking model is solved with a deep convolutional neural network (CNN) that builds the relation between input image pairs and their similarity scores through joint representation learning directly from raw image pixels. The proposed framework allows us to get rid of feature engineering and does not rely on any assumption. An extensive comparative evaluation is given, demonstrating that our approach significantly outperforms all state-of-the-art approaches, including both traditional and CNN-based methods on the challenging VIPeR, CUHK-01 and CAVIAR4REID datasets. Additionally, our approach has better ability to generalize across datasets without fine-tuning.
ER  -


TY  - Preprint
T1  - Boosting-like Deep Learning For Pedestrian Detection
A1  - Lei Wang
A1  - Baochang Zhang
JO  - ArXiv e-prints
Y1  - 25 May, 2015
UR  - https://arxiv.org/abs/1505.06800
N2  - This paper proposes boosting-like deep learning (BDL) framework for pedestrian detection. Due to overtraining on the limited training samples, overfitting is a major problem of deep learning. We incorporate a boosting-like technique into deep learning to weigh the training samples, and thus prevent overtraining in the iterative process. We theoretically give the details of derivation of our algorithm, and report the experimental results on open data sets showing that BDL achieves a better stable performance than the state-of-the-arts. Our approach achieves 15.85% and 3.81% reduction in the average miss rate compared with ACF and JointDeep on the largest Caltech benchmark dataset, respectively.
ER  -


TY  - Preprint
T1  - Instant Learning: Parallel Deep Neural Networks and Convolutional Bootstrapping
A1  - Andrew J. R. Simpson
JO  - ArXiv e-prints
Y1  - 21 May, 2016
UR  - https://arxiv.org/abs/1505.05972
N2  - Although deep neural networks (DNN) are able to scale with direct advances in computational power (e.g., memory and processing speed), they are not well suited to exploit the recent trends for parallel architectures. In particular, gradient descent is a sequential process and the resulting serial dependencies mean that DNN training cannot be parallelized effectively. Here, we show that a DNN may be replicated over a massive parallel architecture and used to provide a cumulative sampling of local solution space which results in rapid and robust learning. We introduce a complimentary convolutional bootstrapping approach that enhances performance of the parallel architecture further. Our parallelized convolutional bootstrapping DNN out-performs an identical fully-trained traditional DNN after only a single iteration of training.
ER  -


TY  - Preprint
T1  - Deep Learning for Semantic Part Segmentation with High-Level Guidance
A1  - S. Tsogkas
A1  - I. Kokkinos
A1  - G. Papandreou
A1  - A. Vedaldi
JO  - ArXiv e-prints
Y1  - 24 November, 2015
UR  - https://arxiv.org/abs/1505.02438
N2  - In this work we address the task of segmenting an object into its parts, or semantic part segmentation. We start by adapting a state-of-the-art semantic segmentation system to this task, and show that a combination of a fully-convolutional Deep CNN system coupled with Dense CRF labelling provides excellent results for a broad range of object categories. Still, this approach remains agnostic to high-level constraints between object parts. We introduce such prior information by means of the Restricted Boltzmann Machine, adapted to our task and train our model in an discriminative fashion, as a hidden CRF, demonstrating that prior information can yield additional improvements. We also investigate the performance of our approach ``in the wild&#39;&#39;, without information concerning the objects&#39; bounding boxes, using an object detector to guide a multi-scale segmentation scheme. We evaluate the performance of our approach on the Penn-Fudan and LFW datasets for the tasks of pedestrian parsing and face labelling respectively. We show superior performance with respect to competitive methods that have been extensively engineered on these benchmarks, as well as realistic qualitative results on part segmentation, even for occluded or deformable objects. We also provide quantitative and extensive qualitative results on three classes from the PASCAL Parts dataset. Finally, we show that our multi-scale segmentation scheme can boost accuracy, recovering segmentations for finer parts.
ER  -


TY  - Preprint
T1  - Deep Learning for Medical Image Segmentation
A1  - Matthew Lai
JO  - ArXiv e-prints
Y1  - 8 May, 2015
UR  - https://arxiv.org/abs/1505.02000
N2  - This report provides an overview of the current state of the art deep learning architectures and optimisation techniques, and uses the ADNI hippocampus MRI dataset as an example to compare the effectiveness and efficiency of different convolutional architectures on the task of patch-based 3-dimensional hippocampal segmentation, which is important in the diagnosis of Alzheimer&#39;s Disease. We found that a slightly unconventional &#34;stacked 2D&#34; approach provides much better classification performance than simple 2D patches without requiring significantly more computational power. We also examined the popular &#34;tri-planar&#34; approach used in some recently published studies, and found that it provides much better results than the 2D approaches, but also with a moderate increase in computational power requirement. Finally, we evaluated a full 3D convolutional architecture, and found that it provides marginally better results than the tri-planar approach, but at the cost of a very significant increase in computational power requirement.
ER  -


TY  - Preprint
T1  - Deep Learning for Object Saliency Detection and Image Segmentation
A1  - Hengyue Pan
A1  - Bo Wang
A1  - Hui Jiang
JO  - ArXiv e-prints
Y1  - 5 May, 2015
UR  - https://arxiv.org/abs/1505.01173
N2  - In this paper, we propose several novel deep learning methods for object saliency detection based on the powerful convolutional neural networks. In our approach, we use a gradient descent method to iteratively modify an input image based on the pixel-wise gradients to reduce a cost function measuring the class-specific objectness of the image. The pixel-wise gradients can be efficiently computed using the back-propagation algorithm. The discrepancy between the modified image and the original one may be used as a saliency map for the image. Moreover, we have further proposed several new training methods to learn saliency-specific convolutional nets for object saliency detection, in order to leverage the available pixel-wise segmentation information. Our methods are extremely computationally efficient (processing 20-40 images per second in one GPU). In this work, we use the computed saliency maps for image segmentation. Experimental results on two benchmark tasks, namely Microsoft COCO and Pascal VOC 2012, have shown that our proposed methods can generate high-quality salience maps, clearly outperforming many existing methods. In particular, our approaches excel in handling many difficult images, which contain complex background, highly-variable salient objects, multiple objects, and/or very small salient objects.
ER  -


TY  - Preprint
T1  - Modeling Representation of Videos for Anomaly Detection using Deep Learning: A Review
A1  - Yong Shean Chong
A1  - Yong Haur Tay
JO  - ArXiv e-prints
Y1  - 4 May, 2015
UR  - https://arxiv.org/abs/1505.00523
N2  - This review article surveys the current progresses made toward video-based anomaly detection. We address the most fundamental aspect for video anomaly detection, that is, video feature representation. Much research works have been done in finding the right representation to perform anomaly detection in video streams accurately with an acceptable false alarm rate. However, this is very challenging due to large variations in environment and human movement, and high space-time complexity due to huge dimensionality of video data. The weakly supervised nature of deep learning algorithms can help in learning representations from the video data itself instead of manually designing the right feature for specific scenes. In this paper, we would like to review the existing methods of modeling video representations using deep learning techniques for the task of anomaly detection and action recognition.
ER  -


TY  - Preprint
T1  - Making Sense of Hidden Layer Information in Deep Networks by Learning Hierarchical Targets
A1  - Abhinav Tushar
JO  - ArXiv e-prints
Y1  - 24 September, 2016
UR  - https://arxiv.org/abs/1505.00384
N2  - This paper proposes an architecture for deep neural networks with hidden layer branches that learn targets of lower hierarchy than final layer targets. The branches provide a channel for enforcing useful information in hidden layer which helps in attaining better accuracy, both for the final layer and hidden layers. The shared layers modify their weights using the gradients of all cost functions higher than the branching layer. This model provides a flexible inference system with many levels of targets which is modular and can be used efficiently in situations requiring different levels of results according to complexity. This paper applies the idea to a text classification task on 20 Newsgroups data set with two level of hierarchical targets and a comparison is made with training without the use of hidden layer branches.
ER  -


TY  - Preprint
T1  - Can deep learning help you find the perfect match?
A1  - Harm de Vries
A1  - Jason Yosinski
JO  - ArXiv e-prints
Y1  - 20 June, 2015
UR  - https://arxiv.org/abs/1505.00359
N2  - Is he/she my type or not? The answer to this question depends on the personal preferences of the one asking it. The individual process of obtaining a full answer may generally be difficult and time consuming, but often an approximate answer can be obtained simply by looking at a photo of the potential match. Such approximate answers based on visual cues can be produced in a fraction of a second, a phenomenon that has led to a series of recently successful dating apps in which users rate others positively or negatively using primarily a single photo. In this paper we explore using convolutional networks to create a model of an individual&#39;s personal preferences based on rated photos. This introduced task is difficult due to the large number of variations in profile pictures and the noise in attractiveness labels. Toward this task we collect a dataset comprised of $9364$ pictures and binary labels for each. We compare performance of convolutional models trained in three ways: first directly on the collected dataset, second with features transferred from a network trained to predict gender, and third with features transferred from a network trained on ImageNet. Our findings show that ImageNet features transfer best, producing a model that attains $68.1\%$ accuracy on the test set and is moderately successful at predicting matches.
ER  -


TY  - Preprint
T1  - Multi-Object Classification and Unsupervised Scene Understanding Using Deep Learning Features and Latent Tree Probabilistic Models
A1  - Tejaswi Nimmagadda
A1  - Anima Anandkumar
JO  - ArXiv e-prints
Y1  - 1 May, 2015
UR  - https://arxiv.org/abs/1505.00308
N2  - Deep learning has shown state-of-art classification performance on datasets such as ImageNet, which contain a single object in each image. However, multi-object classification is far more challenging. We present a unified framework which leverages the strengths of multiple machine learning methods, viz deep learning, probabilistic models and kernel methods to obtain state-of-art performance on Microsoft COCO, consisting of non-iconic images. We incorporate contextual information in natural images through a conditional latent tree probabilistic model (CLTM), where the object co-occurrences are conditioned on the extracted fc7 features from pre-trained Imagenet CNN as input. We learn the CLTM tree structure using conditional pairwise probabilities for object co-occurrences, estimated through kernel methods, and we learn its node and edge potentials by training a new 3-layer neural network, which takes fc7 features as input. Object classification is carried out via inference on the learnt conditional tree model, and we obtain significant gain in precision-recall and F-measures on MS-COCO, especially for difficult object categories. Moreover, the latent variables in the CLTM capture scene information: the images with top activations for a latent node have common themes such as being a grasslands or a food scene, and on on. In addition, we show that a simple k-means clustering of the inferred latent nodes alone significantly improves scene classification performance on the MIT-Indoor dataset, without the need for any retraining, and without using scene labels during training. Thus, we present a unified framework for multi-object classification and unsupervised scene understanding.
ER  -


TY  - Preprint
T1  - Joint Object and Part Segmentation using Deep Learned Potentials
A1  - Peng Wang
A1  - Xiaohui Shen
A1  - Zhe Lin
A1  - Scott Cohen
A1  - Brian Price
A1  - Alan Yuille
JO  - ArXiv e-prints
Y1  - 1 May, 2015
UR  - https://arxiv.org/abs/1505.00276
N2  - Segmenting semantic objects from images and parsing them into their respective semantic parts are fundamental steps towards detailed object understanding in computer vision. In this paper, we propose a joint solution that tackles semantic object and part segmentation simultaneously, in which higher object-level context is provided to guide part segmentation, and more detailed part-level localization is utilized to refine object segmentation. Specifically, we first introduce the concept of semantic compositional parts (SCP) in which similar semantic parts are grouped and shared among different objects. A two-channel fully convolutional network (FCN) is then trained to provide the SCP and object potentials at each pixel. At the same time, a compact set of segments can also be obtained from the SCP predictions of the network. Given the potentials and the generated segments, in order to explore long-range context, we finally construct an efficient fully connected conditional random field (FCRF) to jointly predict the final object and part labels. Extensive evaluation on three different datasets shows that our approach can mutually enhance the performance of object and part segmentation, and outperforms the current state-of-the-art on both tasks.
ER  -


TY  - Preprint
T1  - A Deep Learning Model for Structured Outputs with High-order Interaction
A1  - Hongyu Guo
A1  - Xiaodan Zhu
A1  - Martin Renqiang Min
JO  - ArXiv e-prints
Y1  - 29 April, 2015
UR  - https://arxiv.org/abs/1504.08022
N2  - Many real-world applications are associated with structured data, where not only input but also output has interplay. However, typical classification and regression models often lack the ability of simultaneously exploring high-order interaction within input and that within output. In this paper, we present a deep learning model aiming to generate a powerful nonlinear functional mapping from structured input to structured output. More specifically, we propose to integrate high-order hidden units, guided discriminative pretraining, and high-order auto-encoders for this purpose. We evaluate the model with three datasets, and obtain state-of-the-art performances among competitive methods. Our current work focuses on structured output regression, which is a less explored area, although the model can be extended to handle structured label classification.
ER  -


TY  - Preprint
T1  - Caffe con Troll: Shallow Ideas to Speed Up Deep Learning
A1  - Stefan Hadjis
A1  - Firas Abuzaid
A1  - Ce Zhang
A1  - Christopher RÃ©
JO  - ArXiv e-prints
Y1  - 26 May, 2015
UR  - https://arxiv.org/abs/1504.04343
N2  - We present Caffe con Troll (CcT), a fully compatible end-to-end version of the popular framework Caffe with rebuilt internals. We built CcT to examine the performance characteristics of training and deploying general-purpose convolutional neural networks across different hardware architectures. We find that, by employing standard batching optimizations for CPU training, we achieve a 4.5x throughput improvement over Caffe on popular networks like CaffeNet. Moreover, with these improvements, the end-to-end training time for CNNs is directly proportional to the FLOPS delivered by the CPU, which enables us to efficiently train hybrid CPU-GPU systems for CNNs.
ER  -


TY  - Preprint
T1  - Societal, Economic, Ethical and Legal Challenges of the Digital Revolution: From Big Data to Deep Learning, Artificial Intelligence, and Manipulative Technologies
A1  - Dirk Helbing
JO  - ArXiv e-prints
Y1  - 14 April, 2015
UR  - https://arxiv.org/abs/1504.03751
N2  - In the wake of the on-going digital revolution, we will see a dramatic transformation of our economy and most of our societal institutions. While the benefits of this transformation can be massive, there are also tremendous risks to our society. After the automation of many production processes and the creation of self-driving vehicles, the automation of society is next. This is moving us to a tipping point and to a crossroads: we must decide between a society in which the actions are determined in a top-down way and then implemented by coercion or manipulative technologies (such as personalized ads and nudging) or a society, in which decisions are taken in a free and participatory way and mutually coordinated. Modern information and communication systems (ICT) enable both, but the latter has economic and strategic benefits. The fundaments of human dignity, autonomous decision-making, and democracies are shaking, but I believe that they need to be vigorously defended, as they are not only core principles of livable societies, but also the basis of greater efficiency and success.
ER  -


TY  - Preprint
T1  - Simultaneous Feature Learning and Hash Coding with Deep Neural Networks
A1  - Hanjiang Lai
A1  - Yan Pan
A1  - Ye Liu
A1  - Shuicheng Yan
JO  - ArXiv e-prints
Y1  - 13 April, 2015
UR  - https://arxiv.org/abs/1504.03410
N2  - Similarity-preserving hashing is a widely-used method for nearest neighbour search in large-scale image retrieval tasks. For most existing hashing methods, an image is first encoded as a vector of hand-engineering visual features, followed by another separate projection or quantization step that generates binary codes. However, such visual feature vectors may not be optimally compatible with the coding process, thus producing sub-optimal hashing codes. In this paper, we propose a deep architecture for supervised hashing, in which images are mapped into binary codes via carefully designed deep neural networks. The pipeline of the proposed deep architecture consists of three building blocks: 1) a sub-network with a stack of convolution layers to produce the effective intermediate image features; 2) a divide-and-encode module to divide the intermediate image features into multiple branches, each encoded into one hash bit; and 3) a triplet ranking loss designed to characterize that one image is more similar to the second image than to the third one. Extensive evaluations on several benchmark image datasets show that the proposed simultaneous feature learning and hash coding pipeline brings substantial improvements over other state-of-the-art supervised or unsupervised hashing methods.
ER  -


TY  - Preprint
T1  - Real-world Object Recognition with Off-the-shelf Deep Conv Nets: How Many Objects can iCub Learn?
A1  - Giulia Pasquale
A1  - Carlo Ciliberto
A1  - Francesca Odone
A1  - Lorenzo Rosasco
A1  - Lorenzo Natale
JO  - ArXiv e-prints
Y1  - 14 April, 2015
UR  - https://arxiv.org/abs/1504.03154
N2  - The ability to visually recognize objects is a fundamental skill for robotics systems. Indeed, a large variety of tasks involving manipulation, navigation or interaction with other agents, deeply depends on the accurate understanding of the visual scene. Yet, at the time being, robots are lacking good visual perceptual systems, which often become the main bottleneck preventing the use of autonomous agents for real-world applications.
ER  -


TY  - Preprint
T1  - A Deep Embedding Model for Co-occurrence Learning
A1  - Yelong Shen
A1  - Ruoming Jin
A1  - Jianshu Chen
A1  - Xiaodong He
A1  - Jianfeng Gao
A1  - Li Deng
JO  - ArXiv e-prints
Y1  - 4 June, 2015
UR  - https://arxiv.org/abs/1504.02824
N2  - Co-occurrence Data is a common and important information source in many areas, such as the word co-occurrence in the sentences, friends co-occurrence in social networks and products co-occurrence in commercial transaction data, etc, which contains rich correlation and clustering information about the items. In this paper, we study co-occurrence data using a general energy-based probabilistic model, and we analyze three different categories of energy-based model, namely, the $L_1$, $L_2$ and $L_k$ models, which are able to capture different levels of dependency in the co-occurrence data. We also discuss how several typical existing models are related to these three types of energy models, including the Fully Visible Boltzmann Machine (FVBM) ($L_2$), Matrix Factorization ($L_2$), Log-BiLinear (LBL) models ($L_2$), and the Restricted Boltzmann Machine (RBM) model ($L_k$). Then, we propose a Deep Embedding Model (DEM) (an $L_k$ model) from the energy model in a \emph{principled} manner. Furthermore, motivated by the observation that the partition function in the energy model is intractable and the fact that the major objective of modeling the co-occurrence data is to predict using the conditional probability, we apply the \emph{maximum pseudo-likelihood} method to learn DEM. In consequence, the developed model and its learning method naturally avoid the above difficulties and can be easily used to compute the conditional probability in prediction. Interestingly, our method is equivalent to learning a special structured deep neural network using back-propagation and a special sampling strategy, which makes it scalable on large-scale datasets. Finally, in the experiments, we show that the DEM can achieve comparable or better results than state-of-the-art methods on datasets across several application domains.
ER  -


TY  - Preprint
T1  - What Do Deep CNNs Learn About Objects?
A1  - Xingchao Peng
A1  - Baochen Sun
A1  - Karim Ali
A1  - Kate Saenko
JO  - ArXiv e-prints
Y1  - 9 April, 2015
UR  - https://arxiv.org/abs/1504.02485
N2  - Deep convolutional neural networks learn extremely powerful image representations, yet most of that power is hidden in the millions of deep-layer parameters. What exactly do these parameters represent? Recent work has started to analyse CNN representations, finding that, e.g., they are invariant to some 2D transformations Fischer et al. (2014), but are confused by particular types of image noise Nguyen et al. (2014). In this work, we delve deeper and ask: how invariant are CNNs to object-class variations caused by 3D shape, pose, and photorealism?
ER  -


TY  - Preprint
T1  - A Group Theoretic Perspective on Unsupervised Deep Learning
A1  - Arnab Paul
A1  - Suresh Venkatasubramanian
JO  - ArXiv e-prints
Y1  - 21 April, 2015
UR  - https://arxiv.org/abs/1504.02462
N2  - Why does Deep Learning work? What representations does it capture? How do higher-order representations emerge? We study these questions from the perspective of group theory, thereby opening a new approach towards a theory of Deep learning.
ER  -


TY  - Preprint
T1  - When Face Recognition Meets with Deep Learning: an Evaluation of Convolutional Neural Networks for Face Recognition
A1  - Guosheng Hu
A1  - Yongxin Yang
A1  - Dong Yi
A1  - Josef Kittler
A1  - William Christmas
A1  - Stan Z. Li
A1  - Timothy Hospedales
JO  - ArXiv e-prints
Y1  - 9 April, 2015
UR  - https://arxiv.org/abs/1504.02351
N2  - Deep learning, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a &#39;good&#39; architecture. The existing works tend to focus on reporting CNN architectures that work well for face recognition rather than investigate the reason. In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible. Specifically, we use public database LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing CNNs trained on private databases. We propose three CNN architectures which are the first reported architectures trained using LFW data. This paper quantitatively compares the architectures of CNNs and evaluate the effect of different implementation choices. We identify several useful properties of CNN-FRS. For instance, the dimensionality of the learned features can be significantly reduced without adverse effect on face recognition accuracy. In addition, traditional metric learning method exploiting CNN-learned features is evaluated. Experiments show two crucial factors to good CNN-FRS performance are the fusion of multiple CNNs and metric learning. To make our work reproducible, source code and models will be made publicly available.
ER  -


TY  - Preprint
T1  - Pixel-wise Deep Learning for Contour Detection
A1  - Jyh-Jing Hwang
A1  - Tyng-Luh Liu
JO  - ArXiv e-prints
Y1  - 8 April, 2015
UR  - https://arxiv.org/abs/1504.01989
N2  - We address the problem of contour detection via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different CNN layers and verify their performance on BSDS500.
ER  -


TY  - Preprint
T1  - Autonomous CRM Control via CLV Approximation with Deep Reinforcement Learning in Discrete and Continuous Action Space
A1  - Yegor Tkachenko
JO  - ArXiv e-prints
Y1  - 8 April, 2015
UR  - https://arxiv.org/abs/1504.01840
N2  - The paper outlines a framework for autonomous control of a CRM (customer relationship management) system. First, it explores how a modified version of the widely accepted Recency-Frequency-Monetary Value system of metrics can be used to define the state space of clients or donors. Second, it describes a procedure to determine the optimal direct marketing action in discrete and continuous action space for the given individual, based on his position in the state space. The procedure involves the use of model-free Q-learning to train a deep neural network that relates a client&#39;s position in the state space to rewards associated with possible marketing actions. The estimated value function over the client state space can be interpreted as customer lifetime value, and thus allows for a quick plug-in estimation of CLV for a given client. Experimental results are presented, based on KDD Cup 1998 mailing dataset of donation solicitations.
ER  -


TY  - Preprint
T1  - An Empirical Evaluation of Deep Learning on Highway Driving
A1  - Brody Huval
A1  - Tao Wang
A1  - Sameep Tandon
A1  - Jeff Kiske
A1  - Will Song
A1  - Joel Pazhayampallil
A1  - Mykhaylo Andriluka
A1  - Pranav Rajpurkar
A1  - Toki Migimatsu
A1  - Royce Cheng-Yue
A1  - Fernando Mujica
A1  - Adam Coates
A1  - Andrew Y. Ng
JO  - ArXiv e-prints
Y1  - 16 April, 2015
UR  - https://arxiv.org/abs/1504.01716
N2  - Numerous groups have applied a variety of deep learning techniques to computer vision problems in highway perception scenarios. In this paper, we presented a number of empirical evaluations of recent deep learning advances. Computer vision, combined with deep learning, has the potential to bring about a relatively inexpensive, robust solution to autonomous driving. To prepare deep learning for industry uptake and practical applications, neural networks will require large data sets that represent all possible driving environments and scenarios. We collect a large data set of highway data and apply deep learning and computer vision algorithms to problems such as car and lane detection. We show how existing convolutional neural networks (CNNs) can be used to perform lane and vehicle detection while running at frame rates required for a real-time system. Our results lend credence to the hypothesis that deep learning holds promise for autonomous driving.
ER  -


TY  - Preprint
T1  - Modeling Spatial-Temporal Clues in a Hybrid Deep Learning Framework for Video Classification
A1  - Zuxuan Wu
A1  - Xi Wang
A1  - Yu-Gang Jiang
A1  - Hao Ye
A1  - Xiangyang Xue
JO  - ArXiv e-prints
Y1  - 7 April, 2015
UR  - https://arxiv.org/abs/1504.01561
N2  - Classifying videos according to content semantics is an important problem with a wide range of applications. In this paper, we propose a hybrid deep learning framework for video classification, which is able to model static spatial information, short-term motion, as well as long-term temporal clues in the videos. Specifically, the spatial and the short-term motion features are extracted separately by two Convolutional Neural Networks (CNN). These two types of CNN-based features are then combined in a regularized feature fusion network for classification, which is able to learn and utilize feature relationships for improved performance. In addition, Long Short Term Memory (LSTM) networks are applied on top of the two features to further model longer-term temporal clues. The main contribution of this work is the hybrid learning framework that can model several important aspects of the video data. We also show that (1) combining the spatial and the short-term motion features in the regularized fusion network is better than direct classification and fusion using the CNN with a softmax layer, and (2) the sequence-based LSTM is highly complementary to the traditional classification strategy without considering the temporal frame orders. Extensive experiments are conducted on two popular and challenging benchmarks, the UCF-101 Human Actions and the Columbia Consumer Videos (CCV). On both benchmarks, our framework achieves to-date the best reported performance: $91.3\%$ on the UCF-101 and $83.5\%$ on the CCV.
ER  -


TY  - Preprint
T1  - Implementation of a Practical Distributed Calculation System with Browsers and JavaScript, and Application to Distributed Deep Learning
A1  - Ken Miura
A1  - Tatsuya Harada
JO  - ArXiv e-prints
Y1  - 19 March, 2015
UR  - https://arxiv.org/abs/1503.05743
N2  - Deep learning can achieve outstanding results in various fields. However, it requires so significant computational power that graphics processing units (GPUs) and/or numerous computers are often required for the practical application. We have developed a new distributed calculation framework called &#34;Sashimi&#34; that allows any computer to be used as a distribution node only by accessing a website. We have also developed a new JavaScript neural network framework called &#34;Sukiyaki&#34; that uses general purpose GPUs with web browsers. Sukiyaki performs 30 times faster than a conventional JavaScript library for deep convolutional neural networks (deep CNNs) learning. The combination of Sashimi and Sukiyaki, as well as new distribution algorithms, demonstrates the distributed deep learning of deep CNNs only with web browsers on various devices. The libraries that comprise the proposed methods are available under MIT license at http://mil-tokyo.github.io/.
ER  -


TY  - Preprint
T1  - Deep Unsupervised Learning using Nonequilibrium Thermodynamics
A1  - Jascha Sohl-Dickstein
A1  - Eric A. Weiss
A1  - Niru Maheswaranathan
A1  - Surya Ganguli
JO  - ArXiv e-prints
Y1  - 18 November, 2015
UR  - https://arxiv.org/abs/1503.03585
N2  - A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.
ER  -


TY  - Preprint
T1  - Representation Learning with Deep Extreme Learning Machines for Efficient Image Set Classification
A1  - Muhammad Uzair
A1  - Faisal Shafait
A1  - Bernard Ghanem
A1  - Ajmal Mian
JO  - ArXiv e-prints
Y1  - 1 April, 2015
UR  - https://arxiv.org/abs/1503.02445
N2  - Efficient and accurate joint representation of a collection of images, that belong to the same class, is a major research challenge for practical image set classification. Existing methods either make prior assumptions about the data structure, or perform heavy computations to learn structure from the data itself. In this paper, we propose an efficient image set representation that does not make any prior assumptions about the structure of the underlying data. We learn the non-linear structure of image sets with Deep Extreme Learning Machines (DELM) that are very efficient and generalize well even on a limited number of training samples. Extensive experiments on a broad range of public datasets for image set classification (Honda/UCSD, CMU Mobo, YouTube Celebrities, Celebrity-1000, ETH-80) show that the proposed algorithm consistently outperforms state-of-the-art image set classification methods both in terms of speed and accuracy.
ER  -


TY  - Preprint
T1  - Deep Learning and the Information Bottleneck Principle
A1  - Naftali Tishby
A1  - Noga Zaslavsky
JO  - ArXiv e-prints
Y1  - 9 March, 2015
UR  - https://arxiv.org/abs/1503.02406
N2  - Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network&#39;s simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.
ER  -


TY  - Preprint
T1  - EmoNets: Multimodal deep learning approaches for emotion recognition in video
A1  - Samira Ebrahimi Kahou
A1  - Xavier Bouthillier
A1  - Pascal Lamblin
A1  - Caglar Gulcehre
A1  - Vincent Michalski
A1  - Kishore Konda
A1  - SÃ©bastien Jean
A1  - Pierre Froumenty
A1  - Yann Dauphin
A1  - Nicolas Boulanger-Lewandowski
A1  - Raul Chandias Ferrari
A1  - Mehdi Mirza
A1  - David Warde-Farley
A1  - Aaron Courville
A1  - Pascal Vincent
A1  - Roland Memisevic
A1  - Christopher Pal
A1  - Yoshua Bengio
JO  - ArXiv e-prints
Y1  - 29 March, 2015
UR  - https://arxiv.org/abs/1503.01800
N2  - The task of the emotion recognition in the wild (EmotiW) Challenge is to assign one of seven emotions to short video clips extracted from Hollywood style movies. The videos depict acted-out emotions under realistic conditions with a large degree of variation in attributes such as pose and illumination, making it worthwhile to explore approaches which consider combinations of features from multiple modalities for label assignment. In this paper we present our approach to learning several specialist models using deep learning techniques, each focusing on one modality. Among these are a convolutional neural network, focusing on capturing visual information in detected faces, a deep belief net focusing on the representation of the audio stream, a K-Means based &#34;bag-of-mouths&#34; model, which extracts visual features around the mouth region and a relational autoencoder, which addresses spatio-temporal aspects of videos. We explore multiple methods for the combination of cues from these modalities into one common classifier. This achieves a considerably greater accuracy than predictions from our strongest single-modality classifier. Our method was the winning submission in the 2013 EmotiW challenge and achieved a test set accuracy of 47.67% on the 2014 dataset.
ER  -


TY  - Preprint
T1  - When Are Tree Structures Necessary for Deep Learning of Representations?
A1  - Jiwei Li
A1  - Minh-Thang Luong
A1  - Dan Jurafsky
A1  - Eudard Hovy
JO  - ArXiv e-prints
Y1  - 18 August, 2015
UR  - https://arxiv.org/abs/1503.00185
N2  - Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up, are a popular architecture. But there have not been rigorous evaluations showing for exactly which tasks this syntax-based method is appropriate. In this paper we benchmark {\bf recursive} neural models against sequential {\bf recurrent} neural models (simple recurrent and LSTM models), enforcing apples-to-apples comparison as much as possible. We investigate 4 tasks: (1) sentiment classification at the sentence level and phrase level; (2) matching questions to answer-phrases; (3) discourse parsing; (4) semantic relation extraction (e.g., {\em component-whole} between nouns).
ER  -


TY  - Preprint
T1  - Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields
A1  - Fayao Liu
A1  - Chunhua Shen
A1  - Guosheng Lin
A1  - Ian Reid
JO  - ArXiv e-prints
Y1  - 24 November, 2015
UR  - https://arxiv.org/abs/1502.07411
N2  - In this article, we tackle the problem of depth estimation from single monocular images. Compared with depth estimation using multiple images such as stereo depth perception, depth from monocular images is much more challenging. Prior work typically focuses on exploiting geometric priors or additional sources of information, most using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) set new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimations can be naturally formulated as a continuous conditional random field (CRF) learning problem. Therefore, here we present a deep convolutional neural field model for estimating depths from single monocular images, aiming to jointly explore the capacity of deep CNN and continuous CRF. In particular, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework. We then further propose an equally effective model based on fully convolutional networks and a novel superpixel pooling method, which is $\sim 10$ times faster, to speedup the patch-wise convolutions in the deep model. With this more efficient model, we are able to design deeper networks to pursue better performance. Experiments on both indoor and outdoor scene datasets demonstrate that the proposed method outperforms state-of-the-art depth estimation approaches.
ER  -


TY  - Preprint
T1  - Hands Deep in Deep Learning for Hand Pose Estimation
A1  - Markus Oberweger
A1  - Paul Wohlhart
A1  - Vincent Lepetit
JO  - ArXiv e-prints
Y1  - 2 December, 2016
UR  - https://arxiv.org/abs/1502.06807
N2  - We introduce and evaluate several architectures for Convolutional Neural Networks to predict the 3D joint locations of a hand given a depth map. We first show that a prior on the 3D pose can be easily introduced and significantly improves the accuracy and reliability of the predictions. We also show how to use context efficiently to deal with ambiguities between fingers. These two contributions allow us to significantly outperform the state-of-the-art on several challenging benchmarks, both in terms of accuracy and computation times.
ER  -


TY  - Preprint
T1  - Deep Learning for Multi-label Classification
A1  - Jesse Read
A1  - Fernando Perez-Cruz
JO  - ArXiv e-prints
Y1  - 17 December, 2014
UR  - https://arxiv.org/abs/1502.05988
N2  - In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature
ER  -


TY  - Preprint
T1  - Towards Biologically Plausible Deep Learning
A1  - Yoshua Bengio
A1  - Dong-Hyun Lee
A1  - Jorg Bornschein
A1  - Thomas Mesnard
A1  - Zhouhan Lin
JO  - ArXiv e-prints
Y1  - 8 August, 2016
UR  - https://arxiv.org/abs/1502.04156
N2  - Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.
ER  -


TY  - Preprint
T1  - Abstract Learning via Demodulation in a Deep Neural Network
A1  - Andrew J. R. Simpson
JO  - ArXiv e-prints
Y1  - 13 February, 2015
UR  - https://arxiv.org/abs/1502.04042
N2  - Inspired by the brain, deep neural networks (DNN) are thought to learn abstract representations through their hierarchical architecture. However, at present, how this happens is not well understood. Here, we demonstrate that DNN learn abstract representations by a process of demodulation. We introduce a biased sigmoid activation function and use it to show that DNN learn and perform better when optimized for demodulation. Our findings constitute the first unambiguous evidence that DNN perform abstract learning in practical use. Our findings may also explain abstract learning in the human brain.
ER  -


TY  - Preprint
T1  - Applying deep learning techniques on medical corpora from the World Wide Web: a prototypical system and evaluation
A1  - Jose Antonio MiÃ±arro-GimÃ©nez
A1  - Oscar MarÃ­n-Alonso
A1  - Matthias Samwald
JO  - ArXiv e-prints
Y1  - 12 February, 2015
UR  - https://arxiv.org/abs/1502.03682
N2  - BACKGROUND: The amount of biomedical literature is rapidly growing and it is becoming increasingly difficult to keep manually curated knowledge bases and ontologies up-to-date. In this study we applied the word2vec deep learning toolkit to medical corpora to test its potential for identifying relationships from unstructured text. We evaluated the efficiency of word2vec in identifying properties of pharmaceuticals based on mid-sized, unstructured medical text corpora available on the web. Properties included relationships to diseases (&#39;may treat&#39;) or physiological processes (&#39;has physiological effect&#39;). We compared the relationships identified by word2vec with manually curated information from the National Drug File - Reference Terminology (NDF-RT) ontology as a gold standard. RESULTS: Our results revealed a maximum accuracy of 49.28% which suggests a limited ability of word2vec to capture linguistic regularities on the collected medical corpora compared with other published results. We were able to document the influence of different parameter settings on result accuracy and found and unexpected trade-off between ranking quality and accuracy. Pre-processing corpora to reduce syntactic variability proved to be a good strategy for increasing the utility of the trained vector models. CONCLUSIONS: Word2vec is a very efficient implementation for computing vector representations and for its ability to identify relationships in textual data without any prior domain knowledge. We found that the ranking and retrieved results generated by word2vec were not of sufficient quality for automatic population of knowledge bases and ontologies, but could serve as a starting point for further manual curation.
ER  -


TY  - Preprint
T1  - Large-Scale Deep Learning on the YFCC100M Dataset
A1  - Karl Ni
A1  - Roger Pearce
A1  - Kofi Boakye
A1  - Brian Van Essen
A1  - Damian Borth
A1  - Barry Chen
A1  - Eric Wang
JO  - ArXiv e-prints
Y1  - 11 February, 2015
UR  - https://arxiv.org/abs/1502.03409
N2  - We present a work-in-progress snapshot of learning with a 15 billion parameter deep learning network on HPC architectures applied to the largest publicly available natural image and video dataset released to-date. Recent advancements in unsupervised deep neural networks suggest that scaling up such networks in both model and training dataset size can yield significant improvements in the learning of concepts at the highest layers. We train our three-layer deep neural network on the Yahoo! Flickr Creative Commons 100M dataset. The dataset comprises approximately 99.2 million images and 800,000 user-created videos from Yahoo&#39;s Flickr image and video sharing platform. Training of our network takes eight days on 98 GPU nodes at the High Performance Computing Center at Lawrence Livermore National Laboratory. Encouraging preliminary results and future research directions are presented and discussed.
ER  -


TY  - Preprint
T1  - Using Distance Estimation and Deep Learning to Simplify Calibration in Food Calorie Measurement
A1  - Pallavi Kuhad
A1  - Abdulsalam Yassine
A1  - Shervin Shirmohammadi
JO  - ArXiv e-prints
Y1  - 23 March, 2015
UR  - https://arxiv.org/abs/1502.03302
N2  - High calorie intake in the human body on the one hand, has proved harmful in numerous occasions leading to several diseases and on the other hand, a standard amount of calorie intake has been deemed essential by dieticians to maintain the right balance of calorie content in human body. As such, researchers have proposed a variety of automatic tools and systems to assist users measure their calorie in-take. In this paper, we consider the category of those tools that use image processing to recognize the food, and we propose a method for fully automatic and user-friendly calibration of the dimension of the food portion sizes, which is needed in order to measure food portion weight and its ensuing amount of calories. Experimental results show that our method, which uses deep learning, mobile cloud computing, distance estimation and size calibration inside a mobile device, leads to an accuracy improvement to 95% on average compared to previous work
ER  -


TY  - Preprint
T1  - Learning Transferable Features with Deep Adaptation Networks
A1  - Mingsheng Long
A1  - Yue Cao
A1  - Jianmin Wang
A1  - Michael I. Jordan
JO  - ArXiv e-prints
Y1  - 27 May, 2015
UR  - https://arxiv.org/abs/1502.02791
N2  - Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.
ER  -


TY  - Preprint
T1  - Deep Learning with Limited Numerical Precision
A1  - Suyog Gupta
A1  - Ankur Agrawal
A1  - Kailash Gopalakrishnan
A1  - Pritish Narayanan
JO  - ArXiv e-prints
Y1  - 9 February, 2015
UR  - https://arxiv.org/abs/1502.02551
N2  - Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network&#39;s behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding.
ER  -


TY  - Preprint
T1  - Deep Joint Task Learning for Generic Object Extraction
A1  - Xiaolong Wang
A1  - Liliang Zhang
A1  - Liang Lin
A1  - Zhujin Liang
A1  - Wangmeng Zuo
JO  - ArXiv e-prints
Y1  - 3 February, 2015
UR  - https://arxiv.org/abs/1502.00743
N2  - This paper investigates how to extract objects-of-interest without relying on hand-craft features and sliding windows approaches, that aims to jointly solve two sub-tasks: (i) rapidly localizing salient objects from images, and (ii) accurately segmenting the objects based on the localizations. We present a general joint task learning framework, in which each task (either object localization or object segmentation) is tackled via a multi-layer convolutional neural network, and the two networks work collaboratively to boost performance. In particular, we propose to incorporate latent variables bridging the two networks in a joint optimization manner. The first network directly predicts the positions and scales of salient objects from raw images, and the latent variables adjust the object localizations to feed the second network that produces pixelwise object masks. An EM-type method is presented for the optimization, iterating with two steps: (i) by using the two networks, it estimates the latent variables by employing an MCMC-based sampling method; (ii) it optimizes the parameters of the two networks unitedly via back propagation, with the fixed latent variables. Extensive experiments suggest that our framework significantly outperforms other state-of-the-art approaches in both accuracy and efficiency (e.g. 1000 times faster than competing approaches).
ER  -


TY  - Preprint
T1  - maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell GPUs
A1  - Andrew Lavin
JO  - ArXiv e-prints
Y1  - 30 January, 2015
UR  - https://arxiv.org/abs/1501.06633
N2  - This paper describes maxDNN, a computationally efficient convolution kernel for deep learning with the NVIDIA Maxwell GPU. maxDNN reaches 96.3% computational efficiency on typical deep learning network architectures. The design combines ideas from cuda-convnet2 with the Maxas SGEMM assembly code. We only address forward propagation (FPROP) operation of the network, but we believe that the same techniques used here will be effective for backward propagation (BPROP) as well.
ER  -


TY  - Preprint
T1  - Deep Multimodal Learning for Audio-Visual Speech Recognition
A1  - Youssef Mroueh
A1  - Etienne Marcheret
A1  - Vaibhava Goel
JO  - ArXiv e-prints
Y1  - 22 January, 2015
UR  - https://arxiv.org/abs/1501.05396
N2  - In this paper, we present methods in deep multimodal learning for fusing speech and visual modalities for Audio-Visual Automatic Speech Recognition (AV-ASR). First, we study an approach where uni-modal deep networks are trained separately and their final hidden layers fused to obtain a joint feature space in which another deep network is built. While the audio network alone achieves a phone error rate (PER) of $41\%$ under clean condition on the IBM large vocabulary audio-visual studio dataset, this fusion model achieves a PER of $35.83\%$ demonstrating the tremendous value of the visual channel in phone classification even in audio with high signal to noise ratio. Second, we present a new deep network architecture that uses a bilinear softmax layer to account for class specific correlations between modalities. We show that combining the posteriors from the bilinear networks with those from the fused model mentioned above results in a further significant phone error rate reduction, yielding a final PER of $34.03\%$.
ER  -


TY  - Preprint
T1  - Deep Learning with Nonparametric Clustering
A1  - Gang Chen
JO  - ArXiv e-prints
Y1  - 13 January, 2015
UR  - https://arxiv.org/abs/1501.03084
N2  - Clustering is an essential problem in machine learning and data mining. One vital factor that impacts clustering performance is how to learn or design the data representation (or features). Fortunately, recent advances in deep learning can learn unsupervised features effectively, and have yielded state of the art performance in many classification problems, such as character recognition, object recognition and document categorization. However, little attention has been paid to the potential of deep learning for unsupervised clustering problems. In this paper, we propose a deep belief network with nonparametric clustering. As an unsupervised method, our model first leverages the advantages of deep learning for feature representation and dimension reduction. Then, it performs nonparametric clustering under a maximum margin framework -- a discriminative clustering model and can be trained online efficiently in the code space. Lastly model parameters are refined in the deep belief network. Thus, this model can learn features for clustering and infer model complexity in an unified framework. The experimental results show the advantage of our approach over competitive baselines.
ER  -


TY  - Preprint
T1  - Joint Deep Learning for Car Detection
A1  - Seyedshams Feyzabadi
JO  - ArXiv e-prints
Y1  - 14 July, 2016
UR  - https://arxiv.org/abs/1412.7854
N2  - Traditional object recognition approaches apply feature extraction, part deformation handling, occlusion handling and classification sequentially while they are independent from each other. Ouyang and Wang proposed a model for jointly learning of all of the mentioned processes using one deep neural network. We utilized, and manipulated their toolbox in order to apply it in car detection scenarios where it had not been tested. Creating a single deep architecture from these components, improves the interaction between them and can enhance the performance of the whole system. We believe that the approach can be used as a general purpose object detection toolbox. We tested the algorithm on UIUC car dataset, and achieved an outstanding result. The accuracy of our method was 97 % while the previously reported results showed an accuracy of up to 91 %. We strongly believe that having an experiment on a larger dataset can show the advantage of using deep models over shallow ones.
ER  -


TY  - Preprint
T1  - Learning Deep Temporal Representations for Brain Decoding
A1  - Orhan Firat
A1  - Emre Aksan
A1  - Ilke Oztekin
A1  - Fatos T. Yarman Vural
JO  - ArXiv e-prints
Y1  - 12 January, 2015
UR  - https://arxiv.org/abs/1412.7522
N2  - Functional magnetic resonance imaging produces high dimensional data, with a less then ideal number of labelled samples for brain decoding tasks (predicting brain states). In this study, we propose a new deep temporal convolutional neural network architecture with spatial pooling for brain decoding which aims to reduce dimensionality of feature space along with improved classification performance. Temporal representations (filters) for each layer of the convolutional model are learned by leveraging unlabelled fMRI data in an unsupervised fashion with regularized autoencoders. Learned temporal representations in multiple levels capture the regularities in the temporal domain and are observed to be a rich bank of activation patterns which also exhibit similarities to the actual hemodynamic responses. Further, spatial pooling layers in the convolutional architecture reduce the dimensionality without losing excessive information. By employing the proposed temporal convolutional architecture with spatial pooling, raw input fMRI data is mapped to a non-linear, highly-expressive and low-dimensional feature space where the final classification is conducted. In addition, we propose a simple heuristic approach for hyper-parameter tuning when no validation data is available. Proposed method is tested on a ten class recognition memory experiment with nine subjects. The results support the efficiency and potential of the proposed model, compared to the baseline multi-voxel pattern analysis techniques.
ER  -


TY  - Preprint
T1  - Learning Deep Object Detectors from 3D Models
A1  - Xingchao Peng
A1  - Baochen Sun
A1  - Karim Ali
A1  - Kate Saenko
JO  - ArXiv e-prints
Y1  - 11 October, 2015
UR  - https://arxiv.org/abs/1412.7122
N2  - Crowdsourced 3D CAD models are becoming easily accessible online, and can potentially generate an infinite number of training images for almost any object category.We show that augmenting the training data of contemporary Deep Convolutional Neural Net (DCNN) models with such synthetic data can be effective, especially when real training data is limited or not well matched to the target domain. Most freely available CAD models capture 3D shape but are often missing other low level cues, such as realistic object texture, pose, or background. In a detailed analysis, we use synthetic CAD-rendered images to probe the ability of DCNN to learn without these cues, with surprising findings. In particular, we show that when the DCNN is fine-tuned on the target detection task, it exhibits a large degree of invariance to missing low-level cues, but, when pretrained on generic ImageNet classification, it learns better when the low-level cues are simulated. We show that our synthetic DCNN training approach significantly outperforms previous methods on the PASCAL VOC2007 dataset when learning in the few-shot scenario and improves performance in a domain shift scenario on the Office benchmark.
ER  -


TY  - Preprint
T1  - Learning Activation Functions to Improve Deep Neural Networks
A1  - Forest Agostinelli
A1  - Matthew Hoffman
A1  - Peter Sadowski
A1  - Pierre Baldi
JO  - ArXiv e-prints
Y1  - 21 April, 2015
UR  - https://arxiv.org/abs/1412.6830
N2  - Artificial neural networks typically have a fixed, non-linear activation function at each neuron. We have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes.
ER  -


TY  - Preprint
T1  - Deep learning with Elastic Averaging SGD
A1  - Sixin Zhang
A1  - Anna Choromanska
A1  - Yann LeCun
JO  - ArXiv e-prints
Y1  - 25 October, 2015
UR  - https://arxiv.org/abs/1412.6651
N2  - We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.
ER  -


TY  - Preprint
T1  - Deep metric learning using Triplet network
A1  - Elad Hoffer
A1  - Nir Ailon
JO  - ArXiv e-prints
Y1  - 23 March, 2015
UR  - https://arxiv.org/abs/1412.6622
N2  - Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.
ER  -


TY  - Preprint
T1  - Why does Deep Learning work? - A perspective from Group Theory
A1  - Arnab Paul
A1  - Suresh Venkatasubramanian
JO  - ArXiv e-prints
Y1  - 28 February, 2015
UR  - https://arxiv.org/abs/1412.6621
N2  - Why does Deep Learning work? What representations does it capture? How do higher-order representations emerge? We study these questions from the perspective of group theory, thereby opening a new approach towards a theory of Deep learning.
ER  -


TY  - Preprint
T1  - In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning
A1  - Behnam Neyshabur
A1  - Ryota Tomioka
A1  - Nathan Srebro
JO  - ArXiv e-prints
Y1  - 16 April, 2015
UR  - https://arxiv.org/abs/1412.6614
N2  - We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multilayer feed-forward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.
ER  -


TY  - Preprint
T1  - Purine: A bi-graph based deep learning framework
A1  - Min Lin
A1  - Shuo Li
A1  - Xuan Luo
A1  - Shuicheng Yan
JO  - ArXiv e-prints
Y1  - 16 April, 2015
UR  - https://arxiv.org/abs/1412.6249
N2  - In this paper, we introduce a novel deep learning framework, termed Purine. In Purine, a deep network is expressed as a bipartite graph (bi-graph), which is composed of interconnected operators and data tensors. With the bi-graph abstraction, networks are easily solvable with event-driven task dispatcher. We then demonstrate that different parallelism schemes over GPUs and/or CPUs on single or multiple PCs can be universally implemented by graph composition. This eases researchers from coding for various parallelization schemes, and the same dispatcher can be used for solving variant graphs. Scheduled by the task dispatcher, memory transfers are fully overlapped with other computations, which greatly reduce the communication overhead and help us achieve approximate linear acceleration.
ER  -


TY  - Preprint
T1  - Deep Structured Output Learning for Unconstrained Text Recognition
A1  - Max Jaderberg
A1  - Karen Simonyan
A1  - Andrea Vedaldi
A1  - Andrew Zisserman
JO  - ArXiv e-prints
Y1  - 10 April, 2015
UR  - https://arxiv.org/abs/1412.5903
N2  - We develop a representation suitable for the unconstrained recognition of words in natural images: the general case of no fixed lexicon and unknown length.
ER  -


TY  - Preprint
T1  - Sequential Labeling with online Deep Learning
A1  - Gang Chen
A1  - Ran Xu
A1  - Sargur Srihari
JO  - ArXiv e-prints
Y1  - 3 May, 2015
UR  - https://arxiv.org/abs/1412.3397
N2  - Deep learning has attracted great attention recently and yielded the state of the art performance in dimension reduction and classification problems. However, it cannot effectively handle the structured output prediction, e.g. sequential labeling. In this paper, we propose a deep learning structure, which can learn discriminative features for sequential labeling problems. More specifically, we add the inter-relationship between labels in our deep learning structure, in order to incorporate the context information from the sequential data. Thus, our model is more powerful than linear Conditional Random Fields (CRFs) because the objective function learns latent non-linear features so that target labeling can be better predicted. We pretrain the deep structure with stacked restricted Boltzmann machines (RBMs) for feature learning and optimize our objective function with online learning algorithm, a mixture of perceptron training and stochastic gradient descent. We test our model on different challenge tasks, and show that our model outperforms significantly over the completive baselines.
ER  -


TY  - Preprint
T1  - Multimodal Transfer Deep Learning with Applications in Audio-Visual Recognition
A1  - Seungwhan Moon
A1  - Suyoun Kim
A1  - Haohan Wang
JO  - ArXiv e-prints
Y1  - 18 February, 2016
UR  - https://arxiv.org/abs/1412.3121
N2  - We propose a transfer deep learning (TDL) framework that can transfer the knowledge obtained from a single-modal neural network to a network with a different modality. Specifically, we show that we can leverage speech data to fine-tune the network trained for video recognition, given an initial set of audio-video parallel dataset within the same semantics. Our approach first learns the analogy-preserving embeddings between the abstract representations learned from intermediate layers of each network, allowing for semantics-level transfer between the source and target modalities. We then apply our neural network operation that fine-tunes the target network with the additional knowledge transferred from the source network, while keeping the topology of the target network unchanged. While we present an audio-visual recognition task as an application of our approach, our framework is flexible and thus can work with any multimodal dataset, or with any already-existing deep networks that share the common underlying semantics. In this work in progress report, we aim to provide comprehensive results of different configurations of the proposed approach on two widely used audio-visual datasets, and we discuss potential applications of the proposed approach.
ER  -


TY  - Preprint
T1  - Deep Learning for Answer Sentence Selection
A1  - Lei Yu
A1  - Karl Moritz Hermann
A1  - Phil Blunsom
A1  - Stephen Pulman
JO  - ArXiv e-prints
Y1  - 4 December, 2014
UR  - https://arxiv.org/abs/1412.1632
N2  - Answer sentence selection is the task of identifying sentences that contain the answer to a given question. This is an important problem in its own right as well as in the larger context of open domain question answering. We propose a novel approach to solving this task via means of distributed representations, and learn to match questions with answers by considering their semantic encoding. This contrasts prior work on this task, which typically relies on classifiers with large numbers of hand-crafted syntactic and semantic features and various external resources. Our approach does not require any feature engineering nor does it involve specialist linguistic data, making this model easily applicable to a wide range of domains and languages. Experimental results on a standard benchmark dataset from TREC demonstrate that---despite its simplicity---our model matches state of the art performance on the answer sentence selection task.
ER  -


TY  - Preprint
T1  - Deep Distributed Random Samplings for Supervised Learning: An Alternative to Random Forests?
A1  - Xiao-Lei Zhang
JO  - ArXiv e-prints
Y1  - 28 January, 2015
UR  - https://arxiv.org/abs/1412.1271
N2  - In (\cite{zhang2014nonlinear,zhang2014nonlinear2}), we have viewed machine learning as a coding and dimensionality reduction problem, and further proposed a simple unsupervised dimensionality reduction method, entitled deep distributed random samplings (DDRS). In this paper, we further extend it to supervised learning incrementally. The key idea here is to incorporate label information into the coding process by reformulating that each center in DDRS has multiple output units indicating which class the center belongs to.
ER  -


TY  - Preprint
T1  - Pedestrian Detection aided by Deep Learning Semantic Tasks
A1  - Yonglong Tian
A1  - Ping Luo
A1  - Xiaogang Wang
A1  - Xiaoou Tang
JO  - ArXiv e-prints
Y1  - 28 November, 2014
UR  - https://arxiv.org/abs/1412.0069
N2  - Deep learning methods have achieved great success in pedestrian detection, owing to its ability to learn features from raw pixels. However, they mainly capture middle-level representations, such as pose of pedestrian, but confuse positive with hard negative samples, which have large ambiguity, e.g. the shape and appearance of `tree trunk&#39; or `wire pole&#39; are similar to pedestrian in certain viewpoint. This ambiguity can be distinguished by high-level representation. To this end, this work jointly optimizes pedestrian detection with semantic tasks, including pedestrian attributes (e.g. `carrying backpack&#39;) and scene attributes (e.g. `road&#39;, `tree&#39;, and `horizontal&#39;). Rather than expensively annotating scene attributes, we transfer attributes information from existing scene segmentation datasets to the pedestrian dataset, by proposing a novel deep model to learn high-level features from multiple tasks and multiple data sources. Since distinct tasks have distinct convergence rates and data from different datasets have different distributions, a multi-task objective function is carefully designed to coordinate tasks and reduce discrepancies among datasets. The importance coefficients of tasks and network parameters in this objective function can be iteratively estimated. Extensive evaluations show that the proposed approach outperforms the state-of-the-art on the challenging Caltech and ETH datasets, where it reduces the miss rates of previous deep models by 17 and 5.5 percent, respectively.
ER  -


TY  - Preprint
T1  - Deep Learning Face Attributes in the Wild
A1  - Ziwei Liu
A1  - Ping Luo
A1  - Xiaogang Wang
A1  - Xiaoou Tang
JO  - ArXiv e-prints
Y1  - 24 September, 2015
UR  - https://arxiv.org/abs/1411.7766
N2  - Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation.
ER  -


TY  - Preprint
T1  - Investigating the Role of Prior Disambiguation in Deep-learning Compositional Models of Meaning
A1  - Jianpeng Cheng
A1  - Dimitri Kartsaklis
A1  - Edward Grefenstette
JO  - ArXiv e-prints
Y1  - 15 November, 2014
UR  - https://arxiv.org/abs/1411.4116
N2  - This paper aims to explore the effect of prior disambiguation on neural network- based compositional models, with the hope that better semantic representations for text compounds can be produced. We disambiguate the input word vectors before they are fed into a compositional deep net. A series of evaluations shows the positive effect of prior disambiguation for such deep models.
ER  -


TY  - Preprint
T1  - Deep Multi-Instance Transfer Learning
A1  - Dimitrios Kotzias
A1  - Misha Denil
A1  - Phil Blunsom
A1  - Nando de Freitas
JO  - ArXiv e-prints
Y1  - 10 December, 2014
UR  - https://arxiv.org/abs/1411.3128
N2  - We present a new approach for transferring knowledge from groups to individuals that comprise them. We evaluate our method in text, by inferring the ratings of individual sentences using full-review ratings. This approach, which combines ideas from transfer learning, deep learning and multi-instance learning, reduces the need for laborious human labelling of fine-grained data when abundant labels are available at the group level.
ER  -


TY  - Preprint
T1  - Deep Structured learning for mass segmentation from Mammograms
A1  - Neeraj Dhungel
A1  - Gustavo Carneiro
A1  - Andrew P. Bradley
JO  - ArXiv e-prints
Y1  - 4 December, 2014
UR  - https://arxiv.org/abs/1410.7454
N2  - In this paper, we present a novel method for the segmentation of breast masses from mammograms exploring structured and deep learning. Specifically, using structured support vector machine (SSVM), we formulate a model that combines different types of potential functions, including one that classifies image regions using deep learning. Our main goal with this work is to show the accuracy and efficiency improvements that these relatively new techniques can provide for the segmentation of breast masses from mammograms. We also propose an easily reproducible quantitative analysis to as- sess the performance of breast mass segmentation methodologies based on widely accepted accuracy and running time measurements on public datasets, which will facilitate further comparisons for this segmentation problem. In particular, we use two publicly available datasets (DDSM-BCRP and INbreast) and propose the computa- tion of the running time taken for the methodology to produce a mass segmentation given an input image and the use of the Dice index to quantitatively measure the segmentation accuracy. For both databases, we show that our proposed methodology produces competitive results in terms of accuracy and running time.
ER  -


TY  - Preprint
T1  - Non-parametric Bayesian Learning with Deep Learning Structure and Its Applications in Wireless Networks
A1  - Erte Pan
A1  - Zhu Han
JO  - ArXiv e-prints
Y1  - 23 October, 2014
UR  - https://arxiv.org/abs/1410.4599
N2  - In this paper, we present an infinite hierarchical non-parametric Bayesian model to extract the hidden factors over observed data, where the number of hidden factors for each layer is unknown and can be potentially infinite. Moreover, the number of layers can also be infinite. We construct the model structure that allows continuous values for the hidden factors and weights, which makes the model suitable for various applications. We use the Metropolis-Hastings method to infer the model structure. Then the performance of the algorithm is evaluated by the experiments. Simulation results show that the model fits the underlying structure of simulated data.
ER  -


TY  - Preprint
T1  - cuDNN: Efficient Primitives for Deep Learning
A1  - Sharan Chetlur
A1  - Cliff Woolley
A1  - Philippe Vandermersch
A1  - Jonathan Cohen
A1  - John Tran
A1  - Bryan Catanzaro
A1  - Evan Shelhamer
JO  - ArXiv e-prints
Y1  - 17 December, 2014
UR  - https://arxiv.org/abs/1410.0759
N2  - We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36% on a standard model while also reducing memory consumption.
ER  -


TY  - Preprint
T1  - A Deep Learning Approach to Data-driven Parameterizations for Statistical Parametric Speech Synthesis
A1  - Prasanna Kumar Muthukumar
A1  - Alan W. Black
JO  - ArXiv e-prints
Y1  - 30 September, 2014
UR  - https://arxiv.org/abs/1409.8558
N2  - Nearly all Statistical Parametric Speech Synthesizers today use Mel Cepstral coefficients as the vocal tract parameterization of the speech signal. Mel Cepstral coefficients were never intended to work in a parametric speech synthesis framework, but as yet, there has been little success in creating a better parameterization that is more suited to synthesis. In this paper, we use deep learning algorithms to investigate a data-driven parameterization technique that is designed for the specific requirements of synthesis. We create an invertible, low-dimensional, noise-robust encoding of the Mel Log Spectrum by training a tapered Stacked Denoising Autoencoder (SDA). This SDA is then unwrapped and used as the initialization for a Multi-Layer Perceptron (MLP). The MLP is fine-tuned by training it to reconstruct the input at the output layer. This MLP is then split down the middle to form encoding and decoding networks. These networks produce a parameterization of the Mel Log Spectrum that is intended to better fulfill the requirements of synthesis. Results are reported for experiments conducted using this resulting parameterization with the ClusterGen speech synthesizer.
ER  -


TY  - Preprint
T1  - MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation
A1  - Arjun Jain
A1  - Jonathan Tompson
A1  - Yann LeCun
A1  - Christoph Bregler
JO  - ArXiv e-prints
Y1  - 28 September, 2014
UR  - https://arxiv.org/abs/1409.7963
N2  - In this work, we propose a novel and efficient method for articulated human pose estimation in videos using a convolutional network architecture, which incorporates both color and motion features. We propose a new human body pose dataset, FLIC-motion, that extends the FLIC dataset with additional motion features. We apply our architecture to this dataset and report significantly better performance than current state-of-the-art pose detection systems.
ER  -


TY  - Preprint
T1  - Deep Learning Representation using Autoencoder for 3D Shape Retrieval
A1  - Zhuotun Zhu
A1  - Xinggang Wang
A1  - Song Bai
A1  - Cong Yao
A1  - Xiang Bai
JO  - ArXiv e-prints
Y1  - 25 September, 2014
UR  - https://arxiv.org/abs/1409.7164
N2  - We study the problem of how to build a deep learning representation for 3D shape. Deep learning has shown to be very effective in variety of visual applications, such as image classification and object detection. However, it has not been successfully applied to 3D shape recognition. This is because 3D shape has complex structure in 3D space and there are limited number of 3D shapes for feature learning. To address these problems, we project 3D shapes into 2D space and use autoencoder for feature learning on the 2D images. High accuracy 3D shape retrieval performance is obtained by aggregating the features learned on 2D images. In addition, we show the proposed deep learning feature is complementary to conventional local image descriptors. By combing the global deep learning representation and the local descriptor representation, our method can obtain the state-of-the-art performance on 3D shape retrieval benchmarks.
ER  -


TY  - Preprint
T1  - Transfer Learning for Video Recognition with Scarce Training Data for Deep Convolutional Neural Network
A1  - Yu-Chuan Su
A1  - Tzu-Hsuan Chiu
A1  - Chun-Yen Yeh
A1  - Hsin-Fu Huang
A1  - Winston H. Hsu
JO  - ArXiv e-prints
Y1  - 15 June, 2015
UR  - https://arxiv.org/abs/1409.4127
N2  - Unconstrained video recognition and Deep Convolution Network (DCN) are two active topics in computer vision recently. In this work, we apply DCNs as frame-based recognizers for video recognition. Our preliminary studies, however, show that video corpora with complete ground truth are usually not large and diverse enough to learn a robust model. The networks trained directly on the video data set suffer from significant overfitting and have poor recognition rate on the test set. The same lack-of-training-sample problem limits the usage of deep models on a wide range of computer vision problems where obtaining training data are difficult. To overcome the problem, we perform transfer learning from images to videos to utilize the knowledge in the weakly labeled image corpus for video recognition. The image corpus help to learn important visual patterns for natural images, while these patterns are ignored by models trained only on the video corpus. Therefore, the resultant networks have better generalizability and better recognition rate. We show that by means of transfer learning from image to video, we can learn a frame-based recognizer with only 4k videos. Because the image corpus is weakly labeled, the entire learning process requires only 4k annotated instances, which is far less than the million scale image data sets required by previous works. The same approach may be applied to other visual recognition tasks where only scarce training data is available, and it improves the applicability of DCNs in various computer vision problems. Our experiments also reveal the correlation between meta-parameters and the performance of DCNs, given the properties of the target problem and data. These results lead to a heuristic for meta-parameter selection for future researches, which does not rely on the time consuming meta-parameter search.
ER  -


TY  - Preprint
T1  - Building Program Vector Representations for Deep Learning
A1  - Lili Mou
A1  - Ge Li
A1  - Yuxuan Liu
A1  - Hao Peng
A1  - Zhi Jin
A1  - Yan Xu
A1  - Lu Zhang
JO  - ArXiv e-prints
Y1  - 11 September, 2014
UR  - https://arxiv.org/abs/1409.3358
N2  - Deep learning has made significant breakthroughs in various fields of artificial intelligence. Advantages of deep learning include the ability to capture highly complicated features, weak involvement of human engineering, etc. However, it is still virtually impossible to use deep learning to analyze programs since deep architectures cannot be trained effectively with pure back propagation. In this pioneering paper, we propose the &#34;coding criterion&#34; to build program vector representations, which are the premise of deep learning for program analysis. Our representation learning approach directly makes deep learning a reality in this new field. We evaluate the learned vector representations both qualitatively and quantitatively. We conclude, based on the experiments, the coding criterion is successful in building program representations. To evaluate whether deep learning is beneficial for program analysis, we feed the representations to deep neural networks, and achieve higher accuracy in the program classification task than &#34;shallow&#34; methods, such as logistic regression and the support vector machine. This result confirms the feasibility of deep learning to analyze programs. It also gives primary evidence of its success in this new field. We believe deep learning will become an outstanding technique for program analysis in the near future.
ER  -


TY  - Preprint
T1  - Collaborative Deep Learning for Recommender Systems
A1  - Hao Wang
A1  - Naiyan Wang
A1  - Dit-Yan Yeung
JO  - ArXiv e-prints
Y1  - 18 June, 2015
UR  - https://arxiv.org/abs/1409.2944
N2  - Collaborative filtering (CF) is a successful approach commonly used by many recommender systems. Conventional CF-based methods use the ratings given to items by users as the sole source of information for learning to make recommendation. However, the ratings are often very sparse in many applications, causing CF-based methods to degrade significantly in their recommendation performance. To address this sparsity problem, auxiliary information such as item content information may be utilized. Collaborative topic regression (CTR) is an appealing recent method taking this approach which tightly couples the two components that learn from two different sources of information. Nevertheless, the latent representation learned by CTR may not be very effective when the auxiliary information is very sparse. To address this problem, we generalize recent advances in deep learning from i.i.d. input to non-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian model called collaborative deep learning (CDL), which jointly performs deep representation learning for the content information and collaborative filtering for the ratings (feedback) matrix. Extensive experiments on three real-world datasets from different domains show that CDL can significantly advance the state of the art.
ER  -


TY  - Preprint
T1  - Learning Deep Representation for Face Alignment with Auxiliary Attributes
A1  - Zhanpeng Zhang
A1  - Ping Luo
A1  - Chen Change Loy
A1  - Xiaoou Tang
JO  - ArXiv e-prints
Y1  - 11 August, 2015
UR  - https://arxiv.org/abs/1408.3967
N2  - In this study, we show that landmark detection or face alignment task is not a single and independent problem. Instead, its robustness can be greatly improved with auxiliary information. Specifically, we jointly optimize landmark detection together with the recognition of heterogeneous but subtly correlated facial attributes, such as gender, expression, and appearance attributes. This is non-trivial since different attribute inference tasks have different learning difficulties and convergence rates. To address this problem, we formulate a novel tasks-constrained deep model, which not only learns the inter-task correlation but also employs dynamic task coefficients to facilitate the optimization convergence when learning multiple complex tasks. Extensive evaluations show that the proposed task-constrained learning (i) outperforms existing face alignment methods, especially in dealing with faces with severe occlusion and pose variation, and (ii) reduces model complexity drastically compared to the state-of-the-art methods based on cascaded deep model.
ER  -


TY  - Preprint
T1  - Deep Metric Learning for Practical Person Re-Identification
A1  - Dong Yi
A1  - Zhen Lei
A1  - Stan Z. Li
JO  - ArXiv e-prints
Y1  - 18 July, 2014
UR  - https://arxiv.org/abs/1407.4979
N2  - Various hand-crafted features and metric learning methods prevail in the field of person re-identification. Compared to these methods, this paper proposes a more general way that can learn a similarity metric from image pixels directly. By using a &#34;siamese&#34; deep neural network, the proposed method can jointly learn the color feature, texture feature and metric in a unified framework. The network has a symmetry structure with two sub-networks which are connected by Cosine function. To deal with the big variations of person images, binomial deviance is used to evaluate the cost between similarities and labels, which is proved to be robust to outliers.
ER  -


TY  - Preprint
T1  - Learning Deep Structured Models
A1  - Liang-Chieh Chen
A1  - Alexander G. Schwing
A1  - Alan L. Yuille
A1  - Raquel Urtasun
JO  - ArXiv e-prints
Y1  - 27 April, 2015
UR  - https://arxiv.org/abs/1407.2538
N2  - Many problems in real-world applications involve predicting several random variables which are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such relationships. The goal of this paper is to combine MRFs with deep learning algorithms to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as multi-class classification of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains.
ER  -


TY  - Preprint
T1  - Deep Learning Multi-View Representation for Face Recognition
A1  - Zhenyao Zhu
A1  - Ping Luo
A1  - Xiaogang Wang
A1  - Xiaoou Tang
JO  - ArXiv e-prints
Y1  - 26 June, 2014
UR  - https://arxiv.org/abs/1406.6947
N2  - Various factors, such as identities, views (poses), and illuminations, are coupled in face images. Disentangling the identity and view representations is a major challenge in face recognition. Existing face recognition systems either use handcrafted features or learn features discriminatively to improve recognition accuracy. This is different from the behavior of human brain. Intriguingly, even without accessing 3D data, human not only can recognize face identity, but can also imagine face images of a person under different viewpoints given a single 2D image, making face perception in the brain robust to view changes. In this sense, human brain has learned and encoded 3D face models from 2D images. To take into account this instinct, this paper proposes a novel deep neural net, named multi-view perceptron (MVP), which can untangle the identity and view features, and infer a full spectrum of multi-view images in the meanwhile, given a single 2D face image. The identity features of MVP achieve superior performance on the MultiPIE dataset. MVP is also capable to interpolate and predict images under viewpoints that are unobserved in the training data.
ER  -


TY  - Preprint
T1  - Semi-Supervised Learning with Deep Generative Models
A1  - Diederik P. Kingma
A1  - Danilo J. Rezende
A1  - Shakir Mohamed
A1  - Max Welling
JO  - ArXiv e-prints
Y1  - 31 October, 2014
UR  - https://arxiv.org/abs/1406.5298
N2  - The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.
ER  -


TY  - Preprint
T1  - Deep Learning Face Representation by Joint Identification-Verification
A1  - Yi Sun
A1  - Xiaogang Wang
A1  - Xiaoou Tang
JO  - ArXiv e-prints
Y1  - 18 June, 2014
UR  - https://arxiv.org/abs/1406.4773
N2  - The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15% face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67%.
ER  -


TY  - Preprint
T1  - Heterogeneous Multi-task Learning for Human Pose Estimation with Deep Convolutional Neural Network
A1  - Sijin Li
A1  - Zhi-Qiang Liu
A1  - Antoni B. Chan
JO  - ArXiv e-prints
Y1  - 13 June, 2014
UR  - https://arxiv.org/abs/1406.3474
N2  - We propose an heterogeneous multi-task learning framework for human pose estimation from monocular image with deep convolutional neural network. In particular, we simultaneously learn a pose-joint regressor and a sliding-window body-part detector in a deep network architecture. We show that including the body-part detection task helps to regularize the network, directing it to converge to a good solution. We report competitive and state-of-art results on several data sets. We also empirically show that the learned neurons in the middle layer of our network are tuned to localized body parts.
ER  -


TY  - Preprint
T1  - Deep Learning in Neural Networks: An Overview
A1  - Juergen Schmidhuber
JO  - ArXiv e-prints
Y1  - 8 October, 2014
UR  - https://arxiv.org/abs/1404.7828
N2  - In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning &amp; evolutionary computation, and indirect search for short programs encoding deep and large networks.
ER  -


TY  - Preprint
T1  - Learning Fine-grained Image Similarity with Deep Ranking
A1  - Jiang Wang
A1  - Yang song
A1  - Thomas Leung
A1  - Chuck Rosenberg
A1  - Jinbin Wang
A1  - James Philbin
A1  - Bo Chen
A1  - Ying Wu
JO  - ArXiv e-prints
Y1  - 17 April, 2014
UR  - https://arxiv.org/abs/1404.4661
N2  - Learning fine-grained image similarity is a challenging task. It needs to capture between-class and within-class image differences. This paper proposes a deep ranking model that employs deep learning techniques to learn similarity metric directly from images.It has higher learning capability than models based on hand-crafted features. A novel multiscale network structure has been developed to describe the images effectively. An efficient triplet sampling algorithm is proposed to learn the model with distributed asynchronized stochastic gradient. Extensive experiments show that the proposed algorithm outperforms models based on hand-crafted visual features and deep classification models.
ER  -


TY  - Preprint
T1  - PCANet: A Simple Deep Learning Baseline for Image Classification?
A1  - Tsung-Han Chan
A1  - Kui Jia
A1  - Shenghua Gao
A1  - Jiwen Lu
A1  - Zinan Zeng
A1  - Yi Ma
JO  - ArXiv e-prints
Y1  - 28 August, 2014
UR  - https://arxiv.org/abs/1404.3606
N2  - In this work, we propose a very simple deep learning network for image classification which comprises only the very basic data processing components: cascaded principal component analysis (PCA), binary hashing, and block-wise histograms. In the proposed architecture, PCA is employed to learn multistage filter banks. It is followed by simple binary hashing and block histograms for indexing and pooling. This architecture is thus named as a PCA network (PCANet) and can be designed and learned extremely easily and efficiently. For comparison and better understanding, we also introduce and study two simple variations to the PCANet, namely the RandNet and LDANet. They share the same topology of PCANet but their cascaded filters are either selected randomly or learned from LDA. We have tested these basic networks extensively on many benchmark visual datasets for different tasks, such as LFW for face verification, MultiPIE, Extended Yale B, AR, FERET datasets for face recognition, as well as MNIST for hand-written digits recognition. Surprisingly, for all tasks, such a seemingly naive PCANet model is on par with the state of the art features, either prefixed, highly hand-crafted or carefully learned (by DNNs). Even more surprisingly, it sets new records for many classification tasks in Extended Yale B, AR, FERET datasets, and MNIST variations. Additional experiments on other public datasets also demonstrate the potential of the PCANet serving as a simple but highly competitive baseline for texture classification and object recognition.
ER  -


TY  - Preprint
T1  - Learning Deep Convolutional Features for MRI Based Alzheimer&#39;s Disease Classification
A1  - Fayao Liu
A1  - Chunhua Shen
JO  - ArXiv e-prints
Y1  - 27 April, 2014
UR  - https://arxiv.org/abs/1404.3366
N2  - Effective and accurate diagnosis of Alzheimer&#39;s disease (AD) or mild cognitive impairment (MCI) can be critical for early treatment and thus has attracted more and more attention nowadays. Since first introduced, machine learning methods have been gaining increasing popularity for AD related research. Among the various identified biomarkers, magnetic resonance imaging (MRI) are widely used for the prediction of AD or MCI. However, before a machine learning algorithm can be applied, image features need to be extracted to represent the MRI images. While good representations can be pivotal to the classification performance, almost all the previous studies typically rely on human labelling to find the regions of interest (ROI) which may be correlated to AD, such as hippocampus, amygdala, precuneus, etc. This procedure requires domain knowledge and is costly and tedious.
ER  -


TY  - Preprint
T1  - Sparse Coding: A Deep Learning using Unlabeled Data for High - Level Representation
A1  - R. Vidya
A1  - Dr. G. M. Nasira
A1  - R. P. Jaia Priyankka
JO  - ArXiv e-prints
Y1  - 6 April, 2014
UR  - https://arxiv.org/abs/1404.1559
N2  - Sparse coding algorithm is an learning algorithm mainly for unsupervised feature for finding succinct, a little above high - level Representation of inputs, and it has successfully given a way for Deep learning. Our objective is to use High - Level Representation data in form of unlabeled category to help unsupervised learning task. when compared with labeled data, unlabeled data is easier to acquire because, unlike labeled data it does not follow some particular class labels. This really makes the Deep learning wider and applicable to practical problems and learning. The main problem with sparse coding is it uses Quadratic loss function and Gaussian noise mode. So, its performs is very poor when binary or integer value or other Non- Gaussian type data is applied. Thus first we propose an algorithm for solving the L1 - regularized convex optimization algorithm for the problem to allow High - Level Representation of unlabeled data. Through this we derive a optimal solution for describing an approach to Deep learning algorithm by using sparse code.
ER  -


TY  - Preprint
T1  - Learning Deep Face Representation
A1  - Haoqiang Fan
A1  - Zhimin Cao
A1  - Yuning Jiang
A1  - Qi Yin
A1  - Chinchilla Doudou
JO  - ArXiv e-prints
Y1  - 11 March, 2014
UR  - https://arxiv.org/abs/1403.2802
N2  - Face representation is a crucial step of face recognition systems. An optimal face representation should be discriminative, robust, compact, and very easy-to-implement. While numerous hand-crafted and learning-based representations have been proposed, considerable room for improvement is still present. In this paper, we present a very easy-to-implement deep learning framework for face representation. Our method bases on a new structure of deep network (called Pyramid CNN). The proposed Pyramid CNN adopts a greedy-filter-and-down-sample operation, which enables the training procedure to be very fast and computation-efficient. In addition, the structure of Pyramid CNN can naturally incorporate feature sharing across multi-scale face representations, increasing the discriminative ability of resulting representation. Our basic network is capable of achieving high recognition accuracy ($85.8\%$ on LFW benchmark) with only 8 dimension representation. When extended to feature-sharing Pyramid CNN, our system achieves the state-of-the-art performance ($97.3\%$) on LFW benchmark. We also introduce a new benchmark of realistic face images on social network and validate our proposed representation has a good ability of generalization.
ER  -


TY  - Preprint
T1  - To go deep or wide in learning?
A1  - Gaurav Pandey
A1  - Ambedkar Dukkipati
JO  - ArXiv e-prints
Y1  - 23 February, 2014
UR  - https://arxiv.org/abs/1402.5634
N2  - To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a two-layered supervised learning model, or learn the features directly using a deep (multi-layered) model. While the first approach is very problem-specific, the second approach has computational overheads in learning multiple layers and fine-tuning of the model. In this paper, we propose an approach called wide learning based on arc-cosine kernels, that learns a single layer of infinite width. We propose exact and inexact learning strategies for wide learning and show that wide learning with single layer outperforms single layer as well as deep architectures of finite width for some benchmark datasets.
ER  -


TY  - Preprint
T1  - Deep learning for class-generic object detection
A1  - Brody Huval
A1  - Adam Coates
A1  - Andrew Ng
JO  - ArXiv e-prints
Y1  - 24 December, 2013
UR  - https://arxiv.org/abs/1312.6885
N2  - We investigate the use of deep neural networks for the novel task of class generic object detection. We show that neural networks originally designed for image recognition can be trained to detect objects within images, regardless of their class, including objects for which no bounding box labels have been provided. In addition, we show that bounding box labels yield a 1% performance increase on the ImageNet recognition challenge.
ER  -


TY  - Preprint
T1  - Learning Paired-associate Images with An Unsupervised Deep Learning Architecture
A1  - Ti Wang
A1  - Daniel L. Silver
JO  - ArXiv e-prints
Y1  - 10 January, 2014
UR  - https://arxiv.org/abs/1312.6171
N2  - This paper presents an unsupervised multi-modal learning system that learns associative representation from two input modalities, or channels, such that input on one channel will correctly generate the associated response at the other and vice versa. In this way, the system develops a kind of supervised classification model meant to simulate aspects of human associative memory. The system uses a deep learning architecture (DLA) composed of two input/output channels formed from stacked Restricted Boltzmann Machines (RBM) and an associative memory network that combines the two channels. The DLA is trained on pairs of MNIST handwritten digit images to develop hierarchical features and associative representations that are able to reconstruct one image given its paired-associate. Experiments show that the multi-modal learning system generates models that are as accurate as back-propagation networks but with the advantage of a bi-directional network and unsupervised learning from either paired or non-paired training examples.
ER  -


TY  - Preprint
T1  - Exact solutions to the nonlinear dynamics of learning in deep linear neural networks
A1  - Andrew M. Saxe
A1  - James L. McClelland
A1  - Surya Ganguli
JO  - ArXiv e-prints
Y1  - 19 February, 2014
UR  - https://arxiv.org/abs/1312.6120
N2  - Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.
ER  -


TY  - Preprint
T1  - Deep learning for neuroimaging: a validation study
A1  - Sergey M. Plis
A1  - Devon R. Hjelm
A1  - Ruslan Salakhutdinov
A1  - Vince D. Calhoun
JO  - ArXiv e-prints
Y1  - 19 February, 2014
UR  - https://arxiv.org/abs/1312.5847
N2  - Deep learning methods have recently made notable advances in the tasks of classification and representation learning. These tasks are important for brain imaging and neuroscience discovery, making the methods attractive for porting to a neuroimager&#39;s toolbox. Success of these methods is, in part, explained by the flexibility of deep learning models. However, this flexibility makes the process of porting to new areas a difficult parameter optimization problem. In this work we demonstrate our results (and feasible parameter ranges) in application of deep learning methods to structural and functional brain imaging data. We also describe a novel constraint-based approach to visualizing high dimensional data. We use it to analyze the effect of parameter choices on data transformations. Our results show that deep learning methods are able to learn physiologically important representations and detect latent relations in neuroimaging data.
ER  -


TY  - Preprint
T1  - Unsupervised Feature Learning by Deep Sparse Coding
A1  - Yunlong He
A1  - Koray Kavukcuoglu
A1  - Yun Wang
A1  - Arthur Szlam
A1  - Yanjun Qi
JO  - ArXiv e-prints
Y1  - 19 December, 2013
UR  - https://arxiv.org/abs/1312.5783
N2  - In this paper, we propose a new unsupervised feature learning framework, namely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layer architecture for visual object recognition tasks. The main innovation of the framework is that it connects the sparse-encoders from different layers by a sparse-to-dense module. The sparse-to-dense module is a composition of a local spatial pooling step and a low-dimensional embedding process, which takes advantage of the spatial smoothness information in the image. As a result, the new method is able to learn several levels of sparse representation of the image which capture features at a variety of abstraction levels and simultaneously preserve the spatial smoothness between the neighboring image patches. Combining the feature representations from multiple layers, DeepSC achieves the state-of-the-art performance on multiple object recognition tasks.
ER  -


TY  - Preprint
T1  - Playing Atari with Deep Reinforcement Learning
A1  - Volodymyr Mnih
A1  - Koray Kavukcuoglu
A1  - David Silver
A1  - Alex Graves
A1  - Ioannis Antonoglou
A1  - Daan Wierstra
A1  - Martin Riedmiller
JO  - ArXiv e-prints
Y1  - 19 December, 2013
UR  - https://arxiv.org/abs/1312.5602
N2  - We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.
ER  -


TY  - Preprint
T1  - Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds
A1  - Irina Sergienya
A1  - Hinrich SchÃ¼tze
JO  - ArXiv e-prints
Y1  - 18 February, 2014
UR  - https://arxiv.org/abs/1312.5559
N2  - There are two main approaches to the distributed representation of words: low-dimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional-model vectors - as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.
ER  -


TY  - Preprint
T1  - My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013
A1  - JÃ¼rgen Schmidhuber
JO  - ArXiv e-prints
Y1  - 19 December, 2013
UR  - https://arxiv.org/abs/1312.5548
N2  - Deep Learning has attracted significant attention in recent years. Here I present a brief overview of my first Deep Learner of 1991, and its historic context, with a timeline of Deep Learning highlights.
ER  -


TY  - Preprint
T1  - Generative NeuroEvolution for Deep Learning
A1  - Phillip Verbancsics
A1  - Josh Harguess
JO  - ArXiv e-prints
Y1  - 18 December, 2013
UR  - https://arxiv.org/abs/1312.5355
N2  - An important goal for the machine learning (ML) community is to create approaches that can learn solutions with human-level capability. One domain where humans have held a significant advantage is visual processing. A significant approach to addressing this gap has been machine learning approaches that are inspired from the natural systems, such as artificial neural networks (ANNs), evolutionary computation (EC), and generative and developmental systems (GDS). Research into deep learning has demonstrated that such architectures can achieve performance competitive with humans on some visual tasks; however, these systems have been primarily trained through supervised and unsupervised learning algorithms. Alternatively, research is showing that evolution may have a significant role in the development of visual systems. Thus this paper investigates the role neuro-evolution (NE) can take in deep learning. In particular, the Hypercube-based NeuroEvolution of Augmenting Topologies is a NE approach that can effectively learn large neural structures by training an indirect encoding that compresses the ANN weight pattern as a function of geometry. The results show that HyperNEAT struggles with performing image classification by itself, but can be effective in training a feature extractor that other ML approaches can learn from. Thus NeuroEvolution combined with other ML methods provides an intriguing area of research that can replicate the processes in nature.
ER  -


TY  - Preprint
T1  - Deep Learning Embeddings for Discontinuous Linguistic Units
A1  - Wenpeng Yin
A1  - Hinrich SchÃ¼tze
JO  - ArXiv e-prints
Y1  - 19 December, 2013
UR  - https://arxiv.org/abs/1312.5129
N2  - Deep learning embeddings have been successfully used for many natural language processing problems. Embeddings are mostly computed for word forms although a number of recent papers have extended this to other linguistic units like morphemes and phrases. In this paper, we argue that learning embeddings for discontinuous linguistic units should also be considered. In an experimental evaluation on coreference resolution, we show that such embeddings perform better than word form embeddings.
ER  -


TY  - Preprint
T1  - Learning Deep Representations By Distributed Random Samplings
A1  - Xiao-Lei Zhang
JO  - ArXiv e-prints
Y1  - 16 December, 2013
UR  - https://arxiv.org/abs/1312.4405
N2  - In this paper, we propose an extremely simple deep model for the unsupervised nonlinear dimensionality reduction -- deep distributed random samplings, which performs like a stack of unsupervised bootstrap aggregating. First, its network structure is novel: each layer of the network is a group of mutually independent $k$-centers clusterings. Second, its learning method is extremely simple: the $k$ centers of each clustering are only $k$ randomly selected examples from the training data; for small-scale data sets, the $k$ centers are further randomly reconstructed by a simple cyclic-shift operation. Experimental results on nonlinear dimensionality reduction show that the proposed method can learn abstract representations on both large-scale and small-scale problems, and meanwhile is much faster than deep neural networks on large-scale problems.
ER  -


TY  - Preprint
T1  - Learning Factored Representations in a Deep Mixture of Experts
A1  - David Eigen
A1  - Marc&#39;Aurelio Ranzato
A1  - Ilya Sutskever
JO  - ArXiv e-prints
Y1  - 9 March, 2014
UR  - https://arxiv.org/abs/1312.4314
N2  - Mixtures of Experts combine the outputs of several &#34;expert&#34; networks, each of which specializes in a different part of the input space. This is achieved by training a &#34;gating&#34; network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent (&#34;where&#34;) experts at the first layer, and class-specific (&#34;what&#34;) experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.
ER  -


TY  - Preprint
T1  - Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks
A1  - Caglar Gulcehre
A1  - Kyunghyun Cho
A1  - Razvan Pascanu
A1  - Yoshua Bengio
JO  - ArXiv e-prints
Y1  - 1 September, 2014
UR  - https://arxiv.org/abs/1311.1780
N2  - In this paper we propose and investigate a novel nonlinear unit, called $L_p$ unit, for deep neural networks. The proposed $L_p$ unit receives signals from several projections of a subset of units in the layer below and computes a normalized $L_p$ norm. We notice two interesting interpretations of the $L_p$ unit. First, the proposed unit can be understood as a generalization of a number of conventional pooling operators such as average, root-mean-square and max pooling widely used in, for instance, convolutional neural networks (CNN), HMAX models and neocognitrons. Furthermore, the $L_p$ unit is, to a certain degree, similar to the recently proposed maxout unit (Goodfellow et al., 2013) which achieved the state-of-the-art object recognition results on a number of benchmark datasets. Secondly, we provide a geometrical interpretation of the activation function based on which we argue that the $L_p$ unit is more efficient at representing complex, nonlinear separating boundaries. Each $L_p$ unit defines a superelliptic boundary, with its exact shape defined by the order $p$. We claim that this makes it possible to model arbitrarily shaped, curved boundaries more efficiently by combining a few $L_p$ units of different orders. This insight justifies the need for learning different orders for each unit in the model. We empirically evaluate the proposed $L_p$ units on a number of datasets and show that multilayer perceptrons (MLP) consisting of the $L_p$ units achieve the state-of-the-art results on a number of benchmark datasets. Furthermore, we evaluate the proposed $L_p$ unit on the recently proposed deep recurrent neural networks (RNN).
ER  -


TY  - Preprint
T1  - Provable Bounds for Learning Some Deep Representations
A1  - Sanjeev Arora
A1  - Aditya Bhaskara
A1  - Rong Ge
A1  - Tengyu Ma
JO  - ArXiv e-prints
Y1  - 23 October, 2013
UR  - https://arxiv.org/abs/1310.6343
N2  - We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an $n$ node multilayer neural net that has degree at most $n^Î³$ for some $Î³&lt;1$ and each edge has a random edge weight in $[-1,1]$. Our algorithm learns {\em almost all} networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model.
ER  -


TY  - Preprint
T1  - Learning Deep Representation Without Parameter Inference for Nonlinear Dimensionality Reduction
A1  - Xiao-Lei Zhang
JO  - ArXiv e-prints
Y1  - 2 January, 2014
UR  - https://arxiv.org/abs/1308.4922
N2  - Unsupervised deep learning is one of the most powerful representation learning techniques. Restricted Boltzman machine, sparse coding, regularized auto-encoders, and convolutional neural networks are pioneering building blocks of deep learning. In this paper, we propose a new building block -- distributed random models. The proposed method is a special full implementation of the product of experts: (i) each expert owns multiple hidden units and different experts have different numbers of hidden units; (ii) the model of each expert is a k-center clustering, whose k-centers are only uniformly sampled examples, and whose output (i.e. the hidden units) is a sparse code that only the similarity values from a few nearest neighbors are reserved. The relationship between the pioneering building blocks, several notable research branches and the proposed method is analyzed. Experimental results show that the proposed deep model can learn better representations than deep belief networks and meanwhile can train a much larger network with much less time than deep belief networks.
ER  -


TY  - Preprint
T1  - Deep Learning by Scattering
A1  - StÃ©phane Mallat
A1  - IrÃ¨ne Waldspurger
JO  - ArXiv e-prints
Y1  - 25 June, 2015
UR  - https://arxiv.org/abs/1306.5532
N2  - We introduce general scattering transforms as mathematical models of deep neural networks with l2 pooling. Scattering networks iteratively apply complex valued unitary operators, and the pooling is performed by a complex modulus. An expected scattering defines a contractive representation of a high-dimensional probability distribution, which preserves its mean-square norm. We show that unsupervised learning can be casted as an optimization of the space contraction to preserve the volume occupied by unlabeled examples, at each layer of the network. Supervised learning and classification are performed with an averaged scattering, which provides scattering estimations for multiple classes.
ER  -


TY  - Preprint
T1  - Predicting Parameters in Deep Learning
A1  - Misha Denil
A1  - Babak Shakibi
A1  - Laurent Dinh
A1  - Marc&#39;Aurelio Ranzato
A1  - Nando de Freitas
JO  - ArXiv e-prints
Y1  - 27 October, 2014
UR  - https://arxiv.org/abs/1306.0543
N2  - We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy.
ER  -


TY  - Preprint
T1  - Deep Learning using Linear Support Vector Machines
A1  - Yichuan Tang
JO  - ArXiv e-prints
Y1  - 21 February, 2015
UR  - https://arxiv.org/abs/1306.0239
N2  - Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these &#34;deep learning&#34; models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop&#39;s face expression recognition challenge.
ER  -


TY  - Preprint
T1  - Deep Learning of Representations: Looking Forward
A1  - Yoshua Bengio
JO  - ArXiv e-prints
Y1  - 6 June, 2013
UR  - https://arxiv.org/abs/1305.0445
N2  - Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts. Although the study of deep learning has already led to impressive theoretical results, learning algorithms and breakthrough experiments, several challenges lie ahead. This paper proposes to examine some of these challenges, centering on the questions of scaling deep learning algorithms to much larger models and datasets, reducing optimization difficulties due to ill-conditioning or local minima, designing more efficient and powerful inference and sampling procedures, and learning to disentangle the factors of variation underlying the observed data. It also proposes a few forward-looking research directions aimed at overcoming these challenges.
ER  -


TY  - Preprint
T1  - Transfer Learning for Voice Activity Detection: A Denoising Deep Neural Network Perspective
A1  - Xiao-Lei Zhang
A1  - Ji Wu
JO  - ArXiv e-prints
Y1  - 8 March, 2013
UR  - https://arxiv.org/abs/1303.2104
N2  - Mismatching problem between the source and target noisy corpora severely hinder the practical use of the machine-learning-based voice activity detection (VAD). In this paper, we try to address this problem in the transfer learning prospective. Transfer learning tries to find a common learning machine or a common feature subspace that is shared by both the source corpus and the target corpus. The denoising deep neural network is used as the learning machine. Three transfer techniques, which aim to learn common feature representations, are used for analysis. Experimental results demonstrate the effectiveness of the transfer learning schemes on the mismatch problem.
ER  -


TY  - Preprint
T1  - Understanding Boltzmann Machine and Deep Learning via A Confident Information First Principle
A1  - Xiaozhao Zhao
A1  - Yuexian Hou
A1  - Qian Yu
A1  - Dawei Song
A1  - Wenjie Li
JO  - ArXiv e-prints
Y1  - 9 October, 2013
UR  - https://arxiv.org/abs/1302.3931
N2  - Typical dimensionality reduction methods focus on directly reducing the number of random variables while retaining maximal variations in the data. In this paper, we consider the dimensionality reduction in parameter spaces of binary multivariate distributions. We propose a general Confident-Information-First (CIF) principle to maximally preserve parameters with confident estimates and rule out unreliable or noisy parameters. Formally, the confidence of a parameter can be assessed by its Fisher information, which establishes a connection with the inverse variance of any unbiased estimate for the parameter via the CramÃ©r-Rao bound. We then revisit Boltzmann machines (BM) and theoretically show that both single-layer BM without hidden units (SBM) and restricted BM (RBM) can be solidly derived using the CIF principle. This can not only help us uncover and formalize the essential parts of the target density that SBM and RBM capture, but also suggest that the deep neural network consisting of several layers of RBM can be seen as the layer-wise application of CIF. Guided by the theoretical analysis, we develop a sample-specific CIF-based contrastive divergence (CD-CIF) algorithm for SBM and a CIF-based iterative projection procedure (IP) for RBM. Both CD-CIF and IP are studied in a series of density estimation experiments.
ER  -


TY  - Preprint
T1  - Two SVDs produce more focal deep learning representations
A1  - Hinrich Schuetze
A1  - Christian Scheible
JO  - ArXiv e-prints
Y1  - 11 May, 2013
UR  - https://arxiv.org/abs/1301.3627
N2  - A key characteristic of work on deep learning and neural networks in general is that it relies on representations of the input that support generalization, robust inference, domain adaptation and other desirable functionalities. Much recent progress in the field has focused on efficient and effective methods for computing representations. In this paper, we propose an alternative method that is more efficient than prior work and produces representations that have a property we call focality -- a property we hypothesize to be important for neural network representations. The method consists of a simple application of two consecutive SVDs and is inspired by Anandkumar (2012).
ER  -


TY  - Preprint
T1  - Feature Learning in Deep Neural Networks - Studies on Speech Recognition Tasks
A1  - Dong Yu
A1  - Michael L. Seltzer
A1  - Jinyu Li
A1  - Jui-Ting Huang
A1  - Frank Seide
JO  - ArXiv e-prints
Y1  - 8 March, 2013
UR  - https://arxiv.org/abs/1301.3605
N2  - Recent studies have shown that deep neural networks (DNNs) perform significantly better than shallow networks and Gaussian mixture models (GMMs) on large vocabulary speech recognition tasks. In this paper, we argue that the improved accuracy achieved by the DNNs is the result of their ability to extract discriminative internal representations that are robust to the many sources of variability in speech signals. We show that these representations become increasingly insensitive to small perturbations in the input with increasing network depth, which leads to better speech recognition performance with deeper networks. We also show that DNNs cannot extrapolate to test samples that are substantially different from the training examples. If the training data are sufficiently representative, however, internal features learned by the DNN are relatively stable with respect to speaker differences, bandwidth differences, and environment distortion. This enables DNN-based recognizers to perform as well or better than state-of-the-art systems based on GMMs or shallow networks without the need for explicit model adaptation or feature normalization.
ER  -


TY  - Preprint
T1  - Deep Learning for Detecting Robotic Grasps
A1  - Ian Lenz
A1  - Honglak Lee
A1  - Ashutosh Saxena
JO  - ArXiv e-prints
Y1  - 21 August, 2014
UR  - https://arxiv.org/abs/1301.3592
N2  - We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast, as well as robust, we present a two-step cascaded structure with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs well, for which we present a method to apply structured regularization on the weights based on multimodal group regularization. We demonstrate that our method outperforms the previous state-of-the-art methods in robotic grasp detection, and can be used to successfully execute grasps on two different robotic platforms.
ER  -


TY  - Preprint
T1  - Layer-wise learning of deep generative models
A1  - Ludovic Arnold
A1  - Yann Ollivier
JO  - ArXiv e-prints
Y1  - 16 February, 2013
UR  - https://arxiv.org/abs/1212.1524
N2  - When using deep, multi-layered architectures to build generative models of data, it is difficult to train all layers at once. We propose a layer-wise training procedure admitting a performance guarantee compared to the global optimum. It is based on an optimistic proxy of future performance, the best latent marginal. We interpret auto-encoders in this setting as generative models, by showing that they train a lower bound of this criterion. We test the new learning procedure against a state of the art method (stacked RBMs), and find it to improve performance. Both theory and experiments highlight the importance, when training deep architectures, of using an inference model (from data to hidden variables) richer than the generative model (from hidden variables to data).
ER  -


TY  - Preprint
T1  - Learning where to Attend with Deep Architectures for Image Tracking
A1  - Misha Denil
A1  - Loris Bazzani
A1  - Hugo Larochelle
A1  - Nando de Freitas
JO  - ArXiv e-prints
Y1  - 16 September, 2011
UR  - https://arxiv.org/abs/1109.3737
N2  - We discuss an attentional model for simultaneous object tracking and recognition that is driven by gaze data. Motivated by theories of perception, the model consists of two interacting pathways: identity and control, intended to mirror the what and where pathways in neuroscience models. The identity pathway models object appearance and performs classification using deep (factored)-Restricted Boltzmann Machines. At each point in time the observations consist of foveated images, with decaying resolution toward the periphery of the gaze. The control pathway models the location, orientation, scale and speed of the attended object. The posterior distribution of these states is estimated with particle filtering. Deeper in the control pathway, we encounter an attentional mechanism that learns to select gazes so as to minimize tracking uncertainty. Unlike in our previous work, we introduce gaze selection strategies which operate in the presence of partial information and on a continuous action space. We show that a straightforward extension of the existing approach to the partial information setting results in poor performance, and we propose an alternative method based on modeling the reward surface as a Gaussian Process. This approach gives good performance in the presence of partial information and allows us to expand the action space from a small, discrete set of fixation points to a continuous domain.
ER  -


TY  - Preprint
T1  - Deep Self-Taught Learning for Handwritten Character Recognition
A1  - FrÃ©dÃ©ric Bastien
A1  - Yoshua Bengio
A1  - Arnaud Bergeron
A1  - Nicolas Boulanger-Lewandowski
A1  - Thomas Breuel
A1  - Youssouf Chherawala
A1  - Moustapha Cisse
A1  - Myriam CÃ´tÃ©
A1  - Dumitru Erhan
A1  - Jeremy Eustache
A1  - Xavier Glorot
A1  - Xavier Muller
A1  - Sylvain Pannetier Lebeuf
A1  - Razvan Pascanu
A1  - Salah Rifai
A1  - Francois Savard
A1  - Guillaume Sicard
JO  - ArXiv e-prints
Y1  - 18 September, 2010
UR  - https://arxiv.org/abs/1009.3589
N2  - Recent theoretical and empirical work in statistical machine learning has demonstrated the importance of learning algorithms for deep architectures, i.e., function classes obtained by composing multiple non-linear transformations. Self-taught learning (exploiting unlabeled examples or examples from other distributions) has already been applied to deep learners, but mostly to show the advantage of unlabeled examples. Here we explore the advantage brought by {\em out-of-distribution examples}. For this purpose we developed a powerful generator of stochastic variations and noise processes for character images, including not only affine transformations but also slant, local elastic deformations, changes in thickness, background images, grey level changes, contrast, occlusion, and various types of noise. The out-of-distribution examples are obtained from these highly distorted images or by including examples of object classes different from those in the target test set. We show that {\em deep learners benefit more from out-of-distribution examples than a corresponding shallow learner}, at least in the area of handwritten character recognition. In fact, we show that they beat previously published results and reach human-level performance on both handwritten digit classification and 62-class handwritten character recognition.
ER  -


