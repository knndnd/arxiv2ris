
TY  - Preprint
T1  - Deep Reinforcement Learning for Time Scheduling in RF-Powered Backscatter Cognitive Radio Networks
A1  - Tran The Anh
A1  - Nguyen Cong Luong
A1  - Dusit Niyato
A1  - Ying-Chang Liang
A1  - Dong In Kim
JO  - ArXiv e-prints
Y1  - 3 October, 2018
UR  - https://arxiv.org/abs/1810.04520
N2  - In an RF-powered backscatter cognitive radio network, multiple secondary users communicate with a secondary gateway by backscattering or harvesting energy and actively transmitting their data depending on the primary channel state. To coordinate the transmission of multiple secondary transmitters, the secondary gateway needs to schedule the backscattering time, energy harvesting time, and transmission time among them. However, under the dynamics of the primary channel and the uncertainty of the energy state of the secondary transmitters, it is challenging for the gateway to find a time scheduling mechanism which maximizes the total throughput. In this paper, we propose to use the deep reinforcement learning algorithm to derive an optimal time scheduling policy for the gateway. Specifically, to deal with the problem with large state and action spaces, we adopt a Double Deep-Q Network (DDQN) that enables the gateway to learn the optimal policy. The simulation results clearly show that the proposed deep reinforcement learning algorithm outperforms non-learning schemes in terms of network throughput.
ER  -


TY  - Preprint
T1  - Distributed Wildfire Surveillance with Autonomous Aircraft using Deep Reinforcement Learning
A1  - Kyle D. Julian
A1  - Mykel J. Kochenderfer
JO  - ArXiv e-prints
Y1  - 9 October, 2018
UR  - https://arxiv.org/abs/1810.04244
N2  - Teams of autonomous unmanned aircraft can be used to monitor wildfires, enabling firefighters to make informed decisions. However, controlling multiple autonomous fixed-wing aircraft to maximize forest fire coverage is a complex problem. The state space is high dimensional, the fire propagates stochastically, the sensor information is imperfect, and the aircraft must coordinate with each other to accomplish their mission. This work presents two deep reinforcement learning approaches for training decentralized controllers that accommodate the high dimensionality and uncertainty inherent in the problem. The first approach controls the aircraft using immediate observations of the individual aircraft. The second approach allows aircraft to collaborate on a map of the wildfire&#39;s state and maintain a time history of locations visited, which are used as inputs to the controller. Simulation results show that both approaches allow the aircraft to accurately track wildfire expansions and outperform an online receding horizon controller. Additional simulations demonstrate that the approach scales with different numbers of aircraft and generalizes to different wildfire shapes.
ER  -


TY  - Preprint
T1  - Semi-supervised Deep Reinforcement Learning in Support of IoT and Smart City Services
A1  - Mehdi Mohammadi
A1  - Ala Al-Fuqaha
A1  - Mohsen Guizani
A1  - Jun-Seok Oh
JO  - ArXiv e-prints
Y1  - 9 October, 2018
UR  - https://arxiv.org/abs/1810.04118
N2  - Smart services are an important element of the smart cities and the Internet of Things (IoT) ecosystems where the intelligence behind the services is obtained and improved through the sensory data. Providing a large amount of training data is not always feasible; therefore, we need to consider alternative ways that incorporate unlabeled data as well. In recent years, Deep reinforcement learning (DRL) has gained great success in several application domains. It is an applicable method for IoT and smart city scenarios where auto-generated data can be partially labeled by users&#39; feedback for training purposes. In this paper, we propose a semi-supervised deep reinforcement learning model that fits smart city applications as it consumes both labeled and unlabeled data to improve the performance and accuracy of the learning agent. The model utilizes Variational Autoencoders (VAE) as the inference engine for generalizing optimal policies. To the best of our knowledge, the proposed model is the first investigation that extends deep reinforcement learning to the semi-supervised paradigm. As a case study of smart city applications, we focus on smart buildings and apply the proposed model to the problem of indoor localization based on BLE signal strength. Indoor localization is the main component of smart city services since people spend significant time in indoor environments. Our model learns the best action policies that lead to a close estimation of the target locations with an improvement of 23% in terms of distance to the target and at least 67% more received rewards compared to the supervised DRL model.
ER  -


TY  - Preprint
T1  - A Distributed Reinforcement Learning Solution With Knowledge Transfer Capability for A Bike Rebalancing Problem
A1  - Ian Xiao
JO  - ArXiv e-prints
Y1  - 9 October, 2018
UR  - https://arxiv.org/abs/1810.04058
N2  - Rebalancing is a critical service bottleneck for many transportation services, such as Citi Bike. Citi Bike relies on manual orchestrations of rebalancing bikes between dispatchers and field agents. Motivated by such problem and the lack of smart autonomous solutions in this area, this project explored a new RL architecture called Distributed RL (DiRL) with Transfer Learning (TL) capability. The DiRL solution is adaptive to changing traffic dynamics when keeping bike stock under control at the minimum cost. DiRL achieved a 350% improvement in bike rebalancing autonomously and TL offered a 62.4% performance boost in managing an entire bike network. Lastly, a field trip to the dispatch office of Chariot, a ride-sharing service, provided insights to overcome challenges of deploying an RL solution in the real world.
ER  -


TY  - Preprint
T1  - Continual State Representation Learning for Reinforcement Learning using Generative Replay
A1  - Hugo Caselles-DuprÃ©
A1  - Michael Garcia-Ortiz
A1  - David Filliat
JO  - ArXiv e-prints
Y1  - 9 October, 2018
UR  - https://arxiv.org/abs/1810.03880
N2  - We consider the problem of building a state representation model in a continual fashion. As the environment changes, the aim is to efficiently compress the state&#39;s information without losing past knowledge. The learned features are then fed to a Reinforcement Learning algorithm to learn a policy. We propose to use Variational Auto-Encoders for state representation, and Generative Replay, e.g the use of generated samples, to maintain past knowledge. We also provide a general and statistically sound method for automatic environment change detection. Our method provides efficient state representation as well as forward transfer, and avoids catastrophic forgetting. The resulting model is capable of incrementally learning information without using past data and with a bounded system size.
ER  -


TY  - Preprint
T1  - Reinforcement Learning for Improving Agent Design
A1  - David Ha
JO  - ArXiv e-prints
Y1  - 8 October, 2018
UR  - https://arxiv.org/abs/1810.03779
N2  - In many reinforcement learning tasks, the goal is to learn a policy to manipulate an agent, whose design is fixed, to maximize some notion of cumulative reward. The design of the agent&#39;s physical structure is rarely optimized for the task at hand. In this work, we explore the possibility of learning a version of the agent&#39;s design that is better suited for its task, jointly with the policy. We propose a minor alteration to the OpenAI Gym framework, where we parameterize parts of an environment, and allow an agent to jointly learn to modify these environment parameters along with its policy. We demonstrate that an agent can learn a better structure of its body that is not only better suited for the task, but also facilitates policy learning. Joint learning of policy and structure may even uncover design principles that are useful for assisted-design applications. Videos of results at https://designrl.github.io/
ER  -


TY  - Preprint
T1  - Actor-Critic Deep Reinforcement Learning for Dynamic Multichannel Access
A1  - Chen Zhong
A1  - Ziyang Lu
A1  - M. Cenk Gursoy
A1  - Senem Velipasalar
JO  - ArXiv e-prints
Y1  - 8 October, 2018
UR  - https://arxiv.org/abs/1810.03695
N2  - We consider the dynamic multichannel access problem, which can be formulated as a partially observable Markov decision process (POMDP). We first propose a model-free actor-critic deep reinforcement learning based framework to explore the sensing policy. To evaluate the performance of the proposed sensing policy and the framework&#39;s tolerance against uncertainty, we test the framework in scenarios with different channel switching patterns and consider different switching probabilities. Then, we consider a time-varying environment to identify the adaptive ability of the proposed framework. Additionally, we provide comparisons with the Deep-Q network (DQN) based framework proposed in [1], in terms of both average reward and the time efficiency.
ER  -


TY  - Preprint
T1  - Multi-agent Deep Reinforcement Learning for Zero Energy Communities
A1  - Amit Prasad
A1  - Ivana Dusparic
JO  - ArXiv e-prints
Y1  - 8 October, 2018
UR  - https://arxiv.org/abs/1810.03679
N2  - Advances in renewable energy generation and introduction of the government targets to improve energy efficiency gave rise to a concept of a Zero Energy Building (ZEB). A ZEB is a building whose net energy usage over a year is zero, i.e., its energy use is not larger than its overall renewables generation. A collection of ZEBs forms a Zero Energy Community (ZEC). This paper addresses the problem of energy sharing in such a community. This is different from previously addressed energy sharing between buildings as our focus is on the improvement of community energy status, while traditionally research focused on reducing losses due to transmission and storage, or achieving economic gains. We model this problem in a multi-agent environment and propose a Deep Reinforcement Learning (DRL) based solution. Each building is represented by an intelligent agent that learns over time the appropriate behaviour to share energy. We have evaluated the proposed solution in a multi-agent simulation built using osBrain. Results indicate that with time agents learn to collaborate and learn a policy comparable to the optimal policy, which in turn improves the ZEC&#39;s energy status. Buildings with no renewables preferred to request energy from their neighbours rather than from the supply grid.
ER  -


TY  - Preprint
T1  - SFV: Reinforcement Learning of Physical Skills from Videos
A1  - Xue Bin Peng
A1  - Angjoo Kanazawa
A1  - Jitendra Malik
A1  - Pieter Abbeel
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 8 October, 2018
UR  - https://arxiv.org/abs/1810.03599
N2  - Data-driven character animation based on motion capture can produce highly naturalistic behaviors and, when combined with physics simulation, can provide for natural procedural responses to physical perturbations, environmental changes, and morphological discrepancies. Motion capture remains the most popular source of motion data, but collecting mocap data typically requires heavily instrumented environments and actors. In this paper, we propose a method that enables physically simulated characters to learn skills from videos (SFV). Our approach, based on deep pose estimation and deep reinforcement learning, allows data-driven animation to leverage the abundance of publicly available video clips from the web, such as those from YouTube. This has the potential to enable fast and easy design of character controllers simply by querying for video recordings of the desired behavior. The resulting controllers are robust to perturbations, can be adapted to new settings, can perform basic object interactions, and can be retargeted to new morphologies via reinforcement learning. We further demonstrate that our method can predict potential human motions from still images, by forward simulation of learned controllers initialized from the observed pose. Our framework is able to learn a broad range of dynamic skills, including locomotion, acrobatics, and martial arts.
ER  -


TY  - Preprint
T1  - Internet Congestion Control via Deep Reinforcement Learning
A1  - Nathan Jay
A1  - Noga H. Rotman
A1  - P. Brighten Godfrey
A1  - Michael Schapira
A1  - Aviv Tamar
JO  - ArXiv e-prints
Y1  - 7 October, 2018
UR  - https://arxiv.org/abs/1810.03259
N2  - We present and investigate a novel and timely application domain for deep reinforcement learning (RL): Internet congestion control. Congestion control is the core networking task of modulating traffic sources&#39; data-transmission rates so as to efficiently and fairly allocate network resources. Congestion control is fundamental to computer networking research and practice, and has recently been the subject of extensive attention in light of the advent of challenging Internet applications such as live video, augmented and virtual reality, Internet-of-Things, and more.
ER  -


TY  - Preprint
T1  - Reinforcement Evolutionary Learning Method for self-learning
A1  - Kumarjit Pathak
A1  - Jitin Kapila
JO  - ArXiv e-prints
Y1  - 7 October, 2018
UR  - https://arxiv.org/abs/1810.03198
N2  - In statistical modelling the biggest threat is concept drift which makes the model gradually showing deteriorating performance over time. There are state of the art methodologies to detect the impact of concept drift, however general strategy considered to overcome the issue in performance is to rebuild or re-calibrate the model periodically as the variable patterns for the model changes significantly due to market change or consumer behavior change etc. Quantitative research is the most widely spread application of data science in Marketing or financial domain where applicability of state of the art reinforcement learning for auto-learning is less explored paradigm. Reinforcement learning is heavily dependent on having a simulated environment which is majorly available for gaming or online systems, to learn from the live feedback. However, there are some research happened on the area of online advertisement, pricing etc where due to the nature of the online learning environment scope of reinforcement learning is explored. Our proposed solution is a reinforcement learning based, true self-learning algorithm which can adapt to the data change or concept drift and auto learn and self-calibrate for the new patterns of the data solving the problem of concept drift.
ER  -


TY  - Preprint
T1  - Q-map: a Convolutional Approach for Goal-Oriented Reinforcement Learning
A1  - Fabio Pardo
A1  - Vitaly Levdik
A1  - Petar Kormushev
JO  - ArXiv e-prints
Y1  - 5 October, 2018
UR  - https://arxiv.org/abs/1810.02927
N2  - Goal-oriented learning has become a core concept in reinforcement learning (RL), extending the reward signal as a sole way to define tasks. However, as parameterizing value functions with goals increases the learning complexity, efficiently reusing past experience to update estimates towards several goals at once becomes desirable but usually requires independent updates per goal. Considering that a significant number of RL environments can support spatial coordinates as goals, such as on-screen location of the character in ATARI or SNES games, we propose a novel goal-oriented agent called Q-map that utilizes an autoencoder-like neural network to predict the minimum number of steps towards each coordinate in a single forward pass. This architecture is similar to Horde with parameter sharing and allows the agent to discover correlations between visual patterns and navigation. For example learning how to use a ladder in a game could be transferred to other ladders later. We show how this network can be efficiently trained with a 3D variant of Q-learning to update the estimates towards all goals at once. While the Q-map agent could be used for a wide range of applications, we propose a novel exploration mechanism in place of epsilon-greedy that relies on goal selection at a desired distance followed by several steps taken towards it, allowing long and coherent exploratory steps in the environment. We demonstrate the accuracy and generalization qualities of the Q-map agent on a grid-world environment and then demonstrate the efficiency of the proposed exploration mechanism on the notoriously difficult Montezuma&#39;s Revenge and Super Mario All-Stars games.
ER  -


TY  - Preprint
T1  - Actor-Attention-Critic for Multi-Agent Reinforcement Learning
A1  - Shariq Iqbal
A1  - Fei Sha
JO  - ArXiv e-prints
Y1  - 5 October, 2018
UR  - https://arxiv.org/abs/1810.02912
N2  - Reinforcement learning in multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in single-agent settings. We present an actor-critic algorithm that trains decentralized policies in multi-agent settings, using centrally computed critics that share an attention mechanism which selects relevant information for each agent at every timestep. This attention mechanism enables more effective and scalable learning in complex multi-agent environments, when compared to recent approaches. Our approach is applicable not only to cooperative settings with shared rewards, but also individualized reward settings, including adversarial settings, and it makes no assumptions about the action spaces of the agents. As such, it is flexible enough to be applied to most multi-agent learning problems.
ER  -


TY  - Preprint
T1  - MyCaffe: A Complete C# Re-Write of Caffe with Reinforcement Learning
A1  - David W. Brown
JO  - ArXiv e-prints
Y1  - 4 October, 2018
UR  - https://arxiv.org/abs/1810.02272
N2  - Over the past few years Caffe, from Berkeley AI Research, has gained a strong following in the deep learning community with over 15K forks on the github.com/BLVC/Caffe site. With its well organized, very modular C++ design it is easy to work with and very fast. However, in the world of Windows development, C# has helped accelerate development with many of the enhancements that it offers over C++, such as garbage collection, a very rich .NET programming framework and easy database access via Entity Frameworks. So how can a C# developer use the advances of C# to take full advantage of the benefits offered by the Berkeley Caffe deep learning system? The answer is the fully open source, &#39;MyCaffe&#39; for Windows .NET programmers. MyCaffe is an open source, complete C# language re-write of Berkeley&#39;s Caffe. This article describes the general architecture of MyCaffe including the newly added MyCaffeTrainerRL for Reinforcement Learning. In addition, this article discusses how MyCaffe closely follows the C++ Caffe, while talking efficiently to the low level NVIDIA CUDA hardware to offer a high performance, highly programmable deep learning system for Windows .NET programmers.
ER  -


TY  - Preprint
T1  - Reinforcement Learning Meets Hybrid Zero Dynamics: A Case Study for RABBIT
A1  - Guillermo A. Castillo
A1  - Bowen Weng
A1  - Ayonga Hereid
A1  - Wei Zhang
JO  - ArXiv e-prints
Y1  - 3 October, 2018
UR  - https://arxiv.org/abs/1810.01977
N2  - The design of feedback controllers for bipedal robots is challenging due to the hybrid nature of its dynamics and the complexity imposed by high-dimensional bipedal models. In this paper, we present a novel approach for the design of feedback controllers using Reinforcement Learning (RL) and Hybrid Zero Dynamics (HZD). Existing RL approaches for bipedal walking are inefficient as they do not consider the underlying physics, often requires substantial training, and the resulting controller may not be applicable to real robots. HZD is a powerful tool for bipedal control with local stability guarantees of the walking limit cycles. In this paper, we propose a non traditional RL structure that embeds the HZD framework into the policy learning. More specifically, we propose to use RL to find a control policy that maps from the robot&#39;s reduced order states to a set of parameters that define the desired trajectories for the robot&#39;s joints through the virtual constraints. Then, these trajectories are tracked using an adaptive PD controller. The method results in a stable and robust control policy that is able to track variable speed within a continuous interval. Robustness of the policy is evaluated by applying external forces to the torso of the robot. The proposed RL framework is implemented and demonstrated in OpenAI Gym with the MuJoCo physics engine based on the well-known RABBIT robot model.
ER  -


TY  - Preprint
T1  - Comparison of Reinforcement Learning algorithms applied to the Cart Pole problem
A1  - Savinay Nagendra
A1  - Nikhil Podila
A1  - Rashmi Ugarakhod
A1  - Koshy George
JO  - ArXiv e-prints
Y1  - 3 October, 2018
UR  - https://arxiv.org/abs/1810.01940
N2  - Designing optimal controllers continues to be challenging as systems are becoming complex and are inherently nonlinear. The principal advantage of reinforcement learning (RL) is its ability to learn from the interaction with the environment and provide optimal control strategy. In this paper, RL is explored in the context of control of the benchmark cartpole dynamical system with no prior knowledge of the dynamics. RL algorithms such as temporal-difference, policy gradient actor-critic, and value function approximation are compared in this context with the standard LQR solution. Further, we propose a novel approach to integrate RL and swing-up controllers.
ER  -


TY  - Preprint
T1  - Reinforcement Learning for Model-Free Power Management of Networked Microgrids
A1  - Qianzhi Zhang
A1  - Kaveh Dehghanpour
A1  - Zhaoyu Wang
JO  - ArXiv e-prints
Y1  - 1 October, 2018
UR  - https://arxiv.org/abs/1810.01758
N2  - This paper presents an approximate Reinforcement Learning (RL) methodology for data-driven power management of networked Microgrids (MG) in electric distribution systems. In practice, the system operator has limited or no knowledge of the MG asset behavior and models behind the Point of Common Coupling (PCC). This makes the distribution systems unobservable and impedes a conventional optimization approach for solving the power management problem while satisfying network constraints. To tackle this challenge, we have proposed a bi-level model-free machine learning framework in a retail market environment which functions exclusively based on the exchanged signals between different entities within the market. While at the lower level, each MG provides power-flow-constrained optimal response to price and voltage signals, at the higher level, the system operator performs function approximation to predict the behavior of market participants under incomplete information of MG parametric models. This function approximation scheme is then used within an adaptive RL framework to optimize the price signal and the substation voltage as the system load and solar generation change over time. Numerical experiments have been devised to verify the performance of the proposed learning model.
ER  -


TY  - Preprint
T1  - A Practical Approach to Insertion with Variable Socket Position Using Deep Reinforcement Learning
A1  - Mel Vecerik
A1  - Oleg Sushkov
A1  - David Barker
A1  - Thomas RothÃ¶rl
A1  - Todd Hester
A1  - Jon Scholz
JO  - ArXiv e-prints
Y1  - 8 October, 2018
UR  - https://arxiv.org/abs/1810.01531
N2  - Insertion is a challenging haptic and visual control problem with significant practical value for manufacturing. Existing approaches in the model-based robotics community can be highly effective when task geometry is known, but are complex and cumbersome to implement, and must be tailored to each individual problem by a qualified engineer. Within the learning community there is a long history of insertion research, but existing approaches are typically either too sample-inefficient to run on real robots, or assume access to high-level object features, e.g. socket pose. In this paper we show that relatively minor modifications to an off-the-shelf Deep-RL algorithm (DDPG), combined with a small number of human demonstrations, allows the robot to quickly learn to solve these tasks efficiently and robustly. Our approach requires no modeling or simulation, no parameterized search or alignment behaviors, no vision system aside from raw images, and no reward shaping. We evaluate our approach on a narrow-clearance peg-insertion task and a deformable clip-insertion task, both of which include variability in the socket position. Our results show that these tasks can be solved reliably on the real robot in less than 10 minutes of interaction time, and that the resulting policies are robust to variance in the socket position and orientation.
ER  -


TY  - Preprint
T1  - Near-Optimal Representation Learning for Hierarchical Reinforcement Learning
A1  - Ofir Nachum
A1  - Shixiang Gu
A1  - Honglak Lee
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 2 October, 2018
UR  - https://arxiv.org/abs/1810.01257
N2  - We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation -- the mapping of observation space to goal space -- is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods (see videos at https://sites.google.com/view/representation-hrl).
ER  -


TY  - Preprint
T1  - AlphaSeq: Sequence Discovery with Deep Reinforcement Learning
A1  - Yulin Shao
A1  - Soung Chang Liew
A1  - Taotao Wang
JO  - ArXiv e-prints
Y1  - 26 September, 2018
UR  - https://arxiv.org/abs/1810.01218
N2  - Sequences play an important role in many applications and systems. Discovering sequences with desired properties has long been an interesting intellectual pursuit. This paper puts forth a new paradigm, AlphaSeq, to discover desired sequences algorithmically using deep reinforcement learning (DRL) techniques. AlphaSeq treats the sequence discovery problem as an episodic symbol-filling game, in which a player fills symbols in the vacant positions of a sequence set sequentially during an episode of the game. Each episode ends with a completely-filled sequence set, upon which a reward is given based on the desirability of the sequence set. AlphaSeq models the game as a Markov Decision Process (MDP), and adapts the DRL framework of AlphaGo to solve the MDP. Sequences discovered improve progressively as AlphaSeq, starting as a novice, learns to become an expert game player through many episodes of game playing. Compared with traditional sequence construction by mathematical tools, AlphaSeq is particularly suitable for problems with complex objectives intractable to mathematical analysis. We demonstrate the searching capabilities of AlphaSeq in two applications: 1) AlphaSeq successfully rediscovers a set of ideal complementary codes that can zero-force all potential interferences in multi-carrier CDMA systems. 2) AlphaSeq discovers new sequences that triple the signal-to-interference ratio -- benchmarked against the well-known Legendre sequence -- of a mismatched filter estimator in pulse compression radar systems.
ER  -


TY  - Preprint
T1  - The Dreaming Variational Autoencoder for Reinforcement Learning Environments
A1  - Per-Arne Andersen
A1  - Morten Goodwin
A1  - Ole-Christoffer Granmo
JO  - ArXiv e-prints
Y1  - 2 October, 2018
UR  - https://arxiv.org/abs/1810.01112
N2  - Reinforcement learning has shown great potential in generalizing over raw sensory data using only a single neural network for value optimization. There are several challenges in the current state-of-the-art reinforcement learning algorithms that prevent them from converging towards the global optima. It is likely that the solution to these problems lies in short- and long-term planning, exploration and memory management for reinforcement learning algorithms. Games are often used to benchmark reinforcement learning algorithms as they provide a flexible, reproducible, and easy to control environment. Regardless, few games feature a state-space where results in exploration, memory, and planning are easily perceived. This paper presents The Dreaming Variational Autoencoder (DVAE), a neural network based generative modeling architecture for exploration in environments with sparse feedback. We further present Deep Maze, a novel and flexible maze engine that challenges DVAE in partial and fully-observable state-spaces, long-horizon tasks, and deterministic and stochastic problems. We show initial findings and encourage further work in reinforcement learning driven by generative exploration.
ER  -


TY  - Preprint
T1  - Reinforcement Learning with Perturbed Rewards
A1  - Jingkang Wang
A1  - Yang Liu
A1  - Bo Li
JO  - ArXiv e-prints
Y1  - 5 October, 2018
UR  - https://arxiv.org/abs/1810.01032
N2  - Recent studies have shown the vulnerability of reinforcement learning (RL) models in noisy settings. The sources of noises differ across scenarios. For instance, in practice, the observed reward channel is often subject to noise (e.g., when observed rewards are collected through sensors), and thus observed rewards may not be credible as a result. Also, in applications such as robotics, a deep reinforcement learning (DRL) algorithm can be manipulated to produce arbitrary errors. In this paper, we consider noisy RL problems where observed rewards by RL agents are generated with a reward confusion matrix. We call such observed rewards as perturbed rewards. We develop an unbiased reward estimator aided robust RL framework that enables RL agents to learn in noisy environments while observing only perturbed rewards. Our framework draws upon approaches for supervised learning with noisy data. The core ideas of our solution include estimating a reward confusion matrix and defining a set of unbiased surrogate rewards. We prove the convergence and sample complexity of our approach. Extensive experiments on different DRL platforms show that policies based on our estimated surrogate reward can achieve higher expected rewards, and converge faster than existing baselines. For instance, the state-of-the-art PPO algorithm is able to obtain 67.5% and 46.7% improvements in average on five Atari games, when the error rates are 10% and 30% respectively.
ER  -


TY  - Preprint
T1  - Omega-Regular Objectives in Model-Free Reinforcement Learning
A1  - Ernst Moritz Hahn
A1  - Mateo Perez
A1  - Sven Schewe
A1  - Fabio Somenzi
A1  - Ashutosh Trivedi
A1  - Dominik Wojtczak
JO  - ArXiv e-prints
Y1  - 26 September, 2018
UR  - https://arxiv.org/abs/1810.00950
N2  - We provide the first solution for model-free reinforcement learning of Ï-regular objectives for Markov decision processes (MDPs). We present a constructive reduction from the almost-sure satisfaction of Ï-regular objectives to an almost- sure reachability problem and extend this technique to learning how to control an unknown model so that the chance of satisfying the objective is maximized. A key feature of our technique is the compilation of Ï-regular properties into limit- deterministic Buechi automata instead of the traditional Rabin automata; this choice sidesteps difficulties that have marred previous proposals. Our approach allows us to apply model-free, off-the-shelf reinforcement learning algorithms to compute optimal strategies from the observations of the MDP. We present an experimental evaluation of our technique on benchmark learning problems.
ER  -


TY  - Preprint
T1  - Joint On-line Learning of a Zero-shot Spoken Semantic Parser and a Reinforcement Learning Dialogue Manager
A1  - Matthieu Riou
A1  - Bassam Jabaian
A1  - StÃ©phane Huet
A1  - Fabrice LefÃ¨vre
JO  - ArXiv e-prints
Y1  - 1 October, 2018
UR  - https://arxiv.org/abs/1810.00924
N2  - Despite many recent advances for the design of dialogue systems, a true bottleneck remains the acquisition of data required to train its components. Unlike many other language processing applications, dialogue systems require interactions with users, therefore it is complex to develop them with pre-recorded data. Building on previous works, on-line learning is pursued here as a most convenient way to address the issue. Data collection, annotation and use in learning algorithms are performed in a single process. The main difficulties are then: to bootstrap an initial basic system, and to control the level of additional cost on the user side. Considering that well-performing solutions can be used directly off the shelf for speech recognition and synthesis, the study is focused on learning the spoken language understanding and dialogue management modules only. Several variants of joint learning are investigated and tested with user trials to confirm that the overall on-line learning can be obtained after only a few hundred training dialogues and can overstep an expert-based system.
ER  -


TY  - Preprint
T1  - Bayesian Transfer Reinforcement Learning with Prior Knowledge Rules
A1  - Michalis K. Titsias
A1  - Sotirios Nikoloutsopoulos
JO  - ArXiv e-prints
Y1  - 30 September, 2018
UR  - https://arxiv.org/abs/1810.00468
N2  - We propose a probabilistic framework to directly insert prior knowledge in reinforcement learning (RL) algorithms by defining the behaviour policy as a Bayesian posterior distribution. Such a posterior combines task specific information with prior knowledge, thus allowing to achieve transfer learning across tasks. The resulting method is flexible and it can be easily incorporated to any standard off-policy and on-policy algorithms, such as those based on temporal differences and policy gradients. We develop a specific instance of this Bayesian transfer RL framework by expressing prior knowledge as general deterministic rules that can be useful in a large variety of tasks, such as navigation tasks. Also, we elaborate more on recent probabilistic and entropy-regularised RL by developing a novel temporal learning algorithm and show how to combine it with Bayesian transfer RL. Finally, we demonstrate our method for solving mazes and show that significant speed ups can be obtained.
ER  -


TY  - Preprint
T1  - Using State Predictions for Value Regularization in Curiosity Driven Deep Reinforcement Learning
A1  - Gino Brunner
A1  - Manuel Fritsche
A1  - Oliver Richter
A1  - Roger Wattenhofer
JO  - ArXiv e-prints
Y1  - 30 September, 2018
UR  - https://arxiv.org/abs/1810.00361
N2  - Learning in sparse reward settings remains a challenge in Reinforcement Learning, which is often addressed by using intrinsic rewards. One promising strategy is inspired by human curiosity, requiring the agent to learn to predict the future. In this paper a curiosity-driven agent is extended to use these predictions directly for training. To achieve this, the agent predicts the value function of the next state at any point in time. Subsequently, the consistency of this prediction with the current value function is measured, which is then used as a regularization term in the loss function of the algorithm. Experiments were made on grid-world environments as well as on a 3D navigation task, both with sparse rewards. In the first case the extended agent is able to learn significantly faster than the baselines.
ER  -


TY  - Preprint
T1  - Reinforcement Learning in R
A1  - Nicolas PrÃ¶llochs
A1  - Stefan Feuerriegel
JO  - ArXiv e-prints
Y1  - 29 September, 2018
UR  - https://arxiv.org/abs/1810.00240
N2  - Reinforcement learning refers to a group of methods from artificial intelligence where an agent performs learning through trial and error. It differs from supervised learning, since reinforcement learning requires no explicit labels; instead, the agent interacts continuously with its environment. That is, the agent starts in a specific state and then performs an action, based on which it transitions to a new state and, depending on the outcome, receives a reward. Different strategies (e.g. Q-learning) have been proposed to maximize the overall reward, resulting in a so-called policy, which defines the best possible action in each state. Mathematically, this process can be formalized by a Markov decision process and it has been implemented by packages in R; however, there is currently no package available for reinforcement learning. As a remedy, this paper demonstrates how to perform reinforcement learning in R and, for this purpose, introduces the ReinforcementLearning package. The package provides a remarkably flexible framework and is easily applied to a wide range of different problems. We demonstrate its use by drawing upon common examples from the literature (e.g. finding optimal game strategies).
ER  -


TY  - Preprint
T1  - M^3RL: Mind-aware Multi-agent Management Reinforcement Learning
A1  - Tianmin Shu
A1  - Yuandong Tian
JO  - ArXiv e-prints
Y1  - 29 September, 2018
UR  - https://arxiv.org/abs/1810.00147
N2  - Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly controlling the agents to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not wish to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is maximizing the overall productivity as well as minimizing payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents&#39; minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.
ER  -


TY  - Preprint
T1  - Robot Representing and Reasoning with Knowledge from Reinforcement Learning
A1  - Keting Lu
A1  - Shiqi Zhang
A1  - Peter Stone
A1  - Xiaoping Chen
JO  - ArXiv e-prints
Y1  - 9 October, 2018
UR  - https://arxiv.org/abs/1809.11074
N2  - Reinforcement learning (RL) agents aim at learning by interacting with an environment, and are not designed for representing or reasoning with declarative knowledge. Knowledge representation and reasoning (KRR) paradigms are strong in declarative KRR tasks, but are ill-equipped to learn from such experiences. In this work, we integrate logical-probabilistic KRR with model-based RL, enabling agents to simultaneously reason with declarative knowledge and learn from interaction experiences. The knowledge from humans and RL is unified and used for dynamically computing task-specific planning models under potentially new environments. Experiments were conducted using a mobile robot working on dialog, navigation, and delivery tasks. Results show significant improvements, in comparison to existing model-based RL methods.
ER  -


TY  - Preprint
T1  - Using Deep Reinforcement Learning to Learn High-Level Policies on the ATRIAS Biped
A1  - Tianyu Li
A1  - Akshara Rai
A1  - Hartmut Geyer
A1  - Christopher G. Atkeson
JO  - ArXiv e-prints
Y1  - 27 September, 2018
UR  - https://arxiv.org/abs/1809.10811
N2  - Learning controllers for bipedal robots is a challenging problem, often requiring expert knowledge and extensive tuning of parameters that vary in different situations. Recently, deep reinforcement learning has shown promise at automatically learning controllers for complex systems in simulation. This has been followed by a push towards learning controllers that can be transferred between simulation and hardware, primarily with the use of domain randomization. However, domain randomization can make the problem of finding stable controllers even more challenging, especially for underactuated bipedal robots. In this work, we explore whether policies learned in simulation can be transferred to hardware with the use of high-fidelity simulators and structured controllers. We learn a neural network policy which is a part of a more structured controller. While the neural network is learned in simulation, the rest of the controller stays fixed, and can be tuned by the expert as needed. We show that using this approach can greatly speed up the rate of learning in simulation, as well as enable transfer of policies between simulation and hardware. We present our results on an ATRIAS robot and explore the effect of action spaces and cost functions on the rate of transfer between simulation and hardware. Our results show that structured policies can indeed be learned in simulation and implemented on hardware successfully. This has several advantages, as the structure preserves the intuitive nature of the policy, and the neural network improves the performance of the hand-designed policy. In this way, we propose a way of using neural networks to improve expert designed controllers, while maintaining ease of understanding.
ER  -


TY  - Preprint
T1  - Controllable Neural Story Generation via Reinforcement Learning
A1  - Pradyumna Tambwekar
A1  - Murtaza Dhuliawala
A1  - Animesh Mehta
A1  - Lara J. Martin
A1  - Brent Harrison
A1  - Mark O. Riedl
JO  - ArXiv e-prints
Y1  - 27 September, 2018
UR  - https://arxiv.org/abs/1809.10736
N2  - Open story generation is the problem of automatically creating a story for any domain without retraining. Neural language models can be trained on large corpora across many domains and then used to generate stories. However, stories generated via language models tend to lack direction and coherence. We introduce a policy gradient reinforcement learning approach to open story generation that learns to achieve a given narrative goal state. In this work, the goal is for a story to end with a specific type of event, given in advance. However, a reward based on achieving the given goal is too sparse for effective learning. We use reward shaping to provide the reinforcement learner with a partial reward at every step. We show that our technique can train a model that generates a story that reaches the goal 94% of the time and reduces model perplexity. A human subject evaluation shows that stories generated by our technique are perceived to have significantly higher plausible event ordering and plot coherence over a baseline language modeling technique without perceived degradation of overall quality, enjoyability, or local causality.
ER  -


TY  - Preprint
T1  - Adaptive Tensegrity Locomotion on Rough Terrain via Reinforcement Learning
A1  - David Surovik
A1  - Kun Wang
A1  - Kostas E. Bekris
JO  - ArXiv e-prints
Y1  - 27 September, 2018
UR  - https://arxiv.org/abs/1809.10710
N2  - The dynamical properties of tensegrity robots give them appealing ruggedness and adaptability, but present major challenges with respect to locomotion control. Due to high-dimensionality and complex contact responses, data-driven approaches are apt for producing viable feedback policies. Guided Policy Search (GPS), a sample-efficient and model-free hybrid framework for optimization and reinforcement learning, has recently been used to produce periodic locomotion for a spherical 6-bar tensegrity robot on flat or slightly varied surfaces. This work provides an extension to non-periodic locomotion and achieves rough terrain traversal, which requires more broadly varied, adaptive, and non-periodic rover behavior. The contribution alters the control optimization step of GPS, which locally fits and exploits surrogate models of the dynamics, and employs the existing supervised learning step. The proposed solution incorporates new processes to ensure effective local modeling despite the disorganized nature of sample data in rough terrain locomotion. Demonstrations in simulation reveal that the resulting controller sustains the highly adaptive behavior necessary to reliably traverse rough terrain.
ER  -


TY  - Preprint
T1  - Definition and evaluation of model-free coordination of electrical vehicle charging with reinforcement learning
A1  - Nasrin Sadeghianpourhamami
A1  - Johannes Deleu
A1  - Chris Develder
JO  - ArXiv e-prints
Y1  - 27 September, 2018
UR  - https://arxiv.org/abs/1809.10679
N2  - Initial DR studies mainly adopt model predictive control and thus require accurate models of the control problem (e.g., a customer behavior model), which are to a large extent uncertain for the EV scenario. Hence, model-free approaches, especially based on reinforcement learning (RL) are an attractive alternative. In this paper, we propose a new Markov decision process (MDP) formulation in the RL framework, to jointly coordinate a set of EV charging stations. State-of-the-art algorithms either focus on a single EV, or perform the control of an aggregate of EVs in multiple steps (e.g., aggregate load decisions in one step, then a step translating the aggregate decision to individual connected EVs). On the contrary, we propose an RL approach to jointly control the whole set of EVs at once. We contribute a new MDP formulation, with a scalable state representation that is independent of the number of EV charging stations. Further, we use a batch reinforcement learning algorithm, i.e., an instance of fitted Q-iteration, to learn the optimal charging policy. We analyze its performance using simulation experiments based on a real-world EV charging data. More specifically, we (i) explore the various settings in training the RL policy (e.g., duration of the period with training data), (ii) compare its performance to an oracle all-knowing benchmark (which provides an upper bound for performance, relying on information that is not available or at least imperfect in practice), (iii) analyze performance over time, over the course of a full year to evaluate possible performance fluctuations (e.g, across different seasons), and (iv) demonstrate the generalization capacity of a learned control policy to larger sets of charging stations.
ER  -


TY  - Preprint
T1  - Learning to Coordinate Multiple Reinforcement Learning Agents for Diverse Query Reformulation
A1  - Rodrigo Nogueira
A1  - Jannis Bulian
A1  - Massimiliano Ciaramita
JO  - ArXiv e-prints
Y1  - 27 September, 2018
UR  - https://arxiv.org/abs/1809.10658
N2  - We propose a method to efficiently learn diverse strategies in reinforcement learning for query reformulation in the tasks of document retrieval and question answering. In the proposed framework an agent consists of multiple specialized sub-agents and a meta-agent that learns to aggregate the answers from sub-agents to produce a final answer. Sub-agents are trained on disjoint partitions of the training data, while the meta-agent is trained on the full training set. Our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines, such as an ensemble of agents trained on the full data. We show that the improved performance is due to the increased diversity of reformulation strategies.
ER  -


TY  - Preprint
T1  - Learning through Probing: a decentralized reinforcement learning architecture for social dilemmas
A1  - Nicolas Anastassacos
A1  - Mirco Musolesi
JO  - ArXiv e-prints
Y1  - 26 September, 2018
UR  - https://arxiv.org/abs/1809.10007
N2  - Multi-agent reinforcement learning has received significant interest in recent years notably due to the advancements made in deep reinforcement learning which have allowed for the developments of new architectures and learning algorithms. Using social dilemmas as the training ground, we present a novel learning architecture, Learning through Probing (LTP), where agents utilize a probing mechanism to incorporate how their opponent&#39;s behavior changes when an agent takes an action. We use distinct training phases and adjust rewards according to the overall outcome of the experiences accounting for changes to the opponents behavior. We introduce a parameter eta to determine the significance of these future changes to opponent behavior. When applied to the Iterated Prisoner&#39;s Dilemma (IPD), LTP agents demonstrate that they can learn to cooperate with each other, achieving higher average cumulative rewards than other reinforcement learning methods while also maintaining good performance in playing against static agents that are present in Axelrod tournaments. We compare this method with traditional reinforcement learning algorithms and agent-tracking techniques to highlight key differences and potential applications. We also draw attention to the differences between solving games and societal-like interactions and analyze the training of Q-learning agents in makeshift societies. This is to emphasize how cooperation may emerge in societies and demonstrate this using environments where interactions with opponents are determined through a random encounter format of the IPD.
ER  -


TY  - Preprint
T1  - Floyd-Warshall Reinforcement Learning: Learning from Past Experiences to Reach New Goals
A1  - Vikas Dhiman
A1  - Shurjo Banerjee
A1  - Jeffrey M. Siskind
A1  - Jason J. Corso
JO  - ArXiv e-prints
Y1  - 4 October, 2018
UR  - https://arxiv.org/abs/1809.09318
N2  - Consider mutli-goal tasks that involve static environments and dynamic goals. Examples of such tasks, such as goal-directed navigation and pick-and-place in robotics, abound. Two types of Reinforcement Learning (RL) algorithms are used for such tasks: model-free or model-based. Each of these approaches has limitations. Model-free RL struggles to transfer learned information when the goal location changes, but achieves high asymptotic accuracy in single goal tasks. Model-based RL can transfer learned information to new goal locations by retaining the explicitly learned state-dynamics, but is limited by the fact that small errors in modelling these dynamics accumulate over long-term planning. In this work, we improve upon the limitations of model-free RL in multi-goal domains. We do this by adapting the Floyd-Warshall algorithm for RL and call the adaptation Floyd-Warshall RL (FWRL). The proposed algorithm learns a goal-conditioned action-value function by constraining the value of the optimal path between any two states to be greater than or equal to the value of paths via intermediary states. Experimentally, we show that FWRL is more sample-efficient and learns higher reward strategies in multi-goal tasks as compared to Q-learning, model-based RL and other relevant baselines in a tabular domain.
ER  -


TY  - Preprint
T1  - Visual Diagnostics for Deep Reinforcement Learning Policy Development
A1  - Jieliang Luo
A1  - Sam Green
A1  - Peter Feghali
A1  - George Legrady
A1  - Ãetin Kaya KoÃ§
JO  - ArXiv e-prints
Y1  - 26 September, 2018
UR  - https://arxiv.org/abs/1809.06781
N2  - Modern vision-based reinforcement learning techniques often use convolutional neural networks (CNN) as universal function approximators to choose which action to take for a given visual input. Until recently, CNNs have been treated like black-box functions, but this mindset is especially dangerous when used for control in safety-critical settings. In this paper, we present our extensions of CNN visualization algorithms to the domain of vision-based reinforcement learning. We use a simulated drone environment as an example scenario. These visualization algorithms are an important tool for behavior introspection and provide insight into the qualities and flaws of trained policies when interacting with the physical world. A video may be seen at https://sites.google.com/view/drlvisual .
ER  -


TY  - Preprint
T1  - Multiobjective Reinforcement Learning for Reconfigurable Adaptive Optimal Control of Manufacturing Processes
A1  - Johannes Dornheim
A1  - Norbert Link
JO  - ArXiv e-prints
Y1  - 9 October, 2018
UR  - https://arxiv.org/abs/1809.06750
N2  - In industrial applications of adaptive optimal control often multiple contrary objectives have to be considered. The weights (relative importance) of the objectives are often not known during the design of the control and can change with changing production conditions and requirements. In this work a novel model-free multiobjective reinforcement learning approach for adaptive optimal control of manufacturing processes is proposed. The approach enables sample-efficient learning in sequences of control configurations, given by particular objective weights.
ER  -


TY  - Preprint
T1  - Variance Reduction for Reinforcement Learning in Input-Driven Environments
A1  - Hongzi Mao
A1  - Shaileshh Bojja Venkatakrishnan
A1  - Malte Schwarzkopf
A1  - Mohammad Alizadeh
JO  - ArXiv e-prints
Y1  - 3 October, 2018
UR  - https://arxiv.org/abs/1807.02264
N2  - We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We derive a bias-free, input-dependent baseline to reduce this variance, and analytically show its benefits over state-dependent baselines. We then propose a meta-learning approach to overcome the complexity of learning a baseline that depends on a long sequence of inputs. Our experimental results show that across environments from queuing systems, computer networks, and MuJoCo robotic locomotion, input-dependent baselines consistently improve training stability and result in better eventual policies.
ER  -


TY  - Preprint
T1  - Sim-to-Real Reinforcement Learning for Deformable Object Manipulation
A1  - Jan Matas
A1  - Stephen James
A1  - Andrew J. Davison
JO  - ArXiv e-prints
Y1  - 7 October, 2018
UR  - https://arxiv.org/abs/1806.07851
N2  - We have seen much recent progress in rigid object manipulation, but interaction with deformable objects has notably lagged behind. Due to the large configuration space of deformable objects, solutions using traditional modelling approaches require significant engineering work. Perhaps then, bypassing the need for explicit modelling and instead learning the control in an end-to-end manner serves as a better approach? Despite the growing interest in the use of end-to-end robot learning approaches, only a small amount of work has focused on their applicability to deformable object manipulation. Moreover, due to the large amount of data needed to learn these end-to-end solutions, an emerging trend is to learn control policies in simulation and then transfer them over to the real world. To-date, no work has explored whether it is possible to learn and transfer deformable object policies. We believe that if sim-to-real methods are to be employed further, then it should be possible to learn to interact with a wide variety of objects, and not only rigid objects. In this work, we use a combination of state-of-the-art deep reinforcement learning algorithms to solve the problem of manipulating deformable objects (specifically cloth). We evaluate our approach on three tasks --- folding a towel up to a mark, folding a face towel diagonally, and draping a piece of cloth over a hanger. Our agents are fully trained in simulation with domain randomisation, and then successfully deployed in the real world without having seen any real deformable objects.
ER  -


TY  - Preprint
T1  - Data-Efficient Hierarchical Reinforcement Learning
A1  - Ofir Nachum
A1  - Shixiang Gu
A1  - Honglak Lee
A1  - Sergey Levine
JO  - ArXiv e-prints
Y1  - 5 October, 2018
UR  - https://arxiv.org/abs/1805.08296
N2  - Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher- and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We term the resulting HRL agent HIRO and find that it is generally applicable and highly sample-efficient. Our experiments show that HIRO can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.
ER  -


TY  - Preprint
T1  - Sparse Wide-Area Control of Power Systems using Data-driven Reinforcement Learning
A1  - Amirhassan Fallah Dizche
A1  - Aranya Chakrabortty
A1  - Alexandra Duel-Hallen
JO  - ArXiv e-prints
Y1  - 28 September, 2018
UR  - https://arxiv.org/abs/1804.09827
N2  - In this paper we present an online wide-area oscillation damping control (WAC) design for uncertain models of power systems using ideas from reinforcement learning. We assume that the exact small-signal model of the power system at the onset of a contingency is not known to the operator and use the nominal model and online measurements of the generator states and control inputs to rapidly converge to a state-feedback controller that minimizes a given quadratic energy cost. However, unlike conventional linear quadratic regulators (LQR), we intend our controller to be sparse, so its implementation reduces the communication costs. We, therefore, employ the gradient support pursuit (GraSP) optimization algorithm to impose sparsity constraints on the control gain matrix during learning. The sparse controller is thereafter implemented using distributed communication. Using the IEEE 39-bus power system model with 1149 unknown parameters, it is demonstrated that the proposed learning method provides reliable LQR performance while the controller matched to the nominal model becomes unstable for severely uncertain systems.
ER  -


TY  - Preprint
T1  - Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning
A1  - Andy Zeng
A1  - Shuran Song
A1  - Stefan Welker
A1  - Johnny Lee
A1  - Alberto Rodriguez
A1  - Thomas Funkhouser
JO  - ArXiv e-prints
Y1  - 30 September, 2018
UR  - https://arxiv.org/abs/1803.09956
N2  - Skilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. In this work, we demonstrate that it is possible to discover and learn these synergies from scratch through model-free deep reinforcement learning. Our method involves training two fully convolutional networks that map from visual observations to actions: one infers the utility of pushes for a dense pixel-wise sampling of end effector orientations and locations, while the other does the same for grasping. Both networks are trained jointly in a Q-learning framework and are entirely self-supervised by trial and error, where rewards are provided from successful grasps. In this way, our policy learns pushing motions that enable future grasps, while learning grasps that can leverage past pushes. During picking experiments in both simulation and real-world scenarios, we find that our system quickly learns complex behaviors amid challenging cases of clutter, and achieves better grasping success rates and picking efficiencies than baseline alternatives after only a few hours of training. We further demonstrate that our method is capable of generalizing to novel objects. Qualitative results (videos), code, pre-trained models, and simulation environments are available at http://vpg.cs.princeton.edu
ER  -


TY  - Preprint
T1  - Inverse Reinforcement Learning via Nonparametric Spatio-Temporal Subgoal Modeling
A1  - Adrian Å oÅ¡iÄ
A1  - Elmar Rueckert
A1  - Jan Peters
A1  - Abdelhak M. Zoubir
A1  - Heinz Koeppl
JO  - ArXiv e-prints
Y1  - 4 October, 2018
UR  - https://arxiv.org/abs/1803.00444
N2  - Advances in the field of inverse reinforcement learning (IRL) have led to sophisticated inference frameworks that relax the original modeling assumption of observing an agent behavior that reflects only a single intention. Instead of learning a global behavioral model, recent IRL methods divide the demonstration data into parts, to account for the fact that different trajectories may correspond to different intentions, e.g., because they were generated by different domain experts. In this work, we go one step further: using the intuitive concept of subgoals, we build upon the premise that even a single trajectory can be explained more efficiently locally within a certain context than globally, enabling a more compact representation of the observed behavior. Based on this assumption, we build an implicit intentional model of the agent&#39;s goals to forecast its behavior in unobserved situations. The result is an integrated Bayesian prediction framework that significantly outperforms existing IRL solutions and provides smooth policy estimates consistent with the expert&#39;s plan. Most notably, our framework naturally handles situations where the intentions of the agent change over time and classical IRL algorithms fail. In addition, due to its probabilistic nature, the model can be straightforwardly applied in active learning scenarios to guide the demonstration process of the expert.
ER  -


TY  - Preprint
T1  - Costate-focused models for reinforcement learning
A1  - Bita Behrouzi
A1  - Xuefei Liu
A1  - Douglas Tweed
JO  - ArXiv e-prints
Y1  - 2 October, 2018
UR  - https://arxiv.org/abs/1711.05817
N2  - Many recent algorithms for reinforcement learning are model-free and founded on the Bellman equation. Here we present a method founded on the costate equation and models of the state dynamics. We use the costate -- the gradient of cost with respect to state -- to improve the policy and also to &#34;focus&#34; the model, training it to detect and mimic those features of the environment that are most relevant to its task. We show that this method can handle difficult time-optimal control problems, driving deterministic or stochastic mechanical systems quickly to a target. On these tasks it works well compared to deep deterministic policy gradient, a recent Bellman method. And because it creates a model, the costate method can also learn from mental practice.
ER  -


TY  - Preprint
T1  - Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards
A1  - Mel Vecerik
A1  - Todd Hester
A1  - Jonathan Scholz
A1  - Fumin Wang
A1  - Olivier Pietquin
A1  - Bilal Piot
A1  - Nicolas Heess
A1  - Thomas RothÃ¶rl
A1  - Thomas Lampe
A1  - Martin Riedmiller
JO  - ArXiv e-prints
Y1  - 8 October, 2018
UR  - https://arxiv.org/abs/1707.08817
N2  - We propose a general and model-free approach for Reinforcement Learning (RL) on real robotics with sparse rewards. We build upon the Deep Deterministic Policy Gradient (DDPG) algorithm to use demonstrations. Both demonstrations and actual interactions are used to fill a replay buffer and the sampling ratio between demonstrations and transitions is automatically tuned via a prioritized replay mechanism. Typically, carefully engineered shaping rewards are required to enable the agents to efficiently explore on high dimensional control problems such as robotics. They are also required for model-based acceleration methods relying on local solvers such as iLQG (e.g. Guided Policy Search and Normalized Advantage Function). The demonstrations replace the need for carefully engineered rewards, and reduce the exploration problem encountered by classical RL approaches in these domains. Demonstrations are collected by a robot kinesthetically force-controlled by a human demonstrator. Results on four simulated insertion tasks show that DDPG from demonstrations out-performs DDPG, and does not require engineered rewards. Finally, we demonstrate the method on a real robotics task consisting of inserting a clip (flexible object) into a rigid object.
ER  -


TY  - Preprint
T1  - On the function approximation error for risk-sensitive reinforcement learning
A1  - Prasenjit Karmakar
A1  - Shalabh Bhatnagar
JO  - ArXiv e-prints
Y1  - 7 October, 2018
UR  - https://arxiv.org/abs/1612.07562
N2  - In this paper we obtain several informative error bounds on function approximation for the policy evaluation algorithm proposed by Basu et al. when the aim is to find the risk-sensitive cost represented using exponential utility. The main idea is to use classical Bapat&#39;s inequality and to use Perron-Frobenius eigenvectors (exists if we assume irreducible Markov chain) to get the new bounds. The novelty of our approach is that we use the irreduciblity of Markov chain to get the new bounds whereas the earlier work by Basu et al. used spectral variation bound which is true for any matrix. We also give examples where all our bounds achieve the &#34;actual error&#34; whereas the earlier bound given by Basu et al. is much weaker in comparison. We show that this happens due to the absence of difference term in the earlier bound which is always present in all our bounds when the state space is large. Additionally, we discuss how all our bounds compare with each other.
ER  -


